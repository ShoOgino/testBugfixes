{"path":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6c2782fe88d18fedf3ef67402c9cb5a41978a8c7","date":1328901155,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a78a90fc9701e511308346ea29f4f5e548bb39fe","date":1329489995,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f08557cdb6c60ac7b88a9342c983a20cd236e74f","date":1330954480,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","date":1331075828,"type":3,"author":"Ryan McKinley","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random.nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random.nextBoolean());\n      ft.setStoreTermVectorPositions(random.nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random.nextBoolean()) {\n          text = \"a\";\n        } else if (random.nextBoolean()) {\n          text = \"b\";\n        } else if (random.nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random.nextBoolean() ? 1 : random.nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random.nextBoolean() ? 0 : random.nextInt(5);\n        final int tokenOffset = random.nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random.nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.NO));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":["a78a90fc9701e511308346ea29f4f5e548bb39fe","31f025ae60076ae95274433f3fe8e6ace2857a87"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4","date":1341839195,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.NO));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f2045dc09e45a13e6c9435a1feac37e4752ad228","date":1342130381,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.NO));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.NO));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.NO));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.NO));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"aba371508186796cc6151d8223a5b4e16d02e26e","date":1343474871,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositions.startOffset());\n              assertEquals(token.endOffset(), docsAndPositions.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"322360ac5185a8446d3e0b530b2068bef67cd3d5","date":1343669494,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.NO));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.NO));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"02331260bb246364779cb6f04919ca47900d01bb","date":1343749884,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.NO));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.NO));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.NO));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.NO));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs, true);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, false);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions, true);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ced66195b26fdb1f77ee00e2a77ec6918dedd766","date":1344948886,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.NO));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.NO));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":["913fa4c710b6d1168655966e59f0f4de566907a8","da6d5ac19a80d65b1e864251f155d30960353b7e"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","date":1345029782,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3c188105a9aae04f56c24996f98f8333fc825d2e","date":1345031914,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.NO));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.NO));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1c93396a1df03720cb20e2c2f513a6fa59b21e4c","date":1345032673,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.NO));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.NO));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b05c56a41b733e02a189c48895922b5bd8c7f3d1","date":1345033322,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.NO));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.NO));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(IndexReader reader : r.getSequentialSubReaders()) {\n      // TODO: improve this\n      AtomicReader sub = (AtomicReader) reader;\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1d028314cced5858683a1bb4741423d0f934257b","date":1346596535,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.NO));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f45457a742a53533c348c4b990b1c579ff364467","date":1353197071,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final int docIDToID[] = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8","date":1373996650,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<String,Map<Integer,List<Token>>>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<Token>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.shutdown();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","date":1399816179,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.shutdown();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.shutdown();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"93dd449115a9247533e44bab47e8429e5dccbc6d","date":1400258396,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.shutdown();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.shutdown();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"56572ec06f1407c066d6b7399413178b33176cd8","date":1400495675,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.shutdown();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.shutdown();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(sub, \"id\", false);\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.shutdown();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(AtomicReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      AtomicReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2bb2842e561df4e8e9ad89010605fc86ac265465","date":1414768208,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      PostingsEnum docs = null;\n      PostingsEnum docsAndPositions = null;\n      PostingsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.postings(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.postings(null, docsAndPositions, PostingsEnum.FLAG_ALL);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.postings(null, docsAndPositions, PostingsEnum.FLAG_ALL);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      DocsEnum docs = null;\n      DocsAndPositionsEnum docsAndPositions = null;\n      DocsAndPositionsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.docs(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.docsAndPositions(null, docsAndPositions, DocsAndPositionsEnum.FLAG_PAYLOADS);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.docsAndPositions(null, docsAndPositions);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e73063b92d958076ef4ae8beb5f493e8ccdcecb4","date":1424177215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      PostingsEnum docs = null;\n      PostingsEnum docsAndPositions = null;\n      PostingsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.postings(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.postings(null, docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.postings(null, docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      PostingsEnum docs = null;\n      PostingsEnum docsAndPositions = null;\n      PostingsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.postings(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.postings(null, docsAndPositions, PostingsEnum.FLAG_ALL);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.postings(null, docsAndPositions, PostingsEnum.FLAG_ALL);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator();\n      PostingsEnum docs = null;\n      PostingsEnum docsAndPositions = null;\n      PostingsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.postings(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.postings(null, docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.postings(null, docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator(null);\n      PostingsEnum docs = null;\n      PostingsEnum docsAndPositions = null;\n      PostingsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.postings(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.postings(null, docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.postings(null, docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator();\n      PostingsEnum docs = null;\n      PostingsEnum docsAndPositions = null;\n      PostingsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.postings(docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator();\n      PostingsEnum docs = null;\n      PostingsEnum docsAndPositions = null;\n      PostingsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.postings(null, docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.postings(null, docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.postings(null, docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"770342641f7b505eaa8dccdc666158bff2419109","date":1449868421,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator();\n      PostingsEnum docs = null;\n      PostingsEnum docsAndPositions = null;\n      PostingsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.postings(docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new IntField(\"id\", docCount, Field.Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator();\n      PostingsEnum docs = null;\n      PostingsEnum docsAndPositions = null;\n      PostingsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.postings(docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6652c74b2358a0b13223817a6a793bf1c9d0749d","date":1474465301,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator();\n      PostingsEnum docs = null;\n      PostingsEnum docsAndPositions = null;\n      PostingsEnum docsAndPositionsAndOffsets = null;\n      int[] docIDToID = new int[sub.maxDoc()];\n      NumericDocValues values = DocValues.getNumeric(sub, \"id\");\n      for(int i=0;i<sub.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      \n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.postings(docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator();\n      PostingsEnum docs = null;\n      PostingsEnum docsAndPositions = null;\n      PostingsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.postings(docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator();\n      PostingsEnum docs = null;\n      PostingsEnum docsAndPositions = null;\n      PostingsEnum docsAndPositionsAndOffsets = null;\n      int[] docIDToID = new int[sub.maxDoc()];\n      NumericDocValues values = DocValues.getNumeric(sub, \"id\");\n      for(int i=0;i<sub.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      \n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.postings(docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator();\n      PostingsEnum docs = null;\n      PostingsEnum docsAndPositions = null;\n      PostingsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.postings(docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator();\n      PostingsEnum docs = null;\n      PostingsEnum docsAndPositions = null;\n      PostingsEnum docsAndPositionsAndOffsets = null;\n      int[] docIDToID = new int[sub.maxDoc()];\n      NumericDocValues values = DocValues.getNumeric(sub, \"id\");\n      for(int i=0;i<sub.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      \n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.postings(docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator();\n      PostingsEnum docs = null;\n      PostingsEnum docsAndPositions = null;\n      PostingsEnum docsAndPositionsAndOffsets = null;\n      final NumericDocValues docIDToID = DocValues.getNumeric(sub, \"id\");\n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.postings(docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get((int) docIDToID.get(doc));\n            //System.out.println(\"      doc=\" + docIDToID.get(doc) + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b","date":1497408244,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.terms(\"content\").iterator();\n      PostingsEnum docs = null;\n      PostingsEnum docsAndPositions = null;\n      PostingsEnum docsAndPositionsAndOffsets = null;\n      int[] docIDToID = new int[sub.maxDoc()];\n      NumericDocValues values = DocValues.getNumeric(sub, \"id\");\n      for(int i=0;i<sub.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      \n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.postings(docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator();\n      PostingsEnum docs = null;\n      PostingsEnum docsAndPositions = null;\n      PostingsEnum docsAndPositionsAndOffsets = null;\n      int[] docIDToID = new int[sub.maxDoc()];\n      NumericDocValues values = DocValues.getNumeric(sub, \"id\");\n      for(int i=0;i<sub.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      \n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.postings(docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","date":1498028748,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.terms(\"content\").iterator();\n      PostingsEnum docs = null;\n      PostingsEnum docsAndPositions = null;\n      PostingsEnum docsAndPositionsAndOffsets = null;\n      int[] docIDToID = new int[sub.maxDoc()];\n      NumericDocValues values = DocValues.getNumeric(sub, \"id\");\n      for(int i=0;i<sub.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      \n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.postings(docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator();\n      PostingsEnum docs = null;\n      PostingsEnum docsAndPositions = null;\n      PostingsEnum docsAndPositionsAndOffsets = null;\n      int[] docIDToID = new int[sub.maxDoc()];\n      NumericDocValues values = DocValues.getNumeric(sub, \"id\");\n      for(int i=0;i<sub.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      \n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.postings(docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.terms(\"content\").iterator();\n      PostingsEnum docs = null;\n      PostingsEnum docsAndPositions = null;\n      PostingsEnum docsAndPositionsAndOffsets = null;\n      int[] docIDToID = new int[sub.maxDoc()];\n      NumericDocValues values = DocValues.getNumeric(sub, \"id\");\n      for(int i=0;i<sub.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      \n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.postings(docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    // token -> docID -> tokens\n    final Map<String,Map<Integer,List<Token>>> actualTokens = new HashMap<>();\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);\n\n    final int numDocs = atLeast(20);\n    //final int numDocs = atLeast(5);\n\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n\n    // TODO: randomize what IndexOptions we use; also test\n    // changing this up in one IW buffered segment...:\n    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    if (random().nextBoolean()) {\n      ft.setStoreTermVectors(true);\n      ft.setStoreTermVectorOffsets(random().nextBoolean());\n      ft.setStoreTermVectorPositions(random().nextBoolean());\n    }\n\n    for(int docCount=0;docCount<numDocs;docCount++) {\n      Document doc = new Document();\n      doc.add(new NumericDocValuesField(\"id\", docCount));\n      List<Token> tokens = new ArrayList<>();\n      final int numTokens = atLeast(100);\n      //final int numTokens = atLeast(20);\n      int pos = -1;\n      int offset = 0;\n      //System.out.println(\"doc id=\" + docCount);\n      for(int tokenCount=0;tokenCount<numTokens;tokenCount++) {\n        final String text;\n        if (random().nextBoolean()) {\n          text = \"a\";\n        } else if (random().nextBoolean()) {\n          text = \"b\";\n        } else if (random().nextBoolean()) {\n          text = \"c\";\n        } else {\n          text = \"d\";\n        }       \n        \n        int posIncr = random().nextBoolean() ? 1 : random().nextInt(5);\n        if (tokenCount == 0 && posIncr == 0) {\n          posIncr = 1;\n        }\n        final int offIncr = random().nextBoolean() ? 0 : random().nextInt(5);\n        final int tokenOffset = random().nextInt(5);\n\n        final Token token = makeToken(text, posIncr, offset+offIncr, offset+offIncr+tokenOffset);\n        if (!actualTokens.containsKey(text)) {\n          actualTokens.put(text, new HashMap<Integer,List<Token>>());\n        }\n        final Map<Integer,List<Token>> postingsByDoc = actualTokens.get(text);\n        if (!postingsByDoc.containsKey(docCount)) {\n          postingsByDoc.put(docCount, new ArrayList<Token>());\n        }\n        postingsByDoc.get(docCount).add(token);\n        tokens.add(token);\n        pos += posIncr;\n        // stuff abs position into type:\n        token.setType(\"\"+pos);\n        offset += offIncr + tokenOffset;\n        //System.out.println(\"  \" + token + \" posIncr=\" + token.getPositionIncrement() + \" pos=\" + pos + \" off=\" + token.startOffset() + \"/\" + token.endOffset() + \" (freq=\" + postingsByDoc.get(docCount).size() + \")\");\n      }\n      doc.add(new Field(\"content\", new CannedTokenStream(tokens.toArray(new Token[tokens.size()])), ft));\n      w.addDocument(doc);\n    }\n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    final String[] terms = new String[] {\"a\", \"b\", \"c\", \"d\"};\n    for(LeafReaderContext ctx : r.leaves()) {\n      // TODO: improve this\n      LeafReader sub = ctx.reader();\n      //System.out.println(\"\\nsub=\" + sub);\n      final TermsEnum termsEnum = sub.fields().terms(\"content\").iterator();\n      PostingsEnum docs = null;\n      PostingsEnum docsAndPositions = null;\n      PostingsEnum docsAndPositionsAndOffsets = null;\n      int[] docIDToID = new int[sub.maxDoc()];\n      NumericDocValues values = DocValues.getNumeric(sub, \"id\");\n      for(int i=0;i<sub.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      \n      for(String term : terms) {\n        //System.out.println(\"  term=\" + term);\n        if (termsEnum.seekExact(new BytesRef(term))) {\n          docs = termsEnum.postings(docs);\n          assertNotNull(docs);\n          int doc;\n          //System.out.println(\"    doc/freq\");\n          while((doc = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" docID=\" + doc + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docs.freq());\n          }\n\n          // explicitly exclude offsets here\n          docsAndPositions = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositions);\n          //System.out.println(\"    doc/freq/pos\");\n          while((doc = docsAndPositions.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositions.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositions.nextPosition());\n            }\n          }\n\n          docsAndPositionsAndOffsets = termsEnum.postings(docsAndPositions, PostingsEnum.ALL);\n          assertNotNull(docsAndPositionsAndOffsets);\n          //System.out.println(\"    doc/freq/pos/offs\");\n          while((doc = docsAndPositionsAndOffsets.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            final List<Token> expected = actualTokens.get(term).get(docIDToID[doc]);\n            //System.out.println(\"      doc=\" + docIDToID[doc] + \" \" + expected.size() + \" freq\");\n            assertNotNull(expected);\n            assertEquals(expected.size(), docsAndPositionsAndOffsets.freq());\n            for(Token token : expected) {\n              int pos = Integer.parseInt(token.type());\n              //System.out.println(\"        pos=\" + pos);\n              assertEquals(pos, docsAndPositionsAndOffsets.nextPosition());\n              assertEquals(token.startOffset(), docsAndPositionsAndOffsets.startOffset());\n              assertEquals(token.endOffset(), docsAndPositionsAndOffsets.endOffset());\n            }\n          }\n        }\n      }        \n      // TODO: test advance:\n    }\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"f2045dc09e45a13e6c9435a1feac37e4752ad228":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"3c188105a9aae04f56c24996f98f8333fc825d2e":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":["a78a90fc9701e511308346ea29f4f5e548bb39fe","f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":["1c93396a1df03720cb20e2c2f513a6fa59b21e4c","ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":["d6f074e73200c07d54f242d3880a8da5a35ff97b","ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"56572ec06f1407c066d6b7399413178b33176cd8":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a","93dd449115a9247533e44bab47e8429e5dccbc6d"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"f45457a742a53533c348c4b990b1c579ff364467":["1d028314cced5858683a1bb4741423d0f934257b"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"a78a90fc9701e511308346ea29f4f5e548bb39fe":["6c2782fe88d18fedf3ef67402c9cb5a41978a8c7"],"1d028314cced5858683a1bb4741423d0f934257b":["ced66195b26fdb1f77ee00e2a77ec6918dedd766","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198"],"aba371508186796cc6151d8223a5b4e16d02e26e":["8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4","f2045dc09e45a13e6c9435a1feac37e4752ad228"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["770342641f7b505eaa8dccdc666158bff2419109","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"ced66195b26fdb1f77ee00e2a77ec6918dedd766":["02331260bb246364779cb6f04919ca47900d01bb"],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["f2045dc09e45a13e6c9435a1feac37e4752ad228"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["eee5f2a24465d2c9a5f86ab84b7c35041a30fda8"],"f08557cdb6c60ac7b88a9342c983a20cd236e74f":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"1c93396a1df03720cb20e2c2f513a6fa59b21e4c":["3c188105a9aae04f56c24996f98f8333fc825d2e"],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"51f5280f31484820499077f41fcdfe92d527d9dc":["2bb2842e561df4e8e9ad89010605fc86ac265465"],"770342641f7b505eaa8dccdc666158bff2419109":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"6c2782fe88d18fedf3ef67402c9cb5a41978a8c7":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"28288370235ed02234a64753cdbf0c6ec096304a":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["770342641f7b505eaa8dccdc666158bff2419109","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["1d028314cced5858683a1bb4741423d0f934257b","f45457a742a53533c348c4b990b1c579ff364467"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a","b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["04f07771a2a7dd3a395700665ed839c3dae2def2","f2045dc09e45a13e6c9435a1feac37e4752ad228"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["770342641f7b505eaa8dccdc666158bff2419109"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["fe33227f6805edab2036cbb80645cc4e2d1fa424","02331260bb246364779cb6f04919ca47900d01bb"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["aba371508186796cc6151d8223a5b4e16d02e26e","02331260bb246364779cb6f04919ca47900d01bb"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["28288370235ed02234a64753cdbf0c6ec096304a"],"02331260bb246364779cb6f04919ca47900d01bb":["322360ac5185a8446d3e0b530b2068bef67cd3d5"]},"commit2Childs":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"f2045dc09e45a13e6c9435a1feac37e4752ad228":["aba371508186796cc6151d8223a5b4e16d02e26e","322360ac5185a8446d3e0b530b2068bef67cd3d5","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4":["aba371508186796cc6151d8223a5b4e16d02e26e"],"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b":["28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"3c188105a9aae04f56c24996f98f8333fc825d2e":["1c93396a1df03720cb20e2c2f513a6fa59b21e4c"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":[],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["6c2782fe88d18fedf3ef67402c9cb5a41978a8c7"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["51f5280f31484820499077f41fcdfe92d527d9dc"],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":["1d028314cced5858683a1bb4741423d0f934257b"],"56572ec06f1407c066d6b7399413178b33176cd8":[],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["2bb2842e561df4e8e9ad89010605fc86ac265465"],"f45457a742a53533c348c4b990b1c579ff364467":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["f2045dc09e45a13e6c9435a1feac37e4752ad228","8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"a78a90fc9701e511308346ea29f4f5e548bb39fe":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"1d028314cced5858683a1bb4741423d0f934257b":["f45457a742a53533c348c4b990b1c579ff364467","d4d69c535930b5cce125cff868d40f6373dc27d4"],"aba371508186796cc6151d8223a5b4e16d02e26e":["d6f074e73200c07d54f242d3880a8da5a35ff97b"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b","28288370235ed02234a64753cdbf0c6ec096304a","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":[],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["56572ec06f1407c066d6b7399413178b33176cd8","b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","93dd449115a9247533e44bab47e8429e5dccbc6d"],"ced66195b26fdb1f77ee00e2a77ec6918dedd766":["b05c56a41b733e02a189c48895922b5bd8c7f3d1","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","1d028314cced5858683a1bb4741423d0f934257b"],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["02331260bb246364779cb6f04919ca47900d01bb"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"f08557cdb6c60ac7b88a9342c983a20cd236e74f":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"1c93396a1df03720cb20e2c2f513a6fa59b21e4c":["b05c56a41b733e02a189c48895922b5bd8c7f3d1"],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"51f5280f31484820499077f41fcdfe92d527d9dc":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"6c2782fe88d18fedf3ef67402c9cb5a41978a8c7":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"770342641f7b505eaa8dccdc666158bff2419109":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"28288370235ed02234a64753cdbf0c6ec096304a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["770342641f7b505eaa8dccdc666158bff2419109"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","eee5f2a24465d2c9a5f86ab84b7c35041a30fda8"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["56572ec06f1407c066d6b7399413178b33176cd8","d0ef034a4f10871667ae75181537775ddcf8ade4"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["3c188105a9aae04f56c24996f98f8333fc825d2e"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":[],"02331260bb246364779cb6f04919ca47900d01bb":["ced66195b26fdb1f77ee00e2a77ec6918dedd766","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","b05c56a41b733e02a189c48895922b5bd8c7f3d1","56572ec06f1407c066d6b7399413178b33176cd8","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}