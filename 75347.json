{"path":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","sourceNew":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random, dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString()));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random, dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString()));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","sourceNew":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString()));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random, dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString()));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","sourceNew":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString()));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":["77d4998e63ada9336818d1ebaacc362168f473e8"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d19974432be9aed28ee7dca73bdf01d139e763a9","date":1342822166,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","sourceNew":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newMockDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":["77d4998e63ada9336818d1ebaacc362168f473e8"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","date":1343059585,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","sourceNew":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newMockDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aba371508186796cc6151d8223a5b4e16d02e26e","date":1343474871,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","sourceNew":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newMockDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3b0e8c39ca08b5a02de6edcd33d6f3b90b865173","date":1365631993,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","sourceNew":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newMockDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = newSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newMockDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = new IndexSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","sourceNew":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newMockDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = newSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newMockDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(_TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = newSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","sourceNew":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newMockDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.shutdown();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = newSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newMockDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = newSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","sourceNew":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newMockDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(new MockAnalyzer(random()))\n                                                        .setMaxBufferedDocs(2)\n                                                        .setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.shutdown();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = newSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newMockDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMaxBufferedDocs(2).setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.shutdown();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = newSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","sourceNew":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newMockDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(new MockAnalyzer(random()))\n                                                        .setMaxBufferedDocs(2)\n                                                        .setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = newSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newMockDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(new MockAnalyzer(random()))\n                                                        .setMaxBufferedDocs(2)\n                                                        .setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.shutdown();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = newSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fbaae1c00d39df2c872bbe043af26d02d3818313","date":1409657064,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","sourceNew":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    final MockDirectoryWrapper dir = newMockDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(new MockAnalyzer(random()))\n                                                        .setMaxBufferedDocs(2)\n                                                        .setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = newSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    // NOTE: if we see a fail on this test with \"NestedPulsing\" its because its \n    // reuse isnt perfect (but reasonable). see TestPulsingReuse.testNestedPulsing \n    // for more details\n    final MockDirectoryWrapper dir = newMockDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(new MockAnalyzer(random()))\n                                                        .setMaxBufferedDocs(2)\n                                                        .setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = newSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4329ea1888f09fb692caf2ad95056327be317144","date":1418146038,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","sourceNew":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging and searching:\n  public void test() throws Exception {\n    final MockDirectoryWrapper dir = newMockDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(new MockAnalyzer(random()))\n                                                        .setMaxBufferedDocs(2)\n                                                        .setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), dir.getInputCloneCount() < 500);\n\n    final IndexSearcher s = newSearcher(r);\n    // important: set this after newSearcher, it might have run checkindex\n    final int cloneCount = dir.getInputCloneCount();\n    // dir.setVerboseClone(true);\n    \n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging:\n  public void test() throws Exception {\n    final MockDirectoryWrapper dir = newMockDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(new MockAnalyzer(random()))\n                                                        .setMaxBufferedDocs(2)\n                                                        .setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final int cloneCount = dir.getInputCloneCount();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), cloneCount < 500);\n\n    final IndexSearcher s = newSearcher(r);\n\n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83788ad129a5154d5c6562c4e8ce3db48793aada","date":1532961485,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","sourceNew":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging and searching:\n  public void test() throws Exception {\n    final MockDirectoryWrapper dir = newMockDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(new MockAnalyzer(random()))\n                                                        .setMaxBufferedDocs(2)\n                                                        .setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), dir.getInputCloneCount() < 500);\n\n    final IndexSearcher s = newSearcher(r);\n    // important: set this after newSearcher, it might have run checkindex\n    final int cloneCount = dir.getInputCloneCount();\n    // dir.setVerboseClone(true);\n    \n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits.value > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging and searching:\n  public void test() throws Exception {\n    final MockDirectoryWrapper dir = newMockDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(new MockAnalyzer(random()))\n                                                        .setMaxBufferedDocs(2)\n                                                        .setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), dir.getInputCloneCount() < 500);\n\n    final IndexSearcher s = newSearcher(r);\n    // important: set this after newSearcher, it might have run checkindex\n    final int cloneCount = dir.getInputCloneCount();\n    // dir.setVerboseClone(true);\n    \n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"910afe74efbdc710510f14fa4b997b8f633c64dc","date":1597400071,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestForTooMuchCloning#test().mjava","sourceNew":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging and searching:\n  public void test() throws Exception {\n    final MockDirectoryWrapper dir = newMockDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(new MockAnalyzer(random()))\n                                                        .setMaxBufferedDocs(2)\n                                                        // use a FilterMP otherwise RIW will randomly reconfigure\n                                                        // the MP while the test runs\n                                                        .setMergePolicy(new FilterMergePolicy(tmp)));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), dir.getInputCloneCount() < 500);\n\n    final IndexSearcher s = newSearcher(r);\n    // important: set this after newSearcher, it might have run checkindex\n    final int cloneCount = dir.getInputCloneCount();\n    // dir.setVerboseClone(true);\n    \n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits.value > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure we don't clone IndexInputs too frequently\n  // during merging and searching:\n  public void test() throws Exception {\n    final MockDirectoryWrapper dir = newMockDirectory();\n    final TieredMergePolicy tmp = new TieredMergePolicy();\n    tmp.setMaxMergeAtOnce(2);\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir,\n                                                      newIndexWriterConfig(new MockAnalyzer(random()))\n                                                        .setMaxBufferedDocs(2)\n                                                        .setMergePolicy(tmp));\n    final int numDocs = 20;\n    for(int docs=0;docs<numDocs;docs++) {\n      StringBuilder sb = new StringBuilder();\n      for(int terms=0;terms<100;terms++) {\n        sb.append(TestUtil.randomRealisticUnicodeString(random()));\n        sb.append(' ');\n      }\n      final Document doc = new Document();\n      doc.add(new TextField(\"field\", sb.toString(), Field.Store.NO));\n      w.addDocument(doc);\n    }\n    final IndexReader r = w.getReader();\n    w.close();\n\n    //System.out.println(\"merge clone count=\" + cloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during merging: \" + dir.getInputCloneCount(), dir.getInputCloneCount() < 500);\n\n    final IndexSearcher s = newSearcher(r);\n    // important: set this after newSearcher, it might have run checkindex\n    final int cloneCount = dir.getInputCloneCount();\n    // dir.setVerboseClone(true);\n    \n    // MTQ that matches all terms so the AUTO_REWRITE should\n    // cutover to filter rewrite and reuse a single DocsEnum\n    // across all terms;\n    final TopDocs hits = s.search(new TermRangeQuery(\"field\",\n                                                     new BytesRef(),\n                                                     new BytesRef(\"\\uFFFF\"),\n                                                     true,\n                                                     true), 10);\n    assertTrue(hits.totalHits.value > 0);\n    final int queryCloneCount = dir.getInputCloneCount() - cloneCount;\n    //System.out.println(\"query clone count=\" + queryCloneCount);\n    assertTrue(\"too many calls to IndexInput.clone during TermRangeQuery: \" + queryCloneCount, queryCloneCount < 50);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"6613659748fe4411a7dcf85266e55db1f95f7315":["3b0e8c39ca08b5a02de6edcd33d6f3b90b865173"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"910afe74efbdc710510f14fa4b997b8f633c64dc":["83788ad129a5154d5c6562c4e8ce3db48793aada"],"4329ea1888f09fb692caf2ad95056327be317144":["fbaae1c00d39df2c872bbe043af26d02d3818313"],"3b0e8c39ca08b5a02de6edcd33d6f3b90b865173":["d19974432be9aed28ee7dca73bdf01d139e763a9"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"fbaae1c00d39df2c872bbe043af26d02d3818313":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"aba371508186796cc6151d8223a5b4e16d02e26e":["04f07771a2a7dd3a395700665ed839c3dae2def2","d19974432be9aed28ee7dca73bdf01d139e763a9"],"d19974432be9aed28ee7dca73bdf01d139e763a9":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["6613659748fe4411a7dcf85266e55db1f95f7315"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":["04f07771a2a7dd3a395700665ed839c3dae2def2","d19974432be9aed28ee7dca73bdf01d139e763a9"],"83788ad129a5154d5c6562c4e8ce3db48793aada":["4329ea1888f09fb692caf2ad95056327be317144"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["910afe74efbdc710510f14fa4b997b8f633c64dc"]},"commit2Childs":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"6613659748fe4411a7dcf85266e55db1f95f7315":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"910afe74efbdc710510f14fa4b997b8f633c64dc":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4329ea1888f09fb692caf2ad95056327be317144":["83788ad129a5154d5c6562c4e8ce3db48793aada"],"3b0e8c39ca08b5a02de6edcd33d6f3b90b865173":["6613659748fe4411a7dcf85266e55db1f95f7315"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["aba371508186796cc6151d8223a5b4e16d02e26e","d19974432be9aed28ee7dca73bdf01d139e763a9","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7"],"fbaae1c00d39df2c872bbe043af26d02d3818313":["4329ea1888f09fb692caf2ad95056327be317144"],"aba371508186796cc6151d8223a5b4e16d02e26e":[],"d19974432be9aed28ee7dca73bdf01d139e763a9":["3b0e8c39ca08b5a02de6edcd33d6f3b90b865173","aba371508186796cc6151d8223a5b4e16d02e26e","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["fbaae1c00d39df2c872bbe043af26d02d3818313"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":[],"83788ad129a5154d5c6562c4e8ce3db48793aada":["910afe74efbdc710510f14fa4b997b8f633c64dc"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["aba371508186796cc6151d8223a5b4e16d02e26e","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}