{"path":"lucene/src/java/org/apache/lucene/index/TermVectorsConsumer#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","commits":[{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsConsumer#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (writer != null) {\n      // At least one doc in this run had term vectors enabled\n      try {\n        fill(state.numDocs);\n        assert state.segmentName != null;\n        writer.finish(state.numDocs);\n      } finally {\n        IOUtils.close(writer);\n        writer = null;\n\n        lastDocID = 0;\n        hasVectors = false;\n      }\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsConsumerPerField perField = (TermVectorsConsumerPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","sourceOld":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      IOUtils.close(tvx, tvf, tvd);\n      tvx = tvd = tvf = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumer#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsConsumer#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (writer != null) {\n      // At least one doc in this run had term vectors enabled\n      try {\n        fill(state.numDocs);\n        assert state.segmentName != null;\n        writer.finish(state.numDocs);\n      } finally {\n        IOUtils.close(writer);\n        writer = null;\n\n        lastDocID = 0;\n        hasVectors = false;\n      }\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsConsumerPerField perField = (TermVectorsConsumerPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","sourceOld":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (writer != null) {\n      // At least one doc in this run had term vectors enabled\n      try {\n        fill(state.numDocs);\n        assert state.segmentName != null;\n        writer.finish(state.numDocs);\n      } finally {\n        IOUtils.close(writer);\n        writer = null;\n\n        lastDocID = 0;\n        hasVectors = false;\n      }\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsConsumerPerField perField = (TermVectorsConsumerPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["3cc749c053615f5871f3b95715fe292f34e70a53"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cc749c053615f5871f3b95715fe292f34e70a53":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3cc749c053615f5871f3b95715fe292f34e70a53"],"3cc749c053615f5871f3b95715fe292f34e70a53":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}