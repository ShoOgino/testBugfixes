{"path":"lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsConsumer.NormsWriter#merge(MergeState).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsConsumer.NormsWriter#merge(MergeState).mjava","pathOld":"lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsConsumer.NormsWriter#merge(MergeState).mjava","sourceNew":"    // TODO: we can actually use the defaul DV merge here and drop this specific stuff entirely\n    /** we override merge and bulk-merge norms when there are no deletions */\n    public void merge(MergeState mergeState) throws IOException {\n      int numMergedDocs = 0;\n      for (FieldInfo fi : mergeState.fieldInfos) {\n        if (fi.normsPresent()) {\n          startField(fi);\n          int numMergedDocsForField = 0;\n          for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n            final int maxDoc = reader.reader.maxDoc();\n            byte[] normBuffer;\n            DocValues normValues = reader.reader.normValues(fi.name);\n            if (normValues == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            } else {\n              Source directSource = normValues.getDirectSource();\n              assert directSource.hasArray();\n              normBuffer = (byte[]) directSource.getArray();\n            }\n            if (reader.liveDocs == null) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n              numMergedDocsForField += maxDoc;\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.liveDocs;\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  numMergedDocsForField++;\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            mergeState.checkAbort.work(maxDoc);\n          }\n          assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;\n          numMergedDocs = numMergedDocsForField;\n        }\n      }\n      this.numTotalDocs = numMergedDocs;\n    }\n\n","sourceOld":"    // TODO: we can actually use the defaul DV merge here and drop this specific stuff entirely\n    /** we override merge and bulk-merge norms when there are no deletions */\n    public void merge(MergeState mergeState) throws IOException {\n      int numMergedDocs = 0;\n      for (FieldInfo fi : mergeState.fieldInfos) {\n        if (fi.normsPresent()) {\n          startField(fi);\n          int numMergedDocsForField = 0;\n          for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n            final int maxDoc = reader.reader.maxDoc();\n            byte[] normBuffer;\n            DocValues normValues = reader.reader.normValues(fi.name);\n            if (normValues == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            } else {\n              Source directSource = normValues.getDirectSource();\n              assert directSource.hasArray();\n              normBuffer = (byte[]) directSource.getArray();\n            }\n            if (reader.liveDocs == null) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n              numMergedDocsForField += maxDoc;\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.liveDocs;\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  numMergedDocsForField++;\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            mergeState.checkAbort.work(maxDoc);\n          }\n          assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;\n          numMergedDocs = numMergedDocsForField;\n        }\n      }\n      this.numTotalDocs = numMergedDocs;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"46818a810eab72123f0e37e6ec5f2d426bd47be1","date":1331482161,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsConsumer.NormsWriter#merge(MergeState).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsConsumer.NormsWriter#merge(MergeState).mjava","sourceNew":"    // TODO: we can actually use the defaul DV merge here and drop this specific stuff entirely\n    /** we override merge and bulk-merge norms when there are no deletions */\n    public void merge(MergeState mergeState) throws IOException {\n      int numMergedDocs = 0;\n      for (FieldInfo fi : mergeState.fieldInfos) {\n        if (fi.hasNorms()) {\n          startField(fi);\n          int numMergedDocsForField = 0;\n          for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n            final int maxDoc = reader.reader.maxDoc();\n            byte[] normBuffer;\n            DocValues normValues = reader.reader.normValues(fi.name);\n            if (normValues == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            } else {\n              Source directSource = normValues.getDirectSource();\n              assert directSource.hasArray();\n              normBuffer = (byte[]) directSource.getArray();\n            }\n            if (reader.liveDocs == null) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n              numMergedDocsForField += maxDoc;\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.liveDocs;\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  numMergedDocsForField++;\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            mergeState.checkAbort.work(maxDoc);\n          }\n          assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;\n          numMergedDocs = numMergedDocsForField;\n        }\n      }\n      this.numTotalDocs = numMergedDocs;\n    }\n\n","sourceOld":"    // TODO: we can actually use the defaul DV merge here and drop this specific stuff entirely\n    /** we override merge and bulk-merge norms when there are no deletions */\n    public void merge(MergeState mergeState) throws IOException {\n      int numMergedDocs = 0;\n      for (FieldInfo fi : mergeState.fieldInfos) {\n        if (fi.normsPresent()) {\n          startField(fi);\n          int numMergedDocsForField = 0;\n          for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n            final int maxDoc = reader.reader.maxDoc();\n            byte[] normBuffer;\n            DocValues normValues = reader.reader.normValues(fi.name);\n            if (normValues == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            } else {\n              Source directSource = normValues.getDirectSource();\n              assert directSource.hasArray();\n              normBuffer = (byte[]) directSource.getArray();\n            }\n            if (reader.liveDocs == null) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n              numMergedDocsForField += maxDoc;\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.liveDocs;\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  numMergedDocsForField++;\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            mergeState.checkAbort.work(maxDoc);\n          }\n          assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;\n          numMergedDocs = numMergedDocsForField;\n        }\n      }\n      this.numTotalDocs = numMergedDocs;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"38e3b736c7ca086d61b7dbb841c905ee115490da","date":1331657018,"type":3,"author":"Ryan McKinley","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsConsumer.NormsWriter#merge(MergeState).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsConsumer.NormsWriter#merge(MergeState).mjava","sourceNew":"    // TODO: we can actually use the defaul DV merge here and drop this specific stuff entirely\n    /** we override merge and bulk-merge norms when there are no deletions */\n    public void merge(MergeState mergeState) throws IOException {\n      int numMergedDocs = 0;\n      for (FieldInfo fi : mergeState.fieldInfos) {\n        if (fi.hasNorms()) {\n          startField(fi);\n          int numMergedDocsForField = 0;\n          for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n            final int maxDoc = reader.reader.maxDoc();\n            byte[] normBuffer;\n            DocValues normValues = reader.reader.normValues(fi.name);\n            if (normValues == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            } else {\n              Source directSource = normValues.getDirectSource();\n              assert directSource.hasArray();\n              normBuffer = (byte[]) directSource.getArray();\n            }\n            if (reader.liveDocs == null) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n              numMergedDocsForField += maxDoc;\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.liveDocs;\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  numMergedDocsForField++;\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            mergeState.checkAbort.work(maxDoc);\n          }\n          assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;\n          numMergedDocs = numMergedDocsForField;\n        }\n      }\n      this.numTotalDocs = numMergedDocs;\n    }\n\n","sourceOld":"    // TODO: we can actually use the defaul DV merge here and drop this specific stuff entirely\n    /** we override merge and bulk-merge norms when there are no deletions */\n    public void merge(MergeState mergeState) throws IOException {\n      int numMergedDocs = 0;\n      for (FieldInfo fi : mergeState.fieldInfos) {\n        if (fi.normsPresent()) {\n          startField(fi);\n          int numMergedDocsForField = 0;\n          for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n            final int maxDoc = reader.reader.maxDoc();\n            byte[] normBuffer;\n            DocValues normValues = reader.reader.normValues(fi.name);\n            if (normValues == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            } else {\n              Source directSource = normValues.getDirectSource();\n              assert directSource.hasArray();\n              normBuffer = (byte[]) directSource.getArray();\n            }\n            if (reader.liveDocs == null) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n              numMergedDocsForField += maxDoc;\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.liveDocs;\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  numMergedDocsForField++;\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            mergeState.checkAbort.work(maxDoc);\n          }\n          assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;\n          numMergedDocs = numMergedDocsForField;\n        }\n      }\n      this.numTotalDocs = numMergedDocs;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"57ae3024996ccdb3c36c42cb890e1efb37df4ce8","date":1338343651,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/test-framework/src/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsConsumer.NormsWriter#merge(MergeState).mjava","sourceNew":null,"sourceOld":"    // TODO: we can actually use the defaul DV merge here and drop this specific stuff entirely\n    /** we override merge and bulk-merge norms when there are no deletions */\n    public void merge(MergeState mergeState) throws IOException {\n      int numMergedDocs = 0;\n      for (FieldInfo fi : mergeState.fieldInfos) {\n        if (fi.hasNorms()) {\n          startField(fi);\n          int numMergedDocsForField = 0;\n          for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {\n            final int maxDoc = reader.reader.maxDoc();\n            byte[] normBuffer;\n            DocValues normValues = reader.reader.normValues(fi.name);\n            if (normValues == null) {\n              // Can be null if this segment doesn't have\n              // any docs with this field\n              normBuffer = new byte[maxDoc];\n              Arrays.fill(normBuffer, (byte)0);\n            } else {\n              Source directSource = normValues.getDirectSource();\n              assert directSource.hasArray();\n              normBuffer = (byte[]) directSource.getArray();\n            }\n            if (reader.liveDocs == null) {\n              //optimized case for segments without deleted docs\n              output.writeBytes(normBuffer, maxDoc);\n              numMergedDocsForField += maxDoc;\n            } else {\n              // this segment has deleted docs, so we have to\n              // check for every doc if it is deleted or not\n              final Bits liveDocs = reader.liveDocs;\n              for (int k = 0; k < maxDoc; k++) {\n                if (liveDocs.get(k)) {\n                  numMergedDocsForField++;\n                  output.writeByte(normBuffer[k]);\n                }\n              }\n            }\n            mergeState.checkAbort.work(maxDoc);\n          }\n          assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;\n          numMergedDocs = numMergedDocsForField;\n        }\n      }\n      this.numTotalDocs = numMergedDocs;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"38e3b736c7ca086d61b7dbb841c905ee115490da":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","46818a810eab72123f0e37e6ec5f2d426bd47be1"],"57ae3024996ccdb3c36c42cb890e1efb37df4ce8":["46818a810eab72123f0e37e6ec5f2d426bd47be1"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"46818a810eab72123f0e37e6ec5f2d426bd47be1":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["57ae3024996ccdb3c36c42cb890e1efb37df4ce8"]},"commit2Childs":{"38e3b736c7ca086d61b7dbb841c905ee115490da":[],"57ae3024996ccdb3c36c42cb890e1efb37df4ce8":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["38e3b736c7ca086d61b7dbb841c905ee115490da","46818a810eab72123f0e37e6ec5f2d426bd47be1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"46818a810eab72123f0e37e6ec5f2d426bd47be1":["38e3b736c7ca086d61b7dbb841c905ee115490da","57ae3024996ccdb3c36c42cb890e1efb37df4ce8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["38e3b736c7ca086d61b7dbb841c905ee115490da","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}