{"path":"lucene/src/test-framework/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","commits":[{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test-framework/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random);\n      TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random);\n      TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e6e919043fa85ee891123768dd655a98edbbf63c","date":1322225413,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test-framework/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"lucene/src/test-framework/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random);\n      TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, BytesRef.deepCopyOf(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random);\n      TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":null,"bugIntro":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"lucene/src/test-framework/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random);\n      TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, BytesRef.deepCopyOf(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random);\n      TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, BytesRef.deepCopyOf(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"7b91922b55d15444d554721b352861d028eb8278":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["e6e919043fa85ee891123768dd655a98edbbf63c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e6e919043fa85ee891123768dd655a98edbbf63c":["7b91922b55d15444d554721b352861d028eb8278"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"7b91922b55d15444d554721b352861d028eb8278":["e6e919043fa85ee891123768dd655a98edbbf63c"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7b91922b55d15444d554721b352861d028eb8278"],"e6e919043fa85ee891123768dd655a98edbbf63c":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}