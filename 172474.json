{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestConcatenatingTokenStream#testBasic().mjava","commits":[{"id":"1516d411048ee5b9655104d318ff9a9f0a4a6e5f","date":1498119193,"type":0,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestConcatenatingTokenStream#testBasic().mjava","pathOld":"/dev/null","sourceNew":"  public void testBasic() throws IOException {\n\n    AttributeFactory factory = newAttributeFactory();\n\n    final MockTokenizer first = new MockTokenizer(factory, MockTokenizer.WHITESPACE, false);\n    first.setReader(new StringReader(\"first words \"));\n    final MockTokenizer second = new MockTokenizer(factory, MockTokenizer.WHITESPACE, false);\n    second.setReader(new StringReader(\"second words\"));\n    final MockTokenizer third = new MockTokenizer(factory, MockTokenizer.WHITESPACE, false);\n    third.setReader(new StringReader(\" third words\"));\n\n    TokenStream ts = new ConcatenatingTokenStream(first, second, new EmptyTokenStream(), third);\n    assertTokenStreamContents(ts,\n        new String[] { \"first\", \"words\", \"second\", \"words\", \"third\", \"words\" },\n        new int[]{ 0, 6, 12, 19, 25, 31 },\n        new int[]{ 5, 11, 18, 24, 30, 36 });\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":0,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestConcatenatingTokenStream#testBasic().mjava","pathOld":"/dev/null","sourceNew":"  public void testBasic() throws IOException {\n\n    AttributeFactory factory = newAttributeFactory();\n\n    final MockTokenizer first = new MockTokenizer(factory, MockTokenizer.WHITESPACE, false);\n    first.setReader(new StringReader(\"first words \"));\n    final MockTokenizer second = new MockTokenizer(factory, MockTokenizer.WHITESPACE, false);\n    second.setReader(new StringReader(\"second words\"));\n    final MockTokenizer third = new MockTokenizer(factory, MockTokenizer.WHITESPACE, false);\n    third.setReader(new StringReader(\" third words\"));\n\n    TokenStream ts = new ConcatenatingTokenStream(first, second, new EmptyTokenStream(), third);\n    assertTokenStreamContents(ts,\n        new String[] { \"first\", \"words\", \"second\", \"words\", \"third\", \"words\" },\n        new int[]{ 0, 6, 12, 19, 25, 31 },\n        new int[]{ 5, 11, 18, 24, 30, 36 });\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestConcatenatingTokenStream#testBasic().mjava","pathOld":"/dev/null","sourceNew":"  public void testBasic() throws IOException {\n\n    AttributeFactory factory = newAttributeFactory();\n\n    final MockTokenizer first = new MockTokenizer(factory, MockTokenizer.WHITESPACE, false);\n    first.setReader(new StringReader(\"first words \"));\n    final MockTokenizer second = new MockTokenizer(factory, MockTokenizer.WHITESPACE, false);\n    second.setReader(new StringReader(\"second words\"));\n    final MockTokenizer third = new MockTokenizer(factory, MockTokenizer.WHITESPACE, false);\n    third.setReader(new StringReader(\" third words\"));\n\n    TokenStream ts = new ConcatenatingTokenStream(first, second, new EmptyTokenStream(), third);\n    assertTokenStreamContents(ts,\n        new String[] { \"first\", \"words\", \"second\", \"words\", \"third\", \"words\" },\n        new int[]{ 0, 6, 12, 19, 25, 31 },\n        new int[]{ 5, 11, 18, 24, 30, 36 });\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8c2d40897d81364606f2afb83ea5a6929acc124a","date":1548666001,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestConcatenatingTokenStream#testBasic().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestConcatenatingTokenStream#testBasic().mjava","sourceNew":"  public void testBasic() throws IOException {\n\n    AttributeFactory factory = newAttributeFactory();\n\n    final MockTokenizer first = new MockTokenizer(factory, MockTokenizer.WHITESPACE, false);\n    first.setReader(new StringReader(\"first words \"));\n    final MockTokenizer second = new MockTokenizer(factory, MockTokenizer.WHITESPACE, false);\n    second.setReader(new StringReader(\"second words\"));\n    final MockTokenizer third = new MockTokenizer(factory, MockTokenizer.WHITESPACE, false);\n    third.setReader(new StringReader(\" third words\"));\n\n    TokenStream ts = new ConcatenatingTokenStream(first, second, new EmptyTokenStream(), third);\n    assertTokenStreamContents(ts,\n        new String[] { \"first\", \"words\", \"second\", \"words\", \"third\", \"words\" },\n        new int[]{ 0, 6, 12, 19, 25, 31 },\n        new int[]{ 5, 11, 18, 24, 30, 36 });\n\n    // test re-use\n    first.setReader(new StringReader(\"first words \"));\n    second.setReader(new StringReader(\"second words\"));\n    third.setReader(new StringReader(\" third words\"));\n    assertTokenStreamContents(ts,\n        new String[] { \"first\", \"words\", \"second\", \"words\", \"third\", \"words\" },\n        new int[]{ 0, 6, 12, 19, 25, 31 },\n        new int[]{ 5, 11, 18, 24, 30, 36 },\n        new int[]{ 1, 1, 1, 1, 1, 1 });\n\n  }\n\n","sourceOld":"  public void testBasic() throws IOException {\n\n    AttributeFactory factory = newAttributeFactory();\n\n    final MockTokenizer first = new MockTokenizer(factory, MockTokenizer.WHITESPACE, false);\n    first.setReader(new StringReader(\"first words \"));\n    final MockTokenizer second = new MockTokenizer(factory, MockTokenizer.WHITESPACE, false);\n    second.setReader(new StringReader(\"second words\"));\n    final MockTokenizer third = new MockTokenizer(factory, MockTokenizer.WHITESPACE, false);\n    third.setReader(new StringReader(\" third words\"));\n\n    TokenStream ts = new ConcatenatingTokenStream(first, second, new EmptyTokenStream(), third);\n    assertTokenStreamContents(ts,\n        new String[] { \"first\", \"words\", \"second\", \"words\", \"third\", \"words\" },\n        new int[]{ 0, 6, 12, 19, 25, 31 },\n        new int[]{ 5, 11, 18, 24, 30, 36 });\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"8c2d40897d81364606f2afb83ea5a6929acc124a":["28288370235ed02234a64753cdbf0c6ec096304a"],"1516d411048ee5b9655104d318ff9a9f0a4a6e5f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","1516d411048ee5b9655104d318ff9a9f0a4a6e5f"],"28288370235ed02234a64753cdbf0c6ec096304a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","1516d411048ee5b9655104d318ff9a9f0a4a6e5f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8c2d40897d81364606f2afb83ea5a6929acc124a"]},"commit2Childs":{"8c2d40897d81364606f2afb83ea5a6929acc124a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"1516d411048ee5b9655104d318ff9a9f0a4a6e5f":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1516d411048ee5b9655104d318ff9a9f0a4a6e5f","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"28288370235ed02234a64753cdbf0c6ec096304a":["8c2d40897d81364606f2afb83ea5a6929acc124a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}