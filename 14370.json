{"path":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,byte[],boolean,long,boolean).mjava","commits":[{"id":"7e129bd6cb34a236558a49edf108a49d5c15e0e1","date":1525081316,"type":1,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,byte[],boolean,long,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,byte[],boolean).mjava","sourceNew":"  private static long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState, byte[] updates,\n                                            boolean isNumeric, long delGen,\n                                            boolean segmentPrivateDeletes) throws IOException {\n\n    TermsEnum termsEnum = null;\n    PostingsEnum postingsEnum = null;\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    Map<String, DocValuesFieldUpdates> holder = new HashMap<>();\n\n    ByteArrayDataInput in = new ByteArrayDataInput(updates);\n\n    String termField = null;\n    String updateField = null;\n    BytesRef term = new BytesRef();\n    term.bytes = new byte[16];\n    \n    BytesRef scratch = new BytesRef();\n    scratch.bytes = new byte[16];\n    \n    while (in.getPosition() != updates.length) {\n      int code = in.readVInt();\n      int docIDUpto = in.readVInt();\n      term.length = code >> 2;\n      \n      if ((code & 1) != 0) {\n        termField = in.readString();\n      }\n      if ((code & 2) != 0) {\n        updateField = in.readString();\n      }\n\n      if (term.bytes.length < term.length) {\n        term.bytes = ArrayUtil.grow(term.bytes, term.length);\n      }\n      in.readBytes(term.bytes, 0, term.length);\n\n      final int limit;\n      if (delGen == segState.delGen) {\n        assert segmentPrivateDeletes;\n        limit = docIDUpto;\n      } else {\n        limit = Integer.MAX_VALUE;\n      }\n        \n      // TODO: we traverse the terms in update order (not term order) so that we\n      // apply the updates in the correct order, i.e. if two terms udpate the\n      // same document, the last one that came in wins, irrespective of the\n      // terms lexical order.\n      // we can apply the updates in terms order if we keep an updatesGen (and\n      // increment it with every update) and attach it to each NumericUpdate. Note\n      // that we cannot rely only on docIDUpto because an app may send two updates\n      // which will get same docIDUpto, yet will still need to respect the order\n      // those updates arrived.\n\n      // TODO: we could at least *collate* by field?\n\n      // This is the field used to resolve to docIDs, e.g. an \"id\" field, not the doc values field we are updating!\n      if ((code & 1) != 0) {\n        Terms terms = segState.reader.terms(termField);\n        if (terms != null) {\n          termsEnum = terms.iterator();\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      final BytesRef binaryValue;\n      final long longValue;\n      if (isNumeric) {\n        longValue = NumericDocValuesUpdate.readFrom(in);\n        binaryValue = null;\n      } else {\n        longValue = -1;\n        binaryValue = BinaryDocValuesUpdate.readFrom(in, scratch);\n      }\n\n      if (termsEnum == null) {\n        // no terms in this segment for this field\n        continue;\n      }\n\n      if (termsEnum.seekExact(term)) {\n        // we don't need term frequencies for this\n        postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n        DocValuesFieldUpdates dvUpdates = holder.get(updateField);\n        if (dvUpdates == null) {\n          if (isNumeric) {\n            dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n          } else {\n            dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n          }\n          holder.put(updateField, dvUpdates);\n        }\n        final IntConsumer docIdConsumer;\n        final DocValuesFieldUpdates update = dvUpdates;\n        if (isNumeric) {\n          docIdConsumer = doc -> update.add(doc, longValue);\n        } else {\n          docIdConsumer = doc -> update.add(doc, binaryValue);\n        }\n        final Bits acceptDocs = segState.rld.getLiveDocs();\n        if (segState.rld.sortMap != null && segmentPrivateDeletes) {\n          // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n          int doc;\n          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            if (acceptDocs == null || acceptDocs.get(doc)) {\n              // The limit is in the pre-sorted doc space:\n              if (segState.rld.sortMap.newToOld(doc) < limit) {\n                docIdConsumer.accept(doc);\n                updateCount++;\n              }\n            }\n          }\n        } else {\n          int doc;\n          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            if (doc >= limit) {\n              break; // no more docs that can be updated for this term\n            }\n            if (acceptDocs == null || acceptDocs.get(doc)) {\n              docIdConsumer.accept(doc);\n              updateCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : holder.values()) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","sourceOld":"  private long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState,\n                                     byte[] updates, boolean isNumeric) throws IOException {\n\n    TermsEnum termsEnum = null;\n    PostingsEnum postingsEnum = null;\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    Map<String, DocValuesFieldUpdates> holder = new HashMap<>();\n\n    ByteArrayDataInput in = new ByteArrayDataInput(updates);\n\n    String termField = null;\n    String updateField = null;\n    BytesRef term = new BytesRef();\n    term.bytes = new byte[16];\n    \n    BytesRef scratch = new BytesRef();\n    scratch.bytes = new byte[16];\n    \n    while (in.getPosition() != updates.length) {\n      int code = in.readVInt();\n      int docIDUpto = in.readVInt();\n      term.length = code >> 2;\n      \n      if ((code & 1) != 0) {\n        termField = in.readString();\n      }\n      if ((code & 2) != 0) {\n        updateField = in.readString();\n      }\n\n      if (term.bytes.length < term.length) {\n        term.bytes = ArrayUtil.grow(term.bytes, term.length);\n      }\n      in.readBytes(term.bytes, 0, term.length);\n\n      int limit;\n      if (delGen == segState.delGen) {\n        assert privateSegment != null;\n        limit = docIDUpto;\n      } else {\n        limit = Integer.MAX_VALUE;\n      }\n        \n      // TODO: we traverse the terms in update order (not term order) so that we\n      // apply the updates in the correct order, i.e. if two terms udpate the\n      // same document, the last one that came in wins, irrespective of the\n      // terms lexical order.\n      // we can apply the updates in terms order if we keep an updatesGen (and\n      // increment it with every update) and attach it to each NumericUpdate. Note\n      // that we cannot rely only on docIDUpto because an app may send two updates\n      // which will get same docIDUpto, yet will still need to respect the order\n      // those updates arrived.\n\n      // TODO: we could at least *collate* by field?\n\n      // This is the field used to resolve to docIDs, e.g. an \"id\" field, not the doc values field we are updating!\n      if ((code & 1) != 0) {\n        Terms terms = segState.reader.terms(termField);\n        if (terms != null) {\n          termsEnum = terms.iterator();\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      // TODO: can we avoid boxing here w/o fully forking this method?\n      Object value;\n      if (isNumeric) {\n        value = Long.valueOf(in.readZLong());\n      } else {\n        value = scratch;\n        scratch.length = in.readVInt();\n        if (scratch.bytes.length < scratch.length) {\n          scratch.bytes = ArrayUtil.grow(scratch.bytes, scratch.length);\n        }\n        in.readBytes(scratch.bytes, 0, scratch.length);\n      }\n\n      if (termsEnum == null) {\n        // no terms in this segment for this field\n        continue;\n      }\n\n      if (termsEnum.seekExact(term)) {\n\n        // we don't need term frequencies for this\n        postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n        DocValuesFieldUpdates dvUpdates = holder.get(updateField);\n        if (dvUpdates == null) {\n          if (isNumeric) {\n            dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n          } else {\n            dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n          }\n\n          holder.put(updateField, dvUpdates);\n        }\n\n        if (segState.rld.sortMap != null && privateSegment != null) {\n          // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n          int doc;\n          final Bits acceptDocs = segState.rld.getLiveDocs();\n          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n\n            if (acceptDocs != null && acceptDocs.get(doc) == false) {\n              continue;\n            }\n            \n            // The limit is in the pre-sorted doc space:\n            if (segState.rld.sortMap.newToOld(doc) < limit) {\n              dvUpdates.add(doc, value);\n              updateCount++;\n            }\n          }\n        } else {\n          int doc;\n          final Bits acceptDocs = segState.rld.getLiveDocs();\n          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            if (doc >= limit) {\n              break; // no more docs that can be updated for this term\n            }\n            if (acceptDocs != null && acceptDocs.get(doc) == false) {\n              continue;\n            }\n            dvUpdates.add(doc, value);\n            updateCount++;\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : holder.values()) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f42883db49d143abc1a0f176ba47e3388dafb608","date":1525083166,"type":1,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,byte[],boolean,long,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,byte[],boolean).mjava","sourceNew":"  private static long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState, byte[] updates,\n                                            boolean isNumeric, long delGen,\n                                            boolean segmentPrivateDeletes) throws IOException {\n\n    TermsEnum termsEnum = null;\n    PostingsEnum postingsEnum = null;\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    Map<String, DocValuesFieldUpdates> holder = new HashMap<>();\n\n    ByteArrayDataInput in = new ByteArrayDataInput(updates);\n\n    String termField = null;\n    String updateField = null;\n    BytesRef term = new BytesRef();\n    term.bytes = new byte[16];\n    \n    BytesRef scratch = new BytesRef();\n    scratch.bytes = new byte[16];\n    \n    while (in.getPosition() != updates.length) {\n      int code = in.readVInt();\n      int docIDUpto = in.readVInt();\n      term.length = code >> 2;\n      \n      if ((code & 1) != 0) {\n        termField = in.readString();\n      }\n      if ((code & 2) != 0) {\n        updateField = in.readString();\n      }\n\n      if (term.bytes.length < term.length) {\n        term.bytes = ArrayUtil.grow(term.bytes, term.length);\n      }\n      in.readBytes(term.bytes, 0, term.length);\n\n      final int limit;\n      if (delGen == segState.delGen) {\n        assert segmentPrivateDeletes;\n        limit = docIDUpto;\n      } else {\n        limit = Integer.MAX_VALUE;\n      }\n        \n      // TODO: we traverse the terms in update order (not term order) so that we\n      // apply the updates in the correct order, i.e. if two terms udpate the\n      // same document, the last one that came in wins, irrespective of the\n      // terms lexical order.\n      // we can apply the updates in terms order if we keep an updatesGen (and\n      // increment it with every update) and attach it to each NumericUpdate. Note\n      // that we cannot rely only on docIDUpto because an app may send two updates\n      // which will get same docIDUpto, yet will still need to respect the order\n      // those updates arrived.\n\n      // TODO: we could at least *collate* by field?\n\n      // This is the field used to resolve to docIDs, e.g. an \"id\" field, not the doc values field we are updating!\n      if ((code & 1) != 0) {\n        Terms terms = segState.reader.terms(termField);\n        if (terms != null) {\n          termsEnum = terms.iterator();\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      final BytesRef binaryValue;\n      final long longValue;\n      if (isNumeric) {\n        longValue = NumericDocValuesUpdate.readFrom(in);\n        binaryValue = null;\n      } else {\n        longValue = -1;\n        binaryValue = BinaryDocValuesUpdate.readFrom(in, scratch);\n      }\n\n      if (termsEnum == null) {\n        // no terms in this segment for this field\n        continue;\n      }\n\n      if (termsEnum.seekExact(term)) {\n        // we don't need term frequencies for this\n        postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n        DocValuesFieldUpdates dvUpdates = holder.get(updateField);\n        if (dvUpdates == null) {\n          if (isNumeric) {\n            dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n          } else {\n            dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n          }\n          holder.put(updateField, dvUpdates);\n        }\n        final IntConsumer docIdConsumer;\n        final DocValuesFieldUpdates update = dvUpdates;\n        if (isNumeric) {\n          docIdConsumer = doc -> update.add(doc, longValue);\n        } else {\n          docIdConsumer = doc -> update.add(doc, binaryValue);\n        }\n        final Bits acceptDocs = segState.rld.getLiveDocs();\n        if (segState.rld.sortMap != null && segmentPrivateDeletes) {\n          // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n          int doc;\n          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            if (acceptDocs == null || acceptDocs.get(doc)) {\n              // The limit is in the pre-sorted doc space:\n              if (segState.rld.sortMap.newToOld(doc) < limit) {\n                docIdConsumer.accept(doc);\n                updateCount++;\n              }\n            }\n          }\n        } else {\n          int doc;\n          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            if (doc >= limit) {\n              break; // no more docs that can be updated for this term\n            }\n            if (acceptDocs == null || acceptDocs.get(doc)) {\n              docIdConsumer.accept(doc);\n              updateCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : holder.values()) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","sourceOld":"  private long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState,\n                                     byte[] updates, boolean isNumeric) throws IOException {\n\n    TermsEnum termsEnum = null;\n    PostingsEnum postingsEnum = null;\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    Map<String, DocValuesFieldUpdates> holder = new HashMap<>();\n\n    ByteArrayDataInput in = new ByteArrayDataInput(updates);\n\n    String termField = null;\n    String updateField = null;\n    BytesRef term = new BytesRef();\n    term.bytes = new byte[16];\n    \n    BytesRef scratch = new BytesRef();\n    scratch.bytes = new byte[16];\n    \n    while (in.getPosition() != updates.length) {\n      int code = in.readVInt();\n      int docIDUpto = in.readVInt();\n      term.length = code >> 2;\n      \n      if ((code & 1) != 0) {\n        termField = in.readString();\n      }\n      if ((code & 2) != 0) {\n        updateField = in.readString();\n      }\n\n      if (term.bytes.length < term.length) {\n        term.bytes = ArrayUtil.grow(term.bytes, term.length);\n      }\n      in.readBytes(term.bytes, 0, term.length);\n\n      int limit;\n      if (delGen == segState.delGen) {\n        assert privateSegment != null;\n        limit = docIDUpto;\n      } else {\n        limit = Integer.MAX_VALUE;\n      }\n        \n      // TODO: we traverse the terms in update order (not term order) so that we\n      // apply the updates in the correct order, i.e. if two terms udpate the\n      // same document, the last one that came in wins, irrespective of the\n      // terms lexical order.\n      // we can apply the updates in terms order if we keep an updatesGen (and\n      // increment it with every update) and attach it to each NumericUpdate. Note\n      // that we cannot rely only on docIDUpto because an app may send two updates\n      // which will get same docIDUpto, yet will still need to respect the order\n      // those updates arrived.\n\n      // TODO: we could at least *collate* by field?\n\n      // This is the field used to resolve to docIDs, e.g. an \"id\" field, not the doc values field we are updating!\n      if ((code & 1) != 0) {\n        Terms terms = segState.reader.terms(termField);\n        if (terms != null) {\n          termsEnum = terms.iterator();\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      // TODO: can we avoid boxing here w/o fully forking this method?\n      Object value;\n      if (isNumeric) {\n        value = Long.valueOf(in.readZLong());\n      } else {\n        value = scratch;\n        scratch.length = in.readVInt();\n        if (scratch.bytes.length < scratch.length) {\n          scratch.bytes = ArrayUtil.grow(scratch.bytes, scratch.length);\n        }\n        in.readBytes(scratch.bytes, 0, scratch.length);\n      }\n\n      if (termsEnum == null) {\n        // no terms in this segment for this field\n        continue;\n      }\n\n      if (termsEnum.seekExact(term)) {\n\n        // we don't need term frequencies for this\n        postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n        DocValuesFieldUpdates dvUpdates = holder.get(updateField);\n        if (dvUpdates == null) {\n          if (isNumeric) {\n            dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n          } else {\n            dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n          }\n\n          holder.put(updateField, dvUpdates);\n        }\n\n        if (segState.rld.sortMap != null && privateSegment != null) {\n          // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n          int doc;\n          final Bits acceptDocs = segState.rld.getLiveDocs();\n          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n\n            if (acceptDocs != null && acceptDocs.get(doc) == false) {\n              continue;\n            }\n            \n            // The limit is in the pre-sorted doc space:\n            if (segState.rld.sortMap.newToOld(doc) < limit) {\n              dvUpdates.add(doc, value);\n              updateCount++;\n            }\n          }\n        } else {\n          int doc;\n          final Bits acceptDocs = segState.rld.getLiveDocs();\n          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            if (doc >= limit) {\n              break; // no more docs that can be updated for this term\n            }\n            if (acceptDocs != null && acceptDocs.get(doc) == false) {\n              continue;\n            }\n            dvUpdates.add(doc, value);\n            updateCount++;\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : holder.values()) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"33adea025f43af3243278587a46b8d9fd2e8ccf9","date":1525885077,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,byte[],boolean,long,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,byte[],boolean,long,boolean).mjava","sourceNew":"  private static long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState, byte[] updates,\n                                            boolean isNumeric, long delGen,\n                                            boolean segmentPrivateDeletes) throws IOException {\n\n    TermsEnum termsEnum = null;\n    PostingsEnum postingsEnum = null;\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    Map<String, DocValuesFieldUpdates> holder = new HashMap<>();\n\n    ByteArrayDataInput in = new ByteArrayDataInput(updates);\n\n    String termField = null;\n    String updateField = null;\n    BytesRef term = new BytesRef();\n    term.bytes = new byte[16];\n    \n    BytesRef scratch = new BytesRef();\n    scratch.bytes = new byte[16];\n    \n    while (in.getPosition() != updates.length) {\n      int code = in.readVInt();\n      int docIDUpto = in.readVInt();\n      term.length = code >> 3;\n      \n      if ((code & 1) != 0) {\n        termField = in.readString();\n      }\n      if ((code & 2) != 0) {\n        updateField = in.readString();\n      }\n      boolean hasValue = (code & 4) != 0;\n\n      if (term.bytes.length < term.length) {\n        term.bytes = ArrayUtil.grow(term.bytes, term.length);\n      }\n      in.readBytes(term.bytes, 0, term.length);\n\n      final int limit;\n      if (delGen == segState.delGen) {\n        assert segmentPrivateDeletes;\n        limit = docIDUpto;\n      } else {\n        limit = Integer.MAX_VALUE;\n      }\n        \n      // TODO: we traverse the terms in update order (not term order) so that we\n      // apply the updates in the correct order, i.e. if two terms udpate the\n      // same document, the last one that came in wins, irrespective of the\n      // terms lexical order.\n      // we can apply the updates in terms order if we keep an updatesGen (and\n      // increment it with every update) and attach it to each NumericUpdate. Note\n      // that we cannot rely only on docIDUpto because an app may send two updates\n      // which will get same docIDUpto, yet will still need to respect the order\n      // those updates arrived.\n\n      // TODO: we could at least *collate* by field?\n\n      // This is the field used to resolve to docIDs, e.g. an \"id\" field, not the doc values field we are updating!\n      if ((code & 1) != 0) {\n        Terms terms = segState.reader.terms(termField);\n        if (terms != null) {\n          termsEnum = terms.iterator();\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      final BytesRef binaryValue;\n      final long longValue;\n      if (hasValue == false) {\n        longValue = -1;\n        binaryValue = null;\n      } else if (isNumeric) {\n        longValue = NumericDocValuesUpdate.readFrom(in);\n        binaryValue = null;\n      } else {\n        longValue = -1;\n        binaryValue = BinaryDocValuesUpdate.readFrom(in, scratch);\n      }\n\n      if (termsEnum == null) {\n        // no terms in this segment for this field\n        continue;\n      }\n\n      if (termsEnum.seekExact(term)) {\n        // we don't need term frequencies for this\n        postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n        DocValuesFieldUpdates dvUpdates = holder.get(updateField);\n        if (dvUpdates == null) {\n          if (isNumeric) {\n            dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n          } else {\n            dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n          }\n          holder.put(updateField, dvUpdates);\n        }\n        final IntConsumer docIdConsumer;\n        final DocValuesFieldUpdates update = dvUpdates;\n        if (hasValue == false) {\n          docIdConsumer = doc -> update.reset(doc);\n        } else if (isNumeric) {\n          docIdConsumer = doc -> update.add(doc, longValue);\n        } else {\n          docIdConsumer = doc -> update.add(doc, binaryValue);\n        }\n        final Bits acceptDocs = segState.rld.getLiveDocs();\n        if (segState.rld.sortMap != null && segmentPrivateDeletes) {\n          // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n          int doc;\n          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            if (acceptDocs == null || acceptDocs.get(doc)) {\n              // The limit is in the pre-sorted doc space:\n              if (segState.rld.sortMap.newToOld(doc) < limit) {\n                docIdConsumer.accept(doc);\n                updateCount++;\n              }\n            }\n          }\n        } else {\n          int doc;\n          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            if (doc >= limit) {\n              break; // no more docs that can be updated for this term\n            }\n            if (acceptDocs == null || acceptDocs.get(doc)) {\n              docIdConsumer.accept(doc);\n              updateCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : holder.values()) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","sourceOld":"  private static long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState, byte[] updates,\n                                            boolean isNumeric, long delGen,\n                                            boolean segmentPrivateDeletes) throws IOException {\n\n    TermsEnum termsEnum = null;\n    PostingsEnum postingsEnum = null;\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    Map<String, DocValuesFieldUpdates> holder = new HashMap<>();\n\n    ByteArrayDataInput in = new ByteArrayDataInput(updates);\n\n    String termField = null;\n    String updateField = null;\n    BytesRef term = new BytesRef();\n    term.bytes = new byte[16];\n    \n    BytesRef scratch = new BytesRef();\n    scratch.bytes = new byte[16];\n    \n    while (in.getPosition() != updates.length) {\n      int code = in.readVInt();\n      int docIDUpto = in.readVInt();\n      term.length = code >> 2;\n      \n      if ((code & 1) != 0) {\n        termField = in.readString();\n      }\n      if ((code & 2) != 0) {\n        updateField = in.readString();\n      }\n\n      if (term.bytes.length < term.length) {\n        term.bytes = ArrayUtil.grow(term.bytes, term.length);\n      }\n      in.readBytes(term.bytes, 0, term.length);\n\n      final int limit;\n      if (delGen == segState.delGen) {\n        assert segmentPrivateDeletes;\n        limit = docIDUpto;\n      } else {\n        limit = Integer.MAX_VALUE;\n      }\n        \n      // TODO: we traverse the terms in update order (not term order) so that we\n      // apply the updates in the correct order, i.e. if two terms udpate the\n      // same document, the last one that came in wins, irrespective of the\n      // terms lexical order.\n      // we can apply the updates in terms order if we keep an updatesGen (and\n      // increment it with every update) and attach it to each NumericUpdate. Note\n      // that we cannot rely only on docIDUpto because an app may send two updates\n      // which will get same docIDUpto, yet will still need to respect the order\n      // those updates arrived.\n\n      // TODO: we could at least *collate* by field?\n\n      // This is the field used to resolve to docIDs, e.g. an \"id\" field, not the doc values field we are updating!\n      if ((code & 1) != 0) {\n        Terms terms = segState.reader.terms(termField);\n        if (terms != null) {\n          termsEnum = terms.iterator();\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      final BytesRef binaryValue;\n      final long longValue;\n      if (isNumeric) {\n        longValue = NumericDocValuesUpdate.readFrom(in);\n        binaryValue = null;\n      } else {\n        longValue = -1;\n        binaryValue = BinaryDocValuesUpdate.readFrom(in, scratch);\n      }\n\n      if (termsEnum == null) {\n        // no terms in this segment for this field\n        continue;\n      }\n\n      if (termsEnum.seekExact(term)) {\n        // we don't need term frequencies for this\n        postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n        DocValuesFieldUpdates dvUpdates = holder.get(updateField);\n        if (dvUpdates == null) {\n          if (isNumeric) {\n            dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n          } else {\n            dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n          }\n          holder.put(updateField, dvUpdates);\n        }\n        final IntConsumer docIdConsumer;\n        final DocValuesFieldUpdates update = dvUpdates;\n        if (isNumeric) {\n          docIdConsumer = doc -> update.add(doc, longValue);\n        } else {\n          docIdConsumer = doc -> update.add(doc, binaryValue);\n        }\n        final Bits acceptDocs = segState.rld.getLiveDocs();\n        if (segState.rld.sortMap != null && segmentPrivateDeletes) {\n          // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n          int doc;\n          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            if (acceptDocs == null || acceptDocs.get(doc)) {\n              // The limit is in the pre-sorted doc space:\n              if (segState.rld.sortMap.newToOld(doc) < limit) {\n                docIdConsumer.accept(doc);\n                updateCount++;\n              }\n            }\n          }\n        } else {\n          int doc;\n          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            if (doc >= limit) {\n              break; // no more docs that can be updated for this term\n            }\n            if (acceptDocs == null || acceptDocs.get(doc)) {\n              docIdConsumer.accept(doc);\n              updateCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : holder.values()) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2dca9025b39d48d3ba3337769ffce9a6375be5be","date":1543934174,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,byte[],boolean,long,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,byte[],boolean,long,boolean).mjava","sourceNew":"  private static long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState, byte[] updates,\n                                            boolean isNumeric, long delGen,\n                                            boolean segmentPrivateDeletes) throws IOException {\n\n    TermsEnum termsEnum = null;\n    PostingsEnum postingsEnum = null;\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    Map<String, DocValuesFieldUpdates> holder = new HashMap<>();\n\n    ByteArrayDataInput in = new ByteArrayDataInput(updates);\n\n    String termField = null;\n    String updateField = null;\n    BytesRef term = new BytesRef();\n    term.bytes = new byte[16];\n    \n    BytesRef scratch = new BytesRef();\n    scratch.bytes = new byte[16];\n    \n    while (in.getPosition() != updates.length) {\n      int code = in.readVInt();\n      int docIDUpto = in.readVInt();\n      term.length = code >> 3;\n      \n      if ((code & 1) != 0) {\n        termField = in.readString();\n      }\n      if ((code & 2) != 0) {\n        updateField = in.readString();\n      }\n      boolean hasValue = (code & 4) != 0;\n\n      if (term.bytes.length < term.length) {\n        term.bytes = ArrayUtil.grow(term.bytes, term.length);\n      }\n      in.readBytes(term.bytes, 0, term.length);\n\n      final int limit;\n      if (delGen == segState.delGen) {\n        assert segmentPrivateDeletes;\n        limit = docIDUpto;\n      } else {\n        limit = Integer.MAX_VALUE;\n      }\n        \n      // TODO: we traverse the terms in update order (not term order) so that we\n      // apply the updates in the correct order, i.e. if two terms update the\n      // same document, the last one that came in wins, irrespective of the\n      // terms lexical order.\n      // we can apply the updates in terms order if we keep an updatesGen (and\n      // increment it with every update) and attach it to each NumericUpdate. Note\n      // that we cannot rely only on docIDUpto because an app may send two updates\n      // which will get same docIDUpto, yet will still need to respect the order\n      // those updates arrived.\n\n      // TODO: we could at least *collate* by field?\n\n      // This is the field used to resolve to docIDs, e.g. an \"id\" field, not the doc values field we are updating!\n      if ((code & 1) != 0) {\n        Terms terms = segState.reader.terms(termField);\n        if (terms != null) {\n          termsEnum = terms.iterator();\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      final BytesRef binaryValue;\n      final long longValue;\n      if (hasValue == false) {\n        longValue = -1;\n        binaryValue = null;\n      } else if (isNumeric) {\n        longValue = NumericDocValuesUpdate.readFrom(in);\n        binaryValue = null;\n      } else {\n        longValue = -1;\n        binaryValue = BinaryDocValuesUpdate.readFrom(in, scratch);\n      }\n\n      if (termsEnum == null) {\n        // no terms in this segment for this field\n        continue;\n      }\n\n      if (termsEnum.seekExact(term)) {\n        // we don't need term frequencies for this\n        postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n        DocValuesFieldUpdates dvUpdates = holder.get(updateField);\n        if (dvUpdates == null) {\n          if (isNumeric) {\n            dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n          } else {\n            dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n          }\n          holder.put(updateField, dvUpdates);\n        }\n        final IntConsumer docIdConsumer;\n        final DocValuesFieldUpdates update = dvUpdates;\n        if (hasValue == false) {\n          docIdConsumer = doc -> update.reset(doc);\n        } else if (isNumeric) {\n          docIdConsumer = doc -> update.add(doc, longValue);\n        } else {\n          docIdConsumer = doc -> update.add(doc, binaryValue);\n        }\n        final Bits acceptDocs = segState.rld.getLiveDocs();\n        if (segState.rld.sortMap != null && segmentPrivateDeletes) {\n          // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n          int doc;\n          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            if (acceptDocs == null || acceptDocs.get(doc)) {\n              // The limit is in the pre-sorted doc space:\n              if (segState.rld.sortMap.newToOld(doc) < limit) {\n                docIdConsumer.accept(doc);\n                updateCount++;\n              }\n            }\n          }\n        } else {\n          int doc;\n          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            if (doc >= limit) {\n              break; // no more docs that can be updated for this term\n            }\n            if (acceptDocs == null || acceptDocs.get(doc)) {\n              docIdConsumer.accept(doc);\n              updateCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : holder.values()) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","sourceOld":"  private static long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState, byte[] updates,\n                                            boolean isNumeric, long delGen,\n                                            boolean segmentPrivateDeletes) throws IOException {\n\n    TermsEnum termsEnum = null;\n    PostingsEnum postingsEnum = null;\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    Map<String, DocValuesFieldUpdates> holder = new HashMap<>();\n\n    ByteArrayDataInput in = new ByteArrayDataInput(updates);\n\n    String termField = null;\n    String updateField = null;\n    BytesRef term = new BytesRef();\n    term.bytes = new byte[16];\n    \n    BytesRef scratch = new BytesRef();\n    scratch.bytes = new byte[16];\n    \n    while (in.getPosition() != updates.length) {\n      int code = in.readVInt();\n      int docIDUpto = in.readVInt();\n      term.length = code >> 3;\n      \n      if ((code & 1) != 0) {\n        termField = in.readString();\n      }\n      if ((code & 2) != 0) {\n        updateField = in.readString();\n      }\n      boolean hasValue = (code & 4) != 0;\n\n      if (term.bytes.length < term.length) {\n        term.bytes = ArrayUtil.grow(term.bytes, term.length);\n      }\n      in.readBytes(term.bytes, 0, term.length);\n\n      final int limit;\n      if (delGen == segState.delGen) {\n        assert segmentPrivateDeletes;\n        limit = docIDUpto;\n      } else {\n        limit = Integer.MAX_VALUE;\n      }\n        \n      // TODO: we traverse the terms in update order (not term order) so that we\n      // apply the updates in the correct order, i.e. if two terms udpate the\n      // same document, the last one that came in wins, irrespective of the\n      // terms lexical order.\n      // we can apply the updates in terms order if we keep an updatesGen (and\n      // increment it with every update) and attach it to each NumericUpdate. Note\n      // that we cannot rely only on docIDUpto because an app may send two updates\n      // which will get same docIDUpto, yet will still need to respect the order\n      // those updates arrived.\n\n      // TODO: we could at least *collate* by field?\n\n      // This is the field used to resolve to docIDs, e.g. an \"id\" field, not the doc values field we are updating!\n      if ((code & 1) != 0) {\n        Terms terms = segState.reader.terms(termField);\n        if (terms != null) {\n          termsEnum = terms.iterator();\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      final BytesRef binaryValue;\n      final long longValue;\n      if (hasValue == false) {\n        longValue = -1;\n        binaryValue = null;\n      } else if (isNumeric) {\n        longValue = NumericDocValuesUpdate.readFrom(in);\n        binaryValue = null;\n      } else {\n        longValue = -1;\n        binaryValue = BinaryDocValuesUpdate.readFrom(in, scratch);\n      }\n\n      if (termsEnum == null) {\n        // no terms in this segment for this field\n        continue;\n      }\n\n      if (termsEnum.seekExact(term)) {\n        // we don't need term frequencies for this\n        postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n        DocValuesFieldUpdates dvUpdates = holder.get(updateField);\n        if (dvUpdates == null) {\n          if (isNumeric) {\n            dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n          } else {\n            dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n          }\n          holder.put(updateField, dvUpdates);\n        }\n        final IntConsumer docIdConsumer;\n        final DocValuesFieldUpdates update = dvUpdates;\n        if (hasValue == false) {\n          docIdConsumer = doc -> update.reset(doc);\n        } else if (isNumeric) {\n          docIdConsumer = doc -> update.add(doc, longValue);\n        } else {\n          docIdConsumer = doc -> update.add(doc, binaryValue);\n        }\n        final Bits acceptDocs = segState.rld.getLiveDocs();\n        if (segState.rld.sortMap != null && segmentPrivateDeletes) {\n          // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n          int doc;\n          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            if (acceptDocs == null || acceptDocs.get(doc)) {\n              // The limit is in the pre-sorted doc space:\n              if (segState.rld.sortMap.newToOld(doc) < limit) {\n                docIdConsumer.accept(doc);\n                updateCount++;\n              }\n            }\n          }\n        } else {\n          int doc;\n          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            if (doc >= limit) {\n              break; // no more docs that can be updated for this term\n            }\n            if (acceptDocs == null || acceptDocs.get(doc)) {\n              docIdConsumer.accept(doc);\n              updateCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : holder.values()) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28211671436f185419b3f7e53ccfc3911441ab65","date":1544026960,"type":4,"author":"Simon Willnauer","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#applyDocValuesUpdates(BufferedUpdatesStream.SegmentState,byte[],boolean,long,boolean).mjava","sourceNew":null,"sourceOld":"  private static long applyDocValuesUpdates(BufferedUpdatesStream.SegmentState segState, byte[] updates,\n                                            boolean isNumeric, long delGen,\n                                            boolean segmentPrivateDeletes) throws IOException {\n\n    TermsEnum termsEnum = null;\n    PostingsEnum postingsEnum = null;\n\n    // TODO: we can process the updates per DV field, from last to first so that\n    // if multiple terms affect same document for the same field, we add an update\n    // only once (that of the last term). To do that, we can keep a bitset which\n    // marks which documents have already been updated. So e.g. if term T1\n    // updates doc 7, and then we process term T2 and it updates doc 7 as well,\n    // we don't apply the update since we know T1 came last and therefore wins\n    // the update.\n    // We can also use that bitset as 'liveDocs' to pass to TermEnum.docs(), so\n    // that these documents aren't even returned.\n\n    long updateCount = 0;\n\n    // We first write all our updates private, and only in the end publish to the ReadersAndUpdates */\n    Map<String, DocValuesFieldUpdates> holder = new HashMap<>();\n\n    ByteArrayDataInput in = new ByteArrayDataInput(updates);\n\n    String termField = null;\n    String updateField = null;\n    BytesRef term = new BytesRef();\n    term.bytes = new byte[16];\n    \n    BytesRef scratch = new BytesRef();\n    scratch.bytes = new byte[16];\n    \n    while (in.getPosition() != updates.length) {\n      int code = in.readVInt();\n      int docIDUpto = in.readVInt();\n      term.length = code >> 3;\n      \n      if ((code & 1) != 0) {\n        termField = in.readString();\n      }\n      if ((code & 2) != 0) {\n        updateField = in.readString();\n      }\n      boolean hasValue = (code & 4) != 0;\n\n      if (term.bytes.length < term.length) {\n        term.bytes = ArrayUtil.grow(term.bytes, term.length);\n      }\n      in.readBytes(term.bytes, 0, term.length);\n\n      final int limit;\n      if (delGen == segState.delGen) {\n        assert segmentPrivateDeletes;\n        limit = docIDUpto;\n      } else {\n        limit = Integer.MAX_VALUE;\n      }\n        \n      // TODO: we traverse the terms in update order (not term order) so that we\n      // apply the updates in the correct order, i.e. if two terms update the\n      // same document, the last one that came in wins, irrespective of the\n      // terms lexical order.\n      // we can apply the updates in terms order if we keep an updatesGen (and\n      // increment it with every update) and attach it to each NumericUpdate. Note\n      // that we cannot rely only on docIDUpto because an app may send two updates\n      // which will get same docIDUpto, yet will still need to respect the order\n      // those updates arrived.\n\n      // TODO: we could at least *collate* by field?\n\n      // This is the field used to resolve to docIDs, e.g. an \"id\" field, not the doc values field we are updating!\n      if ((code & 1) != 0) {\n        Terms terms = segState.reader.terms(termField);\n        if (terms != null) {\n          termsEnum = terms.iterator();\n        } else {\n          termsEnum = null;\n        }\n      }\n\n      final BytesRef binaryValue;\n      final long longValue;\n      if (hasValue == false) {\n        longValue = -1;\n        binaryValue = null;\n      } else if (isNumeric) {\n        longValue = NumericDocValuesUpdate.readFrom(in);\n        binaryValue = null;\n      } else {\n        longValue = -1;\n        binaryValue = BinaryDocValuesUpdate.readFrom(in, scratch);\n      }\n\n      if (termsEnum == null) {\n        // no terms in this segment for this field\n        continue;\n      }\n\n      if (termsEnum.seekExact(term)) {\n        // we don't need term frequencies for this\n        postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n        DocValuesFieldUpdates dvUpdates = holder.get(updateField);\n        if (dvUpdates == null) {\n          if (isNumeric) {\n            dvUpdates = new NumericDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n          } else {\n            dvUpdates = new BinaryDocValuesFieldUpdates(delGen, updateField, segState.reader.maxDoc());\n          }\n          holder.put(updateField, dvUpdates);\n        }\n        final IntConsumer docIdConsumer;\n        final DocValuesFieldUpdates update = dvUpdates;\n        if (hasValue == false) {\n          docIdConsumer = doc -> update.reset(doc);\n        } else if (isNumeric) {\n          docIdConsumer = doc -> update.add(doc, longValue);\n        } else {\n          docIdConsumer = doc -> update.add(doc, binaryValue);\n        }\n        final Bits acceptDocs = segState.rld.getLiveDocs();\n        if (segState.rld.sortMap != null && segmentPrivateDeletes) {\n          // This segment was sorted on flush; we must apply seg-private deletes carefully in this case:\n          int doc;\n          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            if (acceptDocs == null || acceptDocs.get(doc)) {\n              // The limit is in the pre-sorted doc space:\n              if (segState.rld.sortMap.newToOld(doc) < limit) {\n                docIdConsumer.accept(doc);\n                updateCount++;\n              }\n            }\n          }\n        } else {\n          int doc;\n          while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n            if (doc >= limit) {\n              break; // no more docs that can be updated for this term\n            }\n            if (acceptDocs == null || acceptDocs.get(doc)) {\n              docIdConsumer.accept(doc);\n              updateCount++;\n            }\n          }\n        }\n      }\n    }\n\n    // now freeze & publish:\n    for (DocValuesFieldUpdates update : holder.values()) {\n      if (update.any()) {\n        update.finish();\n        segState.rld.addDVUpdate(update);\n      }\n    }\n\n    return updateCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"28211671436f185419b3f7e53ccfc3911441ab65":["2dca9025b39d48d3ba3337769ffce9a6375be5be"],"2dca9025b39d48d3ba3337769ffce9a6375be5be":["33adea025f43af3243278587a46b8d9fd2e8ccf9"],"7e129bd6cb34a236558a49edf108a49d5c15e0e1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"33adea025f43af3243278587a46b8d9fd2e8ccf9":["f42883db49d143abc1a0f176ba47e3388dafb608"],"f42883db49d143abc1a0f176ba47e3388dafb608":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","7e129bd6cb34a236558a49edf108a49d5c15e0e1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["28211671436f185419b3f7e53ccfc3911441ab65"]},"commit2Childs":{"28211671436f185419b3f7e53ccfc3911441ab65":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"2dca9025b39d48d3ba3337769ffce9a6375be5be":["28211671436f185419b3f7e53ccfc3911441ab65"],"7e129bd6cb34a236558a49edf108a49d5c15e0e1":["f42883db49d143abc1a0f176ba47e3388dafb608"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7e129bd6cb34a236558a49edf108a49d5c15e0e1","f42883db49d143abc1a0f176ba47e3388dafb608"],"33adea025f43af3243278587a46b8d9fd2e8ccf9":["2dca9025b39d48d3ba3337769ffce9a6375be5be"],"f42883db49d143abc1a0f176ba47e3388dafb608":["33adea025f43af3243278587a46b8d9fd2e8ccf9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}