{"path":"src/test/org/apache/lucene/search/TestStressSort#create().mjava","commits":[{"id":"63bc3238545c6012bd44f5d294077997f236bc4e","date":1233087321,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestStressSort#create().mjava","pathOld":"/dev/null","sourceNew":"  private void create() throws Throwable {\n\n    // NOTE: put seed in here to make failures\n    // deterministic, but do not commit with a seed (to\n    // better test):\n    final Random r = new Random();\n    dir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.setMaxBufferedDocs(17);\n\n    final Document doc = new Document();\n    final Document doc2 = new Document();\n\n    final Field id = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NO);\n    doc.add(id);\n    doc2.add(id);\n\n    final Field contents = new Field(\"contents\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(contents);\n    doc2.add(contents);\n\n    final Field byteField = new Field(\"byte\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(byteField);\n    doc2.add(byteField);\n\n    final Field shortField = new Field(\"short\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(shortField);\n    doc2.add(shortField);\n\n    final Field intField = new Field(\"int\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(intField);\n    doc2.add(intField);\n\n    final Field longField = new Field(\"long\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(longField);\n    doc2.add(longField);\n\n    final Field floatField = new Field(\"float\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(floatField);\n    doc2.add(floatField);\n\n    final Field doubleField = new Field(\"double\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(doubleField);\n    doc2.add(doubleField);\n\n    final Field stringField = new Field(\"string\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringField);\n    // doc2 doesn't have stringField, so we get nulls\n\n    for(int i=0;i<NUM_DOCS;i++) {\n      id.setValue(\"\"+i);\n      if (i % 1000 == 0) {\n        contents.setValue(\"a b c z\");\n      } else if (i % 100 == 0) {\n        contents.setValue(\"a b c y\");\n      } else if (i % 10 == 0) {\n        contents.setValue(\"a b c x\");\n      } else {\n        contents.setValue(\"a b c\");\n      }\n      byteField.setValue(\"\"+nextInt(Byte.MIN_VALUE, Byte.MAX_VALUE));\n      if (nextInt(10) == 3) {\n        shortField.setValue(\"\"+Short.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        shortField.setValue(\"\"+Short.MAX_VALUE);\n      } else {\n        shortField.setValue(\"\"+nextInt(Short.MIN_VALUE, Short.MAX_VALUE));\n      }\n\n      if (nextInt(10) == 3) {\n        intField.setValue(\"\"+Integer.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        intField.setValue(\"\"+Integer.MAX_VALUE);\n      } else {\n        intField.setValue(\"\"+r.nextInt());\n      }\n\n      if (nextInt(10) == 3) {\n        longField.setValue(\"\"+Long.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        longField.setValue(\"\"+Long.MAX_VALUE);\n      } else {\n        longField.setValue(\"\"+r.nextLong());\n      }\n      floatField.setValue(\"\"+r.nextFloat());\n      doubleField.setValue(\"\"+r.nextDouble());\n      if (i % 197 == 0) {\n        writer.addDocument(doc2);\n      } else {\n        stringField.setValue(randomString(nextInt(20)));\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    searcherMultiSegment = new IndexSearcher(dir);\n\n    dir2 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir2, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize();\n    writer.close();\n    searcherSingleSegment = new IndexSearcher(dir2);\n\n    dir3 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir3, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize(3);\n    writer.close();\n    searcherFewSegment = new IndexSearcher(dir3);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cb1066f2afe9450585d0d10063ea4450085236f1","date":1233870820,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestStressSort#create().mjava","pathOld":"src/test/org/apache/lucene/search/TestStressSort#create().mjava","sourceNew":"  private void create() throws Throwable {\n\n    // NOTE: put seed in here to make failures\n    // deterministic, but do not commit with a seed (to\n    // better test):\n    dir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.setMaxBufferedDocs(17);\n\n    final Document doc = new Document();\n    final Document doc2 = new Document();\n\n    final Field id = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NO);\n    doc.add(id);\n    doc2.add(id);\n\n    final Field contents = new Field(\"contents\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(contents);\n    doc2.add(contents);\n\n    final Field byteField = new Field(\"byte\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(byteField);\n    doc2.add(byteField);\n\n    final Field shortField = new Field(\"short\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(shortField);\n    doc2.add(shortField);\n\n    final Field intField = new Field(\"int\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(intField);\n    doc2.add(intField);\n\n    final Field longField = new Field(\"long\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(longField);\n    doc2.add(longField);\n\n    final Field floatField = new Field(\"float\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(floatField);\n    doc2.add(floatField);\n\n    final Field doubleField = new Field(\"double\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(doubleField);\n    doc2.add(doubleField);\n\n    final Field stringField = new Field(\"string\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringField);\n    // doc2 doesn't have stringField, so we get nulls\n\n    for(int i=0;i<NUM_DOCS;i++) {\n      id.setValue(\"\"+i);\n      if (i % 1000 == 0) {\n        contents.setValue(\"a b c z\");\n      } else if (i % 100 == 0) {\n        contents.setValue(\"a b c y\");\n      } else if (i % 10 == 0) {\n        contents.setValue(\"a b c x\");\n      } else {\n        contents.setValue(\"a b c\");\n      }\n      byteField.setValue(\"\"+nextInt(Byte.MIN_VALUE, Byte.MAX_VALUE));\n      if (nextInt(10) == 3) {\n        shortField.setValue(\"\"+Short.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        shortField.setValue(\"\"+Short.MAX_VALUE);\n      } else {\n        shortField.setValue(\"\"+nextInt(Short.MIN_VALUE, Short.MAX_VALUE));\n      }\n\n      if (nextInt(10) == 3) {\n        intField.setValue(\"\"+Integer.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        intField.setValue(\"\"+Integer.MAX_VALUE);\n      } else {\n        intField.setValue(\"\"+r.nextInt());\n      }\n\n      if (nextInt(10) == 3) {\n        longField.setValue(\"\"+Long.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        longField.setValue(\"\"+Long.MAX_VALUE);\n      } else {\n        longField.setValue(\"\"+r.nextLong());\n      }\n      floatField.setValue(\"\"+r.nextFloat());\n      doubleField.setValue(\"\"+r.nextDouble());\n      if (i % 197 == 0) {\n        writer.addDocument(doc2);\n      } else {\n        stringField.setValue(randomString(nextInt(20)));\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    searcherMultiSegment = new IndexSearcher(dir);\n\n    dir2 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir2, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize();\n    writer.close();\n    searcherSingleSegment = new IndexSearcher(dir2);\n\n    dir3 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir3, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize(3);\n    writer.close();\n    searcherFewSegment = new IndexSearcher(dir3);\n  }\n\n","sourceOld":"  private void create() throws Throwable {\n\n    // NOTE: put seed in here to make failures\n    // deterministic, but do not commit with a seed (to\n    // better test):\n    final Random r = new Random();\n    dir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.setMaxBufferedDocs(17);\n\n    final Document doc = new Document();\n    final Document doc2 = new Document();\n\n    final Field id = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NO);\n    doc.add(id);\n    doc2.add(id);\n\n    final Field contents = new Field(\"contents\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(contents);\n    doc2.add(contents);\n\n    final Field byteField = new Field(\"byte\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(byteField);\n    doc2.add(byteField);\n\n    final Field shortField = new Field(\"short\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(shortField);\n    doc2.add(shortField);\n\n    final Field intField = new Field(\"int\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(intField);\n    doc2.add(intField);\n\n    final Field longField = new Field(\"long\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(longField);\n    doc2.add(longField);\n\n    final Field floatField = new Field(\"float\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(floatField);\n    doc2.add(floatField);\n\n    final Field doubleField = new Field(\"double\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(doubleField);\n    doc2.add(doubleField);\n\n    final Field stringField = new Field(\"string\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringField);\n    // doc2 doesn't have stringField, so we get nulls\n\n    for(int i=0;i<NUM_DOCS;i++) {\n      id.setValue(\"\"+i);\n      if (i % 1000 == 0) {\n        contents.setValue(\"a b c z\");\n      } else if (i % 100 == 0) {\n        contents.setValue(\"a b c y\");\n      } else if (i % 10 == 0) {\n        contents.setValue(\"a b c x\");\n      } else {\n        contents.setValue(\"a b c\");\n      }\n      byteField.setValue(\"\"+nextInt(Byte.MIN_VALUE, Byte.MAX_VALUE));\n      if (nextInt(10) == 3) {\n        shortField.setValue(\"\"+Short.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        shortField.setValue(\"\"+Short.MAX_VALUE);\n      } else {\n        shortField.setValue(\"\"+nextInt(Short.MIN_VALUE, Short.MAX_VALUE));\n      }\n\n      if (nextInt(10) == 3) {\n        intField.setValue(\"\"+Integer.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        intField.setValue(\"\"+Integer.MAX_VALUE);\n      } else {\n        intField.setValue(\"\"+r.nextInt());\n      }\n\n      if (nextInt(10) == 3) {\n        longField.setValue(\"\"+Long.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        longField.setValue(\"\"+Long.MAX_VALUE);\n      } else {\n        longField.setValue(\"\"+r.nextLong());\n      }\n      floatField.setValue(\"\"+r.nextFloat());\n      doubleField.setValue(\"\"+r.nextDouble());\n      if (i % 197 == 0) {\n        writer.addDocument(doc2);\n      } else {\n        stringField.setValue(randomString(nextInt(20)));\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    searcherMultiSegment = new IndexSearcher(dir);\n\n    dir2 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir2, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize();\n    writer.close();\n    searcherSingleSegment = new IndexSearcher(dir2);\n\n    dir3 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir3, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize(3);\n    writer.close();\n    searcherFewSegment = new IndexSearcher(dir3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3965529a7891904512492d9c6a0c4dc6323899bc","date":1243970180,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestStressSort#create().mjava","pathOld":"src/test/org/apache/lucene/search/TestStressSort#create().mjava","sourceNew":"  private void create() throws Throwable {\n\n    // NOTE: put seed in here to make failures\n    // deterministic, but do not commit with a seed (to\n    // better test):\n    dir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.setMaxBufferedDocs(17);\n\n    final Document doc = new Document();\n    final Document doc2 = new Document();\n\n    final Field id = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NO);\n    doc.add(id);\n    doc2.add(id);\n\n    final Field contents = new Field(\"contents\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(contents);\n    doc2.add(contents);\n\n    final Field byteField = new Field(\"byte\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(byteField);\n    doc2.add(byteField);\n\n    final Field shortField = new Field(\"short\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(shortField);\n    doc2.add(shortField);\n\n    final Field intField = new Field(\"int\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(intField);\n    doc2.add(intField);\n\n    final Field longField = new Field(\"long\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(longField);\n    doc2.add(longField);\n\n    final Field floatField = new Field(\"float\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(floatField);\n    doc2.add(floatField);\n\n    final Field doubleField = new Field(\"double\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(doubleField);\n    doc2.add(doubleField);\n\n    final Field stringField = new Field(\"string\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringField);\n    // doc2 doesn't have stringField, so we get nulls\n\n    for(int i=0;i<NUM_DOCS;i++) {\n      id.setValue(\"\"+i);\n      if (i % 1000 == 0) {\n        contents.setValue(\"a b c z\");\n      } else if (i % 100 == 0) {\n        contents.setValue(\"a b c y\");\n      } else if (i % 10 == 0) {\n        contents.setValue(\"a b c x\");\n      } else {\n        contents.setValue(\"a b c\");\n      }\n      byteField.setValue(\"\"+nextInt(Byte.MIN_VALUE, Byte.MAX_VALUE));\n      if (nextInt(10) == 3) {\n        shortField.setValue(\"\"+Short.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        shortField.setValue(\"\"+Short.MAX_VALUE);\n      } else {\n        shortField.setValue(\"\"+nextInt(Short.MIN_VALUE, Short.MAX_VALUE));\n      }\n\n      if (nextInt(10) == 3) {\n        intField.setValue(\"\"+Integer.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        intField.setValue(\"\"+Integer.MAX_VALUE);\n      } else {\n        intField.setValue(\"\"+r.nextInt());\n      }\n\n      if (nextInt(10) == 3) {\n        longField.setValue(\"\"+Long.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        longField.setValue(\"\"+Long.MAX_VALUE);\n      } else {\n        longField.setValue(\"\"+r.nextLong());\n      }\n      floatField.setValue(\"\"+r.nextFloat());\n      doubleField.setValue(\"\"+r.nextDouble());\n      if (i % 197 == 0) {\n        writer.addDocument(doc2);\n      } else {\n        stringField.setValue(randomString(nextInt(20)));\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    searcherMultiSegment = new IndexSearcher(dir);\n    searcherMultiSegment.setDefaultFieldSortScoring(true, true);\n\n    dir2 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir2, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize();\n    writer.close();\n    searcherSingleSegment = new IndexSearcher(dir2);\n    searcherSingleSegment.setDefaultFieldSortScoring(true, true);\n    dir3 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir3, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize(3);\n    writer.close();\n    searcherFewSegment = new IndexSearcher(dir3);\n    searcherFewSegment.setDefaultFieldSortScoring(true, true);\n  }\n\n","sourceOld":"  private void create() throws Throwable {\n\n    // NOTE: put seed in here to make failures\n    // deterministic, but do not commit with a seed (to\n    // better test):\n    dir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.setMaxBufferedDocs(17);\n\n    final Document doc = new Document();\n    final Document doc2 = new Document();\n\n    final Field id = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NO);\n    doc.add(id);\n    doc2.add(id);\n\n    final Field contents = new Field(\"contents\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(contents);\n    doc2.add(contents);\n\n    final Field byteField = new Field(\"byte\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(byteField);\n    doc2.add(byteField);\n\n    final Field shortField = new Field(\"short\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(shortField);\n    doc2.add(shortField);\n\n    final Field intField = new Field(\"int\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(intField);\n    doc2.add(intField);\n\n    final Field longField = new Field(\"long\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(longField);\n    doc2.add(longField);\n\n    final Field floatField = new Field(\"float\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(floatField);\n    doc2.add(floatField);\n\n    final Field doubleField = new Field(\"double\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(doubleField);\n    doc2.add(doubleField);\n\n    final Field stringField = new Field(\"string\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringField);\n    // doc2 doesn't have stringField, so we get nulls\n\n    for(int i=0;i<NUM_DOCS;i++) {\n      id.setValue(\"\"+i);\n      if (i % 1000 == 0) {\n        contents.setValue(\"a b c z\");\n      } else if (i % 100 == 0) {\n        contents.setValue(\"a b c y\");\n      } else if (i % 10 == 0) {\n        contents.setValue(\"a b c x\");\n      } else {\n        contents.setValue(\"a b c\");\n      }\n      byteField.setValue(\"\"+nextInt(Byte.MIN_VALUE, Byte.MAX_VALUE));\n      if (nextInt(10) == 3) {\n        shortField.setValue(\"\"+Short.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        shortField.setValue(\"\"+Short.MAX_VALUE);\n      } else {\n        shortField.setValue(\"\"+nextInt(Short.MIN_VALUE, Short.MAX_VALUE));\n      }\n\n      if (nextInt(10) == 3) {\n        intField.setValue(\"\"+Integer.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        intField.setValue(\"\"+Integer.MAX_VALUE);\n      } else {\n        intField.setValue(\"\"+r.nextInt());\n      }\n\n      if (nextInt(10) == 3) {\n        longField.setValue(\"\"+Long.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        longField.setValue(\"\"+Long.MAX_VALUE);\n      } else {\n        longField.setValue(\"\"+r.nextLong());\n      }\n      floatField.setValue(\"\"+r.nextFloat());\n      doubleField.setValue(\"\"+r.nextDouble());\n      if (i % 197 == 0) {\n        writer.addDocument(doc2);\n      } else {\n        stringField.setValue(randomString(nextInt(20)));\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    searcherMultiSegment = new IndexSearcher(dir);\n\n    dir2 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir2, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize();\n    writer.close();\n    searcherSingleSegment = new IndexSearcher(dir2);\n\n    dir3 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir3, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize(3);\n    writer.close();\n    searcherFewSegment = new IndexSearcher(dir3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e69f59b863731d864bf3047235e718f0f88f8841","date":1250105498,"type":3,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestStressSort#create().mjava","pathOld":"src/test/org/apache/lucene/search/TestStressSort#create().mjava","sourceNew":"  private void create() throws Throwable {\n\n    // NOTE: put seed in here to make failures\n    // deterministic, but do not commit with a seed (to\n    // better test):\n    dir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.setMaxBufferedDocs(17);\n\n    final Document doc = new Document();\n    final Document doc2 = new Document();\n\n    final Field id = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NO);\n    doc.add(id);\n    doc2.add(id);\n\n    final Field contents = new Field(\"contents\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(contents);\n    doc2.add(contents);\n\n    final Field byteField = new Field(\"byte\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(byteField);\n    doc2.add(byteField);\n\n    final Field shortField = new Field(\"short\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(shortField);\n    doc2.add(shortField);\n\n    final Field intField = new Field(\"int\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(intField);\n    doc2.add(intField);\n\n    final Field longField = new Field(\"long\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(longField);\n    doc2.add(longField);\n\n    final Field floatField = new Field(\"float\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(floatField);\n    doc2.add(floatField);\n\n    final Field doubleField = new Field(\"double\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(doubleField);\n    doc2.add(doubleField);\n\n    // we use two diff string fields so our FieldCache usage\n    // is less suspicious to cache inspection\n    final Field stringField = new Field(\"string\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringField);\n    final Field stringFieldIdx = new Field(\"stringIdx\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringFieldIdx);\n    // doc2 doesn't have stringField or stringFieldIdx, so we get nulls\n\n    for(int i=0;i<NUM_DOCS;i++) {\n      id.setValue(\"\"+i);\n      if (i % 1000 == 0) {\n        contents.setValue(\"a b c z\");\n      } else if (i % 100 == 0) {\n        contents.setValue(\"a b c y\");\n      } else if (i % 10 == 0) {\n        contents.setValue(\"a b c x\");\n      } else {\n        contents.setValue(\"a b c\");\n      }\n      byteField.setValue(\"\"+nextInt(Byte.MIN_VALUE, Byte.MAX_VALUE));\n      if (nextInt(10) == 3) {\n        shortField.setValue(\"\"+Short.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        shortField.setValue(\"\"+Short.MAX_VALUE);\n      } else {\n        shortField.setValue(\"\"+nextInt(Short.MIN_VALUE, Short.MAX_VALUE));\n      }\n\n      if (nextInt(10) == 3) {\n        intField.setValue(\"\"+Integer.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        intField.setValue(\"\"+Integer.MAX_VALUE);\n      } else {\n        intField.setValue(\"\"+r.nextInt());\n      }\n\n      if (nextInt(10) == 3) {\n        longField.setValue(\"\"+Long.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        longField.setValue(\"\"+Long.MAX_VALUE);\n      } else {\n        longField.setValue(\"\"+r.nextLong());\n      }\n      floatField.setValue(\"\"+r.nextFloat());\n      doubleField.setValue(\"\"+r.nextDouble());\n      if (i % 197 == 0) {\n        writer.addDocument(doc2);\n      } else {\n        String r = randomString(nextInt(20));\n        stringField.setValue(r);\n        stringFieldIdx.setValue(r);\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    searcherMultiSegment = new IndexSearcher(dir);\n    searcherMultiSegment.setDefaultFieldSortScoring(true, true);\n\n    dir2 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir2, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize();\n    writer.close();\n    searcherSingleSegment = new IndexSearcher(dir2);\n    searcherSingleSegment.setDefaultFieldSortScoring(true, true);\n    dir3 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir3, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize(3);\n    writer.close();\n    searcherFewSegment = new IndexSearcher(dir3);\n    searcherFewSegment.setDefaultFieldSortScoring(true, true);\n  }\n\n","sourceOld":"  private void create() throws Throwable {\n\n    // NOTE: put seed in here to make failures\n    // deterministic, but do not commit with a seed (to\n    // better test):\n    dir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.setMaxBufferedDocs(17);\n\n    final Document doc = new Document();\n    final Document doc2 = new Document();\n\n    final Field id = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NO);\n    doc.add(id);\n    doc2.add(id);\n\n    final Field contents = new Field(\"contents\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(contents);\n    doc2.add(contents);\n\n    final Field byteField = new Field(\"byte\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(byteField);\n    doc2.add(byteField);\n\n    final Field shortField = new Field(\"short\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(shortField);\n    doc2.add(shortField);\n\n    final Field intField = new Field(\"int\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(intField);\n    doc2.add(intField);\n\n    final Field longField = new Field(\"long\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(longField);\n    doc2.add(longField);\n\n    final Field floatField = new Field(\"float\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(floatField);\n    doc2.add(floatField);\n\n    final Field doubleField = new Field(\"double\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(doubleField);\n    doc2.add(doubleField);\n\n    final Field stringField = new Field(\"string\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringField);\n    // doc2 doesn't have stringField, so we get nulls\n\n    for(int i=0;i<NUM_DOCS;i++) {\n      id.setValue(\"\"+i);\n      if (i % 1000 == 0) {\n        contents.setValue(\"a b c z\");\n      } else if (i % 100 == 0) {\n        contents.setValue(\"a b c y\");\n      } else if (i % 10 == 0) {\n        contents.setValue(\"a b c x\");\n      } else {\n        contents.setValue(\"a b c\");\n      }\n      byteField.setValue(\"\"+nextInt(Byte.MIN_VALUE, Byte.MAX_VALUE));\n      if (nextInt(10) == 3) {\n        shortField.setValue(\"\"+Short.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        shortField.setValue(\"\"+Short.MAX_VALUE);\n      } else {\n        shortField.setValue(\"\"+nextInt(Short.MIN_VALUE, Short.MAX_VALUE));\n      }\n\n      if (nextInt(10) == 3) {\n        intField.setValue(\"\"+Integer.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        intField.setValue(\"\"+Integer.MAX_VALUE);\n      } else {\n        intField.setValue(\"\"+r.nextInt());\n      }\n\n      if (nextInt(10) == 3) {\n        longField.setValue(\"\"+Long.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        longField.setValue(\"\"+Long.MAX_VALUE);\n      } else {\n        longField.setValue(\"\"+r.nextLong());\n      }\n      floatField.setValue(\"\"+r.nextFloat());\n      doubleField.setValue(\"\"+r.nextDouble());\n      if (i % 197 == 0) {\n        writer.addDocument(doc2);\n      } else {\n        stringField.setValue(randomString(nextInt(20)));\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    searcherMultiSegment = new IndexSearcher(dir);\n    searcherMultiSegment.setDefaultFieldSortScoring(true, true);\n\n    dir2 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir2, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize();\n    writer.close();\n    searcherSingleSegment = new IndexSearcher(dir2);\n    searcherSingleSegment.setDefaultFieldSortScoring(true, true);\n    dir3 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir3, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize(3);\n    writer.close();\n    searcherFewSegment = new IndexSearcher(dir3);\n    searcherFewSegment.setDefaultFieldSortScoring(true, true);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e8d1458a2543cbd30cbfe7929be4dcb5c5251659","date":1254582241,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestStressSort#create().mjava","pathOld":"src/test/org/apache/lucene/search/TestStressSort#create().mjava","sourceNew":"  private void create() throws Throwable {\n\n    // NOTE: put seed in here to make failures\n    // deterministic, but do not commit with a seed (to\n    // better test):\n    dir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.setMaxBufferedDocs(17);\n\n    final Document doc = new Document();\n    final Document doc2 = new Document();\n\n    final Field id = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NO);\n    doc.add(id);\n    doc2.add(id);\n\n    final Field contents = new Field(\"contents\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(contents);\n    doc2.add(contents);\n\n    final Field byteField = new Field(\"byte\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(byteField);\n    doc2.add(byteField);\n\n    final Field shortField = new Field(\"short\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(shortField);\n    doc2.add(shortField);\n\n    final Field intField = new Field(\"int\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(intField);\n    doc2.add(intField);\n\n    final Field longField = new Field(\"long\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(longField);\n    doc2.add(longField);\n\n    final Field floatField = new Field(\"float\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(floatField);\n    doc2.add(floatField);\n\n    final Field doubleField = new Field(\"double\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(doubleField);\n    doc2.add(doubleField);\n\n    // we use two diff string fields so our FieldCache usage\n    // is less suspicious to cache inspection\n    final Field stringField = new Field(\"string\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringField);\n    final Field stringFieldIdx = new Field(\"stringIdx\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringFieldIdx);\n    // doc2 doesn't have stringField or stringFieldIdx, so we get nulls\n\n    for(int i=0;i<NUM_DOCS;i++) {\n      id.setValue(\"\"+i);\n      if (i % 1000 == 0) {\n        contents.setValue(\"a b c z\");\n      } else if (i % 100 == 0) {\n        contents.setValue(\"a b c y\");\n      } else if (i % 10 == 0) {\n        contents.setValue(\"a b c x\");\n      } else {\n        contents.setValue(\"a b c\");\n      }\n      byteField.setValue(\"\"+nextInt(Byte.MIN_VALUE, Byte.MAX_VALUE));\n      if (nextInt(10) == 3) {\n        shortField.setValue(\"\"+Short.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        shortField.setValue(\"\"+Short.MAX_VALUE);\n      } else {\n        shortField.setValue(\"\"+nextInt(Short.MIN_VALUE, Short.MAX_VALUE));\n      }\n\n      if (nextInt(10) == 3) {\n        intField.setValue(\"\"+Integer.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        intField.setValue(\"\"+Integer.MAX_VALUE);\n      } else {\n        intField.setValue(\"\"+r.nextInt());\n      }\n\n      if (nextInt(10) == 3) {\n        longField.setValue(\"\"+Long.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        longField.setValue(\"\"+Long.MAX_VALUE);\n      } else {\n        longField.setValue(\"\"+r.nextLong());\n      }\n      floatField.setValue(\"\"+r.nextFloat());\n      doubleField.setValue(\"\"+r.nextDouble());\n      if (i % 197 == 0) {\n        writer.addDocument(doc2);\n      } else {\n        String r = randomString(nextInt(20));\n        stringField.setValue(r);\n        stringFieldIdx.setValue(r);\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    searcherMultiSegment = new IndexSearcher(dir, true);\n    searcherMultiSegment.setDefaultFieldSortScoring(true, true);\n\n    dir2 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir2, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize();\n    writer.close();\n    searcherSingleSegment = new IndexSearcher(dir2, true);\n    searcherSingleSegment.setDefaultFieldSortScoring(true, true);\n    dir3 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir3, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize(3);\n    writer.close();\n    searcherFewSegment = new IndexSearcher(dir3, true);\n    searcherFewSegment.setDefaultFieldSortScoring(true, true);\n  }\n\n","sourceOld":"  private void create() throws Throwable {\n\n    // NOTE: put seed in here to make failures\n    // deterministic, but do not commit with a seed (to\n    // better test):\n    dir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.setMaxBufferedDocs(17);\n\n    final Document doc = new Document();\n    final Document doc2 = new Document();\n\n    final Field id = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NO);\n    doc.add(id);\n    doc2.add(id);\n\n    final Field contents = new Field(\"contents\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(contents);\n    doc2.add(contents);\n\n    final Field byteField = new Field(\"byte\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(byteField);\n    doc2.add(byteField);\n\n    final Field shortField = new Field(\"short\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(shortField);\n    doc2.add(shortField);\n\n    final Field intField = new Field(\"int\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(intField);\n    doc2.add(intField);\n\n    final Field longField = new Field(\"long\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(longField);\n    doc2.add(longField);\n\n    final Field floatField = new Field(\"float\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(floatField);\n    doc2.add(floatField);\n\n    final Field doubleField = new Field(\"double\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(doubleField);\n    doc2.add(doubleField);\n\n    // we use two diff string fields so our FieldCache usage\n    // is less suspicious to cache inspection\n    final Field stringField = new Field(\"string\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringField);\n    final Field stringFieldIdx = new Field(\"stringIdx\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringFieldIdx);\n    // doc2 doesn't have stringField or stringFieldIdx, so we get nulls\n\n    for(int i=0;i<NUM_DOCS;i++) {\n      id.setValue(\"\"+i);\n      if (i % 1000 == 0) {\n        contents.setValue(\"a b c z\");\n      } else if (i % 100 == 0) {\n        contents.setValue(\"a b c y\");\n      } else if (i % 10 == 0) {\n        contents.setValue(\"a b c x\");\n      } else {\n        contents.setValue(\"a b c\");\n      }\n      byteField.setValue(\"\"+nextInt(Byte.MIN_VALUE, Byte.MAX_VALUE));\n      if (nextInt(10) == 3) {\n        shortField.setValue(\"\"+Short.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        shortField.setValue(\"\"+Short.MAX_VALUE);\n      } else {\n        shortField.setValue(\"\"+nextInt(Short.MIN_VALUE, Short.MAX_VALUE));\n      }\n\n      if (nextInt(10) == 3) {\n        intField.setValue(\"\"+Integer.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        intField.setValue(\"\"+Integer.MAX_VALUE);\n      } else {\n        intField.setValue(\"\"+r.nextInt());\n      }\n\n      if (nextInt(10) == 3) {\n        longField.setValue(\"\"+Long.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        longField.setValue(\"\"+Long.MAX_VALUE);\n      } else {\n        longField.setValue(\"\"+r.nextLong());\n      }\n      floatField.setValue(\"\"+r.nextFloat());\n      doubleField.setValue(\"\"+r.nextDouble());\n      if (i % 197 == 0) {\n        writer.addDocument(doc2);\n      } else {\n        String r = randomString(nextInt(20));\n        stringField.setValue(r);\n        stringFieldIdx.setValue(r);\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    searcherMultiSegment = new IndexSearcher(dir);\n    searcherMultiSegment.setDefaultFieldSortScoring(true, true);\n\n    dir2 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir2, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize();\n    writer.close();\n    searcherSingleSegment = new IndexSearcher(dir2);\n    searcherSingleSegment.setDefaultFieldSortScoring(true, true);\n    dir3 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir3, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize(3);\n    writer.close();\n    searcherFewSegment = new IndexSearcher(dir3);\n    searcherFewSegment.setDefaultFieldSortScoring(true, true);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a046c0c310bc77931fc8441bd920053b607dd14","date":1254584734,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"src/test/org/apache/lucene/search/TestStressSort#create().mjava","pathOld":"src/test/org/apache/lucene/search/TestStressSort#create().mjava","sourceNew":"  private void create() throws Throwable {\n\n    // NOTE: put seed in here to make failures\n    // deterministic, but do not commit with a seed (to\n    // better test):\n    dir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.setMaxBufferedDocs(17);\n\n    final Document doc = new Document();\n    final Document doc2 = new Document();\n\n    final Field id = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NO);\n    doc.add(id);\n    doc2.add(id);\n\n    final Field contents = new Field(\"contents\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(contents);\n    doc2.add(contents);\n\n    final Field byteField = new Field(\"byte\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(byteField);\n    doc2.add(byteField);\n\n    final Field shortField = new Field(\"short\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(shortField);\n    doc2.add(shortField);\n\n    final Field intField = new Field(\"int\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(intField);\n    doc2.add(intField);\n\n    final Field longField = new Field(\"long\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(longField);\n    doc2.add(longField);\n\n    final Field floatField = new Field(\"float\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(floatField);\n    doc2.add(floatField);\n\n    final Field doubleField = new Field(\"double\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(doubleField);\n    doc2.add(doubleField);\n\n    // we use two diff string fields so our FieldCache usage\n    // is less suspicious to cache inspection\n    final Field stringField = new Field(\"string\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringField);\n    final Field stringFieldIdx = new Field(\"stringIdx\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringFieldIdx);\n    // doc2 doesn't have stringField or stringFieldIdx, so we get nulls\n\n    for(int i=0;i<NUM_DOCS;i++) {\n      id.setValue(\"\"+i);\n      if (i % 1000 == 0) {\n        contents.setValue(\"a b c z\");\n      } else if (i % 100 == 0) {\n        contents.setValue(\"a b c y\");\n      } else if (i % 10 == 0) {\n        contents.setValue(\"a b c x\");\n      } else {\n        contents.setValue(\"a b c\");\n      }\n      byteField.setValue(\"\"+nextInt(Byte.MIN_VALUE, Byte.MAX_VALUE));\n      if (nextInt(10) == 3) {\n        shortField.setValue(\"\"+Short.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        shortField.setValue(\"\"+Short.MAX_VALUE);\n      } else {\n        shortField.setValue(\"\"+nextInt(Short.MIN_VALUE, Short.MAX_VALUE));\n      }\n\n      if (nextInt(10) == 3) {\n        intField.setValue(\"\"+Integer.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        intField.setValue(\"\"+Integer.MAX_VALUE);\n      } else {\n        intField.setValue(\"\"+r.nextInt());\n      }\n\n      if (nextInt(10) == 3) {\n        longField.setValue(\"\"+Long.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        longField.setValue(\"\"+Long.MAX_VALUE);\n      } else {\n        longField.setValue(\"\"+r.nextLong());\n      }\n      floatField.setValue(\"\"+r.nextFloat());\n      doubleField.setValue(\"\"+r.nextDouble());\n      if (i % 197 == 0) {\n        writer.addDocument(doc2);\n      } else {\n        String r = randomString(nextInt(20));\n        stringField.setValue(r);\n        stringFieldIdx.setValue(r);\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    searcherMultiSegment = new IndexSearcher(dir, true);\n    searcherMultiSegment.setDefaultFieldSortScoring(true, true);\n\n    dir2 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir2, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize();\n    writer.close();\n    searcherSingleSegment = new IndexSearcher(dir2, true);\n    searcherSingleSegment.setDefaultFieldSortScoring(true, true);\n    dir3 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir3, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize(3);\n    writer.close();\n    searcherFewSegment = new IndexSearcher(dir3, true);\n    searcherFewSegment.setDefaultFieldSortScoring(true, true);\n  }\n\n","sourceOld":"  private void create() throws Throwable {\n\n    // NOTE: put seed in here to make failures\n    // deterministic, but do not commit with a seed (to\n    // better test):\n    dir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.setMaxBufferedDocs(17);\n\n    final Document doc = new Document();\n    final Document doc2 = new Document();\n\n    final Field id = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NO);\n    doc.add(id);\n    doc2.add(id);\n\n    final Field contents = new Field(\"contents\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(contents);\n    doc2.add(contents);\n\n    final Field byteField = new Field(\"byte\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(byteField);\n    doc2.add(byteField);\n\n    final Field shortField = new Field(\"short\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(shortField);\n    doc2.add(shortField);\n\n    final Field intField = new Field(\"int\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(intField);\n    doc2.add(intField);\n\n    final Field longField = new Field(\"long\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(longField);\n    doc2.add(longField);\n\n    final Field floatField = new Field(\"float\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(floatField);\n    doc2.add(floatField);\n\n    final Field doubleField = new Field(\"double\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(doubleField);\n    doc2.add(doubleField);\n\n    // we use two diff string fields so our FieldCache usage\n    // is less suspicious to cache inspection\n    final Field stringField = new Field(\"string\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringField);\n    final Field stringFieldIdx = new Field(\"stringIdx\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringFieldIdx);\n    // doc2 doesn't have stringField or stringFieldIdx, so we get nulls\n\n    for(int i=0;i<NUM_DOCS;i++) {\n      id.setValue(\"\"+i);\n      if (i % 1000 == 0) {\n        contents.setValue(\"a b c z\");\n      } else if (i % 100 == 0) {\n        contents.setValue(\"a b c y\");\n      } else if (i % 10 == 0) {\n        contents.setValue(\"a b c x\");\n      } else {\n        contents.setValue(\"a b c\");\n      }\n      byteField.setValue(\"\"+nextInt(Byte.MIN_VALUE, Byte.MAX_VALUE));\n      if (nextInt(10) == 3) {\n        shortField.setValue(\"\"+Short.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        shortField.setValue(\"\"+Short.MAX_VALUE);\n      } else {\n        shortField.setValue(\"\"+nextInt(Short.MIN_VALUE, Short.MAX_VALUE));\n      }\n\n      if (nextInt(10) == 3) {\n        intField.setValue(\"\"+Integer.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        intField.setValue(\"\"+Integer.MAX_VALUE);\n      } else {\n        intField.setValue(\"\"+r.nextInt());\n      }\n\n      if (nextInt(10) == 3) {\n        longField.setValue(\"\"+Long.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        longField.setValue(\"\"+Long.MAX_VALUE);\n      } else {\n        longField.setValue(\"\"+r.nextLong());\n      }\n      floatField.setValue(\"\"+r.nextFloat());\n      doubleField.setValue(\"\"+r.nextDouble());\n      if (i % 197 == 0) {\n        writer.addDocument(doc2);\n      } else {\n        String r = randomString(nextInt(20));\n        stringField.setValue(r);\n        stringFieldIdx.setValue(r);\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    searcherMultiSegment = new IndexSearcher(dir);\n    searcherMultiSegment.setDefaultFieldSortScoring(true, true);\n\n    dir2 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir2, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize();\n    writer.close();\n    searcherSingleSegment = new IndexSearcher(dir2);\n    searcherSingleSegment.setDefaultFieldSortScoring(true, true);\n    dir3 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir3, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize(3);\n    writer.close();\n    searcherFewSegment = new IndexSearcher(dir3);\n    searcherFewSegment.setDefaultFieldSortScoring(true, true);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4b41b991de69ba7b72d5e90cfcee25699a1a7fc9","date":1256127131,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/search/TestStressSort#create().mjava","pathOld":"src/test/org/apache/lucene/search/TestStressSort#create().mjava","sourceNew":"  private void create() throws Throwable {\n\n    // NOTE: put seed in here to make failures\n    // deterministic, but do not commit with a seed (to\n    // better test):\n    dir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.LIMITED);\n    writer.setMaxBufferedDocs(17);\n\n    final Document doc = new Document();\n    final Document doc2 = new Document();\n\n    final Field id = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NO);\n    doc.add(id);\n    doc2.add(id);\n\n    final Field contents = new Field(\"contents\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(contents);\n    doc2.add(contents);\n\n    final Field byteField = new Field(\"byte\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(byteField);\n    doc2.add(byteField);\n\n    final Field shortField = new Field(\"short\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(shortField);\n    doc2.add(shortField);\n\n    final Field intField = new Field(\"int\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(intField);\n    doc2.add(intField);\n\n    final Field longField = new Field(\"long\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(longField);\n    doc2.add(longField);\n\n    final Field floatField = new Field(\"float\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(floatField);\n    doc2.add(floatField);\n\n    final Field doubleField = new Field(\"double\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(doubleField);\n    doc2.add(doubleField);\n\n    // we use two diff string fields so our FieldCache usage\n    // is less suspicious to cache inspection\n    final Field stringField = new Field(\"string\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringField);\n    final Field stringFieldIdx = new Field(\"stringIdx\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringFieldIdx);\n    // doc2 doesn't have stringField or stringFieldIdx, so we get nulls\n\n    for(int i=0;i<NUM_DOCS;i++) {\n      id.setValue(\"\"+i);\n      if (i % 1000 == 0) {\n        contents.setValue(\"a b c z\");\n      } else if (i % 100 == 0) {\n        contents.setValue(\"a b c y\");\n      } else if (i % 10 == 0) {\n        contents.setValue(\"a b c x\");\n      } else {\n        contents.setValue(\"a b c\");\n      }\n      byteField.setValue(\"\"+nextInt(Byte.MIN_VALUE, Byte.MAX_VALUE));\n      if (nextInt(10) == 3) {\n        shortField.setValue(\"\"+Short.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        shortField.setValue(\"\"+Short.MAX_VALUE);\n      } else {\n        shortField.setValue(\"\"+nextInt(Short.MIN_VALUE, Short.MAX_VALUE));\n      }\n\n      if (nextInt(10) == 3) {\n        intField.setValue(\"\"+Integer.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        intField.setValue(\"\"+Integer.MAX_VALUE);\n      } else {\n        intField.setValue(\"\"+r.nextInt());\n      }\n\n      if (nextInt(10) == 3) {\n        longField.setValue(\"\"+Long.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        longField.setValue(\"\"+Long.MAX_VALUE);\n      } else {\n        longField.setValue(\"\"+r.nextLong());\n      }\n      floatField.setValue(\"\"+r.nextFloat());\n      doubleField.setValue(\"\"+r.nextDouble());\n      if (i % 197 == 0) {\n        writer.addDocument(doc2);\n      } else {\n        String r = randomString(nextInt(20));\n        stringField.setValue(r);\n        stringFieldIdx.setValue(r);\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    searcherMultiSegment = new IndexSearcher(dir, true);\n    searcherMultiSegment.setDefaultFieldSortScoring(true, true);\n\n    dir2 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir2, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize();\n    writer.close();\n    searcherSingleSegment = new IndexSearcher(dir2, true);\n    searcherSingleSegment.setDefaultFieldSortScoring(true, true);\n    dir3 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir3, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize(3);\n    writer.close();\n    searcherFewSegment = new IndexSearcher(dir3, true);\n    searcherFewSegment.setDefaultFieldSortScoring(true, true);\n  }\n\n","sourceOld":"  private void create() throws Throwable {\n\n    // NOTE: put seed in here to make failures\n    // deterministic, but do not commit with a seed (to\n    // better test):\n    dir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.setMaxBufferedDocs(17);\n\n    final Document doc = new Document();\n    final Document doc2 = new Document();\n\n    final Field id = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NO);\n    doc.add(id);\n    doc2.add(id);\n\n    final Field contents = new Field(\"contents\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(contents);\n    doc2.add(contents);\n\n    final Field byteField = new Field(\"byte\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(byteField);\n    doc2.add(byteField);\n\n    final Field shortField = new Field(\"short\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(shortField);\n    doc2.add(shortField);\n\n    final Field intField = new Field(\"int\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(intField);\n    doc2.add(intField);\n\n    final Field longField = new Field(\"long\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(longField);\n    doc2.add(longField);\n\n    final Field floatField = new Field(\"float\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(floatField);\n    doc2.add(floatField);\n\n    final Field doubleField = new Field(\"double\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(doubleField);\n    doc2.add(doubleField);\n\n    // we use two diff string fields so our FieldCache usage\n    // is less suspicious to cache inspection\n    final Field stringField = new Field(\"string\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringField);\n    final Field stringFieldIdx = new Field(\"stringIdx\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringFieldIdx);\n    // doc2 doesn't have stringField or stringFieldIdx, so we get nulls\n\n    for(int i=0;i<NUM_DOCS;i++) {\n      id.setValue(\"\"+i);\n      if (i % 1000 == 0) {\n        contents.setValue(\"a b c z\");\n      } else if (i % 100 == 0) {\n        contents.setValue(\"a b c y\");\n      } else if (i % 10 == 0) {\n        contents.setValue(\"a b c x\");\n      } else {\n        contents.setValue(\"a b c\");\n      }\n      byteField.setValue(\"\"+nextInt(Byte.MIN_VALUE, Byte.MAX_VALUE));\n      if (nextInt(10) == 3) {\n        shortField.setValue(\"\"+Short.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        shortField.setValue(\"\"+Short.MAX_VALUE);\n      } else {\n        shortField.setValue(\"\"+nextInt(Short.MIN_VALUE, Short.MAX_VALUE));\n      }\n\n      if (nextInt(10) == 3) {\n        intField.setValue(\"\"+Integer.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        intField.setValue(\"\"+Integer.MAX_VALUE);\n      } else {\n        intField.setValue(\"\"+r.nextInt());\n      }\n\n      if (nextInt(10) == 3) {\n        longField.setValue(\"\"+Long.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        longField.setValue(\"\"+Long.MAX_VALUE);\n      } else {\n        longField.setValue(\"\"+r.nextLong());\n      }\n      floatField.setValue(\"\"+r.nextFloat());\n      doubleField.setValue(\"\"+r.nextDouble());\n      if (i % 197 == 0) {\n        writer.addDocument(doc2);\n      } else {\n        String r = randomString(nextInt(20));\n        stringField.setValue(r);\n        stringFieldIdx.setValue(r);\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    searcherMultiSegment = new IndexSearcher(dir, true);\n    searcherMultiSegment.setDefaultFieldSortScoring(true, true);\n\n    dir2 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir2, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize();\n    writer.close();\n    searcherSingleSegment = new IndexSearcher(dir2, true);\n    searcherSingleSegment.setDefaultFieldSortScoring(true, true);\n    dir3 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir3, new StandardAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize(3);\n    writer.close();\n    searcherFewSegment = new IndexSearcher(dir3, true);\n    searcherFewSegment.setDefaultFieldSortScoring(true, true);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"90576423481ca5b715ad8c2ebce681817cabb3b1","date":1256332541,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"src/test/org/apache/lucene/search/TestStressSort#create().mjava","sourceNew":null,"sourceOld":"  private void create() throws Throwable {\n\n    // NOTE: put seed in here to make failures\n    // deterministic, but do not commit with a seed (to\n    // better test):\n    dir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.LIMITED);\n    writer.setMaxBufferedDocs(17);\n\n    final Document doc = new Document();\n    final Document doc2 = new Document();\n\n    final Field id = new Field(\"id\", \"\", Field.Store.YES, Field.Index.NO);\n    doc.add(id);\n    doc2.add(id);\n\n    final Field contents = new Field(\"contents\", \"\", Field.Store.NO, Field.Index.ANALYZED);\n    doc.add(contents);\n    doc2.add(contents);\n\n    final Field byteField = new Field(\"byte\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(byteField);\n    doc2.add(byteField);\n\n    final Field shortField = new Field(\"short\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(shortField);\n    doc2.add(shortField);\n\n    final Field intField = new Field(\"int\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(intField);\n    doc2.add(intField);\n\n    final Field longField = new Field(\"long\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(longField);\n    doc2.add(longField);\n\n    final Field floatField = new Field(\"float\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(floatField);\n    doc2.add(floatField);\n\n    final Field doubleField = new Field(\"double\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(doubleField);\n    doc2.add(doubleField);\n\n    // we use two diff string fields so our FieldCache usage\n    // is less suspicious to cache inspection\n    final Field stringField = new Field(\"string\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringField);\n    final Field stringFieldIdx = new Field(\"stringIdx\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(stringFieldIdx);\n    // doc2 doesn't have stringField or stringFieldIdx, so we get nulls\n\n    for(int i=0;i<NUM_DOCS;i++) {\n      id.setValue(\"\"+i);\n      if (i % 1000 == 0) {\n        contents.setValue(\"a b c z\");\n      } else if (i % 100 == 0) {\n        contents.setValue(\"a b c y\");\n      } else if (i % 10 == 0) {\n        contents.setValue(\"a b c x\");\n      } else {\n        contents.setValue(\"a b c\");\n      }\n      byteField.setValue(\"\"+nextInt(Byte.MIN_VALUE, Byte.MAX_VALUE));\n      if (nextInt(10) == 3) {\n        shortField.setValue(\"\"+Short.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        shortField.setValue(\"\"+Short.MAX_VALUE);\n      } else {\n        shortField.setValue(\"\"+nextInt(Short.MIN_VALUE, Short.MAX_VALUE));\n      }\n\n      if (nextInt(10) == 3) {\n        intField.setValue(\"\"+Integer.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        intField.setValue(\"\"+Integer.MAX_VALUE);\n      } else {\n        intField.setValue(\"\"+r.nextInt());\n      }\n\n      if (nextInt(10) == 3) {\n        longField.setValue(\"\"+Long.MIN_VALUE);\n      } else if (nextInt(10) == 7) {\n        longField.setValue(\"\"+Long.MAX_VALUE);\n      } else {\n        longField.setValue(\"\"+r.nextLong());\n      }\n      floatField.setValue(\"\"+r.nextFloat());\n      doubleField.setValue(\"\"+r.nextDouble());\n      if (i % 197 == 0) {\n        writer.addDocument(doc2);\n      } else {\n        String r = randomString(nextInt(20));\n        stringField.setValue(r);\n        stringFieldIdx.setValue(r);\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    searcherMultiSegment = new IndexSearcher(dir, true);\n    searcherMultiSegment.setDefaultFieldSortScoring(true, true);\n\n    dir2 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir2, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize();\n    writer.close();\n    searcherSingleSegment = new IndexSearcher(dir2, true);\n    searcherSingleSegment.setDefaultFieldSortScoring(true, true);\n    dir3 = new MockRAMDirectory(dir);\n    writer = new IndexWriter(dir3, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), IndexWriter.MaxFieldLength.LIMITED);\n    writer.optimize(3);\n    writer.close();\n    searcherFewSegment = new IndexSearcher(dir3, true);\n    searcherFewSegment.setDefaultFieldSortScoring(true, true);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3965529a7891904512492d9c6a0c4dc6323899bc":["cb1066f2afe9450585d0d10063ea4450085236f1"],"cb1066f2afe9450585d0d10063ea4450085236f1":["63bc3238545c6012bd44f5d294077997f236bc4e"],"e8d1458a2543cbd30cbfe7929be4dcb5c5251659":["e69f59b863731d864bf3047235e718f0f88f8841"],"e69f59b863731d864bf3047235e718f0f88f8841":["3965529a7891904512492d9c6a0c4dc6323899bc"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0a046c0c310bc77931fc8441bd920053b607dd14":["e69f59b863731d864bf3047235e718f0f88f8841","e8d1458a2543cbd30cbfe7929be4dcb5c5251659"],"90576423481ca5b715ad8c2ebce681817cabb3b1":["4b41b991de69ba7b72d5e90cfcee25699a1a7fc9"],"4b41b991de69ba7b72d5e90cfcee25699a1a7fc9":["0a046c0c310bc77931fc8441bd920053b607dd14"],"63bc3238545c6012bd44f5d294077997f236bc4e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["90576423481ca5b715ad8c2ebce681817cabb3b1"]},"commit2Childs":{"3965529a7891904512492d9c6a0c4dc6323899bc":["e69f59b863731d864bf3047235e718f0f88f8841"],"cb1066f2afe9450585d0d10063ea4450085236f1":["3965529a7891904512492d9c6a0c4dc6323899bc"],"e8d1458a2543cbd30cbfe7929be4dcb5c5251659":["0a046c0c310bc77931fc8441bd920053b607dd14"],"e69f59b863731d864bf3047235e718f0f88f8841":["e8d1458a2543cbd30cbfe7929be4dcb5c5251659","0a046c0c310bc77931fc8441bd920053b607dd14"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["63bc3238545c6012bd44f5d294077997f236bc4e"],"0a046c0c310bc77931fc8441bd920053b607dd14":["4b41b991de69ba7b72d5e90cfcee25699a1a7fc9"],"90576423481ca5b715ad8c2ebce681817cabb3b1":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4b41b991de69ba7b72d5e90cfcee25699a1a7fc9":["90576423481ca5b715ad8c2ebce681817cabb3b1"],"63bc3238545c6012bd44f5d294077997f236bc4e":["cb1066f2afe9450585d0d10063ea4450085236f1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}