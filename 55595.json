{"path":"lucene/backwards/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/backwards/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","pathOld":"backwards/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","sourceNew":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n\n    int code = textStart;\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate RawPostingList in hash\n    p = postingsHash[hashPos];\n\n    if (p != null && p.textStart != textStart) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        p = postingsHash[hashPos];\n      } while (p != null && p.textStart != textStart);\n    }\n\n    if (p == null) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n\n      // Refill?\n      if (0 == perThread.freePostingsCount)\n        perThread.morePostings();\n\n      // Pull next free RawPostingList from free list\n      p = perThread.freePostings[--perThread.freePostingsCount];\n      assert p != null;\n\n      p.textStart = textStart;\n          \n      assert postingsHash[hashPos] == null;\n      postingsHash[hashPos] = p;\n      numPostings++;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      p.intStart = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      p.byteStart = intUptos[intUptoStart];\n\n      consumer.newTerm(p);\n\n    } else {\n      intUptos = intPool.buffers[p.intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = p.intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(p);\n    }\n  }\n\n","sourceOld":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n\n    int code = textStart;\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate RawPostingList in hash\n    p = postingsHash[hashPos];\n\n    if (p != null && p.textStart != textStart) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        p = postingsHash[hashPos];\n      } while (p != null && p.textStart != textStart);\n    }\n\n    if (p == null) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n\n      // Refill?\n      if (0 == perThread.freePostingsCount)\n        perThread.morePostings();\n\n      // Pull next free RawPostingList from free list\n      p = perThread.freePostings[--perThread.freePostingsCount];\n      assert p != null;\n\n      p.textStart = textStart;\n          \n      assert postingsHash[hashPos] == null;\n      postingsHash[hashPos] = p;\n      numPostings++;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      p.intStart = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      p.byteStart = intUptos[intUptoStart];\n\n      consumer.newTerm(p);\n\n    } else {\n      intUptos = intPool.buffers[p.intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = p.intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(p);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6","date":1272983566,"type":4,"author":"Uwe Schindler","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/backwards/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","sourceNew":null,"sourceOld":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n\n    int code = textStart;\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate RawPostingList in hash\n    p = postingsHash[hashPos];\n\n    if (p != null && p.textStart != textStart) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        p = postingsHash[hashPos];\n      } while (p != null && p.textStart != textStart);\n    }\n\n    if (p == null) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n\n      // Refill?\n      if (0 == perThread.freePostingsCount)\n        perThread.morePostings();\n\n      // Pull next free RawPostingList from free list\n      p = perThread.freePostings[--perThread.freePostingsCount];\n      assert p != null;\n\n      p.textStart = textStart;\n          \n      assert postingsHash[hashPos] == null;\n      postingsHash[hashPos] = p;\n      numPostings++;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      p.intStart = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      p.byteStart = intUptos[intUptoStart];\n\n      consumer.newTerm(p);\n\n    } else {\n      intUptos = intPool.buffers[p.intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = p.intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(p);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}