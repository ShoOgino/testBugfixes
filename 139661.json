{"path":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","commits":[{"id":"b21422ff1d1d56499dec481f193b402e5e8def5b","date":1281472367,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean).mjava","sourceNew":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexWriterConfig conf = newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n  }\n\n","sourceOld":"  public void createIndex(String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1f653cfcf159baeaafe5d01682a911e95bba4012","date":1284122058,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","sourceNew":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n  }\n\n","sourceOld":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexWriterConfig conf = newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"132903c28af3aa6f67284b78de91c0f0a99488c2","date":1284282129,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","sourceNew":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n  }\n\n","sourceOld":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a13a126d15299d5c1e117ea99ddae6fb0fa3f209","date":1291909583,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","sourceNew":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n  }\n\n","sourceOld":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5","date":1292695408,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","sourceNew":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n  }\n\n","sourceOld":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","sourceNew":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n  }\n\n","sourceOld":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = FSDirectory.open(new File(dirName));\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundDocStore(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","pathOld":"/dev/null","sourceNew":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7a54e23e03b47f3d568ab3020bdd386e4b2f0a05","date":1294877328,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","sourceNew":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", Similarity.getDefault().encodeNormValue(1.5f));\n    reader.close();\n  }\n\n","sourceOld":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","sourceNew":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", Similarity.getDefault().encodeNormValue(1.5f));\n    reader.close();\n  }\n\n","sourceOld":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fd9325c7ff9928fabe81c28553b41fc7aa57dfab","date":1295896411,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","sourceNew":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", conf.getSimilarityProvider().get(\"content\").encodeNormValue(1.5f));\n    reader.close();\n  }\n\n","sourceOld":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", Similarity.getDefault().encodeNormValue(1.5f));\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bb9b72f7c3d7827c64dd4ec580ded81778da361d","date":1295897920,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","sourceNew":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", conf.getSimilarityProvider().get(\"content\").encodeNormValue(1.5f));\n    reader.close();\n  }\n\n","sourceOld":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", Similarity.getDefault().encodeNormValue(1.5f));\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cdad2c6b6234338031bcc1f24c001a5ad66f714","date":1296866109,"type":3,"author":"Doron Cohen","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","sourceNew":"  public File createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    File indexDir = _TestUtil.getTempDir(dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    \n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", conf.getSimilarityProvider().get(\"content\").encodeNormValue(1.5f));\n    reader.close();\n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", conf.getSimilarityProvider().get(\"content\").encodeNormValue(1.5f));\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","sourceNew":"  public File createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    File indexDir = _TestUtil.getTempDir(dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    \n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", conf.getSimilarityProvider().get(\"content\").encodeNormValue(1.5f));\n    reader.close();\n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","sourceNew":"  public File createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    File indexDir = _TestUtil.getTempDir(dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    \n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", conf.getSimilarityProvider().get(\"content\").encodeNormValue(1.5f));\n    reader.close();\n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public void createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    rmDir(dirName);\n\n    dirName = fullDir(dirName);\n\n    Directory dir = newFSDirectory(new File(dirName));\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", conf.getSimilarityProvider().get(\"content\").encodeNormValue(1.5f));\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","sourceNew":"  public File createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    File indexDir = _TestUtil.getTempDir(dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    \n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", conf.getSimilarityProvider().get(\"content\").encodeNormValue(1.5f));\n    reader.close();\n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    File indexDir = _TestUtil.getTempDir(dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    \n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", conf.getSimilarityProvider().get(\"content\").encodeNormValue(1.5f));\n    reader.close();\n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","sourceNew":"  public File createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    File indexDir = _TestUtil.getTempDir(dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    \n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", conf.getSimilarityProvider().get(\"content\").encodeNormValue(1.5f));\n    reader.close();\n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    File indexDir = _TestUtil.getTempDir(dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    \n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", conf.getSimilarityProvider().get(\"content\").encodeNormValue(1.5f));\n    reader.close();\n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","sourceNew":"  public File createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    File indexDir = _TestUtil.getTempDir(dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    \n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", conf.getSimilarityProvider().get(\"content\").encodeNormValue(1.5f));\n    reader.close();\n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    File indexDir = _TestUtil.getTempDir(dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    \n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", conf.getSimilarityProvider().get(\"content\").encodeNormValue(1.5f));\n    reader.close();\n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bc6ca4f1e2b53569948e006b93250a2a17d99f47","date":1304761967,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","sourceNew":"  public File createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(LuceneTestCase.TEMP_DIR, dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    \n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", conf.getSimilarityProvider().get(\"content\").encodeNormValue(1.5f));\n    reader.close();\n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    File indexDir = _TestUtil.getTempDir(dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    \n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", conf.getSimilarityProvider().get(\"content\").encodeNormValue(1.5f));\n    reader.close();\n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"08b152af2fb3154484c1ff50057759a26f7b826c","date":1304959586,"type":5,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean optimized) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(LuceneTestCase.TEMP_DIR, dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    \n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (optimized) {\n      writer.optimize();\n    }\n    writer.close();\n\n    if (!optimized) {\n      // open fresh writer so we get no prx file in the added segment\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(10);\n      ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      // Delete one doc so we get a .del file:\n      IndexReader reader = IndexReader.open(dir, false);\n      Term searchTerm = new Term(\"id\", \"7\");\n      int delCount = reader.deleteDocuments(searchTerm);\n      assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n      // Set one norm so we get a .s0 file:\n      reader.setNorm(21, \"content\", conf.getSimilarityProvider().get(\"content\").encodeNormValue(1.5f));\n      reader.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(LuceneTestCase.TEMP_DIR, dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    \n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", conf.getSimilarityProvider().get(\"content\").encodeNormValue(1.5f));\n    reader.close();\n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":5,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(String,boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","sourceNew":"  public File createIndex(String dirName, boolean doCFS, boolean optimized) throws IOException {\n    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:\n    File indexDir = new File(LuceneTestCase.TEMP_DIR, dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    \n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    if (optimized) {\n      writer.optimize();\n    }\n    writer.close();\n\n    if (!optimized) {\n      // open fresh writer so we get no prx file in the added segment\n      conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(10);\n      ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n      writer = new IndexWriter(dir, conf);\n      addNoProxDoc(writer);\n      writer.close();\n\n      // Delete one doc so we get a .del file:\n      IndexReader reader = IndexReader.open(dir, false);\n      Term searchTerm = new Term(\"id\", \"7\");\n      int delCount = reader.deleteDocuments(searchTerm);\n      assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n      // Set one norm so we get a .s0 file:\n      reader.setNorm(21, \"content\", conf.getSimilarityProvider().get(\"content\").encodeNormValue(1.5f));\n      reader.close();\n    }\n    \n    dir.close();\n    \n    return indexDir;\n  }\n\n","sourceOld":"  public File createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    File indexDir = _TestUtil.getTempDir(dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    \n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", conf.getSimilarityProvider().get(\"content\").encodeNormValue(1.5f));\n    reader.close();\n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":4,"author":"Steven Rowe","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#createIndex(Random,String,boolean).mjava","sourceNew":null,"sourceOld":"  public File createIndex(Random random, String dirName, boolean doCFS) throws IOException {\n\n    File indexDir = _TestUtil.getTempDir(dirName);\n    _TestUtil.rmDir(indexDir);\n    Directory dir = newFSDirectory(indexDir);\n    \n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    \n    for(int i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    assertEquals(\"wrong doc count\", 35, writer.maxDoc());\n    writer.close();\n\n    // open fresh writer so we get no prx file in the added segment\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(10);\n    ((LogMergePolicy) conf.getMergePolicy()).setUseCompoundFile(doCFS);\n    writer = new IndexWriter(dir, conf);\n    addNoProxDoc(writer);\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", conf.getSimilarityProvider().get(\"content\").encodeNormValue(1.5f));\n    reader.close();\n    dir.close();\n    \n    return indexDir;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7a54e23e03b47f3d568ab3020bdd386e4b2f0a05":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"a13a126d15299d5c1e117ea99ddae6fb0fa3f209":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"3cdad2c6b6234338031bcc1f24c001a5ad66f714":["fd9325c7ff9928fabe81c28553b41fc7aa57dfab"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["132903c28af3aa6f67284b78de91c0f0a99488c2","4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"962d04139994fce5193143ef35615499a9a96d78":["bde51b089eb7f86171eb3406e38a274743f9b7ac","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"08b152af2fb3154484c1ff50057759a26f7b826c":["bc6ca4f1e2b53569948e006b93250a2a17d99f47"],"bb9b72f7c3d7827c64dd4ec580ded81778da361d":["868da859b43505d9d2a023bfeae6dd0c795f5295","fd9325c7ff9928fabe81c28553b41fc7aa57dfab"],"fd9325c7ff9928fabe81c28553b41fc7aa57dfab":["7a54e23e03b47f3d568ab3020bdd386e4b2f0a05"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["3cdad2c6b6234338031bcc1f24c001a5ad66f714"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["b21422ff1d1d56499dec481f193b402e5e8def5b"],"a3776dccca01c11e7046323cfad46a3b4a471233":["3cdad2c6b6234338031bcc1f24c001a5ad66f714","08b152af2fb3154484c1ff50057759a26f7b826c"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","3cdad2c6b6234338031bcc1f24c001a5ad66f714"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["135621f3a0670a9394eb563224a3b76cc4dddc0f","08b152af2fb3154484c1ff50057759a26f7b826c"],"bc6ca4f1e2b53569948e006b93250a2a17d99f47":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["a13a126d15299d5c1e117ea99ddae6fb0fa3f209"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","7a54e23e03b47f3d568ab3020bdd386e4b2f0a05"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["bb9b72f7c3d7827c64dd4ec580ded81778da361d","3cdad2c6b6234338031bcc1f24c001a5ad66f714"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["08b152af2fb3154484c1ff50057759a26f7b826c"]},"commit2Childs":{"7a54e23e03b47f3d568ab3020bdd386e4b2f0a05":["fd9325c7ff9928fabe81c28553b41fc7aa57dfab","868da859b43505d9d2a023bfeae6dd0c795f5295"],"a13a126d15299d5c1e117ea99ddae6fb0fa3f209":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"3cdad2c6b6234338031bcc1f24c001a5ad66f714":["f2c5f0cb44df114db4228c8f77861714b5cabaea","a3776dccca01c11e7046323cfad46a3b4a471233","29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["a13a126d15299d5c1e117ea99ddae6fb0fa3f209","ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"962d04139994fce5193143ef35615499a9a96d78":[],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"08b152af2fb3154484c1ff50057759a26f7b826c":["a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"bb9b72f7c3d7827c64dd4ec580ded81778da361d":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"fd9325c7ff9928fabe81c28553b41fc7aa57dfab":["3cdad2c6b6234338031bcc1f24c001a5ad66f714","bb9b72f7c3d7827c64dd4ec580ded81778da361d"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","bc6ca4f1e2b53569948e006b93250a2a17d99f47"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","b21422ff1d1d56499dec481f193b402e5e8def5b"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":[],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["7a54e23e03b47f3d568ab3020bdd386e4b2f0a05","ab5cb6a74aefb78aa0569857970b9151dfe2e787","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"bc6ca4f1e2b53569948e006b93250a2a17d99f47":["08b152af2fb3154484c1ff50057759a26f7b826c"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["bb9b72f7c3d7827c64dd4ec580ded81778da361d"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["962d04139994fce5193143ef35615499a9a96d78"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["962d04139994fce5193143ef35615499a9a96d78","a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}