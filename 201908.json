{"path":"lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","commits":[{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[Document],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n      if (INFO_VERBOSE) {\n        message(Thread.currentThread().getName() + \" init seg=\" + segment + \" delQueue=\" + deleteQueue);  \n      }\n    }\n    if (INFO_VERBOSE) {\n      message(Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segment);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n              fieldInfos.revertUncommitted();\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<Document> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n      if (INFO_VERBOSE) {\n        message(Thread.currentThread().getName() + \" init seg=\" + segment + \" delQueue=\" + deleteQueue);  \n      }\n    }\n    if (INFO_VERBOSE) {\n      message(Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segment);\n    }\n    int docCount = 0;\n    try {\n      for(Document doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n              fieldInfos.revertUncommitted();\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n      if (INFO_VERBOSE && infoStream != null) {\n        infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" init seg=\" + segment + \" delQueue=\" + deleteQueue);  \n      }\n    }\n    if (INFO_VERBOSE && infoStream != null) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segment);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n              fieldInfos.revertUncommitted();\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n      if (INFO_VERBOSE) {\n        message(Thread.currentThread().getName() + \" init seg=\" + segment + \" delQueue=\" + deleteQueue);  \n      }\n    }\n    if (INFO_VERBOSE) {\n      message(Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segment);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n              fieldInfos.revertUncommitted();\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"58c6bbc222f074c844e736e6fb23647e3db9cfe3","date":1322743940,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n      if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n        infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" init seg=\" + segment + \" delQueue=\" + deleteQueue);  \n      }\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segment);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n              fieldInfos.revertUncommitted();\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n      if (INFO_VERBOSE && infoStream != null) {\n        infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" init seg=\" + segment + \" delQueue=\" + deleteQueue);  \n      }\n    }\n    if (INFO_VERBOSE && infoStream != null) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segment);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n              fieldInfos.revertUncommitted();\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ecd41ba3cdd1b4d825aa53d1987e4360cf45cb05","date":1323284809,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n      if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n        infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" init seg=\" + segment + \" delQueue=\" + deleteQueue);  \n      }\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segment);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n      if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n        infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" init seg=\" + segment + \" delQueue=\" + deleteQueue);  \n      }\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segment);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n              fieldInfos.revertUncommitted();\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d638301ad1cfcae567b681b893bc8781f0ee48a5","date":1323801546,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n      if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n        infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" init seg=\" + segment + \" delQueue=\" + deleteQueue);  \n      }\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segment);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n      if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n        infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" init seg=\" + segment + \" delQueue=\" + deleteQueue);  \n      }\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segment);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n              fieldInfos.revertUncommitted();\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread#updateDocuments(Iterable[#-extends-Iterable[#-extends-IndexableField]],Analyzer,Term).mjava","sourceNew":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n      if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n        infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" init seg=\" + segment + \" delQueue=\" + deleteQueue);  \n      }\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segment);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","sourceOld":"  public int updateDocuments(Iterable<? extends Iterable<? extends IndexableField>> docs, Analyzer analyzer, Term delTerm) throws IOException {\n    assert writer.testPoint(\"DocumentsWriterPerThread addDocuments start\");\n    assert deleteQueue != null;\n    docState.analyzer = analyzer;\n    if (segment == null) {\n      // this call is synchronized on IndexWriter.segmentInfos\n      segment = writer.newSegmentName();\n      assert numDocsInRAM == 0;\n      if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n        infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" init seg=\" + segment + \" delQueue=\" + deleteQueue);  \n      }\n    }\n    if (INFO_VERBOSE && infoStream.isEnabled(\"DWPT\")) {\n      infoStream.message(\"DWPT\", Thread.currentThread().getName() + \" update delTerm=\" + delTerm + \" docID=\" + docState.docID + \" seg=\" + segment);\n    }\n    int docCount = 0;\n    try {\n      for(Iterable<? extends IndexableField> doc : docs) {\n        docState.doc = doc;\n        docState.docID = numDocsInRAM;\n        docCount++;\n\n        boolean success = false;\n        try {\n          consumer.processDocument(fieldInfos);\n          success = true;\n        } finally {\n          if (!success) {\n            // An exc is being thrown...\n\n            if (!aborting) {\n              // One of the documents hit a non-aborting\n              // exception (eg something happened during\n              // analysis).  We now go and mark any docs\n              // from this batch that we had already indexed\n              // as deleted:\n              int docID = docState.docID;\n              final int endDocID = docID - docCount;\n              while (docID > endDocID) {\n                deleteDocID(docID);\n                docID--;\n              }\n\n              // Incr here because finishDocument will not\n              // be called (because an exc is being thrown):\n              numDocsInRAM++;\n            } else {\n              abort();\n            }\n          }\n        }\n        success = false;\n        try {\n          consumer.finishDocument();\n          success = true;\n        } finally {\n          if (!success) {\n            abort();\n          }\n        }\n\n        finishDocument(null);\n      }\n\n      // Apply delTerm only after all indexing has\n      // succeeded, but apply it only to docs prior to when\n      // this batch started:\n      if (delTerm != null) {\n        deleteQueue.add(delTerm, deleteSlice);\n        assert deleteSlice.isTailItem(delTerm) : \"expected the delete term as the tail item\";\n        deleteSlice.apply(pendingDeletes, numDocsInRAM-docCount);\n      }\n\n    } finally {\n      docState.clear();\n    }\n\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"58c6bbc222f074c844e736e6fb23647e3db9cfe3":["06584e6e98d592b34e1329b384182f368d2025e8"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["d638301ad1cfcae567b681b893bc8781f0ee48a5"],"06584e6e98d592b34e1329b384182f368d2025e8":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"ecd41ba3cdd1b4d825aa53d1987e4360cf45cb05":["58c6bbc222f074c844e736e6fb23647e3db9cfe3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d638301ad1cfcae567b681b893bc8781f0ee48a5":["58c6bbc222f074c844e736e6fb23647e3db9cfe3","ecd41ba3cdd1b4d825aa53d1987e4360cf45cb05"]},"commit2Childs":{"58c6bbc222f074c844e736e6fb23647e3db9cfe3":["ecd41ba3cdd1b4d825aa53d1987e4360cf45cb05","d638301ad1cfcae567b681b893bc8781f0ee48a5"],"06584e6e98d592b34e1329b384182f368d2025e8":["58c6bbc222f074c844e736e6fb23647e3db9cfe3"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["06584e6e98d592b34e1329b384182f368d2025e8"],"ecd41ba3cdd1b4d825aa53d1987e4360cf45cb05":["d638301ad1cfcae567b681b893bc8781f0ee48a5"],"d638301ad1cfcae567b681b893bc8781f0ee48a5":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}