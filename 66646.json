{"path":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(DirectoryReader,int,Map[String,TopTermQueue],Set[String]).mjava","commits":[{"id":"96d207426bd26fa5c1014e26d21d87603aea68b7","date":1327944562,"type":1,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(DirectoryReader,int,Map[String,TopTermQueue],Set[String]).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(IndexReader,int,Map[String,TopTermQueue],Set[String]).mjava","sourceNew":"  public static SimpleOrderedMap<Object> getIndexInfo( DirectoryReader reader, int numTerms,\n                                                       Map<String, TopTermQueue> topTerms,\n                                                       Set<String> fieldList) throws IOException {\n    Directory dir = reader.directory();\n    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<Object>();\n\n    indexInfo.add(\"numDocs\", reader.numDocs());\n    indexInfo.add(\"maxDoc\", reader.maxDoc());\n    final CharsRef spare = new CharsRef();\n    if( numTerms > 0 ) {\n      Fields fields = MultiFields.getFields(reader);\n      long totalTerms = 0;\n      if (fields != null) {\n        FieldsEnum fieldsEnum = fields.iterator();\n        String field;\n        while ((field = fieldsEnum.next()) != null) {\n          Terms terms = fieldsEnum.terms();\n          if (terms == null) {\n            continue;\n          }\n          totalTerms += terms.getUniqueTermCount();\n\n          if (fieldList != null && !fieldList.contains(field)) {\n            continue;\n          }\n\n          TermsEnum termsEnum = terms.iterator(null);\n          BytesRef text;\n          int[] buckets = new int[HIST_ARRAY_SIZE];\n          TopTermQueue tiq = topTerms.get(field);\n          if (tiq == null) {\n            tiq = new TopTermQueue(numTerms + 1);   // Allocating slots for the top N terms to collect freqs.\n            topTerms.put(field, tiq);\n          }\n          while ((text = termsEnum.next()) != null) {\n            int freq = termsEnum.docFreq();  // This calculation seems odd, but it gives the same results as it used to.\n            int slot = 32 - Integer.numberOfLeadingZeros(Math.max(0, freq - 1));\n            buckets[slot] = buckets[slot] + 1;\n            if (freq > tiq.minFreq) {\n              UnicodeUtil.UTF8toUTF16(text, spare);\n              String t = spare.toString();\n              tiq.distinctTerms = new Long(fieldsEnum.terms().getUniqueTermCount()).intValue();\n\n              tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n              if (tiq.size() > numTerms) { // if tiq full\n                tiq.pop(); // remove lowest in tiq\n                tiq.minFreq  = tiq.getTopTermInfo().docFreq;\n              }\n            }\n          }\n          tiq.histogram.add(buckets);\n        }\n      }\n      //Clumsy, but I'm tired.\n      indexInfo.add(\"numTerms\", (new Long(totalTerms)).intValue());\n\n    }\n    indexInfo.add(\"version\", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?\n    indexInfo.add(\"segmentCount\", reader.getSequentialSubReaders().length);\n    indexInfo.add(\"current\", reader.isCurrent() );\n    indexInfo.add(\"hasDeletions\", reader.hasDeletions() );\n    indexInfo.add(\"directory\", dir );\n    String s = reader.getIndexCommit().getUserData().get(SolrIndexWriter.COMMIT_TIME_MSEC_KEY);\n    if (s != null) {\n      indexInfo.add(\"lastModified\", new Date(Long.parseLong(s)));\n    }\n    return indexInfo;\n  }\n  //////////////////////// SolrInfoMBeans methods //////////////////////\n\n","sourceOld":"  public static SimpleOrderedMap<Object> getIndexInfo( IndexReader reader, int numTerms,\n                                                       Map<String, TopTermQueue> topTerms,\n                                                       Set<String> fieldList) throws IOException {\n    Directory dir = reader.directory();\n    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<Object>();\n\n    indexInfo.add(\"numDocs\", reader.numDocs());\n    indexInfo.add(\"maxDoc\", reader.maxDoc());\n    final CharsRef spare = new CharsRef();\n    if( numTerms > 0 ) {\n      Fields fields = MultiFields.getFields(reader);\n      long totalTerms = 0;\n      if (fields != null) {\n        FieldsEnum fieldsEnum = fields.iterator();\n        String field;\n        while ((field = fieldsEnum.next()) != null) {\n          Terms terms = fieldsEnum.terms();\n          if (terms == null) {\n            continue;\n          }\n          totalTerms += terms.getUniqueTermCount();\n\n          if (fieldList != null && !fieldList.contains(field)) {\n            continue;\n          }\n\n          TermsEnum termsEnum = terms.iterator(null);\n          BytesRef text;\n          int[] buckets = new int[HIST_ARRAY_SIZE];\n          TopTermQueue tiq = topTerms.get(field);\n          if (tiq == null) {\n            tiq = new TopTermQueue(numTerms + 1);   // Allocating slots for the top N terms to collect freqs.\n            topTerms.put(field, tiq);\n          }\n          while ((text = termsEnum.next()) != null) {\n            int freq = termsEnum.docFreq();  // This calculation seems odd, but it gives the same results as it used to.\n            int slot = 32 - Integer.numberOfLeadingZeros(Math.max(0, freq - 1));\n            buckets[slot] = buckets[slot] + 1;\n            if (freq > tiq.minFreq) {\n              UnicodeUtil.UTF8toUTF16(text, spare);\n              String t = spare.toString();\n              tiq.distinctTerms = new Long(fieldsEnum.terms().getUniqueTermCount()).intValue();\n\n              tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n              if (tiq.size() > numTerms) { // if tiq full\n                tiq.pop(); // remove lowest in tiq\n                tiq.minFreq  = tiq.getTopTermInfo().docFreq;\n              }\n            }\n          }\n          tiq.histogram.add(buckets);\n        }\n      }\n      //Clumsy, but I'm tired.\n      indexInfo.add(\"numTerms\", (new Long(totalTerms)).intValue());\n\n    }\n    indexInfo.add(\"version\", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?\n    indexInfo.add(\"segmentCount\", reader.getSequentialSubReaders().length);\n    indexInfo.add(\"current\", reader.isCurrent() );\n    indexInfo.add(\"hasDeletions\", reader.hasDeletions() );\n    indexInfo.add(\"directory\", dir );\n    String s = reader.getIndexCommit().getUserData().get(SolrIndexWriter.COMMIT_TIME_MSEC_KEY);\n    if (s != null) {\n      indexInfo.add(\"lastModified\", new Date(Long.parseLong(s)));\n    }\n    return indexInfo;\n  }\n  //////////////////////// SolrInfoMBeans methods //////////////////////\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":1,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(DirectoryReader,int,Map[String,TopTermQueue],Set[String]).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(IndexReader,int,Map[String,TopTermQueue],Set[String]).mjava","sourceNew":"  public static SimpleOrderedMap<Object> getIndexInfo( DirectoryReader reader, int numTerms,\n                                                       Map<String, TopTermQueue> topTerms,\n                                                       Set<String> fieldList) throws IOException {\n    Directory dir = reader.directory();\n    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<Object>();\n\n    indexInfo.add(\"numDocs\", reader.numDocs());\n    indexInfo.add(\"maxDoc\", reader.maxDoc());\n    final CharsRef spare = new CharsRef();\n    if( numTerms > 0 ) {\n      Fields fields = MultiFields.getFields(reader);\n      long totalTerms = 0;\n      if (fields != null) {\n        FieldsEnum fieldsEnum = fields.iterator();\n        String field;\n        while ((field = fieldsEnum.next()) != null) {\n          Terms terms = fieldsEnum.terms();\n          if (terms == null) {\n            continue;\n          }\n          totalTerms += terms.getUniqueTermCount();\n\n          if (fieldList != null && !fieldList.contains(field)) {\n            continue;\n          }\n\n          TermsEnum termsEnum = terms.iterator(null);\n          BytesRef text;\n          int[] buckets = new int[HIST_ARRAY_SIZE];\n          TopTermQueue tiq = topTerms.get(field);\n          if (tiq == null) {\n            tiq = new TopTermQueue(numTerms + 1);   // Allocating slots for the top N terms to collect freqs.\n            topTerms.put(field, tiq);\n          }\n          while ((text = termsEnum.next()) != null) {\n            int freq = termsEnum.docFreq();  // This calculation seems odd, but it gives the same results as it used to.\n            int slot = 32 - Integer.numberOfLeadingZeros(Math.max(0, freq - 1));\n            buckets[slot] = buckets[slot] + 1;\n            if (freq > tiq.minFreq) {\n              UnicodeUtil.UTF8toUTF16(text, spare);\n              String t = spare.toString();\n              tiq.distinctTerms = new Long(fieldsEnum.terms().getUniqueTermCount()).intValue();\n\n              tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n              if (tiq.size() > numTerms) { // if tiq full\n                tiq.pop(); // remove lowest in tiq\n                tiq.minFreq  = tiq.getTopTermInfo().docFreq;\n              }\n            }\n          }\n          tiq.histogram.add(buckets);\n        }\n      }\n      //Clumsy, but I'm tired.\n      indexInfo.add(\"numTerms\", (new Long(totalTerms)).intValue());\n\n    }\n    indexInfo.add(\"version\", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?\n    indexInfo.add(\"segmentCount\", reader.getSequentialSubReaders().length);\n    indexInfo.add(\"current\", reader.isCurrent() );\n    indexInfo.add(\"hasDeletions\", reader.hasDeletions() );\n    indexInfo.add(\"directory\", dir );\n    String s = reader.getIndexCommit().getUserData().get(SolrIndexWriter.COMMIT_TIME_MSEC_KEY);\n    if (s != null) {\n      indexInfo.add(\"lastModified\", new Date(Long.parseLong(s)));\n    }\n    return indexInfo;\n  }\n  //////////////////////// SolrInfoMBeans methods //////////////////////\n\n","sourceOld":"  public static SimpleOrderedMap<Object> getIndexInfo( IndexReader reader, int numTerms,\n                                                       Map<String, TopTermQueue> topTerms,\n                                                       Set<String> fieldList) throws IOException {\n    Directory dir = reader.directory();\n    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<Object>();\n\n    indexInfo.add(\"numDocs\", reader.numDocs());\n    indexInfo.add(\"maxDoc\", reader.maxDoc());\n    final CharsRef spare = new CharsRef();\n    if( numTerms > 0 ) {\n      Fields fields = MultiFields.getFields(reader);\n      long totalTerms = 0;\n      if (fields != null) {\n        FieldsEnum fieldsEnum = fields.iterator();\n        String field;\n        while ((field = fieldsEnum.next()) != null) {\n          Terms terms = fieldsEnum.terms();\n          if (terms == null) {\n            continue;\n          }\n          totalTerms += terms.getUniqueTermCount();\n\n          if (fieldList != null && !fieldList.contains(field)) {\n            continue;\n          }\n\n          TermsEnum termsEnum = terms.iterator(null);\n          BytesRef text;\n          int[] buckets = new int[HIST_ARRAY_SIZE];\n          TopTermQueue tiq = topTerms.get(field);\n          if (tiq == null) {\n            tiq = new TopTermQueue(numTerms + 1);   // Allocating slots for the top N terms to collect freqs.\n            topTerms.put(field, tiq);\n          }\n          while ((text = termsEnum.next()) != null) {\n            int freq = termsEnum.docFreq();  // This calculation seems odd, but it gives the same results as it used to.\n            int slot = 32 - Integer.numberOfLeadingZeros(Math.max(0, freq - 1));\n            buckets[slot] = buckets[slot] + 1;\n            if (freq > tiq.minFreq) {\n              UnicodeUtil.UTF8toUTF16(text, spare);\n              String t = spare.toString();\n              tiq.distinctTerms = new Long(fieldsEnum.terms().getUniqueTermCount()).intValue();\n\n              tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n              if (tiq.size() > numTerms) { // if tiq full\n                tiq.pop(); // remove lowest in tiq\n                tiq.minFreq  = tiq.getTopTermInfo().docFreq;\n              }\n            }\n          }\n          tiq.histogram.add(buckets);\n        }\n      }\n      //Clumsy, but I'm tired.\n      indexInfo.add(\"numTerms\", (new Long(totalTerms)).intValue());\n\n    }\n    indexInfo.add(\"version\", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?\n    indexInfo.add(\"segmentCount\", reader.getSequentialSubReaders().length);\n    indexInfo.add(\"current\", reader.isCurrent() );\n    indexInfo.add(\"hasDeletions\", reader.hasDeletions() );\n    indexInfo.add(\"directory\", dir );\n    String s = reader.getIndexCommit().getUserData().get(SolrIndexWriter.COMMIT_TIME_MSEC_KEY);\n    if (s != null) {\n      indexInfo.add(\"lastModified\", new Date(Long.parseLong(s)));\n    }\n    return indexInfo;\n  }\n  //////////////////////// SolrInfoMBeans methods //////////////////////\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6b05ecbfa2ad21f9ba1f9f694d6c5f5c19e22e4a","date":1328740250,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(DirectoryReader,int,Map[String,TopTermQueue],Set[String]).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(DirectoryReader,int,Map[String,TopTermQueue],Set[String]).mjava","sourceNew":"  public static SimpleOrderedMap<Object> getIndexInfo( DirectoryReader reader, int numTerms,\n                                                       Map<String, TopTermQueue> topTerms,\n                                                       Set<String> fieldList) throws IOException {\n    Directory dir = reader.directory();\n    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<Object>();\n\n    indexInfo.add(\"numDocs\", reader.numDocs());\n    indexInfo.add(\"maxDoc\", reader.maxDoc());\n    final CharsRef spare = new CharsRef();\n    if( numTerms > 0 ) {\n      Fields fields = MultiFields.getFields(reader);\n      long totalTerms = 0;\n      if (fields != null) {\n        FieldsEnum fieldsEnum = fields.iterator();\n        String field;\n        while ((field = fieldsEnum.next()) != null) {\n          Terms terms = fieldsEnum.terms();\n          if (terms == null) {\n            continue;\n          }\n          totalTerms += terms.getUniqueTermCount();\n\n          if (fieldList != null && ! fieldList.contains(field) && ! fieldList.contains(\"*\")) {\n            continue;\n          }\n\n          TermsEnum termsEnum = terms.iterator(null);\n          BytesRef text;\n          int[] buckets = new int[HIST_ARRAY_SIZE];\n          TopTermQueue tiq = topTerms.get(field);\n          if (tiq == null) {\n            tiq = new TopTermQueue(numTerms + 1);   // Allocating slots for the top N terms to collect freqs.\n            topTerms.put(field, tiq);\n          }\n          while ((text = termsEnum.next()) != null) {\n            int freq = termsEnum.docFreq();  // This calculation seems odd, but it gives the same results as it used to.\n            int slot = 32 - Integer.numberOfLeadingZeros(Math.max(0, freq - 1));\n            buckets[slot] = buckets[slot] + 1;\n            if (freq > tiq.minFreq) {\n              UnicodeUtil.UTF8toUTF16(text, spare);\n              String t = spare.toString();\n              tiq.distinctTerms = new Long(fieldsEnum.terms().getUniqueTermCount()).intValue();\n\n              tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n              if (tiq.size() > numTerms) { // if tiq full\n                tiq.pop(); // remove lowest in tiq\n                tiq.minFreq  = tiq.getTopTermInfo().docFreq;\n              }\n            }\n          }\n          tiq.histogram.add(buckets);\n        }\n      }\n      //Clumsy, but I'm tired.\n      indexInfo.add(\"numTerms\", (new Long(totalTerms)).intValue());\n\n    }\n    indexInfo.add(\"version\", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?\n    indexInfo.add(\"segmentCount\", reader.getSequentialSubReaders().length);\n    indexInfo.add(\"current\", reader.isCurrent() );\n    indexInfo.add(\"hasDeletions\", reader.hasDeletions() );\n    indexInfo.add(\"directory\", dir );\n    String s = reader.getIndexCommit().getUserData().get(SolrIndexWriter.COMMIT_TIME_MSEC_KEY);\n    if (s != null) {\n      indexInfo.add(\"lastModified\", new Date(Long.parseLong(s)));\n    }\n    return indexInfo;\n  }\n  //////////////////////// SolrInfoMBeans methods //////////////////////\n\n","sourceOld":"  public static SimpleOrderedMap<Object> getIndexInfo( DirectoryReader reader, int numTerms,\n                                                       Map<String, TopTermQueue> topTerms,\n                                                       Set<String> fieldList) throws IOException {\n    Directory dir = reader.directory();\n    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<Object>();\n\n    indexInfo.add(\"numDocs\", reader.numDocs());\n    indexInfo.add(\"maxDoc\", reader.maxDoc());\n    final CharsRef spare = new CharsRef();\n    if( numTerms > 0 ) {\n      Fields fields = MultiFields.getFields(reader);\n      long totalTerms = 0;\n      if (fields != null) {\n        FieldsEnum fieldsEnum = fields.iterator();\n        String field;\n        while ((field = fieldsEnum.next()) != null) {\n          Terms terms = fieldsEnum.terms();\n          if (terms == null) {\n            continue;\n          }\n          totalTerms += terms.getUniqueTermCount();\n\n          if (fieldList != null && !fieldList.contains(field)) {\n            continue;\n          }\n\n          TermsEnum termsEnum = terms.iterator(null);\n          BytesRef text;\n          int[] buckets = new int[HIST_ARRAY_SIZE];\n          TopTermQueue tiq = topTerms.get(field);\n          if (tiq == null) {\n            tiq = new TopTermQueue(numTerms + 1);   // Allocating slots for the top N terms to collect freqs.\n            topTerms.put(field, tiq);\n          }\n          while ((text = termsEnum.next()) != null) {\n            int freq = termsEnum.docFreq();  // This calculation seems odd, but it gives the same results as it used to.\n            int slot = 32 - Integer.numberOfLeadingZeros(Math.max(0, freq - 1));\n            buckets[slot] = buckets[slot] + 1;\n            if (freq > tiq.minFreq) {\n              UnicodeUtil.UTF8toUTF16(text, spare);\n              String t = spare.toString();\n              tiq.distinctTerms = new Long(fieldsEnum.terms().getUniqueTermCount()).intValue();\n\n              tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n              if (tiq.size() > numTerms) { // if tiq full\n                tiq.pop(); // remove lowest in tiq\n                tiq.minFreq  = tiq.getTopTermInfo().docFreq;\n              }\n            }\n          }\n          tiq.histogram.add(buckets);\n        }\n      }\n      //Clumsy, but I'm tired.\n      indexInfo.add(\"numTerms\", (new Long(totalTerms)).intValue());\n\n    }\n    indexInfo.add(\"version\", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?\n    indexInfo.add(\"segmentCount\", reader.getSequentialSubReaders().length);\n    indexInfo.add(\"current\", reader.isCurrent() );\n    indexInfo.add(\"hasDeletions\", reader.hasDeletions() );\n    indexInfo.add(\"directory\", dir );\n    String s = reader.getIndexCommit().getUserData().get(SolrIndexWriter.COMMIT_TIME_MSEC_KEY);\n    if (s != null) {\n      indexInfo.add(\"lastModified\", new Date(Long.parseLong(s)));\n    }\n    return indexInfo;\n  }\n  //////////////////////// SolrInfoMBeans methods //////////////////////\n\n","bugFix":["eb037ddbc4ef8b427189b9ca13486ea830d0c766"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fc400bb5f8ac2f55f63b8ff9da8747399e29a07c","date":1328928945,"type":3,"author":"Ryan McKinley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(DirectoryReader,int,Map[String,TopTermQueue],Set[String]).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(DirectoryReader,int,Map[String,TopTermQueue],Set[String]).mjava","sourceNew":"  public static SimpleOrderedMap<Object> getIndexInfo( DirectoryReader reader, int numTerms,\n                                                       Map<String, TopTermQueue> topTerms,\n                                                       Set<String> fieldList) throws IOException {\n    Directory dir = reader.directory();\n    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<Object>();\n\n    indexInfo.add(\"numDocs\", reader.numDocs());\n    indexInfo.add(\"maxDoc\", reader.maxDoc());\n    final CharsRef spare = new CharsRef();\n    if( numTerms > 0 ) {\n      Fields fields = MultiFields.getFields(reader);\n      long totalTerms = 0;\n      if (fields != null) {\n        FieldsEnum fieldsEnum = fields.iterator();\n        String field;\n        while ((field = fieldsEnum.next()) != null) {\n          Terms terms = fieldsEnum.terms();\n          if (terms == null) {\n            continue;\n          }\n          totalTerms += terms.getUniqueTermCount();\n\n          if (fieldList != null && ! fieldList.contains(field) && ! fieldList.contains(\"*\")) {\n            continue;\n          }\n\n          TermsEnum termsEnum = terms.iterator(null);\n          BytesRef text;\n          int[] buckets = new int[HIST_ARRAY_SIZE];\n          TopTermQueue tiq = topTerms.get(field);\n          if (tiq == null) {\n            tiq = new TopTermQueue(numTerms + 1);   // Allocating slots for the top N terms to collect freqs.\n            topTerms.put(field, tiq);\n          }\n          while ((text = termsEnum.next()) != null) {\n            int freq = termsEnum.docFreq();  // This calculation seems odd, but it gives the same results as it used to.\n            int slot = 32 - Integer.numberOfLeadingZeros(Math.max(0, freq - 1));\n            buckets[slot] = buckets[slot] + 1;\n            if (freq > tiq.minFreq) {\n              UnicodeUtil.UTF8toUTF16(text, spare);\n              String t = spare.toString();\n              tiq.distinctTerms = new Long(fieldsEnum.terms().getUniqueTermCount()).intValue();\n\n              tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n              if (tiq.size() > numTerms) { // if tiq full\n                tiq.pop(); // remove lowest in tiq\n                tiq.minFreq  = tiq.getTopTermInfo().docFreq;\n              }\n            }\n          }\n          tiq.histogram.add(buckets);\n        }\n      }\n      //Clumsy, but I'm tired.\n      indexInfo.add(\"numTerms\", (new Long(totalTerms)).intValue());\n\n    }\n        \n    indexInfo.add(\"version\", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?\n    indexInfo.add(\"segmentCount\", reader.getSequentialSubReaders().length);\n    indexInfo.add(\"current\", reader.isCurrent() );\n    indexInfo.add(\"hasDeletions\", reader.hasDeletions() );\n    indexInfo.add(\"directory\", dir );\n    indexInfo.add(\"userData\", reader.getIndexCommit().getUserData());\n    String s = reader.getIndexCommit().getUserData().get(SolrIndexWriter.COMMIT_TIME_MSEC_KEY);\n    if (s != null) {\n      indexInfo.add(\"lastModified\", new Date(Long.parseLong(s)));\n    }\n    return indexInfo;\n  }\n  //////////////////////// SolrInfoMBeans methods //////////////////////\n\n","sourceOld":"  public static SimpleOrderedMap<Object> getIndexInfo( DirectoryReader reader, int numTerms,\n                                                       Map<String, TopTermQueue> topTerms,\n                                                       Set<String> fieldList) throws IOException {\n    Directory dir = reader.directory();\n    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<Object>();\n\n    indexInfo.add(\"numDocs\", reader.numDocs());\n    indexInfo.add(\"maxDoc\", reader.maxDoc());\n    final CharsRef spare = new CharsRef();\n    if( numTerms > 0 ) {\n      Fields fields = MultiFields.getFields(reader);\n      long totalTerms = 0;\n      if (fields != null) {\n        FieldsEnum fieldsEnum = fields.iterator();\n        String field;\n        while ((field = fieldsEnum.next()) != null) {\n          Terms terms = fieldsEnum.terms();\n          if (terms == null) {\n            continue;\n          }\n          totalTerms += terms.getUniqueTermCount();\n\n          if (fieldList != null && ! fieldList.contains(field) && ! fieldList.contains(\"*\")) {\n            continue;\n          }\n\n          TermsEnum termsEnum = terms.iterator(null);\n          BytesRef text;\n          int[] buckets = new int[HIST_ARRAY_SIZE];\n          TopTermQueue tiq = topTerms.get(field);\n          if (tiq == null) {\n            tiq = new TopTermQueue(numTerms + 1);   // Allocating slots for the top N terms to collect freqs.\n            topTerms.put(field, tiq);\n          }\n          while ((text = termsEnum.next()) != null) {\n            int freq = termsEnum.docFreq();  // This calculation seems odd, but it gives the same results as it used to.\n            int slot = 32 - Integer.numberOfLeadingZeros(Math.max(0, freq - 1));\n            buckets[slot] = buckets[slot] + 1;\n            if (freq > tiq.minFreq) {\n              UnicodeUtil.UTF8toUTF16(text, spare);\n              String t = spare.toString();\n              tiq.distinctTerms = new Long(fieldsEnum.terms().getUniqueTermCount()).intValue();\n\n              tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n              if (tiq.size() > numTerms) { // if tiq full\n                tiq.pop(); // remove lowest in tiq\n                tiq.minFreq  = tiq.getTopTermInfo().docFreq;\n              }\n            }\n          }\n          tiq.histogram.add(buckets);\n        }\n      }\n      //Clumsy, but I'm tired.\n      indexInfo.add(\"numTerms\", (new Long(totalTerms)).intValue());\n\n    }\n    indexInfo.add(\"version\", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?\n    indexInfo.add(\"segmentCount\", reader.getSequentialSubReaders().length);\n    indexInfo.add(\"current\", reader.isCurrent() );\n    indexInfo.add(\"hasDeletions\", reader.hasDeletions() );\n    indexInfo.add(\"directory\", dir );\n    String s = reader.getIndexCommit().getUserData().get(SolrIndexWriter.COMMIT_TIME_MSEC_KEY);\n    if (s != null) {\n      indexInfo.add(\"lastModified\", new Date(Long.parseLong(s)));\n    }\n    return indexInfo;\n  }\n  //////////////////////// SolrInfoMBeans methods //////////////////////\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b6912d3e0a9ef2865124c6822bc9e4cfd3581c6c","date":1329188942,"type":4,"author":"Erick Erickson","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/handler/admin/LukeRequestHandler#getIndexInfo(DirectoryReader,int,Map[String,TopTermQueue],Set[String]).mjava","sourceNew":null,"sourceOld":"  public static SimpleOrderedMap<Object> getIndexInfo( DirectoryReader reader, int numTerms,\n                                                       Map<String, TopTermQueue> topTerms,\n                                                       Set<String> fieldList) throws IOException {\n    Directory dir = reader.directory();\n    SimpleOrderedMap<Object> indexInfo = new SimpleOrderedMap<Object>();\n\n    indexInfo.add(\"numDocs\", reader.numDocs());\n    indexInfo.add(\"maxDoc\", reader.maxDoc());\n    final CharsRef spare = new CharsRef();\n    if( numTerms > 0 ) {\n      Fields fields = MultiFields.getFields(reader);\n      long totalTerms = 0;\n      if (fields != null) {\n        FieldsEnum fieldsEnum = fields.iterator();\n        String field;\n        while ((field = fieldsEnum.next()) != null) {\n          Terms terms = fieldsEnum.terms();\n          if (terms == null) {\n            continue;\n          }\n          totalTerms += terms.getUniqueTermCount();\n\n          if (fieldList != null && ! fieldList.contains(field) && ! fieldList.contains(\"*\")) {\n            continue;\n          }\n\n          TermsEnum termsEnum = terms.iterator(null);\n          BytesRef text;\n          int[] buckets = new int[HIST_ARRAY_SIZE];\n          TopTermQueue tiq = topTerms.get(field);\n          if (tiq == null) {\n            tiq = new TopTermQueue(numTerms + 1);   // Allocating slots for the top N terms to collect freqs.\n            topTerms.put(field, tiq);\n          }\n          while ((text = termsEnum.next()) != null) {\n            int freq = termsEnum.docFreq();  // This calculation seems odd, but it gives the same results as it used to.\n            int slot = 32 - Integer.numberOfLeadingZeros(Math.max(0, freq - 1));\n            buckets[slot] = buckets[slot] + 1;\n            if (freq > tiq.minFreq) {\n              UnicodeUtil.UTF8toUTF16(text, spare);\n              String t = spare.toString();\n              tiq.distinctTerms = new Long(fieldsEnum.terms().getUniqueTermCount()).intValue();\n\n              tiq.add(new TopTermQueue.TermInfo(new Term(field, t), termsEnum.docFreq()));\n              if (tiq.size() > numTerms) { // if tiq full\n                tiq.pop(); // remove lowest in tiq\n                tiq.minFreq  = tiq.getTopTermInfo().docFreq;\n              }\n            }\n          }\n          tiq.histogram.add(buckets);\n        }\n      }\n      //Clumsy, but I'm tired.\n      indexInfo.add(\"numTerms\", (new Long(totalTerms)).intValue());\n\n    }\n        \n    indexInfo.add(\"version\", reader.getVersion());  // TODO? Is this different then: IndexReader.getCurrentVersion( dir )?\n    indexInfo.add(\"segmentCount\", reader.getSequentialSubReaders().length);\n    indexInfo.add(\"current\", reader.isCurrent() );\n    indexInfo.add(\"hasDeletions\", reader.hasDeletions() );\n    indexInfo.add(\"directory\", dir );\n    indexInfo.add(\"userData\", reader.getIndexCommit().getUserData());\n    String s = reader.getIndexCommit().getUserData().get(SolrIndexWriter.COMMIT_TIME_MSEC_KEY);\n    if (s != null) {\n      indexInfo.add(\"lastModified\", new Date(Long.parseLong(s)));\n    }\n    return indexInfo;\n  }\n  //////////////////////// SolrInfoMBeans methods //////////////////////\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"6b05ecbfa2ad21f9ba1f9f694d6c5f5c19e22e4a":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"96d207426bd26fa5c1014e26d21d87603aea68b7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"b6912d3e0a9ef2865124c6822bc9e4cfd3581c6c":["fc400bb5f8ac2f55f63b8ff9da8747399e29a07c"],"fc400bb5f8ac2f55f63b8ff9da8747399e29a07c":["6b05ecbfa2ad21f9ba1f9f694d6c5f5c19e22e4a"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","96d207426bd26fa5c1014e26d21d87603aea68b7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b6912d3e0a9ef2865124c6822bc9e4cfd3581c6c"]},"commit2Childs":{"6b05ecbfa2ad21f9ba1f9f694d6c5f5c19e22e4a":["fc400bb5f8ac2f55f63b8ff9da8747399e29a07c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["96d207426bd26fa5c1014e26d21d87603aea68b7","5cab9a86bd67202d20b6adc463008c8e982b070a"],"96d207426bd26fa5c1014e26d21d87603aea68b7":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"b6912d3e0a9ef2865124c6822bc9e4cfd3581c6c":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["6b05ecbfa2ad21f9ba1f9f694d6c5f5c19e22e4a"],"fc400bb5f8ac2f55f63b8ff9da8747399e29a07c":["b6912d3e0a9ef2865124c6822bc9e4cfd3581c6c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}