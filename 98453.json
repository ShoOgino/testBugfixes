{"path":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","commits":[{"id":"33ba398fa7984fdcb45fd76b87504d5adf7ca5e3","date":1373907993,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","pathOld":"/dev/null","sourceNew":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) {\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    try {\n      //long t0 = System.currentTimeMillis();\n      TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()));\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      BooleanQuery query = new BooleanQuery();\n      int maxEndOffset = -1;\n      final Set<String> matchedTokens = new HashSet<String>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      String prefixToken = null;\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n      ts.close();\n\n      // TODO: we could allow blended sort here, combining\n      // weight w/ score.  Now we ignore score and sort only\n      // by weight:\n\n      //System.out.println(\"INFIX query=\" + query);\n\n      Query finalQuery = finishQuery(query, allTermsRequired);\n\n      // We sorted postings by weight during indexing, so we\n      // only retrieve the first num hits now:\n      FirstNDocsCollector c = new FirstNDocsCollector(num);\n      try {\n        searcher.search(finalQuery, c);\n      } catch (FirstNDocsCollector.DoneException done) {\n      }\n      TopDocs hits = c.getHits();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, new Sort(new SortField(\"weight\", SortField.Type.LONG, true)));\n\n      List<LookupResult> results = new ArrayList<LookupResult>();\n      BytesRef scratch = new BytesRef();\n      for (int i=0;i<hits.scoreDocs.length;i++) {\n        ScoreDoc sd = hits.scoreDocs[i];\n        textDV.get(sd.doc, scratch);\n        String text = scratch.utf8ToString();\n        if (doHighlight) {\n          text = highlight(text, matchedTokens, prefixToken);\n        }\n        long score = weightsDV.get(sd.doc);\n\n        BytesRef payload;\n        if (payloadsDV != null) {\n          payload = new BytesRef();\n          payloadsDV.get(sd.doc, payload);\n        } else {\n          payload = null;\n        }\n\n        results.add(new LookupResult(text, score, payload));\n      }\n      //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n      //System.out.println(results);\n      return results;\n    } catch (IOException ioe) {\n      throw new RuntimeException(ioe);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["782ed6a4b4ba50ec19734fc8db4e570ee193d627","4637747f71df783fc2014ef1f1e0418466e3bed6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b3aad8246db872dc16fbe6109f893457496b0240","date":1373920172,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","sourceNew":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) {\n\n    if (searcher == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    try {\n      //long t0 = System.currentTimeMillis();\n      TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()));\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      BooleanQuery query = new BooleanQuery();\n      int maxEndOffset = -1;\n      final Set<String> matchedTokens = new HashSet<String>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      String prefixToken = null;\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n      ts.close();\n\n      // TODO: we could allow blended sort here, combining\n      // weight w/ score.  Now we ignore score and sort only\n      // by weight:\n\n      //System.out.println(\"INFIX query=\" + query);\n\n      Query finalQuery = finishQuery(query, allTermsRequired);\n\n      // We sorted postings by weight during indexing, so we\n      // only retrieve the first num hits now:\n      FirstNDocsCollector c = new FirstNDocsCollector(num);\n      try {\n        searcher.search(finalQuery, c);\n      } catch (FirstNDocsCollector.DoneException done) {\n      }\n      TopDocs hits = c.getHits();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, new Sort(new SortField(\"weight\", SortField.Type.LONG, true)));\n\n      List<LookupResult> results = new ArrayList<LookupResult>();\n      BytesRef scratch = new BytesRef();\n      for (int i=0;i<hits.scoreDocs.length;i++) {\n        ScoreDoc sd = hits.scoreDocs[i];\n        textDV.get(sd.doc, scratch);\n        String text = scratch.utf8ToString();\n        if (doHighlight) {\n          text = highlight(text, matchedTokens, prefixToken);\n        }\n        long score = weightsDV.get(sd.doc);\n\n        BytesRef payload;\n        if (payloadsDV != null) {\n          payload = new BytesRef();\n          payloadsDV.get(sd.doc, payload);\n        } else {\n          payload = null;\n        }\n\n        results.add(new LookupResult(text, score, payload));\n      }\n      //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n      //System.out.println(results);\n      return results;\n    } catch (IOException ioe) {\n      throw new RuntimeException(ioe);\n    }\n  }\n\n","sourceOld":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) {\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    try {\n      //long t0 = System.currentTimeMillis();\n      TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()));\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      BooleanQuery query = new BooleanQuery();\n      int maxEndOffset = -1;\n      final Set<String> matchedTokens = new HashSet<String>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      String prefixToken = null;\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n      ts.close();\n\n      // TODO: we could allow blended sort here, combining\n      // weight w/ score.  Now we ignore score and sort only\n      // by weight:\n\n      //System.out.println(\"INFIX query=\" + query);\n\n      Query finalQuery = finishQuery(query, allTermsRequired);\n\n      // We sorted postings by weight during indexing, so we\n      // only retrieve the first num hits now:\n      FirstNDocsCollector c = new FirstNDocsCollector(num);\n      try {\n        searcher.search(finalQuery, c);\n      } catch (FirstNDocsCollector.DoneException done) {\n      }\n      TopDocs hits = c.getHits();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, new Sort(new SortField(\"weight\", SortField.Type.LONG, true)));\n\n      List<LookupResult> results = new ArrayList<LookupResult>();\n      BytesRef scratch = new BytesRef();\n      for (int i=0;i<hits.scoreDocs.length;i++) {\n        ScoreDoc sd = hits.scoreDocs[i];\n        textDV.get(sd.doc, scratch);\n        String text = scratch.utf8ToString();\n        if (doHighlight) {\n          text = highlight(text, matchedTokens, prefixToken);\n        }\n        long score = weightsDV.get(sd.doc);\n\n        BytesRef payload;\n        if (payloadsDV != null) {\n          payload = new BytesRef();\n          payloadsDV.get(sd.doc, payload);\n        } else {\n          payload = null;\n        }\n\n        results.add(new LookupResult(text, score, payload));\n      }\n      //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n      //System.out.println(results);\n      return results;\n    } catch (IOException ioe) {\n      throw new RuntimeException(ioe);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":0,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","pathOld":"/dev/null","sourceNew":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) {\n\n    if (searcher == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    try {\n      //long t0 = System.currentTimeMillis();\n      TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()));\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      BooleanQuery query = new BooleanQuery();\n      int maxEndOffset = -1;\n      final Set<String> matchedTokens = new HashSet<String>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      String prefixToken = null;\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n      ts.close();\n\n      // TODO: we could allow blended sort here, combining\n      // weight w/ score.  Now we ignore score and sort only\n      // by weight:\n\n      //System.out.println(\"INFIX query=\" + query);\n\n      Query finalQuery = finishQuery(query, allTermsRequired);\n\n      // We sorted postings by weight during indexing, so we\n      // only retrieve the first num hits now:\n      FirstNDocsCollector c = new FirstNDocsCollector(num);\n      try {\n        searcher.search(finalQuery, c);\n      } catch (FirstNDocsCollector.DoneException done) {\n      }\n      TopDocs hits = c.getHits();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, new Sort(new SortField(\"weight\", SortField.Type.LONG, true)));\n\n      List<LookupResult> results = new ArrayList<LookupResult>();\n      BytesRef scratch = new BytesRef();\n      for (int i=0;i<hits.scoreDocs.length;i++) {\n        ScoreDoc sd = hits.scoreDocs[i];\n        textDV.get(sd.doc, scratch);\n        String text = scratch.utf8ToString();\n        if (doHighlight) {\n          text = highlight(text, matchedTokens, prefixToken);\n        }\n        long score = weightsDV.get(sd.doc);\n\n        BytesRef payload;\n        if (payloadsDV != null) {\n          payload = new BytesRef();\n          payloadsDV.get(sd.doc, payload);\n        } else {\n          payload = null;\n        }\n\n        results.add(new LookupResult(text, score, payload));\n      }\n      //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n      //System.out.println(results);\n      return results;\n    } catch (IOException ioe) {\n      throw new RuntimeException(ioe);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9dbf99ca10e1ef2ffbfeb7119d644bf20b267368","date":1379006067,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","sourceNew":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) {\n\n    if (searcher == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    try {\n      //long t0 = System.currentTimeMillis();\n      TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()));\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      BooleanQuery query = new BooleanQuery();\n      int maxEndOffset = -1;\n      final Set<String> matchedTokens = new HashSet<String>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      String prefixToken = null;\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n      ts.close();\n\n      // TODO: we could allow blended sort here, combining\n      // weight w/ score.  Now we ignore score and sort only\n      // by weight:\n\n      //System.out.println(\"INFIX query=\" + query);\n\n      Query finalQuery = finishQuery(query, allTermsRequired);\n\n      // We sorted postings by weight during indexing, so we\n      // only retrieve the first num hits now:\n      FirstNDocsCollector c = new FirstNDocsCollector(num);\n      try {\n        searcher.search(finalQuery, c);\n      } catch (FirstNDocsCollector.DoneException done) {\n      }\n      TopDocs hits = c.getHits();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, new Sort(new SortField(\"weight\", SortField.Type.LONG, true)));\n\n      List<LookupResult> results = new ArrayList<LookupResult>();\n      BytesRef scratch = new BytesRef();\n      for (int i=0;i<hits.scoreDocs.length;i++) {\n        ScoreDoc sd = hits.scoreDocs[i];\n        textDV.get(sd.doc, scratch);\n        String text = scratch.utf8ToString();\n        long score = weightsDV.get(sd.doc);\n\n        BytesRef payload;\n        if (payloadsDV != null) {\n          payload = new BytesRef();\n          payloadsDV.get(sd.doc, payload);\n        } else {\n          payload = null;\n        }\n\n        LookupResult result;\n\n        if (doHighlight) {\n          Object highlightKey = highlight(text, matchedTokens, prefixToken);\n          result = new LookupResult(highlightKey.toString(), highlightKey, score, payload);\n        } else {\n          result = new LookupResult(text, score, payload);\n        }\n        results.add(result);\n      }\n      //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n      //System.out.println(results);\n      return results;\n    } catch (IOException ioe) {\n      throw new RuntimeException(ioe);\n    }\n  }\n\n","sourceOld":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) {\n\n    if (searcher == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    try {\n      //long t0 = System.currentTimeMillis();\n      TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()));\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      BooleanQuery query = new BooleanQuery();\n      int maxEndOffset = -1;\n      final Set<String> matchedTokens = new HashSet<String>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      String prefixToken = null;\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n      ts.close();\n\n      // TODO: we could allow blended sort here, combining\n      // weight w/ score.  Now we ignore score and sort only\n      // by weight:\n\n      //System.out.println(\"INFIX query=\" + query);\n\n      Query finalQuery = finishQuery(query, allTermsRequired);\n\n      // We sorted postings by weight during indexing, so we\n      // only retrieve the first num hits now:\n      FirstNDocsCollector c = new FirstNDocsCollector(num);\n      try {\n        searcher.search(finalQuery, c);\n      } catch (FirstNDocsCollector.DoneException done) {\n      }\n      TopDocs hits = c.getHits();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, new Sort(new SortField(\"weight\", SortField.Type.LONG, true)));\n\n      List<LookupResult> results = new ArrayList<LookupResult>();\n      BytesRef scratch = new BytesRef();\n      for (int i=0;i<hits.scoreDocs.length;i++) {\n        ScoreDoc sd = hits.scoreDocs[i];\n        textDV.get(sd.doc, scratch);\n        String text = scratch.utf8ToString();\n        if (doHighlight) {\n          text = highlight(text, matchedTokens, prefixToken);\n        }\n        long score = weightsDV.get(sd.doc);\n\n        BytesRef payload;\n        if (payloadsDV != null) {\n          payload = new BytesRef();\n          payloadsDV.get(sd.doc, payload);\n        } else {\n          payload = null;\n        }\n\n        results.add(new LookupResult(text, score, payload));\n      }\n      //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n      //System.out.println(results);\n      return results;\n    } catch (IOException ioe) {\n      throw new RuntimeException(ioe);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"782ed6a4b4ba50ec19734fc8db4e570ee193d627","date":1381127065,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","sourceNew":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) {\n\n    if (searcher == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      BooleanQuery query = new BooleanQuery();\n      int maxEndOffset = -1;\n      final Set<String> matchedTokens = new HashSet<String>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      String prefixToken = null;\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n      ts.close();\n\n      // TODO: we could allow blended sort here, combining\n      // weight w/ score.  Now we ignore score and sort only\n      // by weight:\n\n      //System.out.println(\"INFIX query=\" + query);\n\n      Query finalQuery = finishQuery(query, allTermsRequired);\n\n      // We sorted postings by weight during indexing, so we\n      // only retrieve the first num hits now:\n      FirstNDocsCollector c = new FirstNDocsCollector(num);\n      try {\n        searcher.search(finalQuery, c);\n      } catch (FirstNDocsCollector.DoneException done) {\n      }\n      TopDocs hits = c.getHits();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, new Sort(new SortField(\"weight\", SortField.Type.LONG, true)));\n\n      List<LookupResult> results = new ArrayList<LookupResult>();\n      BytesRef scratch = new BytesRef();\n      for (int i=0;i<hits.scoreDocs.length;i++) {\n        ScoreDoc sd = hits.scoreDocs[i];\n        textDV.get(sd.doc, scratch);\n        String text = scratch.utf8ToString();\n        long score = weightsDV.get(sd.doc);\n\n        BytesRef payload;\n        if (payloadsDV != null) {\n          payload = new BytesRef();\n          payloadsDV.get(sd.doc, payload);\n        } else {\n          payload = null;\n        }\n\n        LookupResult result;\n\n        if (doHighlight) {\n          Object highlightKey = highlight(text, matchedTokens, prefixToken);\n          result = new LookupResult(highlightKey.toString(), highlightKey, score, payload);\n        } else {\n          result = new LookupResult(text, score, payload);\n        }\n        results.add(result);\n      }\n      //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n      //System.out.println(results);\n      return results;\n    } catch (IOException ioe) {\n      throw new RuntimeException(ioe);\n    }\n  }\n\n","sourceOld":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) {\n\n    if (searcher == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    try {\n      //long t0 = System.currentTimeMillis();\n      TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()));\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      BooleanQuery query = new BooleanQuery();\n      int maxEndOffset = -1;\n      final Set<String> matchedTokens = new HashSet<String>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      String prefixToken = null;\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n      ts.close();\n\n      // TODO: we could allow blended sort here, combining\n      // weight w/ score.  Now we ignore score and sort only\n      // by weight:\n\n      //System.out.println(\"INFIX query=\" + query);\n\n      Query finalQuery = finishQuery(query, allTermsRequired);\n\n      // We sorted postings by weight during indexing, so we\n      // only retrieve the first num hits now:\n      FirstNDocsCollector c = new FirstNDocsCollector(num);\n      try {\n        searcher.search(finalQuery, c);\n      } catch (FirstNDocsCollector.DoneException done) {\n      }\n      TopDocs hits = c.getHits();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, new Sort(new SortField(\"weight\", SortField.Type.LONG, true)));\n\n      List<LookupResult> results = new ArrayList<LookupResult>();\n      BytesRef scratch = new BytesRef();\n      for (int i=0;i<hits.scoreDocs.length;i++) {\n        ScoreDoc sd = hits.scoreDocs[i];\n        textDV.get(sd.doc, scratch);\n        String text = scratch.utf8ToString();\n        long score = weightsDV.get(sd.doc);\n\n        BytesRef payload;\n        if (payloadsDV != null) {\n          payload = new BytesRef();\n          payloadsDV.get(sd.doc, payload);\n        } else {\n          payload = null;\n        }\n\n        LookupResult result;\n\n        if (doHighlight) {\n          Object highlightKey = highlight(text, matchedTokens, prefixToken);\n          result = new LookupResult(highlightKey.toString(), highlightKey, score, payload);\n        } else {\n          result = new LookupResult(text, score, payload);\n        }\n        results.add(result);\n      }\n      //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n      //System.out.println(results);\n      return results;\n    } catch (IOException ioe) {\n      throw new RuntimeException(ioe);\n    }\n  }\n\n","bugFix":["33ba398fa7984fdcb45fd76b87504d5adf7ca5e3"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f13ec1b606a28789743a563929e7c556e8218297","date":1389302034,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","sourceNew":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) {\n\n    if (searcher == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      BooleanQuery query = new BooleanQuery();\n      int maxEndOffset = -1;\n      final Set<String> matchedTokens = new HashSet<String>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      String prefixToken = null;\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n      ts.close();\n\n      // TODO: we could allow blended sort here, combining\n      // weight w/ score.  Now we ignore score and sort only\n      // by weight:\n\n      //System.out.println(\"INFIX query=\" + query);\n\n      Query finalQuery = finishQuery(query, allTermsRequired);\n\n      // We sorted postings by weight during indexing, so we\n      // only retrieve the first num hits now:\n      FirstNDocsCollector c = new FirstNDocsCollector(num);\n      try {\n        searcher.search(finalQuery, c);\n      } catch (FirstNDocsCollector.DoneException done) {\n      }\n      TopDocs hits = c.getHits();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, new Sort(new SortField(\"weight\", SortField.Type.LONG, true)));\n\n      List<LookupResult> results = createResults(hits, num, key, doHighlight, matchedTokens, prefixToken);\n\n      //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n      //System.out.println(results);\n\n      return results;\n\n    } catch (IOException ioe) {\n      throw new RuntimeException(ioe);\n    }\n  }\n\n","sourceOld":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) {\n\n    if (searcher == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      BooleanQuery query = new BooleanQuery();\n      int maxEndOffset = -1;\n      final Set<String> matchedTokens = new HashSet<String>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      String prefixToken = null;\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n      ts.close();\n\n      // TODO: we could allow blended sort here, combining\n      // weight w/ score.  Now we ignore score and sort only\n      // by weight:\n\n      //System.out.println(\"INFIX query=\" + query);\n\n      Query finalQuery = finishQuery(query, allTermsRequired);\n\n      // We sorted postings by weight during indexing, so we\n      // only retrieve the first num hits now:\n      FirstNDocsCollector c = new FirstNDocsCollector(num);\n      try {\n        searcher.search(finalQuery, c);\n      } catch (FirstNDocsCollector.DoneException done) {\n      }\n      TopDocs hits = c.getHits();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, new Sort(new SortField(\"weight\", SortField.Type.LONG, true)));\n\n      List<LookupResult> results = new ArrayList<LookupResult>();\n      BytesRef scratch = new BytesRef();\n      for (int i=0;i<hits.scoreDocs.length;i++) {\n        ScoreDoc sd = hits.scoreDocs[i];\n        textDV.get(sd.doc, scratch);\n        String text = scratch.utf8ToString();\n        long score = weightsDV.get(sd.doc);\n\n        BytesRef payload;\n        if (payloadsDV != null) {\n          payload = new BytesRef();\n          payloadsDV.get(sd.doc, payload);\n        } else {\n          payload = null;\n        }\n\n        LookupResult result;\n\n        if (doHighlight) {\n          Object highlightKey = highlight(text, matchedTokens, prefixToken);\n          result = new LookupResult(highlightKey.toString(), highlightKey, score, payload);\n        } else {\n          result = new LookupResult(text, score, payload);\n        }\n        results.add(result);\n      }\n      //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n      //System.out.println(results);\n      return results;\n    } catch (IOException ioe) {\n      throw new RuntimeException(ioe);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a371aa649cc243e82cb8677ca960a1e0232ecedf","date":1393605574,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","sourceNew":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<String>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<String>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    //System.out.println(\"INFIX query=\" + query);\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(new Sort(new SortField(\"weight\", SortField.Type.LONG, true)),\n                                                   num, true, false, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    Collector c2 = new EarlyTerminatingSortingCollector(c, sorter, num);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = (TopFieldDocs) c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, new Sort(new SortField(\"weight\", SortField.Type.LONG, true)));\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","sourceOld":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) {\n\n    if (searcher == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      BooleanQuery query = new BooleanQuery();\n      int maxEndOffset = -1;\n      final Set<String> matchedTokens = new HashSet<String>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      String prefixToken = null;\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n      ts.close();\n\n      // TODO: we could allow blended sort here, combining\n      // weight w/ score.  Now we ignore score and sort only\n      // by weight:\n\n      //System.out.println(\"INFIX query=\" + query);\n\n      Query finalQuery = finishQuery(query, allTermsRequired);\n\n      // We sorted postings by weight during indexing, so we\n      // only retrieve the first num hits now:\n      FirstNDocsCollector c = new FirstNDocsCollector(num);\n      try {\n        searcher.search(finalQuery, c);\n      } catch (FirstNDocsCollector.DoneException done) {\n      }\n      TopDocs hits = c.getHits();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, new Sort(new SortField(\"weight\", SortField.Type.LONG, true)));\n\n      List<LookupResult> results = createResults(hits, num, key, doHighlight, matchedTokens, prefixToken);\n\n      //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n      //System.out.println(results);\n\n      return results;\n\n    } catch (IOException ioe) {\n      throw new RuntimeException(ioe);\n    }\n  }\n\n","bugFix":null,"bugIntro":["4637747f71df783fc2014ef1f1e0418466e3bed6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b638f37b6d00b06fa8d6875cea1df4b274d6e87a","date":1394120449,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","sourceNew":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<String>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<String>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    //System.out.println(\"INFIX query=\" + query);\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(SORT, num, true, false, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    Collector c2 = new EarlyTerminatingSortingCollector(c, SORT, num);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = (TopFieldDocs) c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, SORT);\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","sourceOld":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<String>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<String>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    //System.out.println(\"INFIX query=\" + query);\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(new Sort(new SortField(\"weight\", SortField.Type.LONG, true)),\n                                                   num, true, false, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    Collector c2 = new EarlyTerminatingSortingCollector(c, sorter, num);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = (TopFieldDocs) c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, new Sort(new SortField(\"weight\", SortField.Type.LONG, true)));\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4637747f71df783fc2014ef1f1e0418466e3bed6","date":1394196311,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","sourceNew":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<String>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<String>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    //System.out.println(\"INFIX query=\" + query);\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(SORT, num, true, false, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    Collector c2 = new EarlyTerminatingSortingCollector(c, SORT, num);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = (TopFieldDocs) c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, SORT);\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","sourceOld":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<String>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<String>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    //System.out.println(\"INFIX query=\" + query);\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(new Sort(new SortField(\"weight\", SortField.Type.LONG, true)),\n                                                   num, true, false, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    Collector c2 = new EarlyTerminatingSortingCollector(c, sorter, num);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = (TopFieldDocs) c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, new Sort(new SortField(\"weight\", SortField.Type.LONG, true)));\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","bugFix":["33ba398fa7984fdcb45fd76b87504d5adf7ca5e3","a371aa649cc243e82cb8677ca960a1e0232ecedf"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"96ea64d994d340044e0d57aeb6a5871539d10ca5","date":1394225445,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","sourceNew":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<String>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<String>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    //System.out.println(\"INFIX query=\" + query);\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(SORT, num, true, false, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    Collector c2 = new EarlyTerminatingSortingCollector(c, SORT, num);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = (TopFieldDocs) c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, SORT);\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","sourceOld":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<String>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<String>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    //System.out.println(\"INFIX query=\" + query);\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(new Sort(new SortField(\"weight\", SortField.Type.LONG, true)),\n                                                   num, true, false, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    Collector c2 = new EarlyTerminatingSortingCollector(c, sorter, num);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = (TopFieldDocs) c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, new Sort(new SortField(\"weight\", SortField.Type.LONG, true)));\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","sourceNew":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    //System.out.println(\"INFIX query=\" + query);\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(SORT, num, true, false, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    Collector c2 = new EarlyTerminatingSortingCollector(c, SORT, num);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = (TopFieldDocs) c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, SORT);\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","sourceOld":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<String>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<String>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    //System.out.println(\"INFIX query=\" + query);\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(SORT, num, true, false, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    Collector c2 = new EarlyTerminatingSortingCollector(c, SORT, num);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = (TopFieldDocs) c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, SORT);\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b","date":1395588343,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","sourceNew":"  /** Lookup, without any context. */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n    return lookup(key, null, num, allTermsRequired, doHighlight);\n  }\n\n","sourceOld":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    //System.out.println(\"INFIX query=\" + query);\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(SORT, num, true, false, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    Collector c2 = new EarlyTerminatingSortingCollector(c, SORT, num);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = (TopFieldDocs) c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, SORT);\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","bugFix":null,"bugIntro":["0977217be8980b17c612feb7ea3d1556ccf4aeed"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0977217be8980b17c612feb7ea3d1556ccf4aeed","date":1415742153,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","sourceNew":"  /** Lookup, without any context. */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n    return lookup(key, (Map<BytesRef, BooleanClause.Occur>)null, num, allTermsRequired, doHighlight);\n  }\n\n","sourceOld":"  /** Lookup, without any context. */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n    return lookup(key, null, num, allTermsRequired, doHighlight);\n  }\n\n","bugFix":["58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d2204dea8cf3dcfbffdf7b4d3459cf5287cc1c30","date":1431701935,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,int,boolean,boolean).mjava","sourceNew":"  /** Lookup, without any context. */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n    return lookup(key, (BooleanQuery)null, num, allTermsRequired, doHighlight);\n  }\n\n","sourceOld":"  /** Lookup, without any context. */\n  public List<LookupResult> lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n    return lookup(key, (Map<BytesRef, BooleanClause.Occur>)null, num, allTermsRequired, doHighlight);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["4637747f71df783fc2014ef1f1e0418466e3bed6"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":["a371aa649cc243e82cb8677ca960a1e0232ecedf","4637747f71df783fc2014ef1f1e0418466e3bed6"],"0977217be8980b17c612feb7ea3d1556ccf4aeed":["58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b"],"58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","b3aad8246db872dc16fbe6109f893457496b0240"],"f13ec1b606a28789743a563929e7c556e8218297":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"b3aad8246db872dc16fbe6109f893457496b0240":["33ba398fa7984fdcb45fd76b87504d5adf7ca5e3"],"9dbf99ca10e1ef2ffbfeb7119d644bf20b267368":["b3aad8246db872dc16fbe6109f893457496b0240"],"782ed6a4b4ba50ec19734fc8db4e570ee193d627":["9dbf99ca10e1ef2ffbfeb7119d644bf20b267368"],"a371aa649cc243e82cb8677ca960a1e0232ecedf":["f13ec1b606a28789743a563929e7c556e8218297"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b638f37b6d00b06fa8d6875cea1df4b274d6e87a":["a371aa649cc243e82cb8677ca960a1e0232ecedf"],"d2204dea8cf3dcfbffdf7b4d3459cf5287cc1c30":["0977217be8980b17c612feb7ea3d1556ccf4aeed"],"4637747f71df783fc2014ef1f1e0418466e3bed6":["a371aa649cc243e82cb8677ca960a1e0232ecedf","b638f37b6d00b06fa8d6875cea1df4b274d6e87a"],"33ba398fa7984fdcb45fd76b87504d5adf7ca5e3":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d2204dea8cf3dcfbffdf7b4d3459cf5287cc1c30"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":[],"0977217be8980b17c612feb7ea3d1556ccf4aeed":["d2204dea8cf3dcfbffdf7b4d3459cf5287cc1c30"],"58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b":["0977217be8980b17c612feb7ea3d1556ccf4aeed"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"b3aad8246db872dc16fbe6109f893457496b0240":["37a0f60745e53927c4c876cfe5b5a58170f0646c","9dbf99ca10e1ef2ffbfeb7119d644bf20b267368"],"f13ec1b606a28789743a563929e7c556e8218297":["a371aa649cc243e82cb8677ca960a1e0232ecedf"],"9dbf99ca10e1ef2ffbfeb7119d644bf20b267368":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"782ed6a4b4ba50ec19734fc8db4e570ee193d627":["f13ec1b606a28789743a563929e7c556e8218297"],"a371aa649cc243e82cb8677ca960a1e0232ecedf":["96ea64d994d340044e0d57aeb6a5871539d10ca5","b638f37b6d00b06fa8d6875cea1df4b274d6e87a","4637747f71df783fc2014ef1f1e0418466e3bed6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["37a0f60745e53927c4c876cfe5b5a58170f0646c","33ba398fa7984fdcb45fd76b87504d5adf7ca5e3"],"b638f37b6d00b06fa8d6875cea1df4b274d6e87a":["4637747f71df783fc2014ef1f1e0418466e3bed6"],"d2204dea8cf3dcfbffdf7b4d3459cf5287cc1c30":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4637747f71df783fc2014ef1f1e0418466e3bed6":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","96ea64d994d340044e0d57aeb6a5871539d10ca5"],"33ba398fa7984fdcb45fd76b87504d5adf7ca5e3":["b3aad8246db872dc16fbe6109f893457496b0240"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["96ea64d994d340044e0d57aeb6a5871539d10ca5","37a0f60745e53927c4c876cfe5b5a58170f0646c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}