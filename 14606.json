{"path":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDGF().mjava","commits":[{"id":"9354b2a0148657f4393d2a4acb438fde7f1d1dad","date":1490673235,"type":1,"author":"Steve Rowe","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDGF().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","sourceNew":"  @Test\n  public void testPositionHistoryWithWDGF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterGraphFilter\");\n    assertNotNull(\"Expcting WordDelimiterGraphFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","sourceOld":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"79073f40991d761d73bb67bb490d1f562da07e53","date":1490873944,"type":1,"author":"Steve Rowe","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDGF().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDF().mjava","sourceNew":"  @Test\n  public void testPositionHistoryWithWDGF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterGraphFilter\");\n    assertNotNull(\"Expcting WordDelimiterGraphFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","sourceOld":"  @Test\n  public void testPositionHistoryWithWDF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter\");\n    assertNotNull(\"Expcting WordDelimiterFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e98520789adb1d5ad05afb4956eca0944a929688","date":1592430701,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDGF().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testPositionHistoryWithWDGF().mjava","sourceNew":"  @Test\n  public void testPositionHistoryWithWDGF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    @SuppressWarnings({\"rawtypes\"})\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterGraphFilter\");\n    assertNotNull(\"Expcting WordDelimiterGraphFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","sourceOld":"  @Test\n  public void testPositionHistoryWithWDGF() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"skutype1\");\n    request.setFieldValue(\"hi, 3456-12 a Test\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"skutype1\");\n    assertNotNull(\"expecting result for field type 'skutype1'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'skutype1'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expcting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi,\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"3456-12\", null, \"word\", 4, 11, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"a\", null, \"word\", 12, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Test\", null, \"word\", 14, 18, 4, new int[]{4}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.miscellaneous.WordDelimiterGraphFilter\");\n    assertNotNull(\"Expcting WordDelimiterGraphFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Test\", null, \"word\", 14, 18, 5, new int[]{4,5}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"hi\", null, \"word\", 0, 2, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"345612\", null, \"word\", 4, 11, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"3456\", null, \"word\", 4, 8, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"12\", null, \"word\", 9, 11, 3, new int[]{2,3,3}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"a\", null, \"word\", 12, 13, 4, new int[]{3,4,4}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"test\", null, \"word\", 14, 18, 5, new int[]{4,5,5}, null, false));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"79073f40991d761d73bb67bb490d1f562da07e53":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"9354b2a0148657f4393d2a4acb438fde7f1d1dad":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e98520789adb1d5ad05afb4956eca0944a929688"],"e98520789adb1d5ad05afb4956eca0944a929688":["9354b2a0148657f4393d2a4acb438fde7f1d1dad"]},"commit2Childs":{"79073f40991d761d73bb67bb490d1f562da07e53":[],"9354b2a0148657f4393d2a4acb438fde7f1d1dad":["e98520789adb1d5ad05afb4956eca0944a929688"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["79073f40991d761d73bb67bb490d1f562da07e53","9354b2a0148657f4393d2a4acb438fde7f1d1dad"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"e98520789adb1d5ad05afb4956eca0944a929688":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["79073f40991d761d73bb67bb490d1f562da07e53","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}