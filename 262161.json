{"path":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","commits":[{"id":"0935c850ea562932997b72c69d93e345f21d7f45","date":1344711506,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","pathOld":"/dev/null","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\"), true));\n    DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertTrue(de.hasPayload());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552","date":1344797146,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\"), true));\n    DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\"), true));\n    DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertTrue(de.hasPayload());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c7869f64c874ebf7f317d22c00baf2b6857797a6","date":1344856617,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","pathOld":"/dev/null","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\"), true));\n    DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","date":1344867506,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","pathOld":"/dev/null","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\"), true));\n    DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8","date":1373996650,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\")));\n    DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\"), true));\n    DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\")));\n    DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\"), true));\n    DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\")));\n    DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(new StringReader(\"here we go\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(new StringReader(\"another\"), MockTokenizer.WHITESPACE, true);\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\")));\n    DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\")));\n    DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.shutdown();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\")));\n    DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"923f36bb0db6f793cf62dbb68723ae3bfbaf1d75","date":1399205975,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\")));\n    DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.shutdown();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    assertFalse(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\")));\n    DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.shutdown();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\")));\n    DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.shutdown();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\")));\n    DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.shutdown();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\")));\n    DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\")));\n    DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.shutdown();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\")));\n    PostingsEnum de = termsEnum.postings(null, null, PostingsEnum.FLAG_ALL);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\")));\n    DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e73063b92d958076ef4ae8beb5f493e8ccdcecb4","date":1424177215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\")));\n    PostingsEnum de = termsEnum.postings(null, null, PostingsEnum.ALL);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\")));\n    PostingsEnum de = termsEnum.postings(null, null, PostingsEnum.FLAG_ALL);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator();\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\")));\n    PostingsEnum de = termsEnum.postings(null, null, PostingsEnum.ALL);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator(null);\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\")));\n    PostingsEnum de = termsEnum.postings(null, null, PostingsEnum.ALL);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors#testMixupDocs().mjava","sourceNew":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator();\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\")));\n    PostingsEnum de = termsEnum.postings(null, PostingsEnum.ALL);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** some docs have payload att, some not */\n  public void testMixupDocs() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_NOT_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorPayloads(true);\n    customType.setStoreTermVectorOffsets(random().nextBoolean());\n    Field field = new Field(\"field\", \"\", customType);\n    TokenStream ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"here we go\"));\n    field.setTokenStream(ts);\n    doc.add(field);\n    writer.addDocument(doc);\n    \n    Token withPayload = new Token(\"withPayload\", 0, 11);\n    withPayload.setPayload(new BytesRef(\"test\"));\n    ts = new CannedTokenStream(withPayload);\n    assertTrue(ts.hasAttribute(PayloadAttribute.class));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    ts = new MockTokenizer(MockTokenizer.WHITESPACE, true);\n    ((Tokenizer)ts).setReader(new StringReader(\"another\"));\n    field.setTokenStream(ts);\n    writer.addDocument(doc);\n    \n    DirectoryReader reader = writer.getReader();\n    Terms terms = reader.getTermVector(1, \"field\");\n    assert terms != null;\n    TermsEnum termsEnum = terms.iterator();\n    assertTrue(termsEnum.seekExact(new BytesRef(\"withPayload\")));\n    PostingsEnum de = termsEnum.postings(null, null, PostingsEnum.ALL);\n    assertEquals(0, de.nextDoc());\n    assertEquals(0, de.nextPosition());\n    assertEquals(new BytesRef(\"test\"), de.getPayload());\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["923f36bb0db6f793cf62dbb68723ae3bfbaf1d75"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["eee5f2a24465d2c9a5f86ab84b7c35041a30fda8"],"923f36bb0db6f793cf62dbb68723ae3bfbaf1d75":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"51f5280f31484820499077f41fcdfe92d527d9dc":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552":["0935c850ea562932997b72c69d93e345f21d7f45"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"0935c850ea562932997b72c69d93e345f21d7f45":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8":["2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0f4464508ee83288c8c4585b533f9faaa93aa314"]},"commit2Childs":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"923f36bb0db6f793cf62dbb68723ae3bfbaf1d75":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"51f5280f31484820499077f41fcdfe92d527d9dc":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":[],"2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552":["c7869f64c874ebf7f317d22c00baf2b6857797a6","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","eee5f2a24465d2c9a5f86ab84b7c35041a30fda8","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c7869f64c874ebf7f317d22c00baf2b6857797a6","0935c850ea562932997b72c69d93e345f21d7f45","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":[],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["923f36bb0db6f793cf62dbb68723ae3bfbaf1d75"],"0935c850ea562932997b72c69d93e345f21d7f45":["2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c7869f64c874ebf7f317d22c00baf2b6857797a6","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}