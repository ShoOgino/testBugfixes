{"path":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene54/Lucene54DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","commits":[{"id":"7f3090f7e0cab5b1f5acf12d21f31f00fe74a262","date":1475755647,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene54/Lucene54DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene54/Lucene54DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","sourceNew":"  /** expert: writes a value dictionary for a sorted/sortedset field */\n  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {\n    // first check if it's a \"fixed-length\" terms dict, and compressibility if so\n    int minLength = Integer.MAX_VALUE;\n    int maxLength = Integer.MIN_VALUE;\n    long numValues = 0;\n    BytesRefBuilder previousValue = new BytesRefBuilder();\n    long prefixSum = 0; // only valid for fixed-width data, as we have a choice there\n    for (BytesRef v : values) {\n      minLength = Math.min(minLength, v.length);\n      maxLength = Math.max(maxLength, v.length);\n      if (minLength == maxLength) {\n        int termPosition = (int) (numValues & INTERVAL_MASK);\n        if (termPosition == 0) {\n          // first term in block, save it away to compare against the last term later\n          previousValue.copyBytes(v);\n        } else if (termPosition == INTERVAL_COUNT - 1) {\n          // last term in block, accumulate shared prefix against first term\n          prefixSum += StringHelper.bytesDifference(previousValue.get(), v);\n        }\n      }\n      numValues++;\n    }\n    // for fixed width data, look at the avg(shared prefix) before deciding how to encode:\n    // prefix compression \"costs\" worst case 2 bytes per term because we must store suffix lengths.\n    // so if we share at least 3 bytes on average, always compress.\n    if (minLength == maxLength && prefixSum <= 3*(numValues >> INTERVAL_SHIFT)) {\n      // no index needed: not very compressible, direct addressing by mult\n      addBinaryField(field, values);\n    } else if (numValues < REVERSE_INTERVAL_COUNT) {\n      // low cardinality: waste a few KB of ram, but can't really use fancy index etc\n      addBinaryField(field, values);\n    } else {\n      assert numValues > 0; // we don't have to handle the empty case\n      // header\n      meta.writeVInt(field.number);\n      meta.writeByte(Lucene54DocValuesFormat.BINARY);\n      meta.writeVInt(BINARY_PREFIX_COMPRESSED);\n      meta.writeLong(-1L);\n      // now write the bytes: sharing prefixes within a block\n      final long startFP = data.getFilePointer();\n      // currently, we have to store the delta from expected for every 1/nth term\n      // we could avoid this, but it's not much and less overall RAM than the previous approach!\n      RAMOutputStream addressBuffer = new RAMOutputStream();\n      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, MONOTONIC_BLOCK_SIZE);\n      // buffers up 16 terms\n      RAMOutputStream bytesBuffer = new RAMOutputStream();\n      // buffers up block header\n      RAMOutputStream headerBuffer = new RAMOutputStream();\n      BytesRefBuilder lastTerm = new BytesRefBuilder();\n      lastTerm.grow(maxLength);\n      long count = 0;\n      int suffixDeltas[] = new int[INTERVAL_COUNT];\n      for (BytesRef v : values) {\n        int termPosition = (int) (count & INTERVAL_MASK);\n        if (termPosition == 0) {\n          termAddresses.add(data.getFilePointer() - startFP);\n          // abs-encode first term\n          headerBuffer.writeVInt(v.length);\n          headerBuffer.writeBytes(v.bytes, v.offset, v.length);\n          lastTerm.copyBytes(v);\n        } else {\n          // prefix-code: we only share at most 255 characters, to encode the length as a single\n          // byte and have random access. Larger terms just get less compression.\n          int sharedPrefix = Math.min(255, StringHelper.bytesDifference(lastTerm.get(), v));\n          bytesBuffer.writeByte((byte) sharedPrefix);\n          bytesBuffer.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);\n          // we can encode one smaller, because terms are unique.\n          suffixDeltas[termPosition] = v.length - sharedPrefix - 1;\n        }\n        \n        count++;\n        // flush block\n        if ((count & INTERVAL_MASK) == 0) {\n          flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n        }\n      }\n      // flush trailing crap\n      int leftover = (int) (count & INTERVAL_MASK);\n      if (leftover > 0) {\n        Arrays.fill(suffixDeltas, leftover, suffixDeltas.length, 0);\n        flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n      }\n      final long indexStartFP = data.getFilePointer();\n      // write addresses of indexed terms\n      termAddresses.finish();\n      addressBuffer.writeTo(data);\n      addressBuffer = null;\n      termAddresses = null;\n      meta.writeVInt(minLength);\n      meta.writeVInt(maxLength);\n      meta.writeVLong(count);\n      meta.writeLong(startFP);\n      meta.writeLong(indexStartFP);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeVInt(MONOTONIC_BLOCK_SIZE);\n      addReverseTermIndex(field, values, maxLength);\n    }\n  }\n\n","sourceOld":"  /** expert: writes a value dictionary for a sorted/sortedset field */\n  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {\n    // first check if it's a \"fixed-length\" terms dict, and compressibility if so\n    int minLength = Integer.MAX_VALUE;\n    int maxLength = Integer.MIN_VALUE;\n    long numValues = 0;\n    BytesRefBuilder previousValue = new BytesRefBuilder();\n    long prefixSum = 0; // only valid for fixed-width data, as we have a choice there\n    for (BytesRef v : values) {\n      minLength = Math.min(minLength, v.length);\n      maxLength = Math.max(maxLength, v.length);\n      if (minLength == maxLength) {\n        int termPosition = (int) (numValues & INTERVAL_MASK);\n        if (termPosition == 0) {\n          // first term in block, save it away to compare against the last term later\n          previousValue.copyBytes(v);\n        } else if (termPosition == INTERVAL_COUNT - 1) {\n          // last term in block, accumulate shared prefix against first term\n          prefixSum += StringHelper.bytesDifference(previousValue.get(), v);\n        }\n      }\n      numValues++;\n    }\n    // for fixed width data, look at the avg(shared prefix) before deciding how to encode:\n    // prefix compression \"costs\" worst case 2 bytes per term because we must store suffix lengths.\n    // so if we share at least 3 bytes on average, always compress.\n    if (minLength == maxLength && prefixSum <= 3*(numValues >> INTERVAL_SHIFT)) {\n      // no index needed: not very compressible, direct addressing by mult\n      addBinaryField(field, values);\n    } else if (numValues < REVERSE_INTERVAL_COUNT) {\n      // low cardinality: waste a few KB of ram, but can't really use fancy index etc\n      addBinaryField(field, values);\n    } else {\n      assert numValues > 0; // we don't have to handle the empty case\n      // header\n      meta.writeVInt(field.number);\n      meta.writeByte(Lucene54DocValuesFormat.BINARY);\n      meta.writeVInt(BINARY_PREFIX_COMPRESSED);\n      meta.writeLong(-1L);\n      // now write the bytes: sharing prefixes within a block\n      final long startFP = data.getFilePointer();\n      // currently, we have to store the delta from expected for every 1/nth term\n      // we could avoid this, but it's not much and less overall RAM than the previous approach!\n      RAMOutputStream addressBuffer = new RAMOutputStream();\n      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, MONOTONIC_BLOCK_SIZE);\n      // buffers up 16 terms\n      RAMOutputStream bytesBuffer = new RAMOutputStream();\n      // buffers up block header\n      RAMOutputStream headerBuffer = new RAMOutputStream();\n      BytesRefBuilder lastTerm = new BytesRefBuilder();\n      lastTerm.grow(maxLength);\n      long count = 0;\n      int suffixDeltas[] = new int[INTERVAL_COUNT];\n      for (BytesRef v : values) {\n        int termPosition = (int) (count & INTERVAL_MASK);\n        if (termPosition == 0) {\n          termAddresses.add(data.getFilePointer() - startFP);\n          // abs-encode first term\n          headerBuffer.writeVInt(v.length);\n          headerBuffer.writeBytes(v.bytes, v.offset, v.length);\n          lastTerm.copyBytes(v);\n        } else {\n          // prefix-code: we only share at most 255 characters, to encode the length as a single\n          // byte and have random access. Larger terms just get less compression.\n          int sharedPrefix = Math.min(255, StringHelper.bytesDifference(lastTerm.get(), v));\n          bytesBuffer.writeByte((byte) sharedPrefix);\n          bytesBuffer.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);\n          // we can encode one smaller, because terms are unique.\n          suffixDeltas[termPosition] = v.length - sharedPrefix - 1;\n        }\n        \n        count++;\n        // flush block\n        if ((count & INTERVAL_MASK) == 0) {\n          flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n        }\n      }\n      // flush trailing crap\n      int leftover = (int) (count & INTERVAL_MASK);\n      if (leftover > 0) {\n        Arrays.fill(suffixDeltas, leftover, suffixDeltas.length, 0);\n        flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n      }\n      final long indexStartFP = data.getFilePointer();\n      // write addresses of indexed terms\n      termAddresses.finish();\n      addressBuffer.writeTo(data);\n      addressBuffer = null;\n      termAddresses = null;\n      meta.writeVInt(minLength);\n      meta.writeVInt(maxLength);\n      meta.writeVLong(count);\n      meta.writeLong(startFP);\n      meta.writeLong(indexStartFP);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeVInt(MONOTONIC_BLOCK_SIZE);\n      addReverseTermIndex(field, values, maxLength);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":1,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene54/Lucene54DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene54/Lucene54DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","sourceNew":"  /** expert: writes a value dictionary for a sorted/sortedset field */\n  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {\n    // first check if it's a \"fixed-length\" terms dict, and compressibility if so\n    int minLength = Integer.MAX_VALUE;\n    int maxLength = Integer.MIN_VALUE;\n    long numValues = 0;\n    BytesRefBuilder previousValue = new BytesRefBuilder();\n    long prefixSum = 0; // only valid for fixed-width data, as we have a choice there\n    for (BytesRef v : values) {\n      minLength = Math.min(minLength, v.length);\n      maxLength = Math.max(maxLength, v.length);\n      if (minLength == maxLength) {\n        int termPosition = (int) (numValues & INTERVAL_MASK);\n        if (termPosition == 0) {\n          // first term in block, save it away to compare against the last term later\n          previousValue.copyBytes(v);\n        } else if (termPosition == INTERVAL_COUNT - 1) {\n          // last term in block, accumulate shared prefix against first term\n          prefixSum += StringHelper.bytesDifference(previousValue.get(), v);\n        }\n      }\n      numValues++;\n    }\n    // for fixed width data, look at the avg(shared prefix) before deciding how to encode:\n    // prefix compression \"costs\" worst case 2 bytes per term because we must store suffix lengths.\n    // so if we share at least 3 bytes on average, always compress.\n    if (minLength == maxLength && prefixSum <= 3*(numValues >> INTERVAL_SHIFT)) {\n      // no index needed: not very compressible, direct addressing by mult\n      addBinaryField(field, values);\n    } else if (numValues < REVERSE_INTERVAL_COUNT) {\n      // low cardinality: waste a few KB of ram, but can't really use fancy index etc\n      addBinaryField(field, values);\n    } else {\n      assert numValues > 0; // we don't have to handle the empty case\n      // header\n      meta.writeVInt(field.number);\n      meta.writeByte(Lucene54DocValuesFormat.BINARY);\n      meta.writeVInt(BINARY_PREFIX_COMPRESSED);\n      meta.writeLong(-1L);\n      // now write the bytes: sharing prefixes within a block\n      final long startFP = data.getFilePointer();\n      // currently, we have to store the delta from expected for every 1/nth term\n      // we could avoid this, but it's not much and less overall RAM than the previous approach!\n      RAMOutputStream addressBuffer = new RAMOutputStream();\n      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, MONOTONIC_BLOCK_SIZE);\n      // buffers up 16 terms\n      RAMOutputStream bytesBuffer = new RAMOutputStream();\n      // buffers up block header\n      RAMOutputStream headerBuffer = new RAMOutputStream();\n      BytesRefBuilder lastTerm = new BytesRefBuilder();\n      lastTerm.grow(maxLength);\n      long count = 0;\n      int suffixDeltas[] = new int[INTERVAL_COUNT];\n      for (BytesRef v : values) {\n        int termPosition = (int) (count & INTERVAL_MASK);\n        if (termPosition == 0) {\n          termAddresses.add(data.getFilePointer() - startFP);\n          // abs-encode first term\n          headerBuffer.writeVInt(v.length);\n          headerBuffer.writeBytes(v.bytes, v.offset, v.length);\n          lastTerm.copyBytes(v);\n        } else {\n          // prefix-code: we only share at most 255 characters, to encode the length as a single\n          // byte and have random access. Larger terms just get less compression.\n          int sharedPrefix = Math.min(255, StringHelper.bytesDifference(lastTerm.get(), v));\n          bytesBuffer.writeByte((byte) sharedPrefix);\n          bytesBuffer.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);\n          // we can encode one smaller, because terms are unique.\n          suffixDeltas[termPosition] = v.length - sharedPrefix - 1;\n        }\n        \n        count++;\n        // flush block\n        if ((count & INTERVAL_MASK) == 0) {\n          flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n        }\n      }\n      // flush trailing crap\n      int leftover = (int) (count & INTERVAL_MASK);\n      if (leftover > 0) {\n        Arrays.fill(suffixDeltas, leftover, suffixDeltas.length, 0);\n        flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n      }\n      final long indexStartFP = data.getFilePointer();\n      // write addresses of indexed terms\n      termAddresses.finish();\n      addressBuffer.writeTo(data);\n      addressBuffer = null;\n      termAddresses = null;\n      meta.writeVInt(minLength);\n      meta.writeVInt(maxLength);\n      meta.writeVLong(count);\n      meta.writeLong(startFP);\n      meta.writeLong(indexStartFP);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeVInt(MONOTONIC_BLOCK_SIZE);\n      addReverseTermIndex(field, values, maxLength);\n    }\n  }\n\n","sourceOld":"  /** expert: writes a value dictionary for a sorted/sortedset field */\n  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {\n    // first check if it's a \"fixed-length\" terms dict, and compressibility if so\n    int minLength = Integer.MAX_VALUE;\n    int maxLength = Integer.MIN_VALUE;\n    long numValues = 0;\n    BytesRefBuilder previousValue = new BytesRefBuilder();\n    long prefixSum = 0; // only valid for fixed-width data, as we have a choice there\n    for (BytesRef v : values) {\n      minLength = Math.min(minLength, v.length);\n      maxLength = Math.max(maxLength, v.length);\n      if (minLength == maxLength) {\n        int termPosition = (int) (numValues & INTERVAL_MASK);\n        if (termPosition == 0) {\n          // first term in block, save it away to compare against the last term later\n          previousValue.copyBytes(v);\n        } else if (termPosition == INTERVAL_COUNT - 1) {\n          // last term in block, accumulate shared prefix against first term\n          prefixSum += StringHelper.bytesDifference(previousValue.get(), v);\n        }\n      }\n      numValues++;\n    }\n    // for fixed width data, look at the avg(shared prefix) before deciding how to encode:\n    // prefix compression \"costs\" worst case 2 bytes per term because we must store suffix lengths.\n    // so if we share at least 3 bytes on average, always compress.\n    if (minLength == maxLength && prefixSum <= 3*(numValues >> INTERVAL_SHIFT)) {\n      // no index needed: not very compressible, direct addressing by mult\n      addBinaryField(field, values);\n    } else if (numValues < REVERSE_INTERVAL_COUNT) {\n      // low cardinality: waste a few KB of ram, but can't really use fancy index etc\n      addBinaryField(field, values);\n    } else {\n      assert numValues > 0; // we don't have to handle the empty case\n      // header\n      meta.writeVInt(field.number);\n      meta.writeByte(Lucene54DocValuesFormat.BINARY);\n      meta.writeVInt(BINARY_PREFIX_COMPRESSED);\n      meta.writeLong(-1L);\n      // now write the bytes: sharing prefixes within a block\n      final long startFP = data.getFilePointer();\n      // currently, we have to store the delta from expected for every 1/nth term\n      // we could avoid this, but it's not much and less overall RAM than the previous approach!\n      RAMOutputStream addressBuffer = new RAMOutputStream();\n      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, MONOTONIC_BLOCK_SIZE);\n      // buffers up 16 terms\n      RAMOutputStream bytesBuffer = new RAMOutputStream();\n      // buffers up block header\n      RAMOutputStream headerBuffer = new RAMOutputStream();\n      BytesRefBuilder lastTerm = new BytesRefBuilder();\n      lastTerm.grow(maxLength);\n      long count = 0;\n      int suffixDeltas[] = new int[INTERVAL_COUNT];\n      for (BytesRef v : values) {\n        int termPosition = (int) (count & INTERVAL_MASK);\n        if (termPosition == 0) {\n          termAddresses.add(data.getFilePointer() - startFP);\n          // abs-encode first term\n          headerBuffer.writeVInt(v.length);\n          headerBuffer.writeBytes(v.bytes, v.offset, v.length);\n          lastTerm.copyBytes(v);\n        } else {\n          // prefix-code: we only share at most 255 characters, to encode the length as a single\n          // byte and have random access. Larger terms just get less compression.\n          int sharedPrefix = Math.min(255, StringHelper.bytesDifference(lastTerm.get(), v));\n          bytesBuffer.writeByte((byte) sharedPrefix);\n          bytesBuffer.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);\n          // we can encode one smaller, because terms are unique.\n          suffixDeltas[termPosition] = v.length - sharedPrefix - 1;\n        }\n        \n        count++;\n        // flush block\n        if ((count & INTERVAL_MASK) == 0) {\n          flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n        }\n      }\n      // flush trailing crap\n      int leftover = (int) (count & INTERVAL_MASK);\n      if (leftover > 0) {\n        Arrays.fill(suffixDeltas, leftover, suffixDeltas.length, 0);\n        flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n      }\n      final long indexStartFP = data.getFilePointer();\n      // write addresses of indexed terms\n      termAddresses.finish();\n      addressBuffer.writeTo(data);\n      addressBuffer = null;\n      termAddresses = null;\n      meta.writeVInt(minLength);\n      meta.writeVInt(maxLength);\n      meta.writeVLong(count);\n      meta.writeLong(startFP);\n      meta.writeLong(indexStartFP);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeVInt(MONOTONIC_BLOCK_SIZE);\n      addReverseTermIndex(field, values, maxLength);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"85b9829d27224bda451a373a47f081afb1c664b8","date":1498846708,"type":4,"author":"Anshum Gupta","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene54/Lucene54DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","sourceNew":null,"sourceOld":"  /** expert: writes a value dictionary for a sorted/sortedset field */\n  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {\n    // first check if it's a \"fixed-length\" terms dict, and compressibility if so\n    int minLength = Integer.MAX_VALUE;\n    int maxLength = Integer.MIN_VALUE;\n    long numValues = 0;\n    BytesRefBuilder previousValue = new BytesRefBuilder();\n    long prefixSum = 0; // only valid for fixed-width data, as we have a choice there\n    for (BytesRef v : values) {\n      minLength = Math.min(minLength, v.length);\n      maxLength = Math.max(maxLength, v.length);\n      if (minLength == maxLength) {\n        int termPosition = (int) (numValues & INTERVAL_MASK);\n        if (termPosition == 0) {\n          // first term in block, save it away to compare against the last term later\n          previousValue.copyBytes(v);\n        } else if (termPosition == INTERVAL_COUNT - 1) {\n          // last term in block, accumulate shared prefix against first term\n          prefixSum += StringHelper.bytesDifference(previousValue.get(), v);\n        }\n      }\n      numValues++;\n    }\n    // for fixed width data, look at the avg(shared prefix) before deciding how to encode:\n    // prefix compression \"costs\" worst case 2 bytes per term because we must store suffix lengths.\n    // so if we share at least 3 bytes on average, always compress.\n    if (minLength == maxLength && prefixSum <= 3*(numValues >> INTERVAL_SHIFT)) {\n      // no index needed: not very compressible, direct addressing by mult\n      addBinaryField(field, values);\n    } else if (numValues < REVERSE_INTERVAL_COUNT) {\n      // low cardinality: waste a few KB of ram, but can't really use fancy index etc\n      addBinaryField(field, values);\n    } else {\n      assert numValues > 0; // we don't have to handle the empty case\n      // header\n      meta.writeVInt(field.number);\n      meta.writeByte(Lucene54DocValuesFormat.BINARY);\n      meta.writeVInt(BINARY_PREFIX_COMPRESSED);\n      meta.writeLong(-1L);\n      // now write the bytes: sharing prefixes within a block\n      final long startFP = data.getFilePointer();\n      // currently, we have to store the delta from expected for every 1/nth term\n      // we could avoid this, but it's not much and less overall RAM than the previous approach!\n      RAMOutputStream addressBuffer = new RAMOutputStream();\n      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, MONOTONIC_BLOCK_SIZE);\n      // buffers up 16 terms\n      RAMOutputStream bytesBuffer = new RAMOutputStream();\n      // buffers up block header\n      RAMOutputStream headerBuffer = new RAMOutputStream();\n      BytesRefBuilder lastTerm = new BytesRefBuilder();\n      lastTerm.grow(maxLength);\n      long count = 0;\n      int suffixDeltas[] = new int[INTERVAL_COUNT];\n      for (BytesRef v : values) {\n        int termPosition = (int) (count & INTERVAL_MASK);\n        if (termPosition == 0) {\n          termAddresses.add(data.getFilePointer() - startFP);\n          // abs-encode first term\n          headerBuffer.writeVInt(v.length);\n          headerBuffer.writeBytes(v.bytes, v.offset, v.length);\n          lastTerm.copyBytes(v);\n        } else {\n          // prefix-code: we only share at most 255 characters, to encode the length as a single\n          // byte and have random access. Larger terms just get less compression.\n          int sharedPrefix = Math.min(255, StringHelper.bytesDifference(lastTerm.get(), v));\n          bytesBuffer.writeByte((byte) sharedPrefix);\n          bytesBuffer.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);\n          // we can encode one smaller, because terms are unique.\n          suffixDeltas[termPosition] = v.length - sharedPrefix - 1;\n        }\n        \n        count++;\n        // flush block\n        if ((count & INTERVAL_MASK) == 0) {\n          flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n        }\n      }\n      // flush trailing crap\n      int leftover = (int) (count & INTERVAL_MASK);\n      if (leftover > 0) {\n        Arrays.fill(suffixDeltas, leftover, suffixDeltas.length, 0);\n        flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n      }\n      final long indexStartFP = data.getFilePointer();\n      // write addresses of indexed terms\n      termAddresses.finish();\n      addressBuffer.writeTo(data);\n      addressBuffer = null;\n      termAddresses = null;\n      meta.writeVInt(minLength);\n      meta.writeVInt(maxLength);\n      meta.writeVLong(count);\n      meta.writeLong(startFP);\n      meta.writeLong(indexStartFP);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeVInt(MONOTONIC_BLOCK_SIZE);\n      addReverseTermIndex(field, values, maxLength);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cc018b79379c67835b40b1259cd3dc931df60944","date":1499109112,"type":4,"author":"Anshum Gupta","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene54/Lucene54DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","sourceNew":null,"sourceOld":"  /** expert: writes a value dictionary for a sorted/sortedset field */\n  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {\n    // first check if it's a \"fixed-length\" terms dict, and compressibility if so\n    int minLength = Integer.MAX_VALUE;\n    int maxLength = Integer.MIN_VALUE;\n    long numValues = 0;\n    BytesRefBuilder previousValue = new BytesRefBuilder();\n    long prefixSum = 0; // only valid for fixed-width data, as we have a choice there\n    for (BytesRef v : values) {\n      minLength = Math.min(minLength, v.length);\n      maxLength = Math.max(maxLength, v.length);\n      if (minLength == maxLength) {\n        int termPosition = (int) (numValues & INTERVAL_MASK);\n        if (termPosition == 0) {\n          // first term in block, save it away to compare against the last term later\n          previousValue.copyBytes(v);\n        } else if (termPosition == INTERVAL_COUNT - 1) {\n          // last term in block, accumulate shared prefix against first term\n          prefixSum += StringHelper.bytesDifference(previousValue.get(), v);\n        }\n      }\n      numValues++;\n    }\n    // for fixed width data, look at the avg(shared prefix) before deciding how to encode:\n    // prefix compression \"costs\" worst case 2 bytes per term because we must store suffix lengths.\n    // so if we share at least 3 bytes on average, always compress.\n    if (minLength == maxLength && prefixSum <= 3*(numValues >> INTERVAL_SHIFT)) {\n      // no index needed: not very compressible, direct addressing by mult\n      addBinaryField(field, values);\n    } else if (numValues < REVERSE_INTERVAL_COUNT) {\n      // low cardinality: waste a few KB of ram, but can't really use fancy index etc\n      addBinaryField(field, values);\n    } else {\n      assert numValues > 0; // we don't have to handle the empty case\n      // header\n      meta.writeVInt(field.number);\n      meta.writeByte(Lucene54DocValuesFormat.BINARY);\n      meta.writeVInt(BINARY_PREFIX_COMPRESSED);\n      meta.writeLong(-1L);\n      // now write the bytes: sharing prefixes within a block\n      final long startFP = data.getFilePointer();\n      // currently, we have to store the delta from expected for every 1/nth term\n      // we could avoid this, but it's not much and less overall RAM than the previous approach!\n      RAMOutputStream addressBuffer = new RAMOutputStream();\n      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, MONOTONIC_BLOCK_SIZE);\n      // buffers up 16 terms\n      RAMOutputStream bytesBuffer = new RAMOutputStream();\n      // buffers up block header\n      RAMOutputStream headerBuffer = new RAMOutputStream();\n      BytesRefBuilder lastTerm = new BytesRefBuilder();\n      lastTerm.grow(maxLength);\n      long count = 0;\n      int suffixDeltas[] = new int[INTERVAL_COUNT];\n      for (BytesRef v : values) {\n        int termPosition = (int) (count & INTERVAL_MASK);\n        if (termPosition == 0) {\n          termAddresses.add(data.getFilePointer() - startFP);\n          // abs-encode first term\n          headerBuffer.writeVInt(v.length);\n          headerBuffer.writeBytes(v.bytes, v.offset, v.length);\n          lastTerm.copyBytes(v);\n        } else {\n          // prefix-code: we only share at most 255 characters, to encode the length as a single\n          // byte and have random access. Larger terms just get less compression.\n          int sharedPrefix = Math.min(255, StringHelper.bytesDifference(lastTerm.get(), v));\n          bytesBuffer.writeByte((byte) sharedPrefix);\n          bytesBuffer.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);\n          // we can encode one smaller, because terms are unique.\n          suffixDeltas[termPosition] = v.length - sharedPrefix - 1;\n        }\n        \n        count++;\n        // flush block\n        if ((count & INTERVAL_MASK) == 0) {\n          flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n        }\n      }\n      // flush trailing crap\n      int leftover = (int) (count & INTERVAL_MASK);\n      if (leftover > 0) {\n        Arrays.fill(suffixDeltas, leftover, suffixDeltas.length, 0);\n        flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n      }\n      final long indexStartFP = data.getFilePointer();\n      // write addresses of indexed terms\n      termAddresses.finish();\n      addressBuffer.writeTo(data);\n      addressBuffer = null;\n      termAddresses = null;\n      meta.writeVInt(minLength);\n      meta.writeVInt(maxLength);\n      meta.writeVLong(count);\n      meta.writeLong(startFP);\n      meta.writeLong(indexStartFP);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeVInt(MONOTONIC_BLOCK_SIZE);\n      addReverseTermIndex(field, values, maxLength);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30c8e5574b55d57947e989443dfde611646530ee","date":1499131153,"type":4,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene54/Lucene54DocValuesConsumer#addTermsDict(FieldInfo,Iterable[BytesRef]).mjava","sourceNew":null,"sourceOld":"  /** expert: writes a value dictionary for a sorted/sortedset field */\n  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {\n    // first check if it's a \"fixed-length\" terms dict, and compressibility if so\n    int minLength = Integer.MAX_VALUE;\n    int maxLength = Integer.MIN_VALUE;\n    long numValues = 0;\n    BytesRefBuilder previousValue = new BytesRefBuilder();\n    long prefixSum = 0; // only valid for fixed-width data, as we have a choice there\n    for (BytesRef v : values) {\n      minLength = Math.min(minLength, v.length);\n      maxLength = Math.max(maxLength, v.length);\n      if (minLength == maxLength) {\n        int termPosition = (int) (numValues & INTERVAL_MASK);\n        if (termPosition == 0) {\n          // first term in block, save it away to compare against the last term later\n          previousValue.copyBytes(v);\n        } else if (termPosition == INTERVAL_COUNT - 1) {\n          // last term in block, accumulate shared prefix against first term\n          prefixSum += StringHelper.bytesDifference(previousValue.get(), v);\n        }\n      }\n      numValues++;\n    }\n    // for fixed width data, look at the avg(shared prefix) before deciding how to encode:\n    // prefix compression \"costs\" worst case 2 bytes per term because we must store suffix lengths.\n    // so if we share at least 3 bytes on average, always compress.\n    if (minLength == maxLength && prefixSum <= 3*(numValues >> INTERVAL_SHIFT)) {\n      // no index needed: not very compressible, direct addressing by mult\n      addBinaryField(field, values);\n    } else if (numValues < REVERSE_INTERVAL_COUNT) {\n      // low cardinality: waste a few KB of ram, but can't really use fancy index etc\n      addBinaryField(field, values);\n    } else {\n      assert numValues > 0; // we don't have to handle the empty case\n      // header\n      meta.writeVInt(field.number);\n      meta.writeByte(Lucene54DocValuesFormat.BINARY);\n      meta.writeVInt(BINARY_PREFIX_COMPRESSED);\n      meta.writeLong(-1L);\n      // now write the bytes: sharing prefixes within a block\n      final long startFP = data.getFilePointer();\n      // currently, we have to store the delta from expected for every 1/nth term\n      // we could avoid this, but it's not much and less overall RAM than the previous approach!\n      RAMOutputStream addressBuffer = new RAMOutputStream();\n      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, MONOTONIC_BLOCK_SIZE);\n      // buffers up 16 terms\n      RAMOutputStream bytesBuffer = new RAMOutputStream();\n      // buffers up block header\n      RAMOutputStream headerBuffer = new RAMOutputStream();\n      BytesRefBuilder lastTerm = new BytesRefBuilder();\n      lastTerm.grow(maxLength);\n      long count = 0;\n      int suffixDeltas[] = new int[INTERVAL_COUNT];\n      for (BytesRef v : values) {\n        int termPosition = (int) (count & INTERVAL_MASK);\n        if (termPosition == 0) {\n          termAddresses.add(data.getFilePointer() - startFP);\n          // abs-encode first term\n          headerBuffer.writeVInt(v.length);\n          headerBuffer.writeBytes(v.bytes, v.offset, v.length);\n          lastTerm.copyBytes(v);\n        } else {\n          // prefix-code: we only share at most 255 characters, to encode the length as a single\n          // byte and have random access. Larger terms just get less compression.\n          int sharedPrefix = Math.min(255, StringHelper.bytesDifference(lastTerm.get(), v));\n          bytesBuffer.writeByte((byte) sharedPrefix);\n          bytesBuffer.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);\n          // we can encode one smaller, because terms are unique.\n          suffixDeltas[termPosition] = v.length - sharedPrefix - 1;\n        }\n        \n        count++;\n        // flush block\n        if ((count & INTERVAL_MASK) == 0) {\n          flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n        }\n      }\n      // flush trailing crap\n      int leftover = (int) (count & INTERVAL_MASK);\n      if (leftover > 0) {\n        Arrays.fill(suffixDeltas, leftover, suffixDeltas.length, 0);\n        flushTermsDictBlock(headerBuffer, bytesBuffer, suffixDeltas);\n      }\n      final long indexStartFP = data.getFilePointer();\n      // write addresses of indexed terms\n      termAddresses.finish();\n      addressBuffer.writeTo(data);\n      addressBuffer = null;\n      termAddresses = null;\n      meta.writeVInt(minLength);\n      meta.writeVInt(maxLength);\n      meta.writeVLong(count);\n      meta.writeLong(startFP);\n      meta.writeLong(indexStartFP);\n      meta.writeVInt(PackedInts.VERSION_CURRENT);\n      meta.writeVInt(MONOTONIC_BLOCK_SIZE);\n      addReverseTermIndex(field, values, maxLength);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7f3090f7e0cab5b1f5acf12d21f31f00fe74a262":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cc018b79379c67835b40b1259cd3dc931df60944":["7f3090f7e0cab5b1f5acf12d21f31f00fe74a262","85b9829d27224bda451a373a47f081afb1c664b8"],"30c8e5574b55d57947e989443dfde611646530ee":["7f3090f7e0cab5b1f5acf12d21f31f00fe74a262","cc018b79379c67835b40b1259cd3dc931df60944"],"85b9829d27224bda451a373a47f081afb1c664b8":["7f3090f7e0cab5b1f5acf12d21f31f00fe74a262"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","7f3090f7e0cab5b1f5acf12d21f31f00fe74a262"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["cc018b79379c67835b40b1259cd3dc931df60944"]},"commit2Childs":{"7f3090f7e0cab5b1f5acf12d21f31f00fe74a262":["cc018b79379c67835b40b1259cd3dc931df60944","30c8e5574b55d57947e989443dfde611646530ee","85b9829d27224bda451a373a47f081afb1c664b8","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7f3090f7e0cab5b1f5acf12d21f31f00fe74a262","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"cc018b79379c67835b40b1259cd3dc931df60944":["30c8e5574b55d57947e989443dfde611646530ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"30c8e5574b55d57947e989443dfde611646530ee":[],"85b9829d27224bda451a373a47f081afb1c664b8":["cc018b79379c67835b40b1259cd3dc931df60944"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["30c8e5574b55d57947e989443dfde611646530ee","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}