{"path":"lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadBasics#beforeClass().mjava","commits":[{"id":"c4f600f812447b5512daeaf8e5c9df5dbcc4a254","date":1428874774,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadBasics#beforeClass().mjava","pathOld":"/dev/null","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    Analyzer simplePayloadAnalyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n          return new TokenStreamComponents(tokenizer, new SimplePayloadFilter(tokenizer));\n        }\n    };\n  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(simplePayloadAnalyzer)\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"field\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8a8dd51777c3f17c83f8aac170bd0f68a029d174","date":1442220758,"type":5,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadCheckQuery#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/payloads/TestPayloadBasics#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    Analyzer simplePayloadAnalyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n          return new TokenStreamComponents(tokenizer, new SimplePayloadFilter(tokenizer));\n        }\n    };\n  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(simplePayloadAnalyzer)\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"field\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    Analyzer simplePayloadAnalyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName) {\n          Tokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n          return new TokenStreamComponents(tokenizer, new SimplePayloadFilter(tokenizer));\n        }\n    };\n  \n    directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n        newIndexWriterConfig(simplePayloadAnalyzer)\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000)).setMergePolicy(newLogMergePolicy()));\n    //writer.infoStream = System.out;\n    for (int i = 0; i < 2000; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"field\", English.intToEnglish(i), Field.Store.YES));\n      writer.addDocument(doc);\n    }\n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"8a8dd51777c3f17c83f8aac170bd0f68a029d174":["c4f600f812447b5512daeaf8e5c9df5dbcc4a254"],"c4f600f812447b5512daeaf8e5c9df5dbcc4a254":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8a8dd51777c3f17c83f8aac170bd0f68a029d174"]},"commit2Childs":{"8a8dd51777c3f17c83f8aac170bd0f68a029d174":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c4f600f812447b5512daeaf8e5c9df5dbcc4a254":["8a8dd51777c3f17c83f8aac170bd0f68a029d174"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c4f600f812447b5512daeaf8e5c9df5dbcc4a254"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}