{"path":"lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","commits":[{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      TokenStream ts = a.tokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertAnalyzesToReuse(a, text, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions));\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              toIntArray(positions));\n        } else if (offsetAtt != null) {\n          // offset\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets));\n        } else {\n          // terms only\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      TokenStream ts = a.tokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertAnalyzesToReuse(a, text, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions));\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              toIntArray(positions));\n        } else if (offsetAtt != null) {\n          // offset\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets));\n        } else {\n          // terms only\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1c6ce79c21a8da5e778ef9179243bc50d6d42c52","date":1324010267,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","pathOld":"lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 4)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        case 2:\n          text = _TestUtil.randomHtmlishString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      TokenStream ts = a.tokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertAnalyzesToReuse(a, text, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions));\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              toIntArray(positions));\n        } else if (offsetAtt != null) {\n          // offset\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets));\n        } else {\n          // terms only\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 3)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      TokenStream ts = a.tokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertAnalyzesToReuse(a, text, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions));\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              toIntArray(positions));\n        } else if (offsetAtt != null) {\n          // offset\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets));\n        } else {\n          // terms only\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e477c2108982ba9974f73aa8800270c75cb4971","date":1327277332,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","pathOld":"lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    checkRandomData(random, a, iterations, maxWordLength, random.nextBoolean());\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 4)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        case 2:\n          text = _TestUtil.randomHtmlishString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      TokenStream ts = a.tokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertAnalyzesToReuse(a, text, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions));\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              toIntArray(positions));\n        } else if (offsetAtt != null) {\n          // offset\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets));\n        } else {\n          // terms only\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a89676536a5d3e2e875a9eed6b3f22a63cca643","date":1327356915,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","pathOld":"lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    checkRandomData(random, a, iterations, maxWordLength, random.nextBoolean());\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 4)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        case 2:\n          text = _TestUtil.randomHtmlishString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      TokenStream ts = a.tokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertAnalyzesToReuse(a, text, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions));\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              toIntArray(positions));\n        } else if (offsetAtt != null) {\n          // offset\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets));\n        } else {\n          // terms only\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","date":1327523564,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","pathOld":"lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    checkRandomData(random, a, iterations, maxWordLength, random.nextBoolean());\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    for (int i = 0; i < iterations; i++) {\n      String text;\n      switch(_TestUtil.nextInt(random, 0, 4)) {\n        case 0: \n          text = _TestUtil.randomSimpleString(random);\n          break;\n        case 1:\n          text = _TestUtil.randomRealisticUnicodeString(random, maxWordLength);\n          break;\n        case 2:\n          text = _TestUtil.randomHtmlishString(random, maxWordLength);\n          break;\n        default:\n          text = _TestUtil.randomUnicodeString(random, maxWordLength);\n      }\n\n      if (VERBOSE) {\n        System.out.println(\"NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n      }\n\n      TokenStream ts = a.tokenStream(\"dummy\", new StringReader(text));\n      assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n      PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n      TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n      List<String> tokens = new ArrayList<String>();\n      List<String> types = new ArrayList<String>();\n      List<Integer> positions = new ArrayList<Integer>();\n      List<Integer> startOffsets = new ArrayList<Integer>();\n      List<Integer> endOffsets = new ArrayList<Integer>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        tokens.add(termAtt.toString());\n        if (typeAtt != null) types.add(typeAtt.type());\n        if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n        if (offsetAtt != null) {\n          startOffsets.add(offsetAtt.startOffset());\n          endOffsets.add(offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n      ts.close();\n      // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n      if (!tokens.isEmpty()) {\n        if (VERBOSE) {\n          System.out.println(\"NOTE: BaseTokenStreamTestCase: re-run analysis\");\n        }\n        if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n          // offset + pos + type\n          assertAnalyzesToReuse(a, text, \n            tokens.toArray(new String[tokens.size()]),\n            toIntArray(startOffsets),\n            toIntArray(endOffsets),\n            types.toArray(new String[types.size()]),\n            toIntArray(positions));\n        } else if (posIncAtt != null && offsetAtt != null) {\n          // offset + pos\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets),\n              toIntArray(positions));\n        } else if (offsetAtt != null) {\n          // offset\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]),\n              toIntArray(startOffsets),\n              toIntArray(endOffsets));\n        } else {\n          // terms only\n          assertAnalyzesToReuse(a, text, \n              tokens.toArray(new String[tokens.size()]));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","pathOld":"lucene/src/test-framework/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkRandomData(Random,Analyzer,int,int).mjava","sourceNew":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    checkRandomData(random, a, iterations, maxWordLength, random.nextBoolean());\n  }\n\n","sourceOld":"  public static void checkRandomData(Random random, Analyzer a, int iterations, int maxWordLength) throws IOException {\n    checkRandomData(random, a, iterations, maxWordLength, random.nextBoolean());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7b91922b55d15444d554721b352861d028eb8278":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5a89676536a5d3e2e875a9eed6b3f22a63cca643":["1c6ce79c21a8da5e778ef9179243bc50d6d42c52","7e477c2108982ba9974f73aa8800270c75cb4971"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["7e477c2108982ba9974f73aa8800270c75cb4971"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d":["1c6ce79c21a8da5e778ef9179243bc50d6d42c52","7e477c2108982ba9974f73aa8800270c75cb4971"],"1c6ce79c21a8da5e778ef9179243bc50d6d42c52":["7b91922b55d15444d554721b352861d028eb8278"],"7e477c2108982ba9974f73aa8800270c75cb4971":["1c6ce79c21a8da5e778ef9179243bc50d6d42c52"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"7b91922b55d15444d554721b352861d028eb8278":["1c6ce79c21a8da5e778ef9179243bc50d6d42c52"],"5a89676536a5d3e2e875a9eed6b3f22a63cca643":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7b91922b55d15444d554721b352861d028eb8278"],"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d":[],"1c6ce79c21a8da5e778ef9179243bc50d6d42c52":["5a89676536a5d3e2e875a9eed6b3f22a63cca643","a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","7e477c2108982ba9974f73aa8800270c75cb4971"],"7e477c2108982ba9974f73aa8800270c75cb4971":["5a89676536a5d3e2e875a9eed6b3f22a63cca643","3a119bbc8703c10faa329ec201c654b3a35a1e3e","a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5a89676536a5d3e2e875a9eed6b3f22a63cca643","a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}