{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","commits":[{"id":"1bae040fd1d5e03e0d8d695a9c25cf4f402e7ffe","date":1381909398,"type":1,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testNoLostDeletesOrUpdatesOnIOException().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n\n      try {\n\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + (docBase+i) + \" to value \" + value);\n              }\n              w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"f\", value);\n              w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"cf\", value * 2);\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + (docBase+i));\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+(docBase+i)));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.close();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.close() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      for (AtomicReaderContext context : r.leaves()) {\n        Bits liveDocs = context.reader().getLiveDocs();\n        NumericDocValues f = context.reader().getNumericDocValues(\"f\");\n        NumericDocValues cf = context.reader().getNumericDocValues(\"cf\");\n        for (int i = 0; i < context.reader().maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdatesOnIOException() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n\n      try {\n\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + (docBase+i) + \" to value \" + value);\n              }\n              w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"f\", value);\n              w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"cf\", value * 2);\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + (docBase+i));\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+(docBase+i)));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.close();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.close() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      for (AtomicReaderContext context : r.leaves()) {\n        Bits liveDocs = context.reader().getLiveDocs();\n        NumericDocValues f = context.reader().getNumericDocValues(\"f\");\n        NumericDocValues cf = context.reader().getNumericDocValues(\"cf\");\n        for (int i = 0; i < context.reader().maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06805da26538ed636bd89b10c2699cc3834032ae","date":1395132972,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n\n      try {\n\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + (docBase+i) + \" to value \" + value);\n              }\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"f\", value);\n                w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"cf\", value * 2);\n              } else if (random().nextBoolean()) {\n                w.updateBinaryDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"bf\", TestBinaryDocValuesUpdates.toBytes(value));\n                w.updateBinaryDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"bcf\", TestBinaryDocValuesUpdates.toBytes(value * 2));\n              } else {\n                w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"f\", value);\n                w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"cf\", value * 2);\n                w.updateBinaryDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"bf\", TestBinaryDocValuesUpdates.toBytes(value));\n                w.updateBinaryDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"bcf\", TestBinaryDocValuesUpdates.toBytes(value * 2));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + (docBase+i));\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+(docBase+i)));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.close();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.close() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i, scratch), TestBinaryDocValuesUpdates.getValue(bf, i, scratch) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n\n      try {\n\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + (docBase+i) + \" to value \" + value);\n              }\n              w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"f\", value);\n              w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"cf\", value * 2);\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + (docBase+i));\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+(docBase+i)));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.close();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.close() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      for (AtomicReaderContext context : r.leaves()) {\n        Bits liveDocs = context.reader().getLiveDocs();\n        NumericDocValues f = context.reader().getNumericDocValues(\"f\");\n        NumericDocValues cf = context.reader().getNumericDocValues(\"cf\");\n        for (int i = 0; i < context.reader().maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n\n      try {\n\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + (docBase+i) + \" to value \" + value);\n              }\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"f\", value);\n                w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"cf\", value * 2);\n              } else if (random().nextBoolean()) {\n                w.updateBinaryDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"bf\", TestBinaryDocValuesUpdates.toBytes(value));\n                w.updateBinaryDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"bcf\", TestBinaryDocValuesUpdates.toBytes(value * 2));\n              } else {\n                w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"f\", value);\n                w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"cf\", value * 2);\n                w.updateBinaryDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"bf\", TestBinaryDocValuesUpdates.toBytes(value));\n                w.updateBinaryDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"bcf\", TestBinaryDocValuesUpdates.toBytes(value * 2));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + (docBase+i));\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+(docBase+i)));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.shutdown();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.shutdown() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        try {\n          w.shutdown();\n        } catch (AlreadyClosedException ace) {\n          // OK\n        }\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i, scratch), TestBinaryDocValuesUpdates.getValue(bf, i, scratch) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.shutdown();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.shutdown();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n\n      try {\n\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + (docBase+i) + \" to value \" + value);\n              }\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"f\", value);\n                w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"cf\", value * 2);\n              } else if (random().nextBoolean()) {\n                w.updateBinaryDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"bf\", TestBinaryDocValuesUpdates.toBytes(value));\n                w.updateBinaryDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"bcf\", TestBinaryDocValuesUpdates.toBytes(value * 2));\n              } else {\n                w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"f\", value);\n                w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"cf\", value * 2);\n                w.updateBinaryDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"bf\", TestBinaryDocValuesUpdates.toBytes(value));\n                w.updateBinaryDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"bcf\", TestBinaryDocValuesUpdates.toBytes(value * 2));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + (docBase+i));\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+(docBase+i)));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.close();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.close() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i, scratch), TestBinaryDocValuesUpdates.getValue(bf, i, scratch) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cd492efeeb99e4839e0a2d683894f2cc26ca1833","date":1399905171,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      int updatingDocID = -1;\n      long updatingValue = -1;\n      try {\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              updatingDocID = docid; // record that we're updating that document\n              updatingValue = value; // and its updating value\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateNumericDocValue(idTerm, \"f\", value);\n                w.updateNumericDocValue(idTerm, \"cf\", value * 2);\n              } else if (random().nextBoolean()) {\n                w.updateBinaryDocValue(idTerm, \"bf\", TestBinaryDocValuesUpdates.toBytes(value));\n                w.updateBinaryDocValue(idTerm, \"bcf\", TestBinaryDocValuesUpdates.toBytes(value * 2));\n              } else {\n                w.updateNumericDocValue(idTerm, \"f\", value);\n                w.updateNumericDocValue(idTerm, \"cf\", value * 2);\n                w.updateBinaryDocValue(idTerm, \"bf\", TestBinaryDocValuesUpdates.toBytes(value));\n                w.updateBinaryDocValue(idTerm, \"bcf\", TestBinaryDocValuesUpdates.toBytes(value * 2));\n              }\n              // record that we successfully updated the document. this is\n              // important when we later assert the value of the DV fields of\n              // that document - since we update two fields that depend on each\n              // other, could be that one of the fields successfully updates,\n              // while the other fails (since we turn on random exceptions).\n              // while this is supported, it makes the test raise false alarms.\n              updatingDocID = -1;\n              updatingValue = -1;\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.shutdown();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.shutdown() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      if (updatingDocID != -1) {\n        // Updating this document did not succeed. Since the fields we assert on\n        // depend on each other, and the update may have gone through halfway,\n        // replay the update on both numeric and binary DV fields, so later\n        // asserts succeed.\n        Term idTerm = new Term(\"id\", \"\"+updatingDocID);\n        w.updateNumericDocValue(idTerm, \"f\", updatingValue);\n        w.updateNumericDocValue(idTerm, \"cf\", updatingValue * 2);\n        w.updateBinaryDocValue(idTerm, \"bf\", TestBinaryDocValuesUpdates.toBytes(updatingValue));\n        w.updateBinaryDocValue(idTerm, \"bcf\", TestBinaryDocValuesUpdates.toBytes(updatingValue * 2));\n      }\n      \n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        try {\n          w.shutdown();\n        } catch (AlreadyClosedException ace) {\n          // OK\n        }\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i, scratch), TestBinaryDocValuesUpdates.getValue(bf, i, scratch) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.shutdown();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.shutdown();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n\n      try {\n\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + (docBase+i) + \" to value \" + value);\n              }\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"f\", value);\n                w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"cf\", value * 2);\n              } else if (random().nextBoolean()) {\n                w.updateBinaryDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"bf\", TestBinaryDocValuesUpdates.toBytes(value));\n                w.updateBinaryDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"bcf\", TestBinaryDocValuesUpdates.toBytes(value * 2));\n              } else {\n                w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"f\", value);\n                w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"cf\", value * 2);\n                w.updateBinaryDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"bf\", TestBinaryDocValuesUpdates.toBytes(value));\n                w.updateBinaryDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"bcf\", TestBinaryDocValuesUpdates.toBytes(value * 2));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + (docBase+i));\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+(docBase+i)));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.shutdown();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.shutdown() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        try {\n          w.shutdown();\n        } catch (AlreadyClosedException ace) {\n          // OK\n        }\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i, scratch), TestBinaryDocValuesUpdates.getValue(bf, i, scratch) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.shutdown();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.shutdown();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a957bf27202eab1c9ddabc5aa30c7a0db04bbf36","date":1400053604,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      int updatingDocID = -1;\n      long updatingValue = -1;\n      try {\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              updatingDocID = docid; // record that we're updating that document\n              updatingValue = value; // and its updating value\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateNumericDocValue(idTerm, \"f\", value);\n                w.updateNumericDocValue(idTerm, \"cf\", value * 2);\n              } else if (random().nextBoolean()) {\n                w.updateBinaryDocValue(idTerm, \"bf\", TestBinaryDocValuesUpdates.toBytes(value));\n                w.updateBinaryDocValue(idTerm, \"bcf\", TestBinaryDocValuesUpdates.toBytes(value * 2));\n              } else {\n                w.updateNumericDocValue(idTerm, \"f\", value);\n                w.updateNumericDocValue(idTerm, \"cf\", value * 2);\n                w.updateBinaryDocValue(idTerm, \"bf\", TestBinaryDocValuesUpdates.toBytes(value));\n                w.updateBinaryDocValue(idTerm, \"bcf\", TestBinaryDocValuesUpdates.toBytes(value * 2));\n              }\n              // record that we successfully updated the document. this is\n              // important when we later assert the value of the DV fields of\n              // that document - since we update two fields that depend on each\n              // other, could be that one of the fields successfully updates,\n              // while the other fails (since we turn on random exceptions).\n              // while this is supported, it makes the test raise false alarms.\n              updatingDocID = -1;\n              updatingValue = -1;\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.shutdown();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.shutdown() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      if (updatingDocID != -1) {\n        // Updating this document did not succeed. Since the fields we assert on\n        // depend on each other, and the update may have gone through halfway,\n        // replay the update on both numeric and binary DV fields, so later\n        // asserts succeed.\n        Term idTerm = new Term(\"id\", \"\"+updatingDocID);\n        w.updateNumericDocValue(idTerm, \"f\", updatingValue);\n        w.updateNumericDocValue(idTerm, \"cf\", updatingValue * 2);\n        w.updateBinaryDocValue(idTerm, \"bf\", TestBinaryDocValuesUpdates.toBytes(updatingValue));\n        w.updateBinaryDocValue(idTerm, \"bcf\", TestBinaryDocValuesUpdates.toBytes(updatingValue * 2));\n      }\n      \n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        try {\n          w.shutdown();\n        } catch (AlreadyClosedException ace) {\n          // OK\n        }\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i, scratch), TestBinaryDocValuesUpdates.getValue(bf, i, scratch) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.shutdown();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.shutdown();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n\n      try {\n\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + (docBase+i) + \" to value \" + value);\n              }\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"f\", value);\n                w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"cf\", value * 2);\n              } else if (random().nextBoolean()) {\n                w.updateBinaryDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"bf\", TestBinaryDocValuesUpdates.toBytes(value));\n                w.updateBinaryDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"bcf\", TestBinaryDocValuesUpdates.toBytes(value * 2));\n              } else {\n                w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"f\", value);\n                w.updateNumericDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"cf\", value * 2);\n                w.updateBinaryDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"bf\", TestBinaryDocValuesUpdates.toBytes(value));\n                w.updateBinaryDocValue(new Term(\"id\", Integer.toString(docBase + i)), \"bcf\", TestBinaryDocValuesUpdates.toBytes(value * 2));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + (docBase+i));\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+(docBase+i)));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.shutdown();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.shutdown() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        try {\n          w.shutdown();\n        } catch (AlreadyClosedException ace) {\n          // OK\n        }\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i, scratch), TestBinaryDocValuesUpdates.getValue(bf, i, scratch) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.shutdown();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.shutdown();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70285ef5917fa2c8feec026d4be4d9c20fa89162","date":1401366288,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.shutdown();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.shutdown() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        try {\n          w.shutdown();\n        } catch (AlreadyClosedException ace) {\n          // OK\n        }\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i, scratch), TestBinaryDocValuesUpdates.getValue(bf, i, scratch) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.shutdown();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.shutdown();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      int updatingDocID = -1;\n      long updatingValue = -1;\n      try {\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              updatingDocID = docid; // record that we're updating that document\n              updatingValue = value; // and its updating value\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateNumericDocValue(idTerm, \"f\", value);\n                w.updateNumericDocValue(idTerm, \"cf\", value * 2);\n              } else if (random().nextBoolean()) {\n                w.updateBinaryDocValue(idTerm, \"bf\", TestBinaryDocValuesUpdates.toBytes(value));\n                w.updateBinaryDocValue(idTerm, \"bcf\", TestBinaryDocValuesUpdates.toBytes(value * 2));\n              } else {\n                w.updateNumericDocValue(idTerm, \"f\", value);\n                w.updateNumericDocValue(idTerm, \"cf\", value * 2);\n                w.updateBinaryDocValue(idTerm, \"bf\", TestBinaryDocValuesUpdates.toBytes(value));\n                w.updateBinaryDocValue(idTerm, \"bcf\", TestBinaryDocValuesUpdates.toBytes(value * 2));\n              }\n              // record that we successfully updated the document. this is\n              // important when we later assert the value of the DV fields of\n              // that document - since we update two fields that depend on each\n              // other, could be that one of the fields successfully updates,\n              // while the other fails (since we turn on random exceptions).\n              // while this is supported, it makes the test raise false alarms.\n              updatingDocID = -1;\n              updatingValue = -1;\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.shutdown();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.shutdown() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      if (updatingDocID != -1) {\n        // Updating this document did not succeed. Since the fields we assert on\n        // depend on each other, and the update may have gone through halfway,\n        // replay the update on both numeric and binary DV fields, so later\n        // asserts succeed.\n        Term idTerm = new Term(\"id\", \"\"+updatingDocID);\n        w.updateNumericDocValue(idTerm, \"f\", updatingValue);\n        w.updateNumericDocValue(idTerm, \"cf\", updatingValue * 2);\n        w.updateBinaryDocValue(idTerm, \"bf\", TestBinaryDocValuesUpdates.toBytes(updatingValue));\n        w.updateBinaryDocValue(idTerm, \"bcf\", TestBinaryDocValuesUpdates.toBytes(updatingValue * 2));\n      }\n      \n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        try {\n          w.shutdown();\n        } catch (AlreadyClosedException ace) {\n          // OK\n        }\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i, scratch), TestBinaryDocValuesUpdates.getValue(bf, i, scratch) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.shutdown();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.shutdown();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf","date":1401983689,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.shutdown();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.shutdown() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        try {\n          w.shutdown();\n        } catch (AlreadyClosedException ace) {\n          // OK\n        }\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.shutdown();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.shutdown();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.shutdown();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.shutdown() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        try {\n          w.shutdown();\n        } catch (AlreadyClosedException ace) {\n          // OK\n        }\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i, scratch), TestBinaryDocValuesUpdates.getValue(bf, i, scratch) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.shutdown();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.shutdown();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.shutdown();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.shutdown() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        try {\n          w.shutdown();\n        } catch (AlreadyClosedException ace) {\n          // OK\n        }\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.shutdown();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.shutdown();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.shutdown();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.shutdown() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        try {\n          w.shutdown();\n        } catch (AlreadyClosedException ace) {\n          // OK\n        }\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.shutdown();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.shutdown();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.close() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        try {\n          w.close();\n        } catch (AlreadyClosedException ace) {\n          // OK\n        }\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.shutdown();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.shutdown() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        try {\n          w.shutdown();\n        } catch (AlreadyClosedException ace) {\n          // OK\n        }\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.shutdown();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.shutdown();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["865877758b369e6bdf9a485e27627ed6fbf391b0"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5ea6955c7bfffe05682ad0a667afb2a6fa85e9da","date":1408027865,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.close() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.close() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        try {\n          w.close();\n        } catch (AlreadyClosedException ace) {\n          // OK\n        }\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ad2a673349939e48652bf304cccf673c3412198f","date":1409585169,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.close() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.close() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"402ad3ddc9da7b70da1b167667a60ece6a1381fb","date":1409656478,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.close() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        boolean defaultCodecSupportsFieldUpdates = defaultCodecSupportsFieldUpdates();\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = defaultCodecSupportsFieldUpdates && random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.close() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","bugFix":["d2e0ff8328a5ae40b00d09c5ac85919aa46a75ba"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.close() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (LeafReaderContext context : r.leaves()) {\n        LeafReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.close() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (AtomicReaderContext context : r.leaves()) {\n        AtomicReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9ae6725e6ae382af525653dca26303eb0661c71f","date":1417051373,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        if (shouldFail.get() == false) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          return;\n        }\n        \n        if (random().nextInt(3) != 2) {\n          return;\n        }\n\n        StackTraceElement[] trace = Thread.currentThread().getStackTrace();\n\n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false) {\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.close() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (LeafReaderContext context : r.leaves()) {\n        LeafReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        StackTraceElement[] trace = new Exception().getStackTrace();\n        if (shouldFail.get() == false) {\n          return;\n        }\n        \n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false && random().nextInt(3) == 2) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.close() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (LeafReaderContext context : r.leaves()) {\n        LeafReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5faf65b6692f15cca0f87bf8666c87899afc619f","date":1420468108,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        if (shouldFail.get() == false) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          return;\n        }\n        \n        if (random().nextInt(3) != 2) {\n          return;\n        }\n\n        StackTraceElement[] trace = Thread.currentThread().getStackTrace();\n\n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false) {\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Directory dir, Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(dir, exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.close() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (LeafReaderContext context : r.leaves()) {\n        LeafReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        if (shouldFail.get() == false) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          return;\n        }\n        \n        if (random().nextInt(3) != 2) {\n          return;\n        }\n\n        StackTraceElement[] trace = Thread.currentThread().getStackTrace();\n\n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false) {\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          suppressFakeIOE.setMergeThreadPriority(cms.getMergeThreadPriority());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.close() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (LeafReaderContext context : r.leaves()) {\n        LeafReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["f83d8858d121721403246759848f7edf175cc704"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"865877758b369e6bdf9a485e27627ed6fbf391b0","date":1436878671,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Throwable {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        if (shouldFail.get() == false) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          return;\n        }\n        \n        if (random().nextInt(3) != 2) {\n          return;\n        }\n\n        StackTraceElement[] trace = Thread.currentThread().getStackTrace();\n\n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false) {\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    boolean tragic = false;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Directory dir, Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(dir, exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (Throwable t) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (t instanceof FakeIOException || (t.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: hit expected IOE\");\n          }\n          if (t instanceof AlreadyClosedException) {\n            // FakeIOExc struck during merge and writer is now closed:\n            w = null;\n            tragic = true;\n          }\n        } else {\n          throw t;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      if (tragic == false) {\n        assertEquals(docCount-deleteCount, r.numDocs());\n      }\n      BytesRef scratch = new BytesRef();\n      for (LeafReaderContext context : r.leaves()) {\n        LeafReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    if (tragic == false) {\n      IndexReader r = DirectoryReader.open(dir);\n      assertEquals(docCount-deleteCount, r.numDocs());\n      r.close();\n    }\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Exception {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        if (shouldFail.get() == false) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          return;\n        }\n        \n        if (random().nextInt(3) != 2) {\n          return;\n        }\n\n        StackTraceElement[] trace = Thread.currentThread().getStackTrace();\n\n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false) {\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Directory dir, Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(dir, exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (IOException ioe) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (ioe instanceof FakeIOException || (ioe.getCause() != null && ioe.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: w.close() hit expected IOE\");\n          }\n        } else {\n          throw ioe;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      assertEquals(docCount-deleteCount, r.numDocs());\n      BytesRef scratch = new BytesRef();\n      for (LeafReaderContext context : r.leaves()) {\n        LeafReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    IndexReader r = DirectoryReader.open(dir);\n    assertEquals(docCount-deleteCount, r.numDocs());\n    r.close();\n\n    dir.close();\n  }\n\n","bugFix":["d2e0ff8328a5ae40b00d09c5ac85919aa46a75ba","d0ef034a4f10871667ae75181537775ddcf8ade4","1bae040fd1d5e03e0d8d695a9c25cf4f402e7ffe"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f83d8858d121721403246759848f7edf175cc704","date":1442165977,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Throwable {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        if (shouldFail.get() == false) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          return;\n        }\n        \n        if (random().nextInt(3) != 2) {\n          return;\n        }\n\n        StackTraceElement[] trace = Thread.currentThread().getStackTrace();\n\n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false) {\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    boolean tragic = false;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (Throwable t) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (t instanceof FakeIOException || (t.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: hit expected IOE\");\n          }\n          if (t instanceof AlreadyClosedException) {\n            // FakeIOExc struck during merge and writer is now closed:\n            w = null;\n            tragic = true;\n          }\n        } else {\n          throw t;\n        }\n      }\n      shouldFail.set(false);\n\n      if (w != null) {\n        MergeScheduler ms = w.w.getConfig().getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          ((ConcurrentMergeScheduler) ms).sync();\n        }\n\n        if (w.w.getTragicException() != null) {\n          // Tragic exc in CMS closed the writer\n          w = null;\n        }\n      }\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      if (tragic == false) {\n        assertEquals(docCount-deleteCount, r.numDocs());\n      }\n      BytesRef scratch = new BytesRef();\n      for (LeafReaderContext context : r.leaves()) {\n        LeafReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    if (tragic == false) {\n      IndexReader r = DirectoryReader.open(dir);\n      assertEquals(docCount-deleteCount, r.numDocs());\n      r.close();\n    }\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Throwable {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        if (shouldFail.get() == false) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          return;\n        }\n        \n        if (random().nextInt(3) != 2) {\n          return;\n        }\n\n        StackTraceElement[] trace = Thread.currentThread().getStackTrace();\n\n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false) {\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    boolean tragic = false;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        final MergeScheduler ms = iwc.getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          final ConcurrentMergeScheduler suppressFakeIOE = new ConcurrentMergeScheduler() {\n              @Override\n              protected void handleMergeException(Directory dir, Throwable exc) {\n                // suppress only FakeIOException:\n                if (!(exc instanceof FakeIOException)) {\n                  super.handleMergeException(dir, exc);\n                }\n              }\n            };\n          final ConcurrentMergeScheduler cms = (ConcurrentMergeScheduler) ms;\n          suppressFakeIOE.setMaxMergesAndThreads(cms.getMaxMergeCount(), cms.getMaxThreadCount());\n          iwc.setMergeScheduler(suppressFakeIOE);\n        }\n        \n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (Throwable t) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (t instanceof FakeIOException || (t.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: hit expected IOE\");\n          }\n          if (t instanceof AlreadyClosedException) {\n            // FakeIOExc struck during merge and writer is now closed:\n            w = null;\n            tragic = true;\n          }\n        } else {\n          throw t;\n        }\n      }\n      shouldFail.set(false);\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      if (tragic == false) {\n        assertEquals(docCount-deleteCount, r.numDocs());\n      }\n      BytesRef scratch = new BytesRef();\n      for (LeafReaderContext context : r.leaves()) {\n        LeafReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    if (tragic == false) {\n      IndexReader r = DirectoryReader.open(dir);\n      assertEquals(docCount-deleteCount, r.numDocs());\n      r.close();\n    }\n\n    dir.close();\n  }\n\n","bugFix":["5faf65b6692f15cca0f87bf8666c87899afc619f","d2e0ff8328a5ae40b00d09c5ac85919aa46a75ba"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7b516a692d03225c8f0e81a13ceed2dc32bb457d","date":1453411951,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Throwable {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        if (shouldFail.get() == false) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          return;\n        }\n        \n        if (random().nextInt(3) != 2) {\n          return;\n        }\n\n        StackTraceElement[] trace = Thread.currentThread().getStackTrace();\n\n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false) {\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    boolean tragic = false;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader();\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (Throwable t) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (t instanceof FakeIOException || (t.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: hit expected IOE\");\n          }\n          if (t instanceof AlreadyClosedException) {\n            // FakeIOExc struck during merge and writer is now closed:\n            w = null;\n            tragic = true;\n          }\n        } else {\n          throw t;\n        }\n      }\n      shouldFail.set(false);\n\n      if (w != null) {\n        MergeScheduler ms = w.w.getConfig().getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          ((ConcurrentMergeScheduler) ms).sync();\n        }\n\n        if (w.w.getTragicException() != null) {\n          // Tragic exc in CMS closed the writer\n          w = null;\n        }\n      }\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      if (tragic == false) {\n        assertEquals(docCount-deleteCount, r.numDocs());\n      }\n      BytesRef scratch = new BytesRef();\n      for (LeafReaderContext context : r.leaves()) {\n        LeafReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    if (tragic == false) {\n      IndexReader r = DirectoryReader.open(dir);\n      assertEquals(docCount-deleteCount, r.numDocs());\n      r.close();\n    }\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Throwable {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        if (shouldFail.get() == false) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          return;\n        }\n        \n        if (random().nextInt(3) != 2) {\n          return;\n        }\n\n        StackTraceElement[] trace = Thread.currentThread().getStackTrace();\n\n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false) {\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    boolean tragic = false;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader(true);\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (Throwable t) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (t instanceof FakeIOException || (t.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: hit expected IOE\");\n          }\n          if (t instanceof AlreadyClosedException) {\n            // FakeIOExc struck during merge and writer is now closed:\n            w = null;\n            tragic = true;\n          }\n        } else {\n          throw t;\n        }\n      }\n      shouldFail.set(false);\n\n      if (w != null) {\n        MergeScheduler ms = w.w.getConfig().getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          ((ConcurrentMergeScheduler) ms).sync();\n        }\n\n        if (w.w.getTragicException() != null) {\n          // Tragic exc in CMS closed the writer\n          w = null;\n        }\n      }\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      if (tragic == false) {\n        assertEquals(docCount-deleteCount, r.numDocs());\n      }\n      BytesRef scratch = new BytesRef();\n      for (LeafReaderContext context : r.leaves()) {\n        LeafReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    if (tragic == false) {\n      IndexReader r = DirectoryReader.open(dir);\n      assertEquals(docCount-deleteCount, r.numDocs());\n      r.close();\n    }\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6652c74b2358a0b13223817a6a793bf1c9d0749d","date":1474465301,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Throwable {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        if (shouldFail.get() == false) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          return;\n        }\n        \n        if (random().nextInt(3) != 2) {\n          return;\n        }\n\n        StackTraceElement[] trace = Thread.currentThread().getStackTrace();\n\n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false) {\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    boolean tragic = false;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader();\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (Throwable t) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (t instanceof FakeIOException || (t.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: hit expected IOE\");\n          }\n          if (t instanceof AlreadyClosedException) {\n            // FakeIOExc struck during merge and writer is now closed:\n            w = null;\n            tragic = true;\n          }\n        } else {\n          throw t;\n        }\n      }\n      shouldFail.set(false);\n\n      if (w != null) {\n        MergeScheduler ms = w.w.getConfig().getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          ((ConcurrentMergeScheduler) ms).sync();\n        }\n\n        if (w.w.getTragicException() != null) {\n          // Tragic exc in CMS closed the writer\n          w = null;\n        }\n      }\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      if (tragic == false) {\n        assertEquals(docCount-deleteCount, r.numDocs());\n      }\n      BytesRef scratch = new BytesRef();\n      for (LeafReaderContext context : r.leaves()) {\n        LeafReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(i, f.advance(i));\n            assertEquals(i, cf.advance(i));\n            assertEquals(i, bf.advance(i));\n            assertEquals(i, bcf.advance(i));\n            assertEquals(\"doc=\" + (docBase + i), cf.longValue(), f.longValue() * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf), TestBinaryDocValuesUpdates.getValue(bf) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    if (tragic == false) {\n      IndexReader r = DirectoryReader.open(dir);\n      assertEquals(docCount-deleteCount, r.numDocs());\n      r.close();\n    }\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Throwable {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        if (shouldFail.get() == false) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          return;\n        }\n        \n        if (random().nextInt(3) != 2) {\n          return;\n        }\n\n        StackTraceElement[] trace = Thread.currentThread().getStackTrace();\n\n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false) {\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    boolean tragic = false;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader();\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (Throwable t) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (t instanceof FakeIOException || (t.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: hit expected IOE\");\n          }\n          if (t instanceof AlreadyClosedException) {\n            // FakeIOExc struck during merge and writer is now closed:\n            w = null;\n            tragic = true;\n          }\n        } else {\n          throw t;\n        }\n      }\n      shouldFail.set(false);\n\n      if (w != null) {\n        MergeScheduler ms = w.w.getConfig().getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          ((ConcurrentMergeScheduler) ms).sync();\n        }\n\n        if (w.w.getTragicException() != null) {\n          // Tragic exc in CMS closed the writer\n          w = null;\n        }\n      }\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      if (tragic == false) {\n        assertEquals(docCount-deleteCount, r.numDocs());\n      }\n      BytesRef scratch = new BytesRef();\n      for (LeafReaderContext context : r.leaves()) {\n        LeafReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    if (tragic == false) {\n      IndexReader r = DirectoryReader.open(dir);\n      assertEquals(docCount-deleteCount, r.numDocs());\n      r.close();\n    }\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Throwable {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        if (shouldFail.get() == false) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          return;\n        }\n        \n        if (random().nextInt(3) != 2) {\n          return;\n        }\n\n        StackTraceElement[] trace = Thread.currentThread().getStackTrace();\n\n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false) {\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    boolean tragic = false;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader();\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (Throwable t) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (t instanceof FakeIOException || (t.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: hit expected IOE\");\n          }\n          if (t instanceof AlreadyClosedException) {\n            // FakeIOExc struck during merge and writer is now closed:\n            w = null;\n            tragic = true;\n          }\n        } else {\n          throw t;\n        }\n      }\n      shouldFail.set(false);\n\n      if (w != null) {\n        MergeScheduler ms = w.w.getConfig().getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          ((ConcurrentMergeScheduler) ms).sync();\n        }\n\n        if (w.w.getTragicException() != null) {\n          // Tragic exc in CMS closed the writer\n          w = null;\n        }\n      }\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      if (tragic == false) {\n        assertEquals(docCount-deleteCount, r.numDocs());\n      }\n      BytesRef scratch = new BytesRef();\n      for (LeafReaderContext context : r.leaves()) {\n        LeafReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(i, f.advance(i));\n            assertEquals(i, cf.advance(i));\n            assertEquals(i, bf.advance(i));\n            assertEquals(i, bcf.advance(i));\n            assertEquals(\"doc=\" + (docBase + i), cf.longValue(), f.longValue() * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf), TestBinaryDocValuesUpdates.getValue(bf) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    if (tragic == false) {\n      IndexReader r = DirectoryReader.open(dir);\n      assertEquals(docCount-deleteCount, r.numDocs());\n      r.close();\n    }\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Throwable {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        if (shouldFail.get() == false) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          return;\n        }\n        \n        if (random().nextInt(3) != 2) {\n          return;\n        }\n\n        StackTraceElement[] trace = Thread.currentThread().getStackTrace();\n\n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false) {\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    boolean tragic = false;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader();\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (Throwable t) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (t instanceof FakeIOException || (t.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: hit expected IOE\");\n          }\n          if (t instanceof AlreadyClosedException) {\n            // FakeIOExc struck during merge and writer is now closed:\n            w = null;\n            tragic = true;\n          }\n        } else {\n          throw t;\n        }\n      }\n      shouldFail.set(false);\n\n      if (w != null) {\n        MergeScheduler ms = w.w.getConfig().getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          ((ConcurrentMergeScheduler) ms).sync();\n        }\n\n        if (w.w.getTragicException() != null) {\n          // Tragic exc in CMS closed the writer\n          w = null;\n        }\n      }\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      if (tragic == false) {\n        assertEquals(docCount-deleteCount, r.numDocs());\n      }\n      BytesRef scratch = new BytesRef();\n      for (LeafReaderContext context : r.leaves()) {\n        LeafReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    if (tragic == false) {\n      IndexReader r = DirectoryReader.open(dir);\n      assertEquals(docCount-deleteCount, r.numDocs());\n      r.close();\n    }\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Throwable {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        if (shouldFail.get() == false) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          return;\n        }\n        \n        if (random().nextInt(3) != 2) {\n          return;\n        }\n\n        StackTraceElement[] trace = Thread.currentThread().getStackTrace();\n\n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false) {\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    boolean tragic = false;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader();\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (Throwable t) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (t instanceof FakeIOException || (t.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: hit expected IOE\");\n          }\n          if (t instanceof AlreadyClosedException) {\n            // FakeIOExc struck during merge and writer is now closed:\n            w = null;\n            tragic = true;\n          }\n        } else {\n          throw t;\n        }\n      }\n      shouldFail.set(false);\n\n      if (w != null) {\n        MergeScheduler ms = w.w.getConfig().getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          ((ConcurrentMergeScheduler) ms).sync();\n        }\n\n        if (w.w.getTragicException() != null) {\n          // Tragic exc in CMS closed the writer\n          w = null;\n        }\n      }\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      if (tragic == false) {\n        assertEquals(docCount-deleteCount, r.numDocs());\n      }\n      BytesRef scratch = new BytesRef();\n      for (LeafReaderContext context : r.leaves()) {\n        LeafReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(i, f.advance(i));\n            assertEquals(i, cf.advance(i));\n            assertEquals(i, bf.advance(i));\n            assertEquals(i, bcf.advance(i));\n            assertEquals(\"doc=\" + (docBase + i), cf.longValue(), f.longValue() * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf), TestBinaryDocValuesUpdates.getValue(bf) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    if (tragic == false) {\n      IndexReader r = DirectoryReader.open(dir);\n      assertEquals(docCount-deleteCount, r.numDocs());\n      r.close();\n    }\n\n    dir.close();\n  }\n\n","sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Throwable {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        if (shouldFail.get() == false) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          return;\n        }\n        \n        if (random().nextInt(3) != 2) {\n          return;\n        }\n\n        StackTraceElement[] trace = Thread.currentThread().getStackTrace();\n\n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false) {\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    boolean tragic = false;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader();\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (Throwable t) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (t instanceof FakeIOException || (t.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: hit expected IOE\");\n          }\n          if (t instanceof AlreadyClosedException) {\n            // FakeIOExc struck during merge and writer is now closed:\n            w = null;\n            tragic = true;\n          }\n        } else {\n          throw t;\n        }\n      }\n      shouldFail.set(false);\n\n      if (w != null) {\n        MergeScheduler ms = w.w.getConfig().getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          ((ConcurrentMergeScheduler) ms).sync();\n        }\n\n        if (w.w.getTragicException() != null) {\n          // Tragic exc in CMS closed the writer\n          w = null;\n        }\n      }\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      if (tragic == false) {\n        assertEquals(docCount-deleteCount, r.numDocs());\n      }\n      BytesRef scratch = new BytesRef();\n      for (LeafReaderContext context : r.leaves()) {\n        LeafReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(\"doc=\" + (docBase + i), cf.get(i), f.get(i) * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf, i), TestBinaryDocValuesUpdates.getValue(bf, i) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    if (tragic == false) {\n      IndexReader r = DirectoryReader.open(dir);\n      assertEquals(docCount-deleteCount, r.numDocs());\n      r.close();\n    }\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":4,"author":"Mike McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":null,"sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Throwable {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        if (shouldFail.get() == false) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          return;\n        }\n        \n        if (random().nextInt(3) != 2) {\n          return;\n        }\n\n        StackTraceElement[] trace = Thread.currentThread().getStackTrace();\n\n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false) {\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    boolean tragic = false;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader();\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (Throwable t) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (t instanceof FakeIOException || (t.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: hit expected IOE\");\n          }\n          if (t instanceof AlreadyClosedException) {\n            // FakeIOExc struck during merge and writer is now closed:\n            w = null;\n            tragic = true;\n          }\n        } else {\n          throw t;\n        }\n      }\n      shouldFail.set(false);\n\n      if (w != null) {\n        MergeScheduler ms = w.w.getConfig().getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          ((ConcurrentMergeScheduler) ms).sync();\n        }\n\n        if (w.w.getTragicException() != null) {\n          // Tragic exc in CMS closed the writer\n          w = null;\n        }\n      }\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      if (tragic == false) {\n        assertEquals(docCount-deleteCount, r.numDocs());\n      }\n      BytesRef scratch = new BytesRef();\n      for (LeafReaderContext context : r.leaves()) {\n        LeafReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(i, f.advance(i));\n            assertEquals(i, cf.advance(i));\n            assertEquals(i, bf.advance(i));\n            assertEquals(i, bcf.advance(i));\n            assertEquals(\"doc=\" + (docBase + i), cf.longValue(), f.longValue() * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf), TestBinaryDocValuesUpdates.getValue(bf) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    if (tragic == false) {\n      IndexReader r = DirectoryReader.open(dir);\n      assertEquals(docCount-deleteCount, r.numDocs());\n      r.close();\n    }\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":4,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":null,"sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Throwable {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        if (shouldFail.get() == false) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          return;\n        }\n        \n        if (random().nextInt(3) != 2) {\n          return;\n        }\n\n        StackTraceElement[] trace = Thread.currentThread().getStackTrace();\n\n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false) {\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    boolean tragic = false;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader();\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (Throwable t) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (t instanceof FakeIOException || (t.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: hit expected IOE\");\n          }\n          if (t instanceof AlreadyClosedException) {\n            // FakeIOExc struck during merge and writer is now closed:\n            w = null;\n            tragic = true;\n          }\n        } else {\n          throw t;\n        }\n      }\n      shouldFail.set(false);\n\n      if (w != null) {\n        MergeScheduler ms = w.w.getConfig().getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          ((ConcurrentMergeScheduler) ms).sync();\n        }\n\n        if (w.w.getTragicException() != null) {\n          // Tragic exc in CMS closed the writer\n          w = null;\n        }\n      }\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      if (tragic == false) {\n        assertEquals(docCount-deleteCount, r.numDocs());\n      }\n      BytesRef scratch = new BytesRef();\n      for (LeafReaderContext context : r.leaves()) {\n        LeafReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(i, f.advance(i));\n            assertEquals(i, cf.advance(i));\n            assertEquals(i, bf.advance(i));\n            assertEquals(i, bcf.advance(i));\n            assertEquals(\"doc=\" + (docBase + i), cf.longValue(), f.longValue() * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf), TestBinaryDocValuesUpdates.getValue(bf) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    if (tragic == false) {\n      IndexReader r = DirectoryReader.open(dir);\n      assertEquals(docCount-deleteCount, r.numDocs());\n      r.close();\n    }\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":4,"author":"Karl Wright","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testNoLostDeletesOrUpdates().mjava","sourceNew":null,"sourceOld":"  // Make sure if we hit a transient IOException (e.g., disk\n  // full), and then the exception stops (e.g., disk frees\n  // up), so we successfully close IW or open an NRT\n  // reader, we don't lose any deletes or updates:\n  public void testNoLostDeletesOrUpdates() throws Throwable {\n    int deleteCount = 0;\n    int docBase = 0;\n    int docCount = 0;\n\n    MockDirectoryWrapper dir = newMockDirectory();\n    final AtomicBoolean shouldFail = new AtomicBoolean();\n    dir.failOn(new MockDirectoryWrapper.Failure() {\n      \n      @Override\n      public void eval(MockDirectoryWrapper dir) throws IOException {\n        if (shouldFail.get() == false) {\n          // Only sometimes throw the exc, so we get\n          // it sometimes on creating the file, on\n          // flushing buffer, on closing the file:\n          return;\n        }\n        \n        if (random().nextInt(3) != 2) {\n          return;\n        }\n\n        StackTraceElement[] trace = Thread.currentThread().getStackTrace();\n\n        boolean sawSeal = false;\n        boolean sawWrite = false;\n        for (int i = 0; i < trace.length; i++) {\n          if (\"sealFlushedSegment\".equals(trace[i].getMethodName())) {\n            sawSeal = true;\n            break;\n          }\n          if (\"writeLiveDocs\".equals(trace[i].getMethodName()) || \"writeFieldUpdates\".equals(trace[i].getMethodName())) {\n            sawWrite = true;\n          }\n        }\n        \n        // Don't throw exc if we are \"flushing\", else\n        // the segment is aborted and docs are lost:\n        if (sawWrite && sawSeal == false) {\n          if (VERBOSE) {\n            System.out.println(\"TEST: now fail; thread=\" + Thread.currentThread().getName() + \" exc:\");\n            new Throwable().printStackTrace(System.out);\n          }\n          shouldFail.set(false);\n          throw new FakeIOException();\n        }\n      }\n    });\n    \n    RandomIndexWriter w = null;\n\n    boolean tragic = false;\n\n    for(int iter=0;iter<10*RANDOM_MULTIPLIER;iter++) {\n      int numDocs = atLeast(100);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" numDocs=\" + numDocs + \" docBase=\" + docBase + \" delCount=\" + deleteCount);\n      }\n      if (w == null) {\n        IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n        w = new RandomIndexWriter(random(), dir, iwc);\n        // Since we hit exc during merging, a partial\n        // forceMerge can easily return when there are still\n        // too many segments in the index:\n        w.setDoRandomForceMergeAssert(false);\n      }\n      for(int i=0;i<numDocs;i++) {\n        Document doc = new Document();\n        doc.add(new StringField(\"id\", \"\"+(docBase+i), Field.Store.NO));\n        doc.add(new NumericDocValuesField(\"f\", 1L));\n        doc.add(new NumericDocValuesField(\"cf\", 2L));\n        doc.add(new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(1L)));\n        doc.add(new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(2L)));\n        w.addDocument(doc);\n      }\n      docCount += numDocs;\n\n      // TODO: we could make the test more evil, by letting\n      // it throw more than one exc, randomly, before \"recovering\"\n\n      // TODO: we could also install an infoStream and try\n      // to fail in \"more evil\" places inside BDS\n\n      shouldFail.set(true);\n      boolean doClose = false;\n      try {\n        for(int i=0;i<numDocs;i++) {\n          if (random().nextInt(10) == 7) {\n            boolean fieldUpdate = random().nextBoolean();\n            int docid = docBase + i;\n            if (fieldUpdate) {\n              long value = iter;\n              if (VERBOSE) {\n                System.out.println(\"  update id=\" + docid + \" to value \" + value);\n              }\n              Term idTerm = new Term(\"id\", Integer.toString(docid));\n              if (random().nextBoolean()) { // update only numeric field\n                w.updateDocValues(idTerm, new NumericDocValuesField(\"f\", value), new NumericDocValuesField(\"cf\", value*2));\n              } else if (random().nextBoolean()) {\n                w.updateDocValues(idTerm, new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              } else {\n                w.updateDocValues(idTerm, \n                    new NumericDocValuesField(\"f\", value), \n                    new NumericDocValuesField(\"cf\", value*2),\n                    new BinaryDocValuesField(\"bf\", TestBinaryDocValuesUpdates.toBytes(value)),\n                    new BinaryDocValuesField(\"bcf\", TestBinaryDocValuesUpdates.toBytes(value*2)));\n              }\n            }\n            \n            // sometimes do both deletes and updates\n            if (!fieldUpdate || random().nextBoolean()) {\n              if (VERBOSE) {\n                System.out.println(\"  delete id=\" + docid);\n              }\n              deleteCount++;\n              w.deleteDocuments(new Term(\"id\", \"\"+docid));\n            }\n          }\n        }\n\n        // Trigger writeLiveDocs + writeFieldUpdates so we hit fake exc:\n        IndexReader r = w.getReader();\n\n        // Sometimes we will make it here (we only randomly\n        // throw the exc):\n        assertEquals(docCount-deleteCount, r.numDocs());\n        r.close();\n        \n        // Sometimes close, so the disk full happens on close:\n        if (random().nextBoolean()) {\n          if (VERBOSE) {\n            System.out.println(\"  now close writer\");\n          }\n          doClose = true;\n          w.commit();\n          w.close();\n          w = null;\n        }\n\n      } catch (Throwable t) {\n        // FakeIOException can be thrown from mergeMiddle, in which case IW\n        // registers it before our CMS gets to suppress it. IW.forceMerge later\n        // throws it as a wrapped IOE, so don't fail in this case.\n        if (t instanceof FakeIOException || (t.getCause() instanceof FakeIOException)) {\n          // expected\n          if (VERBOSE) {\n            System.out.println(\"TEST: hit expected IOE\");\n          }\n          if (t instanceof AlreadyClosedException) {\n            // FakeIOExc struck during merge and writer is now closed:\n            w = null;\n            tragic = true;\n          }\n        } else {\n          throw t;\n        }\n      }\n      shouldFail.set(false);\n\n      if (w != null) {\n        MergeScheduler ms = w.w.getConfig().getMergeScheduler();\n        if (ms instanceof ConcurrentMergeScheduler) {\n          ((ConcurrentMergeScheduler) ms).sync();\n        }\n\n        if (w.w.getTragicException() != null) {\n          // Tragic exc in CMS closed the writer\n          w = null;\n        }\n      }\n\n      IndexReader r;\n\n      if (doClose && w != null) {\n        if (VERBOSE) {\n          System.out.println(\"  now 2nd close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      if (w == null || random().nextBoolean()) {\n        // Open non-NRT reader, to make sure the \"on\n        // disk\" bits are good:\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against non-NRT reader\");\n        }\n        if (w != null) {\n          w.commit();\n        }\n        r = DirectoryReader.open(dir);\n      } else {\n        if (VERBOSE) {\n          System.out.println(\"TEST: verify against NRT reader\");\n        }\n        r = w.getReader();\n      }\n      if (tragic == false) {\n        assertEquals(docCount-deleteCount, r.numDocs());\n      }\n      BytesRef scratch = new BytesRef();\n      for (LeafReaderContext context : r.leaves()) {\n        LeafReader reader = context.reader();\n        Bits liveDocs = reader.getLiveDocs();\n        NumericDocValues f = reader.getNumericDocValues(\"f\");\n        NumericDocValues cf = reader.getNumericDocValues(\"cf\");\n        BinaryDocValues bf = reader.getBinaryDocValues(\"bf\");\n        BinaryDocValues bcf = reader.getBinaryDocValues(\"bcf\");\n        for (int i = 0; i < reader.maxDoc(); i++) {\n          if (liveDocs == null || liveDocs.get(i)) {\n            assertEquals(i, f.advance(i));\n            assertEquals(i, cf.advance(i));\n            assertEquals(i, bf.advance(i));\n            assertEquals(i, bcf.advance(i));\n            assertEquals(\"doc=\" + (docBase + i), cf.longValue(), f.longValue() * 2);\n            assertEquals(\"doc=\" + (docBase + i), TestBinaryDocValuesUpdates.getValue(bcf), TestBinaryDocValuesUpdates.getValue(bf) * 2);\n          }\n        }\n      }\n\n      r.close();\n\n      // Sometimes re-use RIW, other times open new one:\n      if (w != null && random().nextBoolean()) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: close writer\");\n        }\n        w.close();\n        w = null;\n      }\n\n      docBase += numDocs;\n    }\n\n    if (w != null) {\n      w.close();\n    }\n\n    // Final verify:\n    if (tragic == false) {\n      IndexReader r = DirectoryReader.open(dir);\n      assertEquals(docCount-deleteCount, r.numDocs());\n      r.close();\n    }\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"06805da26538ed636bd89b10c2699cc3834032ae":["1bae040fd1d5e03e0d8d695a9c25cf4f402e7ffe"],"cd492efeeb99e4839e0a2d683894f2cc26ca1833":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["402ad3ddc9da7b70da1b167667a60ece6a1381fb"],"5ea6955c7bfffe05682ad0a667afb2a6fa85e9da":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"1bae040fd1d5e03e0d8d695a9c25cf4f402e7ffe":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["7b516a692d03225c8f0e81a13ceed2dc32bb457d","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"865877758b369e6bdf9a485e27627ed6fbf391b0":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["06805da26538ed636bd89b10c2699cc3834032ae"],"a957bf27202eab1c9ddabc5aa30c7a0db04bbf36":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a","cd492efeeb99e4839e0a2d683894f2cc26ca1833"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf"],"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf":["70285ef5917fa2c8feec026d4be4d9c20fa89162"],"7b516a692d03225c8f0e81a13ceed2dc32bb457d":["f83d8858d121721403246759848f7edf175cc704"],"9ae6725e6ae382af525653dca26303eb0661c71f":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"f83d8858d121721403246759848f7edf175cc704":["865877758b369e6bdf9a485e27627ed6fbf391b0"],"70285ef5917fa2c8feec026d4be4d9c20fa89162":["cd492efeeb99e4839e0a2d683894f2cc26ca1833"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"28288370235ed02234a64753cdbf0c6ec096304a":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["7b516a692d03225c8f0e81a13ceed2dc32bb457d","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"402ad3ddc9da7b70da1b167667a60ece6a1381fb":["5ea6955c7bfffe05682ad0a667afb2a6fa85e9da","ad2a673349939e48652bf304cccf673c3412198f"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["7b516a692d03225c8f0e81a13ceed2dc32bb457d"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["9ae6725e6ae382af525653dca26303eb0661c71f"],"ad2a673349939e48652bf304cccf673c3412198f":["5ea6955c7bfffe05682ad0a667afb2a6fa85e9da"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["28288370235ed02234a64753cdbf0c6ec096304a"]},"commit2Childs":{"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"06805da26538ed636bd89b10c2699cc3834032ae":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"cd492efeeb99e4839e0a2d683894f2cc26ca1833":["a957bf27202eab1c9ddabc5aa30c7a0db04bbf36","70285ef5917fa2c8feec026d4be4d9c20fa89162"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["9ae6725e6ae382af525653dca26303eb0661c71f"],"5ea6955c7bfffe05682ad0a667afb2a6fa85e9da":["402ad3ddc9da7b70da1b167667a60ece6a1381fb","ad2a673349939e48652bf304cccf673c3412198f"],"1bae040fd1d5e03e0d8d695a9c25cf4f402e7ffe":["06805da26538ed636bd89b10c2699cc3834032ae"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1bae040fd1d5e03e0d8d695a9c25cf4f402e7ffe"],"865877758b369e6bdf9a485e27627ed6fbf391b0":["f83d8858d121721403246759848f7edf175cc704"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["cd492efeeb99e4839e0a2d683894f2cc26ca1833","a957bf27202eab1c9ddabc5aa30c7a0db04bbf36"],"a957bf27202eab1c9ddabc5aa30c7a0db04bbf36":[],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"7b516a692d03225c8f0e81a13ceed2dc32bb457d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"9ae6725e6ae382af525653dca26303eb0661c71f":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"f83d8858d121721403246759848f7edf175cc704":["7b516a692d03225c8f0e81a13ceed2dc32bb457d"],"70285ef5917fa2c8feec026d4be4d9c20fa89162":["53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"28288370235ed02234a64753cdbf0c6ec096304a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"402ad3ddc9da7b70da1b167667a60ece6a1381fb":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["5ea6955c7bfffe05682ad0a667afb2a6fa85e9da"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["865877758b369e6bdf9a485e27627ed6fbf391b0"],"ad2a673349939e48652bf304cccf673c3412198f":["402ad3ddc9da7b70da1b167667a60ece6a1381fb"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a957bf27202eab1c9ddabc5aa30c7a0db04bbf36","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}