{"path":"lucene/src/test/org/apache/lucene/document/TestDocument#testTransitionAPI().mjava","commits":[{"id":"fa0f44f887719e97183771e977cfc4bfb485b766","date":1326668713,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/document/TestDocument#testTransitionAPI().mjava","pathOld":"/dev/null","sourceNew":"  // LUCENE-3682\n  public void testTransitionAPI() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n\n    Document doc = new Document();\n    doc.add(new Field(\"stored\", \"abc\", Field.Store.YES, Field.Index.NO));\n    doc.add(new Field(\"stored_indexed\", \"abc xyz\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"stored_tokenized\", \"abc xyz\", Field.Store.YES, Field.Index.ANALYZED));\n    doc.add(new Field(\"indexed\", \"abc xyz\", Field.Store.NO, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"tokenized\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED));\n    doc.add(new Field(\"tokenized_reader\", new StringReader(\"abc xyz\")));\n    doc.add(new Field(\"tokenized_tokenstream\", w.w.getAnalyzer().tokenStream(\"tokenized_tokenstream\", new StringReader(\"abc xyz\"))));\n    doc.add(new Field(\"binary\", new byte[10]));\n    doc.add(new Field(\"tv\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"tv_pos\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"tv_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n    doc.add(new Field(\"tv_pos_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    doc = r.document(0);\n    // 4 stored fields\n    assertEquals(4, doc.getFields().size());\n    assertEquals(\"abc\", doc.get(\"stored\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_indexed\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_tokenized\"));\n    final BytesRef br = doc.getBinaryValue(\"binary\");\n    assertNotNull(br);\n    assertEquals(10, br.length);\n\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"xyz\")), 1).totalHits);\n\n    for(String field : new String[] {\"tv\", \"tv_pos\", \"tv_off\", \"tv_pos_off\"}) {\n      Fields tvFields = r.getTermVectors(0);\n      Terms tvs = tvFields.terms(field);\n      assertNotNull(tvs);\n      assertEquals(2, tvs.getUniqueTermCount());\n      TermsEnum tvsEnum = tvs.iterator(null);\n      assertEquals(new BytesRef(\"abc\"), tvsEnum.next());\n      final DocsAndPositionsEnum dpEnum = tvsEnum.docsAndPositions(null, null);\n      if (field.equals(\"tv\")) {\n        assertNull(dpEnum);\n      } else {\n        assertNotNull(dpEnum);\n      }\n      assertEquals(new BytesRef(\"xyz\"), tvsEnum.next());\n      assertNull(tvsEnum.next());\n    }\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31f025ae60076ae95274433f3fe8e6ace2857a87","date":1326669465,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/document/TestDocument#testTransitionAPI().mjava","pathOld":"lucene/src/test/org/apache/lucene/document/TestDocument#testTransitionAPI().mjava","sourceNew":"  // LUCENE-3682\n  public void testTransitionAPI() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n\n    Document doc = new Document();\n    doc.add(new Field(\"stored\", \"abc\", Field.Store.YES, Field.Index.NO));\n    doc.add(new Field(\"stored_indexed\", \"abc xyz\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"stored_tokenized\", \"abc xyz\", Field.Store.YES, Field.Index.ANALYZED));\n    doc.add(new Field(\"indexed\", \"abc xyz\", Field.Store.NO, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"tokenized\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED));\n    doc.add(new Field(\"tokenized_reader\", new StringReader(\"abc xyz\")));\n    doc.add(new Field(\"tokenized_tokenstream\", w.w.getAnalyzer().tokenStream(\"tokenized_tokenstream\", new StringReader(\"abc xyz\"))));\n    doc.add(new Field(\"binary\", new byte[10]));\n    doc.add(new Field(\"tv\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"tv_pos\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"tv_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n    doc.add(new Field(\"tv_pos_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    doc = r.document(0);\n    // 4 stored fields\n    assertEquals(4, doc.getFields().size());\n    assertEquals(\"abc\", doc.get(\"stored\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_indexed\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_tokenized\"));\n    final BytesRef br = doc.getBinaryValue(\"binary\");\n    assertNotNull(br);\n    assertEquals(10, br.length);\n\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"xyz\")), 1).totalHits);\n\n    for(String field : new String[] {\"tv\", \"tv_pos\", \"tv_off\", \"tv_pos_off\"}) {\n      Fields tvFields = r.getTermVectors(0);\n      Terms tvs = tvFields.terms(field);\n      assertNotNull(tvs);\n      assertEquals(2, tvs.getUniqueTermCount());\n      TermsEnum tvsEnum = tvs.iterator(null);\n      assertEquals(new BytesRef(\"abc\"), tvsEnum.next());\n      final DocsAndPositionsEnum dpEnum = tvsEnum.docsAndPositions(null, null, false);\n      if (field.equals(\"tv\")) {\n        assertNull(dpEnum);\n      } else {\n        assertNotNull(dpEnum);\n      }\n      assertEquals(new BytesRef(\"xyz\"), tvsEnum.next());\n      assertNull(tvsEnum.next());\n    }\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3682\n  public void testTransitionAPI() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n\n    Document doc = new Document();\n    doc.add(new Field(\"stored\", \"abc\", Field.Store.YES, Field.Index.NO));\n    doc.add(new Field(\"stored_indexed\", \"abc xyz\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"stored_tokenized\", \"abc xyz\", Field.Store.YES, Field.Index.ANALYZED));\n    doc.add(new Field(\"indexed\", \"abc xyz\", Field.Store.NO, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"tokenized\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED));\n    doc.add(new Field(\"tokenized_reader\", new StringReader(\"abc xyz\")));\n    doc.add(new Field(\"tokenized_tokenstream\", w.w.getAnalyzer().tokenStream(\"tokenized_tokenstream\", new StringReader(\"abc xyz\"))));\n    doc.add(new Field(\"binary\", new byte[10]));\n    doc.add(new Field(\"tv\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"tv_pos\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"tv_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n    doc.add(new Field(\"tv_pos_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    doc = r.document(0);\n    // 4 stored fields\n    assertEquals(4, doc.getFields().size());\n    assertEquals(\"abc\", doc.get(\"stored\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_indexed\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_tokenized\"));\n    final BytesRef br = doc.getBinaryValue(\"binary\");\n    assertNotNull(br);\n    assertEquals(10, br.length);\n\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"xyz\")), 1).totalHits);\n\n    for(String field : new String[] {\"tv\", \"tv_pos\", \"tv_off\", \"tv_pos_off\"}) {\n      Fields tvFields = r.getTermVectors(0);\n      Terms tvs = tvFields.terms(field);\n      assertNotNull(tvs);\n      assertEquals(2, tvs.getUniqueTermCount());\n      TermsEnum tvsEnum = tvs.iterator(null);\n      assertEquals(new BytesRef(\"abc\"), tvsEnum.next());\n      final DocsAndPositionsEnum dpEnum = tvsEnum.docsAndPositions(null, null);\n      if (field.equals(\"tv\")) {\n        assertNull(dpEnum);\n      } else {\n        assertNotNull(dpEnum);\n      }\n      assertEquals(new BytesRef(\"xyz\"), tvsEnum.next());\n      assertNull(tvsEnum.next());\n    }\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/document/TestDocument#testTransitionAPI().mjava","pathOld":"lucene/src/test/org/apache/lucene/document/TestDocument#testTransitionAPI().mjava","sourceNew":"  // LUCENE-3682\n  public void testTransitionAPI() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n\n    Document doc = new Document();\n    doc.add(new Field(\"stored\", \"abc\", Field.Store.YES, Field.Index.NO));\n    doc.add(new Field(\"stored_indexed\", \"abc xyz\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"stored_tokenized\", \"abc xyz\", Field.Store.YES, Field.Index.ANALYZED));\n    doc.add(new Field(\"indexed\", \"abc xyz\", Field.Store.NO, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"tokenized\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED));\n    doc.add(new Field(\"tokenized_reader\", new StringReader(\"abc xyz\")));\n    doc.add(new Field(\"tokenized_tokenstream\", w.w.getAnalyzer().tokenStream(\"tokenized_tokenstream\", new StringReader(\"abc xyz\"))));\n    doc.add(new Field(\"binary\", new byte[10]));\n    doc.add(new Field(\"tv\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"tv_pos\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"tv_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n    doc.add(new Field(\"tv_pos_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    doc = r.document(0);\n    // 4 stored fields\n    assertEquals(4, doc.getFields().size());\n    assertEquals(\"abc\", doc.get(\"stored\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_indexed\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_tokenized\"));\n    final BytesRef br = doc.getBinaryValue(\"binary\");\n    assertNotNull(br);\n    assertEquals(10, br.length);\n\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"xyz\")), 1).totalHits);\n\n    for(String field : new String[] {\"tv\", \"tv_pos\", \"tv_off\", \"tv_pos_off\"}) {\n      Fields tvFields = r.getTermVectors(0);\n      Terms tvs = tvFields.terms(field);\n      assertNotNull(tvs);\n      assertEquals(2, tvs.getUniqueTermCount());\n      TermsEnum tvsEnum = tvs.iterator(null);\n      assertEquals(new BytesRef(\"abc\"), tvsEnum.next());\n      final DocsAndPositionsEnum dpEnum = tvsEnum.docsAndPositions(null, null, false);\n      if (field.equals(\"tv\")) {\n        assertNull(dpEnum);\n      } else {\n        assertNotNull(dpEnum);\n      }\n      assertEquals(new BytesRef(\"xyz\"), tvsEnum.next());\n      assertNull(tvsEnum.next());\n    }\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3682\n  public void testTransitionAPI() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n\n    Document doc = new Document();\n    doc.add(new Field(\"stored\", \"abc\", Field.Store.YES, Field.Index.NO));\n    doc.add(new Field(\"stored_indexed\", \"abc xyz\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"stored_tokenized\", \"abc xyz\", Field.Store.YES, Field.Index.ANALYZED));\n    doc.add(new Field(\"indexed\", \"abc xyz\", Field.Store.NO, Field.Index.NOT_ANALYZED));\n    doc.add(new Field(\"tokenized\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED));\n    doc.add(new Field(\"tokenized_reader\", new StringReader(\"abc xyz\")));\n    doc.add(new Field(\"tokenized_tokenstream\", w.w.getAnalyzer().tokenStream(\"tokenized_tokenstream\", new StringReader(\"abc xyz\"))));\n    doc.add(new Field(\"binary\", new byte[10]));\n    doc.add(new Field(\"tv\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"tv_pos\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"tv_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n    doc.add(new Field(\"tv_pos_off\", \"abc xyz\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    w.addDocument(doc);\n    IndexReader r = w.getReader();\n    w.close();\n\n    doc = r.document(0);\n    // 4 stored fields\n    assertEquals(4, doc.getFields().size());\n    assertEquals(\"abc\", doc.get(\"stored\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_indexed\"));\n    assertEquals(\"abc xyz\", doc.get(\"stored_tokenized\"));\n    final BytesRef br = doc.getBinaryValue(\"binary\");\n    assertNotNull(br);\n    assertEquals(10, br.length);\n\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"stored_tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"indexed\", \"abc xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_reader\", \"xyz\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"abc\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"tokenized_tokenstream\", \"xyz\")), 1).totalHits);\n\n    for(String field : new String[] {\"tv\", \"tv_pos\", \"tv_off\", \"tv_pos_off\"}) {\n      Fields tvFields = r.getTermVectors(0);\n      Terms tvs = tvFields.terms(field);\n      assertNotNull(tvs);\n      assertEquals(2, tvs.getUniqueTermCount());\n      TermsEnum tvsEnum = tvs.iterator(null);\n      assertEquals(new BytesRef(\"abc\"), tvsEnum.next());\n      final DocsAndPositionsEnum dpEnum = tvsEnum.docsAndPositions(null, null, false);\n      if (field.equals(\"tv\")) {\n        assertNull(dpEnum);\n      } else {\n        assertNotNull(dpEnum);\n      }\n      assertEquals(new BytesRef(\"xyz\"), tvsEnum.next());\n      assertNull(tvsEnum.next());\n    }\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fa0f44f887719e97183771e977cfc4bfb485b766":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["fa0f44f887719e97183771e977cfc4bfb485b766"]},"commit2Childs":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["fa0f44f887719e97183771e977cfc4bfb485b766"],"fa0f44f887719e97183771e977cfc4bfb485b766":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}