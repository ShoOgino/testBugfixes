{"path":"modules/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/kuromoji/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","commits":[{"id":"6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2","date":1326399048,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/kuromoji/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"/dev/null","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        dictionary.noteInflection(formatted);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          dictionary.noteInflection(formatted);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    dictionary.finalizeInflections();\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, fstOutput.get(ord));\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    FST<Long> fst = fstBuilder.finish();\n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["8405d98acebb7e287bf7ac40e937ba05b8661285","7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a904819476fbc333a72b5914d82b948e7dab0205","date":1326741588,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/kuromoji/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"modules/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/kuromoji/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, fstOutput.get(ord));\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    FST<Long> fst = fstBuilder.finish();\n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        dictionary.noteInflection(formatted);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          dictionary.noteInflection(formatted);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    dictionary.finalizeInflections();\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, fstOutput.get(ord));\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    FST<Long> fst = fstBuilder.finish();\n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b3be20ca1091c0b7cdb2308b9023606a5e451cec","date":1327877325,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/kuromoji/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"modules/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/kuromoji/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish().pack(2, 100000);\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, fstOutput.get(ord));\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    FST<Long> fst = fstBuilder.finish();\n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"817882884229bace7dc5d1b75f6b0e4aa1e47122","date":1327879145,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"modules/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/kuromoji/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"modules/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/kuromoji/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish().pack(2, 100000);\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, fstOutput.get(ord));\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    FST<Long> fst = fstBuilder.finish();\n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5b6fdfce35d0adb18836cf8711abe487a934df33","date":1327946200,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"modules/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/kuromoji/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"modules/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/kuromoji/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish().pack(2, 100000);\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, fstOutput);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, fstOutput.get(ord));\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    FST<Long> fst = fstBuilder.finish();\n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"98d45c1ff2c99694b6de2201175f9b8b8b27b597","date":1332757908,"type":5,"author":"Christian Moen","isMerge":false,"pathNew":"modules/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"modules/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/kuromoji/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish().pack(2, 100000);\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish().pack(2, 100000);\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"b3be20ca1091c0b7cdb2308b9023606a5e451cec":["a904819476fbc333a72b5914d82b948e7dab0205"],"a904819476fbc333a72b5914d82b948e7dab0205":["6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2"],"817882884229bace7dc5d1b75f6b0e4aa1e47122":["a904819476fbc333a72b5914d82b948e7dab0205","b3be20ca1091c0b7cdb2308b9023606a5e451cec"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"98d45c1ff2c99694b6de2201175f9b8b8b27b597":["b3be20ca1091c0b7cdb2308b9023606a5e451cec"],"5b6fdfce35d0adb18836cf8711abe487a934df33":["a904819476fbc333a72b5914d82b948e7dab0205","b3be20ca1091c0b7cdb2308b9023606a5e451cec"],"6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["98d45c1ff2c99694b6de2201175f9b8b8b27b597"]},"commit2Childs":{"b3be20ca1091c0b7cdb2308b9023606a5e451cec":["817882884229bace7dc5d1b75f6b0e4aa1e47122","98d45c1ff2c99694b6de2201175f9b8b8b27b597","5b6fdfce35d0adb18836cf8711abe487a934df33"],"a904819476fbc333a72b5914d82b948e7dab0205":["b3be20ca1091c0b7cdb2308b9023606a5e451cec","817882884229bace7dc5d1b75f6b0e4aa1e47122","5b6fdfce35d0adb18836cf8711abe487a934df33"],"817882884229bace7dc5d1b75f6b0e4aa1e47122":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2"],"98d45c1ff2c99694b6de2201175f9b8b8b27b597":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5b6fdfce35d0adb18836cf8711abe487a934df33":[],"6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2":["a904819476fbc333a72b5914d82b948e7dab0205"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["817882884229bace7dc5d1b75f6b0e4aa1e47122","5b6fdfce35d0adb18836cf8711abe487a934df33","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}