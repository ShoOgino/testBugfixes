{"path":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean,float).mjava","commits":[{"id":"02e175abd2c4c1611c5a9647486ae8ba249a94c1","date":1468327116,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int numDocs = termsEnum.docFreq();\n            DocIdSetBuilder.BulkAdder adder = builder.grow(numDocs);\n            for (int i = 0; i < numDocs; ++i) {\n              int docId = docs.nextDoc();\n              adder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int numDocs = termsEnum.docFreq();\n            DocIdSetBuilder.BulkAdder adder = builder.grow(numDocs);\n            for (int i = 0; i < numDocs; ++i) {\n              int docId = docs.nextDoc();\n              adder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6652c74b2358a0b13223817a6a793bf1c9d0749d","date":1474465301,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int numDocs = termsEnum.docFreq();\n            DocIdSetBuilder.BulkAdder adder = builder.grow(numDocs);\n            for (int i = 0; i < numDocs; ++i) {\n              int docId = docs.nextDoc();\n              adder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              if (docId > sdv.docID()) {\n                sdv.advance(docId);\n              }\n              if (docId == sdv.docID()) {\n                int count = sdv.docValueCount();\n                for (int i = 0; i < count; i++) {\n                  long hash = sdv.nextValue();\n                  if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                    return true;\n                  }\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int numDocs = termsEnum.docFreq();\n            DocIdSetBuilder.BulkAdder adder = builder.grow(numDocs);\n            for (int i = 0; i < numDocs; ++i) {\n              int docId = docs.nextDoc();\n              adder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int numDocs = termsEnum.docFreq();\n            DocIdSetBuilder.BulkAdder adder = builder.grow(numDocs);\n            for (int i = 0; i < numDocs; ++i) {\n              int docId = docs.nextDoc();\n              adder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              if (docId > sdv.docID()) {\n                sdv.advance(docId);\n              }\n              if (docId == sdv.docID()) {\n                int count = sdv.docValueCount();\n                for (int i = 0; i < count; i++) {\n                  long hash = sdv.nextValue();\n                  if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                    return true;\n                  }\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int numDocs = termsEnum.docFreq();\n            DocIdSetBuilder.BulkAdder adder = builder.grow(numDocs);\n            for (int i = 0; i < numDocs; ++i) {\n              int docId = docs.nextDoc();\n              adder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              sdv.setDocument(docId);\n              int count = sdv.count();\n              for (int i = 0; i < count; i++) {\n                long hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                  return true;\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean,float).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int numDocs = termsEnum.docFreq();\n            DocIdSetBuilder.BulkAdder adder = builder.grow(numDocs);\n            for (int i = 0; i < numDocs; ++i) {\n              int docId = docs.nextDoc();\n              adder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              if (docId > sdv.docID()) {\n                sdv.advance(docId);\n              }\n              if (docId == sdv.docID()) {\n                int count = sdv.docValueCount();\n                for (int i = 0; i < count; i++) {\n                  long hash = sdv.nextValue();\n                  if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                    return true;\n                  }\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7dc53adde67e2fa905526fcf1e14e3c9e1e64821","date":1486416055,"type":4,"author":"Mike McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/spatial/src/java/org/apache/lucene/spatial/geopoint/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean,float).mjava","sourceNew":null,"sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores, float boost) throws IOException {\n    return new ConstantScoreWeight(this, boost) {\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return null;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        // approximation (postfiltering has not yet been applied)\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc(), terms);\n        // subset of documents that need no postfiltering, this is purely an optimization\n        final BitSet preApproved;\n        // dumb heuristic: if the field is really sparse, use a sparse impl\n        if (terms.getDocCount() * 100L < reader.maxDoc()) {\n          preApproved = new SparseFixedBitSet(reader.maxDoc());\n        } else {\n          preApproved = new FixedBitSet(reader.maxDoc());\n        }\n        PostingsEnum docs = null;\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering\n          if (termsEnum.boundaryTerm()) {\n            builder.add(docs);\n          } else {\n            int numDocs = termsEnum.docFreq();\n            DocIdSetBuilder.BulkAdder adder = builder.grow(numDocs);\n            for (int i = 0; i < numDocs; ++i) {\n              int docId = docs.nextDoc();\n              adder.add(docId);\n              preApproved.set(docId);\n            }\n          }\n        }\n\n        DocIdSet set = builder.build();\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n\n        // return two-phase iterator using docvalues to postfilter candidates\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n        TwoPhaseIterator iterator = new TwoPhaseIterator(disi) {\n          @Override\n          public boolean matches() throws IOException {\n            int docId = disi.docID();\n            if (preApproved.get(docId)) {\n              return true;\n            } else {\n              if (docId > sdv.docID()) {\n                sdv.advance(docId);\n              }\n              if (docId == sdv.docID()) {\n                int count = sdv.docValueCount();\n                for (int i = 0; i < count; i++) {\n                  long hash = sdv.nextValue();\n                  if (termsEnum.postFilter(GeoPointField.decodeLatitude(hash), GeoPointField.decodeLongitude(hash))) {\n                    return true;\n                  }\n                }\n              }\n              return false;\n            }\n          }\n\n          @Override\n          public float matchCost() {\n            return 20; // TODO: make this fancier\n          }\n        };\n        return new ConstantScoreScorer(this, score(), iterator);\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7dc53adde67e2fa905526fcf1e14e3c9e1e64821":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["02e175abd2c4c1611c5a9647486ae8ba249a94c1","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["02e175abd2c4c1611c5a9647486ae8ba249a94c1"],"02e175abd2c4c1611c5a9647486ae8ba249a94c1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["7dc53adde67e2fa905526fcf1e14e3c9e1e64821"]},"commit2Childs":{"7dc53adde67e2fa905526fcf1e14e3c9e1e64821":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["7dc53adde67e2fa905526fcf1e14e3c9e1e64821","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["02e175abd2c4c1611c5a9647486ae8ba249a94c1","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"02e175abd2c4c1611c5a9647486ae8ba249a94c1":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}