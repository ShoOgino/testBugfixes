{"path":"src/java/org/apache/lucene/search/MultiSearcher#createQueryWeight(Query).mjava","commits":[{"id":"052fac7830290bd38a04cddee1a121ee07656b56","date":1245780702,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/search/MultiSearcher#createQueryWeight(Query).mjava","pathOld":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","sourceNew":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  protected QueryWeight createQueryWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set terms = new HashSet();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap dfMap = new HashMap();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], new Integer(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.queryWeight(cacheSim);\n  }\n\n","sourceOld":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set terms = new HashSet();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap dfMap = new HashMap();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], new Integer(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe941135bdfc28c81e20b4d21422f8726af34925","date":1250040150,"type":5,"author":"Mark Robert Miller","isMerge":false,"pathNew":"src/java/org/apache/lucene/search/MultiSearcher#createWeight(Query).mjava","pathOld":"src/java/org/apache/lucene/search/MultiSearcher#createQueryWeight(Query).mjava","sourceNew":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  protected Weight createWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set terms = new HashSet();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap dfMap = new HashMap();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], new Integer(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.weight(cacheSim);\n  }\n\n","sourceOld":"  /**\n   * Create weight in multiple index scenario.\n   * \n   * Distributed query processing is done in the following steps:\n   * 1. rewrite query\n   * 2. extract necessary terms\n   * 3. collect dfs for these terms from the Searchables\n   * 4. create query weight using aggregate dfs.\n   * 5. distribute that weight to Searchables\n   * 6. merge results\n   *\n   * Steps 1-4 are done here, 5+6 in the search() methods\n   *\n   * @return rewritten queries\n   */\n  protected QueryWeight createQueryWeight(Query original) throws IOException {\n    // step 1\n    Query rewrittenQuery = rewrite(original);\n\n    // step 2\n    Set terms = new HashSet();\n    rewrittenQuery.extractTerms(terms);\n\n    // step3\n    Term[] allTermsArray = new Term[terms.size()];\n    terms.toArray(allTermsArray);\n    int[] aggregatedDfs = new int[terms.size()];\n    for (int i = 0; i < searchables.length; i++) {\n      int[] dfs = searchables[i].docFreqs(allTermsArray);\n      for(int j=0; j<aggregatedDfs.length; j++){\n        aggregatedDfs[j] += dfs[j];\n      }\n    }\n\n    HashMap dfMap = new HashMap();\n    for(int i=0; i<allTermsArray.length; i++) {\n      dfMap.put(allTermsArray[i], new Integer(aggregatedDfs[i]));\n    }\n\n    // step4\n    int numDocs = maxDoc();\n    CachedDfSource cacheSim = new CachedDfSource(dfMap, numDocs, getSimilarity());\n\n    return rewrittenQuery.queryWeight(cacheSim);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fe941135bdfc28c81e20b4d21422f8726af34925":["052fac7830290bd38a04cddee1a121ee07656b56"],"052fac7830290bd38a04cddee1a121ee07656b56":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["fe941135bdfc28c81e20b4d21422f8726af34925"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["052fac7830290bd38a04cddee1a121ee07656b56"],"fe941135bdfc28c81e20b4d21422f8726af34925":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"052fac7830290bd38a04cddee1a121ee07656b56":["fe941135bdfc28c81e20b4d21422f8726af34925"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}