{"path":"solr/core/src/test/org/apache/solr/search/facet/TestCloudJSONFacetSKG#testBespoke().mjava","commits":[{"id":"92910727264a23a47b7a6c94b0f75d655537b9ea","date":1540414655,"type":1,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/facet/TestCloudJSONFacetSKG#testBespoke().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudJSONFacetSKG#testBespoke().mjava","sourceNew":"  /** \n   * Test some small, hand crafted, but non-trivial queries that are\n   * easier to trace/debug then a pure random monstrosity.\n   * (ie: if something obvious gets broken, this test may fail faster and in a more obvious way then testRandom)\n   */\n  public void testBespoke() throws Exception {\n    { // trivial single level facet\n      Map<String,TermFacet> facets = new LinkedHashMap<>();\n      TermFacet top = new TermFacet(strfield(9), UNIQUE_FIELD_VALS, 0, null);\n      facets.put(\"top1\", top);\n      final AtomicInteger maxBuckets = new AtomicInteger(UNIQUE_FIELD_VALS);\n      assertFacetSKGsAreCorrect(maxBuckets, facets, strfield(7)+\":11\", strfield(5)+\":9\", \"*:*\");\n      assertTrue(\"Didn't check a single bucket???\", maxBuckets.get() < UNIQUE_FIELD_VALS);\n    }\n    \n    { // trivial single level facet w/sorting on skg\n      Map<String,TermFacet> facets = new LinkedHashMap<>();\n      TermFacet top = new TermFacet(strfield(9), UNIQUE_FIELD_VALS, 0, \"skg desc\");\n      facets.put(\"top2\", top);\n      final AtomicInteger maxBuckets = new AtomicInteger(UNIQUE_FIELD_VALS);\n      assertFacetSKGsAreCorrect(maxBuckets, facets, strfield(7)+\":11\", strfield(5)+\":9\", \"*:*\");\n      assertTrue(\"Didn't check a single bucket???\", maxBuckets.get() < UNIQUE_FIELD_VALS);\n    }\n\n    { // trivial single level facet w/ 2 diff ways to request \"limit = (effectively) Infinite\"\n      // to sanity check refinement of buckets missing from other shard in both cases\n      \n      // NOTE that these two queries & facets *should* effectively identical given that the\n      // very large limit value is big enough no shard will ever return that may terms,\n      // but the \"limit=-1\" case it actaully triggers slightly different code paths\n      // because it causes FacetField.returnsPartial() to be \"true\"\n      for (int limit : new int[] { 999999999, -1 }) {\n        Map<String,TermFacet> facets = new LinkedHashMap<>();\n        facets.put(\"top_facet_limit__\" + limit, new TermFacet(strfield(9), limit, 0, \"skg desc\"));\n        final AtomicInteger maxBuckets = new AtomicInteger(UNIQUE_FIELD_VALS);\n        assertFacetSKGsAreCorrect(maxBuckets, facets, strfield(7)+\":11\", strfield(5)+\":9\", \"*:*\");\n        assertTrue(\"Didn't check a single bucket???\", maxBuckets.get() < UNIQUE_FIELD_VALS);\n      }\n    }\n  }\n\n","sourceOld":"  /** \n   * Test some small, hand crafted, but non-trivial queries that are\n   * easier to trace/debug then a pure random monstrosity.\n   * (ie: if something obvious gets broken, this test may fail faster and in a more obvious way then testRandom)\n   */\n  public void testBespoke() throws Exception {\n    { // trivial single level facet\n      Map<String,TermFacet> facets = new LinkedHashMap<>();\n      TermFacet top = new TermFacet(strfield(9), UNIQUE_FIELD_VALS, 0, null);\n      facets.put(\"top1\", top);\n      final AtomicInteger maxBuckets = new AtomicInteger(UNIQUE_FIELD_VALS);\n      assertFacetSKGsAreCorrect(maxBuckets, facets, strfield(7)+\":11\", strfield(5)+\":9\", \"*:*\");\n      assertTrue(\"Didn't check a single bucket???\", maxBuckets.get() < UNIQUE_FIELD_VALS);\n    }\n    \n    { // trivial single level facet w/sorting on skg\n      Map<String,TermFacet> facets = new LinkedHashMap<>();\n      TermFacet top = new TermFacet(strfield(9), UNIQUE_FIELD_VALS, 0, \"skg desc\");\n      facets.put(\"top2\", top);\n      final AtomicInteger maxBuckets = new AtomicInteger(UNIQUE_FIELD_VALS);\n      assertFacetSKGsAreCorrect(maxBuckets, facets, strfield(7)+\":11\", strfield(5)+\":9\", \"*:*\");\n      assertTrue(\"Didn't check a single bucket???\", maxBuckets.get() < UNIQUE_FIELD_VALS);\n    }\n\n    { // trivial single level facet w/ 2 diff ways to request \"limit = (effectively) Infinite\"\n      // to sanity check refinement of buckets missing from other shard in both cases\n      \n      // NOTE that these two queries & facets *should* effectively identical given that the\n      // very large limit value is big enough no shard will ever return that may terms,\n      // but the \"limit=-1\" case it actaully triggers slightly different code paths\n      // because it causes FacetField.returnsPartial() to be \"true\"\n      for (int limit : new int[] { 999999999, -1 }) {\n        Map<String,TermFacet> facets = new LinkedHashMap<>();\n        facets.put(\"top_facet_limit__\" + limit, new TermFacet(strfield(9), limit, 0, \"skg desc\"));\n        final AtomicInteger maxBuckets = new AtomicInteger(UNIQUE_FIELD_VALS);\n        assertFacetSKGsAreCorrect(maxBuckets, facets, strfield(7)+\":11\", strfield(5)+\":9\", \"*:*\");\n        assertTrue(\"Didn't check a single bucket???\", maxBuckets.get() < UNIQUE_FIELD_VALS);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c5ec3c464e62e57df598ba20e010313bf6d5d7b4","date":1589998565,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/facet/TestCloudJSONFacetSKG#testBespoke().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/facet/TestCloudJSONFacetSKG#testBespoke().mjava","sourceNew":"  /** \n   * Test some small, hand crafted, but non-trivial queries that are\n   * easier to trace/debug then a pure random monstrosity.\n   * (ie: if something obvious gets broken, this test may fail faster and in a more obvious way then testRandom)\n   */\n  public void testBespoke() throws Exception {\n    { // trivial single level facet\n      Map<String,TermFacet> facets = new LinkedHashMap<>();\n      TermFacet top = new TermFacet(multiStrField(9), UNIQUE_FIELD_VALS, 0, null);\n      facets.put(\"top1\", top);\n      final AtomicInteger maxBuckets = new AtomicInteger(UNIQUE_FIELD_VALS);\n      assertFacetSKGsAreCorrect(maxBuckets, facets, multiStrField(7)+\":11\", multiStrField(5)+\":9\", \"*:*\");\n      assertTrue(\"Didn't check a single bucket???\", maxBuckets.get() < UNIQUE_FIELD_VALS);\n    }\n    \n    { // trivial single level facet w/sorting on skg\n      Map<String,TermFacet> facets = new LinkedHashMap<>();\n      TermFacet top = new TermFacet(multiStrField(9), UNIQUE_FIELD_VALS, 0, \"skg desc\");\n      facets.put(\"top2\", top);\n      final AtomicInteger maxBuckets = new AtomicInteger(UNIQUE_FIELD_VALS);\n      assertFacetSKGsAreCorrect(maxBuckets, facets, multiStrField(7)+\":11\", multiStrField(5)+\":9\", \"*:*\");\n      assertTrue(\"Didn't check a single bucket???\", maxBuckets.get() < UNIQUE_FIELD_VALS);\n    }\n\n    { // trivial single level facet w/ 2 diff ways to request \"limit = (effectively) Infinite\"\n      // to sanity check refinement of buckets missing from other shard in both cases\n      \n      // NOTE that these two queries & facets *should* effectively identical given that the\n      // very large limit value is big enough no shard will ever return that may terms,\n      // but the \"limit=-1\" case it actaully triggers slightly different code paths\n      // because it causes FacetField.returnsPartial() to be \"true\"\n      for (int limit : new int[] { 999999999, -1 }) {\n        Map<String,TermFacet> facets = new LinkedHashMap<>();\n        facets.put(\"top_facet_limit__\" + limit, new TermFacet(multiStrField(9), limit, 0, \"skg desc\"));\n        final AtomicInteger maxBuckets = new AtomicInteger(UNIQUE_FIELD_VALS);\n        assertFacetSKGsAreCorrect(maxBuckets, facets, multiStrField(7)+\":11\", multiStrField(5)+\":9\", \"*:*\");\n        assertTrue(\"Didn't check a single bucket???\", maxBuckets.get() < UNIQUE_FIELD_VALS);\n      }\n    }\n  }\n\n","sourceOld":"  /** \n   * Test some small, hand crafted, but non-trivial queries that are\n   * easier to trace/debug then a pure random monstrosity.\n   * (ie: if something obvious gets broken, this test may fail faster and in a more obvious way then testRandom)\n   */\n  public void testBespoke() throws Exception {\n    { // trivial single level facet\n      Map<String,TermFacet> facets = new LinkedHashMap<>();\n      TermFacet top = new TermFacet(strfield(9), UNIQUE_FIELD_VALS, 0, null);\n      facets.put(\"top1\", top);\n      final AtomicInteger maxBuckets = new AtomicInteger(UNIQUE_FIELD_VALS);\n      assertFacetSKGsAreCorrect(maxBuckets, facets, strfield(7)+\":11\", strfield(5)+\":9\", \"*:*\");\n      assertTrue(\"Didn't check a single bucket???\", maxBuckets.get() < UNIQUE_FIELD_VALS);\n    }\n    \n    { // trivial single level facet w/sorting on skg\n      Map<String,TermFacet> facets = new LinkedHashMap<>();\n      TermFacet top = new TermFacet(strfield(9), UNIQUE_FIELD_VALS, 0, \"skg desc\");\n      facets.put(\"top2\", top);\n      final AtomicInteger maxBuckets = new AtomicInteger(UNIQUE_FIELD_VALS);\n      assertFacetSKGsAreCorrect(maxBuckets, facets, strfield(7)+\":11\", strfield(5)+\":9\", \"*:*\");\n      assertTrue(\"Didn't check a single bucket???\", maxBuckets.get() < UNIQUE_FIELD_VALS);\n    }\n\n    { // trivial single level facet w/ 2 diff ways to request \"limit = (effectively) Infinite\"\n      // to sanity check refinement of buckets missing from other shard in both cases\n      \n      // NOTE that these two queries & facets *should* effectively identical given that the\n      // very large limit value is big enough no shard will ever return that may terms,\n      // but the \"limit=-1\" case it actaully triggers slightly different code paths\n      // because it causes FacetField.returnsPartial() to be \"true\"\n      for (int limit : new int[] { 999999999, -1 }) {\n        Map<String,TermFacet> facets = new LinkedHashMap<>();\n        facets.put(\"top_facet_limit__\" + limit, new TermFacet(strfield(9), limit, 0, \"skg desc\"));\n        final AtomicInteger maxBuckets = new AtomicInteger(UNIQUE_FIELD_VALS);\n        assertFacetSKGsAreCorrect(maxBuckets, facets, strfield(7)+\":11\", strfield(5)+\":9\", \"*:*\");\n        assertTrue(\"Didn't check a single bucket???\", maxBuckets.get() < UNIQUE_FIELD_VALS);\n      }\n    }\n  }\n\n","bugFix":["2c705a0d590cf911e7c942df49563ca2ea176e22"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"22d2c45da6e47ff0ada29a8f98566b76f0b278de","date":1591372739,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/facet/TestCloudJSONFacetSKG#testBespoke().mjava","pathOld":"solr/core/src/test/org/apache/solr/search/facet/TestCloudJSONFacetSKG#testBespoke().mjava","sourceNew":"  /** \n   * Test some small, hand crafted, but non-trivial queries that are\n   * easier to trace/debug then a pure random monstrosity.\n   * (ie: if something obvious gets broken, this test may fail faster and in a more obvious way then testRandom)\n   */\n  public void testBespoke() throws Exception {\n    { // trivial single level facet\n      Map<String,TermFacet> facets = new LinkedHashMap<>();\n      TermFacet top = new TermFacet(multiStrField(9), UNIQUE_FIELD_VALS, 0, null);\n      facets.put(\"top1\", top);\n      final AtomicInteger maxBuckets = new AtomicInteger(UNIQUE_FIELD_VALS);\n      assertFacetSKGsAreCorrect(maxBuckets, facets, multiStrField(7)+\":11\", multiStrField(5)+\":9\", \"*:*\");\n      assertTrue(\"Didn't check a single bucket???\", maxBuckets.get() < UNIQUE_FIELD_VALS);\n    }\n    \n    { // trivial single level facet w/sorting on skg\n      Map<String,TermFacet> facets = new LinkedHashMap<>();\n      TermFacet top = new TermFacet(multiStrField(9), UNIQUE_FIELD_VALS, 0, \"skg desc\");\n      facets.put(\"top2\", top);\n      final AtomicInteger maxBuckets = new AtomicInteger(UNIQUE_FIELD_VALS);\n      assertFacetSKGsAreCorrect(maxBuckets, facets, multiStrField(7)+\":11\", multiStrField(5)+\":9\", \"*:*\");\n      assertTrue(\"Didn't check a single bucket???\", maxBuckets.get() < UNIQUE_FIELD_VALS);\n    }\n\n    { // trivial single level facet w/ 2 diff ways to request \"limit = (effectively) Infinite\"\n      // to sanity check refinement of buckets missing from other shard in both cases\n      \n      // NOTE that these two queries & facets *should* effectively identical given that the\n      // very large limit value is big enough no shard will ever return that may terms,\n      // but the \"limit=-1\" case it actaully triggers slightly different code paths\n      // because it causes FacetField.returnsPartial() to be \"true\"\n      for (int limit : new int[] { 999999999, -1 }) {\n        Map<String,TermFacet> facets = new LinkedHashMap<>();\n        facets.put(\"top_facet_limit__\" + limit, new TermFacet(multiStrField(9), limit, 0, \"skg desc\"));\n        final AtomicInteger maxBuckets = new AtomicInteger(UNIQUE_FIELD_VALS);\n        assertFacetSKGsAreCorrect(maxBuckets, facets, multiStrField(7)+\":11\", multiStrField(5)+\":9\", \"*:*\");\n        assertTrue(\"Didn't check a single bucket???\", maxBuckets.get() < UNIQUE_FIELD_VALS);\n      }\n    }\n    { // allBuckets should have no impact...\n      for (Boolean allBuckets : Arrays.asList( null, false, true )) {\n        Map<String,TermFacet> facets = new LinkedHashMap<>();\n        facets.put(\"allb__\" + allBuckets, new TermFacet(multiStrField(9),\n                                                        map(\"allBuckets\", allBuckets,\n                                                            \"sort\", \"skg desc\"))); \n        final AtomicInteger maxBuckets = new AtomicInteger(UNIQUE_FIELD_VALS);\n        assertFacetSKGsAreCorrect(maxBuckets, facets, multiStrField(7)+\":11\", multiStrField(5)+\":9\", \"*:*\");\n        assertTrue(\"Didn't check a single bucket???\", maxBuckets.get() < UNIQUE_FIELD_VALS);\n      }\n    }\n  }\n\n","sourceOld":"  /** \n   * Test some small, hand crafted, but non-trivial queries that are\n   * easier to trace/debug then a pure random monstrosity.\n   * (ie: if something obvious gets broken, this test may fail faster and in a more obvious way then testRandom)\n   */\n  public void testBespoke() throws Exception {\n    { // trivial single level facet\n      Map<String,TermFacet> facets = new LinkedHashMap<>();\n      TermFacet top = new TermFacet(multiStrField(9), UNIQUE_FIELD_VALS, 0, null);\n      facets.put(\"top1\", top);\n      final AtomicInteger maxBuckets = new AtomicInteger(UNIQUE_FIELD_VALS);\n      assertFacetSKGsAreCorrect(maxBuckets, facets, multiStrField(7)+\":11\", multiStrField(5)+\":9\", \"*:*\");\n      assertTrue(\"Didn't check a single bucket???\", maxBuckets.get() < UNIQUE_FIELD_VALS);\n    }\n    \n    { // trivial single level facet w/sorting on skg\n      Map<String,TermFacet> facets = new LinkedHashMap<>();\n      TermFacet top = new TermFacet(multiStrField(9), UNIQUE_FIELD_VALS, 0, \"skg desc\");\n      facets.put(\"top2\", top);\n      final AtomicInteger maxBuckets = new AtomicInteger(UNIQUE_FIELD_VALS);\n      assertFacetSKGsAreCorrect(maxBuckets, facets, multiStrField(7)+\":11\", multiStrField(5)+\":9\", \"*:*\");\n      assertTrue(\"Didn't check a single bucket???\", maxBuckets.get() < UNIQUE_FIELD_VALS);\n    }\n\n    { // trivial single level facet w/ 2 diff ways to request \"limit = (effectively) Infinite\"\n      // to sanity check refinement of buckets missing from other shard in both cases\n      \n      // NOTE that these two queries & facets *should* effectively identical given that the\n      // very large limit value is big enough no shard will ever return that may terms,\n      // but the \"limit=-1\" case it actaully triggers slightly different code paths\n      // because it causes FacetField.returnsPartial() to be \"true\"\n      for (int limit : new int[] { 999999999, -1 }) {\n        Map<String,TermFacet> facets = new LinkedHashMap<>();\n        facets.put(\"top_facet_limit__\" + limit, new TermFacet(multiStrField(9), limit, 0, \"skg desc\"));\n        final AtomicInteger maxBuckets = new AtomicInteger(UNIQUE_FIELD_VALS);\n        assertFacetSKGsAreCorrect(maxBuckets, facets, multiStrField(7)+\":11\", multiStrField(5)+\":9\", \"*:*\");\n        assertTrue(\"Didn't check a single bucket???\", maxBuckets.get() < UNIQUE_FIELD_VALS);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c5ec3c464e62e57df598ba20e010313bf6d5d7b4":["92910727264a23a47b7a6c94b0f75d655537b9ea"],"22d2c45da6e47ff0ada29a8f98566b76f0b278de":["c5ec3c464e62e57df598ba20e010313bf6d5d7b4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"92910727264a23a47b7a6c94b0f75d655537b9ea":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["22d2c45da6e47ff0ada29a8f98566b76f0b278de"]},"commit2Childs":{"c5ec3c464e62e57df598ba20e010313bf6d5d7b4":["22d2c45da6e47ff0ada29a8f98566b76f0b278de"],"22d2c45da6e47ff0ada29a8f98566b76f0b278de":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["92910727264a23a47b7a6c94b0f75d655537b9ea"],"92910727264a23a47b7a6c94b0f75d655537b9ea":["c5ec3c464e62e57df598ba20e010313bf6d5d7b4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}