{"path":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/FetcherMain#main(String[]).mjava","commits":[{"id":"05d36e0b328ec96237035fbcca240e73631396e5","date":1020520725,"type":0,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/FetcherMain#main(String[]).mjava","pathOld":"/dev/null","sourceNew":"    /**\n     * The main program. parsed\n     *\n     * @param args  The command line arguments\n     */\n    public static void main(String[] args)\n    {\n        int nrThreads = 10;\n\n        String startURL = \"\";\n        String restrictTo = \"http://141.84.120.82/ll/cmarschn/.*\";\n        boolean gui = false;\n        boolean showInfo = false;\n        System.out.println(\"LARM - LANLab Retrieval Machine - Fetcher - V 1.00 - (C) LANLab 2000-02\");\n        for (int i = 0; i < args.length; i++)\n        {\n            if (args[i].equals(\"-start\"))\n            {\n                i++;\n                startURL = args[i];\n                System.out.println(\"Start-URL set to: \" + startURL);\n            }\n            else if (args[i].equals(\"-restrictto\"))\n            {\n                i++;\n                restrictTo = args[i];\n                System.out.println(\"Restricting URLs to \" + restrictTo);\n            }\n            else if (args[i].equals(\"-threads\"))\n            {\n                i++;\n                nrThreads = Integer.parseInt(args[i]);\n                System.out.println(\"Threads set to \" + nrThreads);\n            }\n            else if (args[i].equals(\"-gui\"))\n            {\n                gui = true;\n            }\n            else if (args[i].equals(\"-?\"))\n            {\n                showInfo = true;\n            }\n            else\n            {\n                System.out.println(\"Unknown option: \" + args[i] + \"; use -? to get syntax\");\n                System.exit(0);\n            }\n        }\n\n        //URL.setURLStreamHandlerFactory(new HttpTimeoutFactory(500));\n        // replaced by HTTPClient\n\n        FetcherMain f = new FetcherMain(nrThreads);\n        if (showInfo || (startURL.equals(\"\") && gui == false))\n        {\n            System.out.println(\"Usage: FetcherMain -start <URL> -restrictto <RegEx> [-threads <nr=10>]\"); // [-gui]\n            System.exit(0);\n        }\n        try\n        {\n            f.setRexString(restrictTo);\n\n            if (gui)\n            {\n                // f.initGui(f, startURL);\n            }\n            else\n            {\n                try\n                {\n                    f.startMonitor();\n                    f.putURL(new URL(startURL), false);\n                }\n                catch (MalformedURLException e)\n                {\n                    System.out.println(\"Malformed URL\");\n\n                }\n            }\n        }\n        catch (MalformedPatternException e)\n        {\n            System.out.println(\"Wrong RegEx syntax. Must be a valid PERL RE\");\n        }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a383cc27dffe14c5d655eb57a87a1c7ae80b23a","date":1032050294,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/FetcherMain#main(String[]).mjava","pathOld":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/FetcherMain#main(String[]).mjava","sourceNew":"    /**\n     * The main program.\n     *\n     * @param args  The command line arguments\n     */\n    public static void main(String[] args)\n    {\n        int nrThreads = 10;\n\n        String startURL = \"\";\n        String restrictTo = \"http://141.84.120.82/ll/cmarschn/.*\";\n        boolean gui = false;\n        boolean showInfo = false;\n        System.out.println(\"LARM - LANLab Retrieval Machine - Fetcher - V 1.00 - (C) LANLab 2000-02\");\n\n\t// FIXME: consider using Jakarta Commons' CLI package for command line parameters\n        for (int i = 0; i < args.length; i++)\n        {\n            if (args[i].equals(\"-start\"))\n            {\n                i++;\n                startURL = args[i];\n                System.out.println(\"Start-URL set to: \" + startURL);\n            }\n            else if (args[i].equals(\"-restrictto\"))\n            {\n                i++;\n                restrictTo = args[i];\n                System.out.println(\"Restricting URLs to \" + restrictTo);\n            }\n            else if (args[i].equals(\"-threads\"))\n            {\n                i++;\n                nrThreads = Integer.parseInt(args[i]);\n                System.out.println(\"Threads set to \" + nrThreads);\n            }\n            else if (args[i].equals(\"-gui\"))\n            {\n                gui = true;\n            }\n            else if (args[i].equals(\"-?\"))\n            {\n                showInfo = true;\n            }\n            else\n            {\n                System.out.println(\"Unknown option: \" + args[i] + \"; use -? to get syntax\");\n                System.exit(0);\n            }\n        }\n\n        //URL.setURLStreamHandlerFactory(new HttpTimeoutFactory(500));\n        // replaced by HTTPClient\n\n        FetcherMain f = new FetcherMain(nrThreads);\n        if (showInfo || (startURL.equals(\"\") && gui == false))\n        {\n            System.out.println(\"Usage: FetcherMain -start <URL> -restrictto <RegEx> [-threads <nr=10>]\"); // [-gui]\n            System.exit(0);\n        }\n        try\n        {\n            f.setRexString(restrictTo);\n\n            if (gui)\n            {\n                // f.initGui(f, startURL);\n            }\n            else\n            {\n                try\n                {\n                    f.startMonitor();\n                    f.putURL(new URL(startURL), false);\n                }\n                catch (MalformedURLException e)\n                {\n                    System.out.println(\"Malformed URL\");\n                }\n            }\n        }\n        catch (MalformedPatternException e)\n        {\n            System.out.println(\"Wrong RegEx syntax. Must be a valid PERL RE\");\n        }\n    }\n\n","sourceOld":"    /**\n     * The main program. parsed\n     *\n     * @param args  The command line arguments\n     */\n    public static void main(String[] args)\n    {\n        int nrThreads = 10;\n\n        String startURL = \"\";\n        String restrictTo = \"http://141.84.120.82/ll/cmarschn/.*\";\n        boolean gui = false;\n        boolean showInfo = false;\n        System.out.println(\"LARM - LANLab Retrieval Machine - Fetcher - V 1.00 - (C) LANLab 2000-02\");\n        for (int i = 0; i < args.length; i++)\n        {\n            if (args[i].equals(\"-start\"))\n            {\n                i++;\n                startURL = args[i];\n                System.out.println(\"Start-URL set to: \" + startURL);\n            }\n            else if (args[i].equals(\"-restrictto\"))\n            {\n                i++;\n                restrictTo = args[i];\n                System.out.println(\"Restricting URLs to \" + restrictTo);\n            }\n            else if (args[i].equals(\"-threads\"))\n            {\n                i++;\n                nrThreads = Integer.parseInt(args[i]);\n                System.out.println(\"Threads set to \" + nrThreads);\n            }\n            else if (args[i].equals(\"-gui\"))\n            {\n                gui = true;\n            }\n            else if (args[i].equals(\"-?\"))\n            {\n                showInfo = true;\n            }\n            else\n            {\n                System.out.println(\"Unknown option: \" + args[i] + \"; use -? to get syntax\");\n                System.exit(0);\n            }\n        }\n\n        //URL.setURLStreamHandlerFactory(new HttpTimeoutFactory(500));\n        // replaced by HTTPClient\n\n        FetcherMain f = new FetcherMain(nrThreads);\n        if (showInfo || (startURL.equals(\"\") && gui == false))\n        {\n            System.out.println(\"Usage: FetcherMain -start <URL> -restrictto <RegEx> [-threads <nr=10>]\"); // [-gui]\n            System.exit(0);\n        }\n        try\n        {\n            f.setRexString(restrictTo);\n\n            if (gui)\n            {\n                // f.initGui(f, startURL);\n            }\n            else\n            {\n                try\n                {\n                    f.startMonitor();\n                    f.putURL(new URL(startURL), false);\n                }\n                catch (MalformedURLException e)\n                {\n                    System.out.println(\"Malformed URL\");\n\n                }\n            }\n        }\n        catch (MalformedPatternException e)\n        {\n            System.out.println(\"Wrong RegEx syntax. Must be a valid PERL RE\");\n        }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2b24dfe829236e25670a0fb6bd671abc4e2a91f9","date":1035299107,"type":3,"author":"cmarschner","isMerge":false,"pathNew":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/FetcherMain#main(String[]).mjava","pathOld":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/FetcherMain#main(String[]).mjava","sourceNew":"    /**\n     * The main program.\n     *\n     * @param args  The command line arguments\n     */\n    public static void main(String[] args) throws Exception\n    {\n        int nrThreads = 10;\n\n        ArrayList startURLs = new ArrayList();\n        String restrictTo = \".*\";\n        boolean gui = false;\n        boolean showInfo = false;\n        String hostResolverFile = \"\";\n        System.out.println(\"LARM - LANLab Retrieval Machine - Fetcher - V 1.00 - B.20020914\");\n\t// FIXME: consider using Jakarta Commons' CLI package for command line parameters\n\n        for (int i = 0; i < args.length; i++)\n        {\n            if (args[i].equals(\"-start\"))\n            {\n                i++;\n                String arg = args[i];\n                if(arg.startsWith(\"@\"))\n                {\n                    // input is a file with one URL per line\n                    String fileName = arg.substring(1);\n                    System.out.println(\"reading URL file \" + fileName);\n                    try\n                    {\n                        BufferedReader r = new BufferedReader(new FileReader(fileName));\n                        String line;\n                        int count=0;\n                        while((line = r.readLine()) != null)\n                        {\n                            try\n                            {\n                                startURLs.add(new URL(line));\n                                count++;\n                            }\n                            catch (MalformedURLException e)\n                            {\n                                System.out.println(\"Malformed URL '\" + line + \"' in line \" + (count+1) + \" of file \" + fileName);\n\n                            }\n                        }\n                        r.close();\n                        System.out.println(\"added \" + count + \" URLs from \" + fileName);\n                    }\n                    catch(IOException e)\n                    {\n                        System.out.println(\"Couldn't read '\" + fileName + \"': \" + e);\n                    }\n                }\n                else\n                {\n                    System.out.println(\"got URL \" + arg);\n                    try\n                    {\n                        startURLs.add(new URL(arg));\n                        System.out.println(\"Start-URL added: \" + arg);\n                    }\n                    catch (MalformedURLException e)\n                    {\n                        System.out.println(\"Malformed URL '\" + arg + \"'\");\n\n                    }\n                }\n            }\n            else if (args[i].equals(\"-restrictto\"))\n            {\n                i++;\n                restrictTo = args[i];\n                System.out.println(\"Restricting URLs to \" + restrictTo);\n            }\n            else if (args[i].equals(\"-threads\"))\n            {\n                i++;\n                nrThreads = Integer.parseInt(args[i]);\n                System.out.println(\"Threads set to \" + nrThreads);\n            }\n            else if (args[i].equals(\"-hostresolver\"))\n            {\n                i++;\n                hostResolverFile = args[i];\n                System.out.println(\"reading host resolver props from  '\" + hostResolverFile + \"'\");\n\n            }\n            else if (args[i].equals(\"-gui\"))\n            {\n                gui = true;\n            }\n            else if (args[i].equals(\"-?\"))\n            {\n                showInfo = true;\n            }\n            else\n            {\n                System.out.println(\"Unknown option: \" + args[i] + \"; use -? to get syntax\");\n                System.exit(0);\n            }\n        }\n\n        //URL.setURLStreamHandlerFactory(new HttpTimeoutFactory(500));\n        // replaced by HTTPClient\n\n        FetcherMain f = new FetcherMain(nrThreads, hostResolverFile);\n        if (showInfo || \"\".equals(hostResolverFile) || (startURLs.isEmpty() && gui == false))\n        {\n            System.out.println(\"The LARM crawler\\n\" +\n                               \"\\n\" +\n                               \"The LARM crawler is a fast parallel crawler, currently designed for\\n\" +\n                               \"large intranets (up to a couple hundred hosts with some hundred thousand\\n\" +\n                               \"documents). It is currently restricted by a relatively high memory overhead\\n\" +\n                               \"per crawled host, and by a HashMap of already crawled URLs which is also held\\n\" +\n                               \"in memory.\\n\" +\n                               \"\\n\" +\n                               \"Usage:   FetcherMain <-start <URL>|@<filename>>+ -restrictto <RegEx>\\n\" +\n                               \"                    [-threads <nr=10>] [-hostresolver <filename>]\\n\" +\n                               \"\\n\" +\n                               \"Commands:\\n\" +\n                               \"         -start specify one or more URLs to start with. You can as well specify a file\" +\n                               \"                that contains URLs, one each line\\n\" +\n                               \"         -restrictto a Perl 5 regular expression each URL must match. It is run against the\\n\" +\n                               \"                     _complete_ URL, including the http:// part\\n\" +\n                               \"         -threads  the number of crawling threads. defaults to 10\\n\" +\n                               \"         -hostresolver specify a file that contains rules for changing the host part of \\n\" +\n                               \"                       a URL during the normalization process (experimental).\\n\" +\n                               \"Caution: The <RegEx> is applied to the _normalized_ form of a URL.\\n\" +\n                               \"         See URLNormalizer for details\\n\" +\n                               \"Example:\\n\" +\n                               \"    -start @urls1.txt -start @urls2.txt -start http://localhost/ \" +\n                               \"    -restrictto http://[^/]*\\\\.localhost/.* -threads 25\\n\" +\n                               \"\\n\" +\n                               \"The host resolver file may contain the following commands: \\n\" +\n                               \"  startsWith(part1) = part2\\n\" +\n                               \"      if host starts with part1, this part will be replaced by part2\\n\" +\n                               \"   endsWith(part1) = part2\\n\" +\n                               \"       if host ends with part1, this part will be replaced by part2. This is done after\\n\" +\n                               \"       startsWith was processed\\n\" +\n                               \"   synonym(host1) = host2\\n\" +\n                               \"       the keywords startsWith, endsWith and synonym are case sensitive\\n\" +\n                               \"       host1 will be replaced with host2. this is done _after_ startsWith and endsWith was \\n\" +\n                               \"       processed. Due to a bug in BeanUtils, dots are not allowed in the keys (in parentheses)\\n\" +\n                               \"       and have to be escaped with commas. To simplify, commas are also replaced in property \\n\" +\n                               \"       values. So just use commas instead of dots. The resulting host names are only used for \\n\" +\n                               \"       comparisons and do not have to be existing URLs (although the syntax has to be valid).\\n\" +\n                               \"       However, the names will often be passed to java.net.URL which will try to make a DNS name\\n\" +\n                               \"       resolution, which will time out if the server can't be found. \\n\" +\n                               \"   Example:\" +\n                               \"     synonym(www1,host,com) = host,com\\n\" +\n                               \"     startsWith(www,) = ,\\n\" +\n                               \"     endsWith(host1,com) = host,com\\n\" +\n                               \"The crawler will show a status message every 5 seconds, which is printed by ThreadMonitor.java\\n\" +\n                               \"It will stop after the ThreadMonitor found the message queue and the crawling threads to be idle a \\n\" +\n                               \"couple of times.\\n\" +\n                               \"The crawled data will be saved within a logs/ directory. A cachingqueue/ directory is used for\\n\" +\n                               \"temporary queues.\\n\" +\n                               \"Note that this implementation is experimental, and that the command line options cover only a part \\n\" +\n                               \"of the parameters. Much of the configuration can only be done by modifying FetcherMain.java\\n\");\n            System.exit(0);\n        }\n        try\n        {\n            f.setRexString(restrictTo);\n\n            if (gui)\n            {\n                // f.initGui(f, startURL);\n                // the GUI is not longer supported\n            }\n            else\n            {\n                f.startMonitor();\n                for(Iterator it = startURLs.iterator(); it.hasNext(); )\n                {\n                    f.putURL((URL)it.next(), false);\n                }\n            }\n        }\n        catch (MalformedPatternException e)\n        {\n            System.out.println(\"Wrong RegEx syntax. Must be a valid PERL RE\");\n        }\n    }\n\n","sourceOld":"    /**\n     * The main program.\n     *\n     * @param args  The command line arguments\n     */\n    public static void main(String[] args)\n    {\n        int nrThreads = 10;\n\n        String startURL = \"\";\n        String restrictTo = \"http://141.84.120.82/ll/cmarschn/.*\";\n        boolean gui = false;\n        boolean showInfo = false;\n        System.out.println(\"LARM - LANLab Retrieval Machine - Fetcher - V 1.00 - (C) LANLab 2000-02\");\n\n\t// FIXME: consider using Jakarta Commons' CLI package for command line parameters\n        for (int i = 0; i < args.length; i++)\n        {\n            if (args[i].equals(\"-start\"))\n            {\n                i++;\n                startURL = args[i];\n                System.out.println(\"Start-URL set to: \" + startURL);\n            }\n            else if (args[i].equals(\"-restrictto\"))\n            {\n                i++;\n                restrictTo = args[i];\n                System.out.println(\"Restricting URLs to \" + restrictTo);\n            }\n            else if (args[i].equals(\"-threads\"))\n            {\n                i++;\n                nrThreads = Integer.parseInt(args[i]);\n                System.out.println(\"Threads set to \" + nrThreads);\n            }\n            else if (args[i].equals(\"-gui\"))\n            {\n                gui = true;\n            }\n            else if (args[i].equals(\"-?\"))\n            {\n                showInfo = true;\n            }\n            else\n            {\n                System.out.println(\"Unknown option: \" + args[i] + \"; use -? to get syntax\");\n                System.exit(0);\n            }\n        }\n\n        //URL.setURLStreamHandlerFactory(new HttpTimeoutFactory(500));\n        // replaced by HTTPClient\n\n        FetcherMain f = new FetcherMain(nrThreads);\n        if (showInfo || (startURL.equals(\"\") && gui == false))\n        {\n            System.out.println(\"Usage: FetcherMain -start <URL> -restrictto <RegEx> [-threads <nr=10>]\"); // [-gui]\n            System.exit(0);\n        }\n        try\n        {\n            f.setRexString(restrictTo);\n\n            if (gui)\n            {\n                // f.initGui(f, startURL);\n            }\n            else\n            {\n                try\n                {\n                    f.startMonitor();\n                    f.putURL(new URL(startURL), false);\n                }\n                catch (MalformedURLException e)\n                {\n                    System.out.println(\"Malformed URL\");\n                }\n            }\n        }\n        catch (MalformedPatternException e)\n        {\n            System.out.println(\"Wrong RegEx syntax. Must be a valid PERL RE\");\n        }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"00827ccb785ffa5db0167c3ae7dc48969c3ba4f9","date":1050068621,"type":3,"author":"cmarschner","isMerge":false,"pathNew":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/FetcherMain#main(String[]).mjava","pathOld":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/FetcherMain#main(String[]).mjava","sourceNew":"    /**\n     * The main program.\n     *\n     * @param args  The command line arguments\n     */\n    public static void main(String[] args) throws Exception\n    {\n        int nrThreads = 10;\n\n        ArrayList startURLs = new ArrayList();\n        String restrictTo = \".*\";\n        boolean gui = false;\n        boolean showInfo = false;\n        String hostResolverFile = \"\";\n        System.out.println(\"LARM - LANLab Retrieval Machine - Fetcher - V 1.00 - B.20020914\");\n\t// FIXME: consider using Jakarta Commons' CLI package for command line parameters\n\n        for (int i = 0; i < args.length; i++)\n        {\n            if (args[i].equals(\"-start\"))\n            {\n                i++;\n                String arg = args[i];\n                if(arg.startsWith(\"@\"))\n                {\n                    // input is a file with one URL per line\n                    String fileName = arg.substring(1);\n                    System.out.println(\"reading URL file \" + fileName);\n                    try\n                    {\n                        BufferedReader r = new BufferedReader(new FileReader(fileName));\n                        String line;\n                        int count=0;\n                        while((line = r.readLine()) != null)\n                        {\n                            try\n                            {\n                                startURLs.add(new URL(line));\n                                count++;\n                            }\n                            catch (MalformedURLException e)\n                            {\n                                System.out.println(\"Malformed URL '\" + line + \"' in line \" + (count+1) + \" of file \" + fileName);\n\n                            }\n                        }\n                        r.close();\n                        System.out.println(\"added \" + count + \" URLs from \" + fileName);\n                    }\n                    catch(IOException e)\n                    {\n                        System.out.println(\"Couldn't read '\" + fileName + \"': \" + e);\n                    }\n                }\n                else\n                {\n                    System.out.println(\"got URL \" + arg);\n                    try\n                    {\n                        startURLs.add(new URL(arg));\n                        System.out.println(\"Start-URL added: \" + arg);\n                    }\n                    catch (MalformedURLException e)\n                    {\n                        System.out.println(\"Malformed URL '\" + arg + \"'\");\n\n                    }\n                }\n            }\n            else if (args[i].equals(\"-restrictto\"))\n            {\n                i++;\n                restrictTo = args[i];\n                System.out.println(\"Restricting URLs to \" + restrictTo);\n            }\n            else if (args[i].equals(\"-threads\"))\n            {\n                i++;\n                nrThreads = Integer.parseInt(args[i]);\n                System.out.println(\"Threads set to \" + nrThreads);\n            }\n            else if (args[i].equals(\"-hostresolver\"))\n            {\n                i++;\n                hostResolverFile = args[i];\n                System.out.println(\"reading host resolver props from  '\" + hostResolverFile + \"'\");\n\n            }\n            else if (args[i].equals(\"-gui\"))\n            {\n                gui = true;\n            }\n            else if (args[i].equals(\"-?\"))\n            {\n                showInfo = true;\n            }\n            else\n            {\n                System.out.println(\"Unknown option: \" + args[i] + \"; use -? to get syntax\");\n                System.exit(0);\n            }\n        }\n\n        //URL.setURLStreamHandlerFactory(new HttpTimeoutFactory(500));\n        // replaced by HTTPClient\n\n        FetcherMain f = new FetcherMain(nrThreads, hostResolverFile);\n        if (showInfo || (startURLs.isEmpty() && gui == false))\n        {\n            System.out.println(\"The LARM crawler\\n\" +\n                               \"\\n\" +\n                               \"The LARM crawler is a fast parallel crawler, currently designed for\\n\" +\n                               \"large intranets (up to a couple hundred hosts with some hundred thousand\\n\" +\n                               \"documents). It is currently restricted by a relatively high memory overhead\\n\" +\n                               \"per crawled host, and by a HashMap of already crawled URLs which is also held\\n\" +\n                               \"in memory.\\n\" +\n                               \"\\n\" +\n                               \"Usage:   FetcherMain <-start <URL>|@<filename>>+ -restrictto <RegEx>\\n\" +\n                               \"                    [-threads <nr=10>] [-hostresolver <filename>]\\n\" +\n                               \"\\n\" +\n                               \"Commands:\\n\" +\n                               \"         -start specify one or more URLs to start with. You can as well specify a file\" +\n                               \"                that contains URLs, one each line\\n\" +\n                               \"         -restrictto a Perl 5 regular expression each URL must match. It is run against the\\n\" +\n                               \"                     _complete_ URL, including the http:// part\\n\" +\n                               \"         -threads  the number of crawling threads. defaults to 10\\n\" +\n                               \"         -hostresolver specify a file that contains rules for changing the host part of \\n\" +\n                               \"                       a URL during the normalization process (experimental).\\n\" +\n                               \"Caution: The <RegEx> is applied to the _normalized_ form of a URL.\\n\" +\n                               \"         See URLNormalizer for details\\n\" +\n                               \"Example:\\n\" +\n                               \"    -start @urls1.txt -start @urls2.txt -start http://localhost/ \" +\n                               \"    -restrictto http://[^/]*\\\\.localhost/.* -threads 25\\n\" +\n                               \"\\n\" +\n                               \"The host resolver file may contain the following commands: \\n\" +\n                               \"  startsWith(part1) = part2\\n\" +\n                               \"      if host starts with part1, this part will be replaced by part2\\n\" +\n                               \"   endsWith(part1) = part2\\n\" +\n                               \"       if host ends with part1, this part will be replaced by part2. This is done after\\n\" +\n                               \"       startsWith was processed\\n\" +\n                               \"   synonym(host1) = host2\\n\" +\n                               \"       the keywords startsWith, endsWith and synonym are case sensitive\\n\" +\n                               \"       host1 will be replaced with host2. this is done _after_ startsWith and endsWith was \\n\" +\n                               \"       processed. Due to a bug in BeanUtils, dots are not allowed in the keys (in parentheses)\\n\" +\n                               \"       and have to be escaped with commas. To simplify, commas are also replaced in property \\n\" +\n                               \"       values. So just use commas instead of dots. The resulting host names are only used for \\n\" +\n                               \"       comparisons and do not have to be existing URLs (although the syntax has to be valid).\\n\" +\n                               \"       However, the names will often be passed to java.net.URL which will try to make a DNS name\\n\" +\n                               \"       resolution, which will time out if the server can't be found. \\n\" +\n                               \"   Example:\" +\n                               \"     synonym(www1,host,com) = host,com\\n\" +\n                               \"     startsWith(www,) = ,\\n\" +\n                               \"     endsWith(host1,com) = host,com\\n\" +\n                               \"The crawler will show a status message every 5 seconds, which is printed by ThreadMonitor.java\\n\" +\n                               \"It will stop after the ThreadMonitor found the message queue and the crawling threads to be idle a \\n\" +\n                               \"couple of times.\\n\" +\n                               \"The crawled data will be saved within a logs/ directory. A cachingqueue/ directory is used for\\n\" +\n                               \"temporary queues.\\n\" +\n                               \"Note that this implementation is experimental, and that the command line options cover only a part \\n\" +\n                               \"of the parameters. Much of the configuration can only be done by modifying FetcherMain.java\\n\");\n            System.exit(0);\n        }\n        try\n        {\n            f.setRexString(restrictTo);\n\n            if (gui)\n            {\n                // f.initGui(f, startURL);\n                // the GUI is not longer supported\n            }\n            else\n            {\n                f.startMonitor();\n                for(Iterator it = startURLs.iterator(); it.hasNext(); )\n                {\n                    f.putURL((URL)it.next(), false);\n                }\n            }\n        }\n        catch (MalformedPatternException e)\n        {\n            System.out.println(\"Wrong RegEx syntax. Must be a valid PERL RE\");\n        }\n    }\n\n","sourceOld":"    /**\n     * The main program.\n     *\n     * @param args  The command line arguments\n     */\n    public static void main(String[] args) throws Exception\n    {\n        int nrThreads = 10;\n\n        ArrayList startURLs = new ArrayList();\n        String restrictTo = \".*\";\n        boolean gui = false;\n        boolean showInfo = false;\n        String hostResolverFile = \"\";\n        System.out.println(\"LARM - LANLab Retrieval Machine - Fetcher - V 1.00 - B.20020914\");\n\t// FIXME: consider using Jakarta Commons' CLI package for command line parameters\n\n        for (int i = 0; i < args.length; i++)\n        {\n            if (args[i].equals(\"-start\"))\n            {\n                i++;\n                String arg = args[i];\n                if(arg.startsWith(\"@\"))\n                {\n                    // input is a file with one URL per line\n                    String fileName = arg.substring(1);\n                    System.out.println(\"reading URL file \" + fileName);\n                    try\n                    {\n                        BufferedReader r = new BufferedReader(new FileReader(fileName));\n                        String line;\n                        int count=0;\n                        while((line = r.readLine()) != null)\n                        {\n                            try\n                            {\n                                startURLs.add(new URL(line));\n                                count++;\n                            }\n                            catch (MalformedURLException e)\n                            {\n                                System.out.println(\"Malformed URL '\" + line + \"' in line \" + (count+1) + \" of file \" + fileName);\n\n                            }\n                        }\n                        r.close();\n                        System.out.println(\"added \" + count + \" URLs from \" + fileName);\n                    }\n                    catch(IOException e)\n                    {\n                        System.out.println(\"Couldn't read '\" + fileName + \"': \" + e);\n                    }\n                }\n                else\n                {\n                    System.out.println(\"got URL \" + arg);\n                    try\n                    {\n                        startURLs.add(new URL(arg));\n                        System.out.println(\"Start-URL added: \" + arg);\n                    }\n                    catch (MalformedURLException e)\n                    {\n                        System.out.println(\"Malformed URL '\" + arg + \"'\");\n\n                    }\n                }\n            }\n            else if (args[i].equals(\"-restrictto\"))\n            {\n                i++;\n                restrictTo = args[i];\n                System.out.println(\"Restricting URLs to \" + restrictTo);\n            }\n            else if (args[i].equals(\"-threads\"))\n            {\n                i++;\n                nrThreads = Integer.parseInt(args[i]);\n                System.out.println(\"Threads set to \" + nrThreads);\n            }\n            else if (args[i].equals(\"-hostresolver\"))\n            {\n                i++;\n                hostResolverFile = args[i];\n                System.out.println(\"reading host resolver props from  '\" + hostResolverFile + \"'\");\n\n            }\n            else if (args[i].equals(\"-gui\"))\n            {\n                gui = true;\n            }\n            else if (args[i].equals(\"-?\"))\n            {\n                showInfo = true;\n            }\n            else\n            {\n                System.out.println(\"Unknown option: \" + args[i] + \"; use -? to get syntax\");\n                System.exit(0);\n            }\n        }\n\n        //URL.setURLStreamHandlerFactory(new HttpTimeoutFactory(500));\n        // replaced by HTTPClient\n\n        FetcherMain f = new FetcherMain(nrThreads, hostResolverFile);\n        if (showInfo || \"\".equals(hostResolverFile) || (startURLs.isEmpty() && gui == false))\n        {\n            System.out.println(\"The LARM crawler\\n\" +\n                               \"\\n\" +\n                               \"The LARM crawler is a fast parallel crawler, currently designed for\\n\" +\n                               \"large intranets (up to a couple hundred hosts with some hundred thousand\\n\" +\n                               \"documents). It is currently restricted by a relatively high memory overhead\\n\" +\n                               \"per crawled host, and by a HashMap of already crawled URLs which is also held\\n\" +\n                               \"in memory.\\n\" +\n                               \"\\n\" +\n                               \"Usage:   FetcherMain <-start <URL>|@<filename>>+ -restrictto <RegEx>\\n\" +\n                               \"                    [-threads <nr=10>] [-hostresolver <filename>]\\n\" +\n                               \"\\n\" +\n                               \"Commands:\\n\" +\n                               \"         -start specify one or more URLs to start with. You can as well specify a file\" +\n                               \"                that contains URLs, one each line\\n\" +\n                               \"         -restrictto a Perl 5 regular expression each URL must match. It is run against the\\n\" +\n                               \"                     _complete_ URL, including the http:// part\\n\" +\n                               \"         -threads  the number of crawling threads. defaults to 10\\n\" +\n                               \"         -hostresolver specify a file that contains rules for changing the host part of \\n\" +\n                               \"                       a URL during the normalization process (experimental).\\n\" +\n                               \"Caution: The <RegEx> is applied to the _normalized_ form of a URL.\\n\" +\n                               \"         See URLNormalizer for details\\n\" +\n                               \"Example:\\n\" +\n                               \"    -start @urls1.txt -start @urls2.txt -start http://localhost/ \" +\n                               \"    -restrictto http://[^/]*\\\\.localhost/.* -threads 25\\n\" +\n                               \"\\n\" +\n                               \"The host resolver file may contain the following commands: \\n\" +\n                               \"  startsWith(part1) = part2\\n\" +\n                               \"      if host starts with part1, this part will be replaced by part2\\n\" +\n                               \"   endsWith(part1) = part2\\n\" +\n                               \"       if host ends with part1, this part will be replaced by part2. This is done after\\n\" +\n                               \"       startsWith was processed\\n\" +\n                               \"   synonym(host1) = host2\\n\" +\n                               \"       the keywords startsWith, endsWith and synonym are case sensitive\\n\" +\n                               \"       host1 will be replaced with host2. this is done _after_ startsWith and endsWith was \\n\" +\n                               \"       processed. Due to a bug in BeanUtils, dots are not allowed in the keys (in parentheses)\\n\" +\n                               \"       and have to be escaped with commas. To simplify, commas are also replaced in property \\n\" +\n                               \"       values. So just use commas instead of dots. The resulting host names are only used for \\n\" +\n                               \"       comparisons and do not have to be existing URLs (although the syntax has to be valid).\\n\" +\n                               \"       However, the names will often be passed to java.net.URL which will try to make a DNS name\\n\" +\n                               \"       resolution, which will time out if the server can't be found. \\n\" +\n                               \"   Example:\" +\n                               \"     synonym(www1,host,com) = host,com\\n\" +\n                               \"     startsWith(www,) = ,\\n\" +\n                               \"     endsWith(host1,com) = host,com\\n\" +\n                               \"The crawler will show a status message every 5 seconds, which is printed by ThreadMonitor.java\\n\" +\n                               \"It will stop after the ThreadMonitor found the message queue and the crawling threads to be idle a \\n\" +\n                               \"couple of times.\\n\" +\n                               \"The crawled data will be saved within a logs/ directory. A cachingqueue/ directory is used for\\n\" +\n                               \"temporary queues.\\n\" +\n                               \"Note that this implementation is experimental, and that the command line options cover only a part \\n\" +\n                               \"of the parameters. Much of the configuration can only be done by modifying FetcherMain.java\\n\");\n            System.exit(0);\n        }\n        try\n        {\n            f.setRexString(restrictTo);\n\n            if (gui)\n            {\n                // f.initGui(f, startURL);\n                // the GUI is not longer supported\n            }\n            else\n            {\n                f.startMonitor();\n                for(Iterator it = startURLs.iterator(); it.hasNext(); )\n                {\n                    f.putURL((URL)it.next(), false);\n                }\n            }\n        }\n        catch (MalformedPatternException e)\n        {\n            System.out.println(\"Wrong RegEx syntax. Must be a valid PERL RE\");\n        }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"afc16d717d9ed1a8e45371668ca6de674164d624","date":1103345442,"type":4,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"/dev/null","pathOld":"sandbox/contributions/webcrawler-LARM/src/de/lanlab/larm/fetcher/FetcherMain#main(String[]).mjava","sourceNew":null,"sourceOld":"    /**\n     * The main program.\n     *\n     * @param args  The command line arguments\n     */\n    public static void main(String[] args) throws Exception\n    {\n        int nrThreads = 10;\n\n        ArrayList startURLs = new ArrayList();\n        String restrictTo = \".*\";\n        boolean gui = false;\n        boolean showInfo = false;\n        String hostResolverFile = \"\";\n        System.out.println(\"LARM - LANLab Retrieval Machine - Fetcher - V 1.00 - B.20020914\");\n\t// FIXME: consider using Jakarta Commons' CLI package for command line parameters\n\n        for (int i = 0; i < args.length; i++)\n        {\n            if (args[i].equals(\"-start\"))\n            {\n                i++;\n                String arg = args[i];\n                if(arg.startsWith(\"@\"))\n                {\n                    // input is a file with one URL per line\n                    String fileName = arg.substring(1);\n                    System.out.println(\"reading URL file \" + fileName);\n                    try\n                    {\n                        BufferedReader r = new BufferedReader(new FileReader(fileName));\n                        String line;\n                        int count=0;\n                        while((line = r.readLine()) != null)\n                        {\n                            try\n                            {\n                                startURLs.add(new URL(line));\n                                count++;\n                            }\n                            catch (MalformedURLException e)\n                            {\n                                System.out.println(\"Malformed URL '\" + line + \"' in line \" + (count+1) + \" of file \" + fileName);\n\n                            }\n                        }\n                        r.close();\n                        System.out.println(\"added \" + count + \" URLs from \" + fileName);\n                    }\n                    catch(IOException e)\n                    {\n                        System.out.println(\"Couldn't read '\" + fileName + \"': \" + e);\n                    }\n                }\n                else\n                {\n                    System.out.println(\"got URL \" + arg);\n                    try\n                    {\n                        startURLs.add(new URL(arg));\n                        System.out.println(\"Start-URL added: \" + arg);\n                    }\n                    catch (MalformedURLException e)\n                    {\n                        System.out.println(\"Malformed URL '\" + arg + \"'\");\n\n                    }\n                }\n            }\n            else if (args[i].equals(\"-restrictto\"))\n            {\n                i++;\n                restrictTo = args[i];\n                System.out.println(\"Restricting URLs to \" + restrictTo);\n            }\n            else if (args[i].equals(\"-threads\"))\n            {\n                i++;\n                nrThreads = Integer.parseInt(args[i]);\n                System.out.println(\"Threads set to \" + nrThreads);\n            }\n            else if (args[i].equals(\"-hostresolver\"))\n            {\n                i++;\n                hostResolverFile = args[i];\n                System.out.println(\"reading host resolver props from  '\" + hostResolverFile + \"'\");\n\n            }\n            else if (args[i].equals(\"-gui\"))\n            {\n                gui = true;\n            }\n            else if (args[i].equals(\"-?\"))\n            {\n                showInfo = true;\n            }\n            else\n            {\n                System.out.println(\"Unknown option: \" + args[i] + \"; use -? to get syntax\");\n                System.exit(0);\n            }\n        }\n\n        //URL.setURLStreamHandlerFactory(new HttpTimeoutFactory(500));\n        // replaced by HTTPClient\n\n        FetcherMain f = new FetcherMain(nrThreads, hostResolverFile);\n        if (showInfo || (startURLs.isEmpty() && gui == false))\n        {\n            System.out.println(\"The LARM crawler\\n\" +\n                               \"\\n\" +\n                               \"The LARM crawler is a fast parallel crawler, currently designed for\\n\" +\n                               \"large intranets (up to a couple hundred hosts with some hundred thousand\\n\" +\n                               \"documents). It is currently restricted by a relatively high memory overhead\\n\" +\n                               \"per crawled host, and by a HashMap of already crawled URLs which is also held\\n\" +\n                               \"in memory.\\n\" +\n                               \"\\n\" +\n                               \"Usage:   FetcherMain <-start <URL>|@<filename>>+ -restrictto <RegEx>\\n\" +\n                               \"                    [-threads <nr=10>] [-hostresolver <filename>]\\n\" +\n                               \"\\n\" +\n                               \"Commands:\\n\" +\n                               \"         -start specify one or more URLs to start with. You can as well specify a file\" +\n                               \"                that contains URLs, one each line\\n\" +\n                               \"         -restrictto a Perl 5 regular expression each URL must match. It is run against the\\n\" +\n                               \"                     _complete_ URL, including the http:// part\\n\" +\n                               \"         -threads  the number of crawling threads. defaults to 10\\n\" +\n                               \"         -hostresolver specify a file that contains rules for changing the host part of \\n\" +\n                               \"                       a URL during the normalization process (experimental).\\n\" +\n                               \"Caution: The <RegEx> is applied to the _normalized_ form of a URL.\\n\" +\n                               \"         See URLNormalizer for details\\n\" +\n                               \"Example:\\n\" +\n                               \"    -start @urls1.txt -start @urls2.txt -start http://localhost/ \" +\n                               \"    -restrictto http://[^/]*\\\\.localhost/.* -threads 25\\n\" +\n                               \"\\n\" +\n                               \"The host resolver file may contain the following commands: \\n\" +\n                               \"  startsWith(part1) = part2\\n\" +\n                               \"      if host starts with part1, this part will be replaced by part2\\n\" +\n                               \"   endsWith(part1) = part2\\n\" +\n                               \"       if host ends with part1, this part will be replaced by part2. This is done after\\n\" +\n                               \"       startsWith was processed\\n\" +\n                               \"   synonym(host1) = host2\\n\" +\n                               \"       the keywords startsWith, endsWith and synonym are case sensitive\\n\" +\n                               \"       host1 will be replaced with host2. this is done _after_ startsWith and endsWith was \\n\" +\n                               \"       processed. Due to a bug in BeanUtils, dots are not allowed in the keys (in parentheses)\\n\" +\n                               \"       and have to be escaped with commas. To simplify, commas are also replaced in property \\n\" +\n                               \"       values. So just use commas instead of dots. The resulting host names are only used for \\n\" +\n                               \"       comparisons and do not have to be existing URLs (although the syntax has to be valid).\\n\" +\n                               \"       However, the names will often be passed to java.net.URL which will try to make a DNS name\\n\" +\n                               \"       resolution, which will time out if the server can't be found. \\n\" +\n                               \"   Example:\" +\n                               \"     synonym(www1,host,com) = host,com\\n\" +\n                               \"     startsWith(www,) = ,\\n\" +\n                               \"     endsWith(host1,com) = host,com\\n\" +\n                               \"The crawler will show a status message every 5 seconds, which is printed by ThreadMonitor.java\\n\" +\n                               \"It will stop after the ThreadMonitor found the message queue and the crawling threads to be idle a \\n\" +\n                               \"couple of times.\\n\" +\n                               \"The crawled data will be saved within a logs/ directory. A cachingqueue/ directory is used for\\n\" +\n                               \"temporary queues.\\n\" +\n                               \"Note that this implementation is experimental, and that the command line options cover only a part \\n\" +\n                               \"of the parameters. Much of the configuration can only be done by modifying FetcherMain.java\\n\");\n            System.exit(0);\n        }\n        try\n        {\n            f.setRexString(restrictTo);\n\n            if (gui)\n            {\n                // f.initGui(f, startURL);\n                // the GUI is not longer supported\n            }\n            else\n            {\n                f.startMonitor();\n                for(Iterator it = startURLs.iterator(); it.hasNext(); )\n                {\n                    f.putURL((URL)it.next(), false);\n                }\n            }\n        }\n        catch (MalformedPatternException e)\n        {\n            System.out.println(\"Wrong RegEx syntax. Must be a valid PERL RE\");\n        }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"2b24dfe829236e25670a0fb6bd671abc4e2a91f9":["2a383cc27dffe14c5d655eb57a87a1c7ae80b23a"],"2a383cc27dffe14c5d655eb57a87a1c7ae80b23a":["05d36e0b328ec96237035fbcca240e73631396e5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"afc16d717d9ed1a8e45371668ca6de674164d624":["00827ccb785ffa5db0167c3ae7dc48969c3ba4f9"],"00827ccb785ffa5db0167c3ae7dc48969c3ba4f9":["2b24dfe829236e25670a0fb6bd671abc4e2a91f9"],"05d36e0b328ec96237035fbcca240e73631396e5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["afc16d717d9ed1a8e45371668ca6de674164d624"]},"commit2Childs":{"2b24dfe829236e25670a0fb6bd671abc4e2a91f9":["00827ccb785ffa5db0167c3ae7dc48969c3ba4f9"],"2a383cc27dffe14c5d655eb57a87a1c7ae80b23a":["2b24dfe829236e25670a0fb6bd671abc4e2a91f9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["05d36e0b328ec96237035fbcca240e73631396e5"],"afc16d717d9ed1a8e45371668ca6de674164d624":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"00827ccb785ffa5db0167c3ae7dc48969c3ba4f9":["afc16d717d9ed1a8e45371668ca6de674164d624"],"05d36e0b328ec96237035fbcca240e73631396e5":["2a383cc27dffe14c5d655eb57a87a1c7ae80b23a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}