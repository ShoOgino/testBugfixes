{"path":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","commits":[{"id":"4ec5d4cc15bae497db86ea4e1f7ea8ee7c1b9e5b","date":1275753210,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom() throws Exception {\n\n    for(int iter=0;iter<2*_TestUtil.getRandomMultiplier();iter++) {\n      Directory dir = new MockRAMDirectory();\n      IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Random r = new Random();\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = r.nextInt(100*_TestUtil.getRandomMultiplier());\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        int count = 0;\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n            count++;\n          }\n        }\n        //System.out.println(\"c=\" + count + \" t=\" + term);\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c4300cda4bfcf74345e3fbb422f79b4128174824","date":1276767246,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    for(int iter=0;iter<2*_TestUtil.getRandomMultiplier();iter++) {\n      Directory dir = new MockRAMDirectory();\n      IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Random r = new Random();\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(r, 1, 100*_TestUtil.getRandomMultiplier());\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        int count = 0;\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n            count++;\n          }\n        }\n        //System.out.println(\"c=\" + count + \" t=\" + term);\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    for(int iter=0;iter<2*_TestUtil.getRandomMultiplier();iter++) {\n      Directory dir = new MockRAMDirectory();\n      IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Random r = new Random();\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = r.nextInt(100*_TestUtil.getRandomMultiplier());\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        int count = 0;\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n            count++;\n          }\n        }\n        //System.out.println(\"c=\" + count + \" t=\" + term);\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"35bc676cb5a9ea7713a7d8245cfbba0d3fea63c9","date":1279033387,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    for(int iter=0;iter<2*_TestUtil.getRandomMultiplier();iter++) {\n      Directory dir = new MockRAMDirectory();\n      IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Random r = new Random();\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(r, 1, 100*_TestUtil.getRandomMultiplier());\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    for(int iter=0;iter<2*_TestUtil.getRandomMultiplier();iter++) {\n      Directory dir = new MockRAMDirectory();\n      IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Random r = new Random();\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(r, 1, 100*_TestUtil.getRandomMultiplier());\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        int count = 0;\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n            count++;\n          }\n        }\n        //System.out.println(\"c=\" + count + \" t=\" + term);\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    for(int iter=0;iter<2*_TestUtil.getRandomMultiplier();iter++) {\n      Directory dir = new MockRAMDirectory();\n      IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Random r = new Random();\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(r, 1, 100*_TestUtil.getRandomMultiplier());\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    for(int iter=0;iter<2*_TestUtil.getRandomMultiplier();iter++) {\n      Directory dir = new MockRAMDirectory();\n      IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Random r = new Random();\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = r.nextInt(100*_TestUtil.getRandomMultiplier());\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        int count = 0;\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n            count++;\n          }\n        }\n        //System.out.println(\"c=\" + count + \" t=\" + term);\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"163fe85a71d778fd2b7747f65ca27b54829e2e57","date":1279898785,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    Random r = newRandom();\n\n    for(int iter=0;iter<2*_TestUtil.getRandomMultiplier();iter++) {\n      Directory dir = new MockRAMDirectory();\n\n      IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(r, 1, 100*_TestUtil.getRandomMultiplier());\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      // nocommit\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    for(int iter=0;iter<2*_TestUtil.getRandomMultiplier();iter++) {\n      Directory dir = new MockRAMDirectory();\n      IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Random r = new Random();\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(r, 1, 100*_TestUtil.getRandomMultiplier());\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"24b91b08ba3110a0904b8ba9803276bf9a9b5f6d","date":1279972526,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    Random r = newRandom();\n\n    for(int iter=0;iter<2*_TestUtil.getRandomMultiplier();iter++) {\n      Directory dir = new MockRAMDirectory();\n\n      IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(r, 1, 100*_TestUtil.getRandomMultiplier());\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    Random r = newRandom();\n\n    for(int iter=0;iter<2*_TestUtil.getRandomMultiplier();iter++) {\n      Directory dir = new MockRAMDirectory();\n\n      IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(r, 1, 100*_TestUtil.getRandomMultiplier());\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      // nocommit\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4b103252dee6afa1b6d7a622c773d178788eb85a","date":1280180143,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    Random r = newRandom();\n\n    for(int iter=0;iter<2*_TestUtil.getRandomMultiplier();iter++) {\n      Directory dir = new MockRAMDirectory();\n\n      IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(r, 1, 100*_TestUtil.getRandomMultiplier());\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    for(int iter=0;iter<2*_TestUtil.getRandomMultiplier();iter++) {\n      Directory dir = new MockRAMDirectory();\n      IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Random r = new Random();\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(r, 1, 100*_TestUtil.getRandomMultiplier());\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0e45742e10e8e3b98e854babe6dbb07a4197b71","date":1280230285,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    Random r = newRandom();\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = new MockRAMDirectory();\n\n      IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(r, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    Random r = newRandom();\n\n    for(int iter=0;iter<2*_TestUtil.getRandomMultiplier();iter++) {\n      Directory dir = new MockRAMDirectory();\n\n      IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(r, 1, 100*_TestUtil.getRandomMultiplier());\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3242a09f703274d3b9283f2064a1a33064b53a1b","date":1280263474,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    Random r = newRandom();\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = new MockRAMDirectory();\n\n      IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(r, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    for(int iter=0;iter<2*_TestUtil.getRandomMultiplier();iter++) {\n      Directory dir = new MockRAMDirectory();\n      IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Random r = new Random();\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(r, 1, 100*_TestUtil.getRandomMultiplier());\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b21422ff1d1d56499dec481f193b402e5e8def5b","date":1281472367,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    Random r = newRandom();\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = new MockRAMDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(r, TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(r, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    Random r = newRandom();\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = new MockRAMDirectory();\n\n      IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(r, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c","date":1281646583,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    Random r = newRandom();\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory(r);\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(r, TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(r, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    Random r = newRandom();\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = new MockRAMDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(r, TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(r, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1f653cfcf159baeaafe5d01682a911e95bba4012","date":1284122058,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    Random r = newRandom();\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory(r);\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(r, TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(r, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"132903c28af3aa6f67284b78de91c0f0a99488c2","date":1284282129,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    Random r = newRandom();\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = new MockRAMDirectory();\n\n      IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(r, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = new Field(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = r.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && r.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(r.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(r, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (r.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && r.nextInt(20) == 1) {\n          int delID = r.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(r.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b1add9ddc0005b07550d4350720aac22dc9886b3","date":1295549635,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e79a6d080bdd5b2a8f56342cf571b5476de04180","date":1295638686,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3","date":1297940445,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f1bdbf92da222965b46c0a942c3857ba56e5c638","date":1298297608,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f83af14a2a8131b14d7aee6274c740334e0363d3","date":1307579822,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"77cf4379b2824f6ea34b091c495d6e95c38ff9e2","date":1307610475,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","date":1307729864,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = 2 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","date":1309960478,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(liveDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(liveDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(liveDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits delDocs = MultiFields.getDeletedDocs(reader);\n      for(int delDoc : deleted) {\n        assertTrue(delDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(delDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"60ba444201d2570214b6fcf1d15600dc1a01f548","date":1313868045,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n      w.setInfoStream(VERBOSE ? System.out : null);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = terms2.docs(liveDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()));\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      //System.out.println(\"TEST reader=\" + reader);\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek to term= \"+ UnicodeUtil.toHexString(term.utf8ToString()));\n        }\n        \n        DocsEnum docsEnum = terms2.docs(liveDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n      w.setInfoStream(VERBOSE ? System.out : null);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = terms2.docs(liveDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n      w.setInfoStream(VERBOSE ? System.out : null);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = terms2.docs(liveDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = terms2.docs(liveDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n      w.setInfoStream(VERBOSE ? System.out : null);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = terms2.docs(liveDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e2297162a22c55456e200caef2cbcb00fe381120","date":1321551342,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = MultiFields.getTermDocsEnum(reader, liveDocs, \"field\", term);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n      Terms terms2 = MultiFields.getTerms(reader, \"field\");\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = terms2.docs(liveDocs, term, null);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"872cff1d3a554e0cd64014cd97f88d3002b0f491","date":1323024658,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random, reader, \"field\", term, liveDocs, null, false);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = MultiFields.getTermDocsEnum(reader, liveDocs, \"field\", term);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b65b350ca9588f9fc76ce7d6804160d06c45ff42","date":1323026297,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random, reader, \"field\", term, liveDocs, null, false);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = MultiFields.getTermDocsEnum(reader, liveDocs, \"field\", term);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestMultiFields#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random, reader, \"field\", term, liveDocs, null, false);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n\n    int num = atLeast(2);\n    for (int iter = 0; iter < num; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      Directory dir = newDirectory();\n\n      IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(NoMergePolicy.COMPOUND_FILES));\n      _TestUtil.keepFullyDeletedSegments(w);\n\n      Map<BytesRef,List<Integer>> docs = new HashMap<BytesRef,List<Integer>>();\n      Set<Integer> deleted = new HashSet<Integer>();\n      List<BytesRef> terms = new ArrayList<BytesRef>();\n\n      int numDocs = _TestUtil.nextInt(random, 1, 100 * RANDOM_MULTIPLIER);\n      Document doc = new Document();\n      Field f = newField(\"field\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(f);\n      Field id = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(id);\n\n      boolean onlyUniqueTerms = random.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: onlyUniqueTerms=\" + onlyUniqueTerms + \" numDocs=\" + numDocs);\n      }\n      Set<BytesRef> uniqueTerms = new HashSet<BytesRef>();\n      for(int i=0;i<numDocs;i++) {\n\n        if (!onlyUniqueTerms && random.nextBoolean() && terms.size() > 0) {\n          // re-use existing term\n          BytesRef term = terms.get(random.nextInt(terms.size()));\n          docs.get(term).add(i);\n          f.setValue(term.utf8ToString());\n        } else {\n          String s = _TestUtil.randomUnicodeString(random, 10);\n          BytesRef term = new BytesRef(s);\n          if (!docs.containsKey(term)) {\n            docs.put(term, new ArrayList<Integer>());\n          }\n          docs.get(term).add(i);\n          terms.add(term);\n          uniqueTerms.add(term);\n          f.setValue(s);\n        }\n        id.setValue(\"\"+i);\n        w.addDocument(doc);\n        if (random.nextInt(4) == 1) {\n          w.commit();\n        }\n        if (i > 0 && random.nextInt(20) == 1) {\n          int delID = random.nextInt(i);\n          deleted.add(delID);\n          w.deleteDocuments(new Term(\"id\", \"\"+delID));\n          if (VERBOSE) {\n            System.out.println(\"TEST: delete \" + delID);\n          }\n        }\n      }\n\n      if (VERBOSE) {\n        List<BytesRef> termsList = new ArrayList<BytesRef>(uniqueTerms);\n        Collections.sort(termsList, BytesRef.getUTF8SortedAsUTF16Comparator());\n        System.out.println(\"TEST: terms in UTF16 order:\");\n        for(BytesRef b : termsList) {\n          System.out.println(\"  \" + UnicodeUtil.toHexString(b.utf8ToString()) + \" \" + b);\n          for(int docID : docs.get(b)) {\n            if (deleted.contains(docID)) {\n              System.out.println(\"    \" + docID + \" (deleted)\");\n            } else {\n              System.out.println(\"    \" + docID);\n            }\n          }\n        }\n      }\n\n      IndexReader reader = w.getReader();\n      w.close();\n      if (VERBOSE) {\n        System.out.println(\"TEST: reader=\" + reader);\n      }\n\n      Bits liveDocs = MultiFields.getLiveDocs(reader);\n      for(int delDoc : deleted) {\n        assertFalse(liveDocs.get(delDoc));\n      }\n\n      for(int i=0;i<100;i++) {\n        BytesRef term = terms.get(random.nextInt(terms.size()));\n        if (VERBOSE) {\n          System.out.println(\"TEST: seek term=\"+ UnicodeUtil.toHexString(term.utf8ToString()) + \" \" + term);\n        }\n        \n        DocsEnum docsEnum = _TestUtil.docs(random, reader, \"field\", term, liveDocs, null, false);\n        assertNotNull(docsEnum);\n\n        for(int docID : docs.get(term)) {\n          if (!deleted.contains(docID)) {\n            assertEquals(docID, docsEnum.nextDoc());\n          }\n        }\n        assertEquals(docsEnum.NO_MORE_DOCS, docsEnum.nextDoc());\n      }\n\n      reader.close();\n      dir.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e45742e10e8e3b98e854babe6dbb07a4197b71":["4b103252dee6afa1b6d7a622c773d178788eb85a"],"f83af14a2a8131b14d7aee6274c740334e0363d3":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"f1bdbf92da222965b46c0a942c3857ba56e5c638":["29ef99d61cda9641b6250bf9567329a6e65f901d","e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["872cff1d3a554e0cd64014cd97f88d3002b0f491"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["f83af14a2a8131b14d7aee6274c740334e0363d3"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"b1add9ddc0005b07550d4350720aac22dc9886b3":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["3242a09f703274d3b9283f2064a1a33064b53a1b","132903c28af3aa6f67284b78de91c0f0a99488c2"],"35bc676cb5a9ea7713a7d8245cfbba0d3fea63c9":["c4300cda4bfcf74345e3fbb422f79b4128174824"],"e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3":["b1add9ddc0005b07550d4350720aac22dc9886b3"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["132903c28af3aa6f67284b78de91c0f0a99488c2","b1add9ddc0005b07550d4350720aac22dc9886b3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["60ba444201d2570214b6fcf1d15600dc1a01f548"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["a0e45742e10e8e3b98e854babe6dbb07a4197b71"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["e79a6d080bdd5b2a8f56342cf571b5476de04180","e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3"],"24b91b08ba3110a0904b8ba9803276bf9a9b5f6d":["163fe85a71d778fd2b7747f65ca27b54829e2e57"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["a3776dccca01c11e7046323cfad46a3b4a471233","f83af14a2a8131b14d7aee6274c740334e0363d3"],"3242a09f703274d3b9283f2064a1a33064b53a1b":["5f4e87790277826a2aea119328600dfb07761f32","a0e45742e10e8e3b98e854babe6dbb07a4197b71"],"4b103252dee6afa1b6d7a622c773d178788eb85a":["35bc676cb5a9ea7713a7d8245cfbba0d3fea63c9","24b91b08ba3110a0904b8ba9803276bf9a9b5f6d"],"163fe85a71d778fd2b7747f65ca27b54829e2e57":["35bc676cb5a9ea7713a7d8245cfbba0d3fea63c9"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["b21422ff1d1d56499dec481f193b402e5e8def5b"],"06584e6e98d592b34e1329b384182f368d2025e8":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["f1bdbf92da222965b46c0a942c3857ba56e5c638","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"e2297162a22c55456e200caef2cbcb00fe381120":["06584e6e98d592b34e1329b384182f368d2025e8"],"872cff1d3a554e0cd64014cd97f88d3002b0f491":["e2297162a22c55456e200caef2cbcb00fe381120"],"5f4e87790277826a2aea119328600dfb07761f32":["4ec5d4cc15bae497db86ea4e1f7ea8ee7c1b9e5b","35bc676cb5a9ea7713a7d8245cfbba0d3fea63c9"],"962d04139994fce5193143ef35615499a9a96d78":["bde51b089eb7f86171eb3406e38a274743f9b7ac","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["f83af14a2a8131b14d7aee6274c740334e0363d3","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"e79a6d080bdd5b2a8f56342cf571b5476de04180":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","b1add9ddc0005b07550d4350720aac22dc9886b3"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"60ba444201d2570214b6fcf1d15600dc1a01f548":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"b65b350ca9588f9fc76ce7d6804160d06c45ff42":["e2297162a22c55456e200caef2cbcb00fe381120","872cff1d3a554e0cd64014cd97f88d3002b0f491"],"c4300cda4bfcf74345e3fbb422f79b4128174824":["4ec5d4cc15bae497db86ea4e1f7ea8ee7c1b9e5b"],"a3776dccca01c11e7046323cfad46a3b4a471233":["e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"4ec5d4cc15bae497db86ea4e1f7ea8ee7c1b9e5b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":["135621f3a0670a9394eb563224a3b76cc4dddc0f","f83af14a2a8131b14d7aee6274c740334e0363d3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"a0e45742e10e8e3b98e854babe6dbb07a4197b71":["b21422ff1d1d56499dec481f193b402e5e8def5b","3242a09f703274d3b9283f2064a1a33064b53a1b"],"f83af14a2a8131b14d7aee6274c740334e0363d3":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","d083e83f225b11e5fdd900e83d26ddb385b6955c","77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"f1bdbf92da222965b46c0a942c3857ba56e5c638":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","60ba444201d2570214b6fcf1d15600dc1a01f548"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["b1add9ddc0005b07550d4350720aac22dc9886b3","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","29ef99d61cda9641b6250bf9567329a6e65f901d"],"b1add9ddc0005b07550d4350720aac22dc9886b3":["e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3","29ef99d61cda9641b6250bf9567329a6e65f901d","e79a6d080bdd5b2a8f56342cf571b5476de04180"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["e79a6d080bdd5b2a8f56342cf571b5476de04180"],"35bc676cb5a9ea7713a7d8245cfbba0d3fea63c9":["4b103252dee6afa1b6d7a622c773d178788eb85a","163fe85a71d778fd2b7747f65ca27b54829e2e57","5f4e87790277826a2aea119328600dfb07761f32"],"e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3":["f1bdbf92da222965b46c0a942c3857ba56e5c638","f2c5f0cb44df114db4228c8f77861714b5cabaea","bde51b089eb7f86171eb3406e38a274743f9b7ac","a3776dccca01c11e7046323cfad46a3b4a471233"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["f83af14a2a8131b14d7aee6274c740334e0363d3","135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","a3776dccca01c11e7046323cfad46a3b4a471233"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["f1bdbf92da222965b46c0a942c3857ba56e5c638"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4ec5d4cc15bae497db86ea4e1f7ea8ee7c1b9e5b"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["06584e6e98d592b34e1329b384182f368d2025e8"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["962d04139994fce5193143ef35615499a9a96d78"],"24b91b08ba3110a0904b8ba9803276bf9a9b5f6d":["4b103252dee6afa1b6d7a622c773d178788eb85a"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["817d8435e9135b756f08ce6710ab0baac51bdf88"],"3242a09f703274d3b9283f2064a1a33064b53a1b":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"4b103252dee6afa1b6d7a622c773d178788eb85a":["a0e45742e10e8e3b98e854babe6dbb07a4197b71"],"163fe85a71d778fd2b7747f65ca27b54829e2e57":["24b91b08ba3110a0904b8ba9803276bf9a9b5f6d"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"06584e6e98d592b34e1329b384182f368d2025e8":["e2297162a22c55456e200caef2cbcb00fe381120"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"e2297162a22c55456e200caef2cbcb00fe381120":["872cff1d3a554e0cd64014cd97f88d3002b0f491","b65b350ca9588f9fc76ce7d6804160d06c45ff42"],"872cff1d3a554e0cd64014cd97f88d3002b0f491":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","b65b350ca9588f9fc76ce7d6804160d06c45ff42"],"5f4e87790277826a2aea119328600dfb07761f32":["3242a09f703274d3b9283f2064a1a33064b53a1b"],"962d04139994fce5193143ef35615499a9a96d78":[],"e79a6d080bdd5b2a8f56342cf571b5476de04180":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"817d8435e9135b756f08ce6710ab0baac51bdf88":[],"60ba444201d2570214b6fcf1d15600dc1a01f548":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"b65b350ca9588f9fc76ce7d6804160d06c45ff42":[],"c4300cda4bfcf74345e3fbb422f79b4128174824":["35bc676cb5a9ea7713a7d8245cfbba0d3fea63c9"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655"],"4ec5d4cc15bae497db86ea4e1f7ea8ee7c1b9e5b":["5f4e87790277826a2aea119328600dfb07761f32","c4300cda4bfcf74345e3fbb422f79b4128174824"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["962d04139994fce5193143ef35615499a9a96d78","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","b65b350ca9588f9fc76ce7d6804160d06c45ff42","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}