{"path":"lucene/misc/src/test/org/apache/lucene/index/TestBlockJoinSorter#test().mjava","commits":[{"id":"82dc51e5dcd7a80b71e40ebe8959b1c43b63f95e","date":1419346542,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/index/TestBlockJoinSorter#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","sourceNew":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final LeafReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null).bits();\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final LeafReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null).bits();\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"60596f28be69b10c37a56a303c2dbea07b2ca4ba","date":1425060541,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/index/TestBlockJoinSorter#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/index/TestBlockJoinSorter#test().mjava","sourceNew":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final LeafReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new BitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null).bits();\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final LeafReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null).bits();\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/misc/src/test/org/apache/lucene/index/TestBlockJoinSorter#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/index/TestBlockJoinSorter#test().mjava","sourceNew":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final LeafReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new BitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null).bits();\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final LeafReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null).bits();\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d52e48927ca4ef3655a261f2230b968b6fdf3608","date":1444652107,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/index/TestBlockJoinSorter#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/index/TestBlockJoinSorter#test().mjava","sourceNew":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final LeafReader reader = getOnlySegmentReader(indexReader);\n    final Query parentsFilter = new TermQuery(new Term(\"parent\", \"true\"));\n    IndexSearcher searcher = newSearcher(reader);\n    final Weight weight = searcher.createNormalizedWeight(parentsFilter, false);\n    final DocIdSetIterator parents = weight.scorer(reader.getContext());\n    final BitSet parentBits = BitSet.of(parents, reader.maxDoc());\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final LeafReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new BitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null).bits();\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":null,"bugIntro":["d2dce5e63b0228e94e989139c2503dd4018d8b45"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"78d7843057b09f6de43cf650943dd904affb00c3","date":1444666279,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/index/TestBlockJoinSorter#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/index/TestBlockJoinSorter#test().mjava","sourceNew":"  @AwaitsFix(bugUrl=\"https://issues.apache.org/jira/browse/LUCENE-6836\")\n  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final LeafReader reader = getOnlySegmentReader(indexReader);\n    final Query parentsFilter = new TermQuery(new Term(\"parent\", \"true\"));\n    IndexSearcher searcher = newSearcher(reader);\n    final Weight weight = searcher.createNormalizedWeight(parentsFilter, false);\n    final DocIdSetIterator parents = weight.scorer(reader.getContext());\n    final BitSet parentBits = BitSet.of(parents, reader.maxDoc());\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final LeafReader reader = getOnlySegmentReader(indexReader);\n    final Query parentsFilter = new TermQuery(new Term(\"parent\", \"true\"));\n    IndexSearcher searcher = newSearcher(reader);\n    final Weight weight = searcher.createNormalizedWeight(parentsFilter, false);\n    final DocIdSetIterator parents = weight.scorer(reader.getContext());\n    final BitSet parentBits = BitSet.of(parents, reader.maxDoc());\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":null,"bugIntro":["d2dce5e63b0228e94e989139c2503dd4018d8b45"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d2dce5e63b0228e94e989139c2503dd4018d8b45","date":1444678473,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/index/TestBlockJoinSorter#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/index/TestBlockJoinSorter#test().mjava","sourceNew":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    IndexReader indexReader = writer.getReader();\n    writer.close();\n\n    IndexSearcher searcher = newSearcher(indexReader);\n    indexReader = searcher.getIndexReader(); // newSearcher may have wrapped it\n    assertEquals(1, indexReader.leaves().size());\n    final LeafReader reader = indexReader.leaves().get(0).reader();\n    final Query parentsFilter = new TermQuery(new Term(\"parent\", \"true\"));\n\n    final Weight weight = searcher.createNormalizedWeight(parentsFilter, false);\n    final DocIdSetIterator parents = weight.scorer(indexReader.leaves().get(0));\n    final BitSet parentBits = BitSet.of(parents, reader.maxDoc());\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","sourceOld":"  @AwaitsFix(bugUrl=\"https://issues.apache.org/jira/browse/LUCENE-6836\")\n  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final LeafReader reader = getOnlySegmentReader(indexReader);\n    final Query parentsFilter = new TermQuery(new Term(\"parent\", \"true\"));\n    IndexSearcher searcher = newSearcher(reader);\n    final Weight weight = searcher.createNormalizedWeight(parentsFilter, false);\n    final DocIdSetIterator parents = weight.scorer(reader.getContext());\n    final BitSet parentBits = BitSet.of(parents, reader.maxDoc());\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":["c9fb5f46e264daf5ba3860defe623a89d202dd87","d52e48927ca4ef3655a261f2230b968b6fdf3608","7a2926dd4be586592be94a073d71c94db8bc7645","78d7843057b09f6de43cf650943dd904affb00c3"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7dd748bb245633a8195281556bb0e68a6ea97d18","date":1449755030,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/index/TestBlockJoinSorter#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/index/TestBlockJoinSorter#test().mjava","sourceNew":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    IndexReader indexReader = writer.getReader();\n    writer.close();\n\n    IndexSearcher searcher = newSearcher(indexReader);\n    indexReader = searcher.getIndexReader(); // newSearcher may have wrapped it\n    assertEquals(1, indexReader.leaves().size());\n    final LeafReader reader = indexReader.leaves().get(0).reader();\n    final Query parentsFilter = new TermQuery(new Term(\"parent\", \"true\"));\n\n    final Weight weight = searcher.createNormalizedWeight(parentsFilter, false);\n    final Scorer parents = weight.scorer(indexReader.leaves().get(0));\n    final BitSet parentBits = BitSet.of(parents.iterator(), reader.maxDoc());\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    IndexReader indexReader = writer.getReader();\n    writer.close();\n\n    IndexSearcher searcher = newSearcher(indexReader);\n    indexReader = searcher.getIndexReader(); // newSearcher may have wrapped it\n    assertEquals(1, indexReader.leaves().size());\n    final LeafReader reader = indexReader.leaves().get(0).reader();\n    final Query parentsFilter = new TermQuery(new Term(\"parent\", \"true\"));\n\n    final Weight weight = searcher.createNormalizedWeight(parentsFilter, false);\n    final DocIdSetIterator parents = weight.scorer(indexReader.leaves().get(0));\n    final BitSet parentBits = BitSet.of(parents, reader.maxDoc());\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5e03940e6e9044943de4b7ac08f8581da37a9534","date":1462870173,"type":4,"author":"Mike McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/misc/src/test/org/apache/lucene/index/TestBlockJoinSorter#test().mjava","sourceNew":null,"sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    IndexReader indexReader = writer.getReader();\n    writer.close();\n\n    IndexSearcher searcher = newSearcher(indexReader);\n    indexReader = searcher.getIndexReader(); // newSearcher may have wrapped it\n    assertEquals(1, indexReader.leaves().size());\n    final LeafReader reader = indexReader.leaves().get(0).reader();\n    final Query parentsFilter = new TermQuery(new Term(\"parent\", \"true\"));\n\n    final Weight weight = searcher.createNormalizedWeight(parentsFilter, false);\n    final Scorer parents = weight.scorer(indexReader.leaves().get(0));\n    final BitSet parentBits = BitSet.of(parents.iterator(), reader.maxDoc());\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3d33e731a93d4b57e662ff094f64f94a745422d4","date":1463128289,"type":4,"author":"Mike McCandless","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/misc/src/test/org/apache/lucene/index/TestBlockJoinSorter#test().mjava","sourceNew":null,"sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    IndexReader indexReader = writer.getReader();\n    writer.close();\n\n    IndexSearcher searcher = newSearcher(indexReader);\n    indexReader = searcher.getIndexReader(); // newSearcher may have wrapped it\n    assertEquals(1, indexReader.leaves().size());\n    final LeafReader reader = indexReader.leaves().get(0).reader();\n    final Query parentsFilter = new TermQuery(new Term(\"parent\", \"true\"));\n\n    final Weight weight = searcher.createNormalizedWeight(parentsFilter, false);\n    final Scorer parents = weight.scorer(indexReader.leaves().get(0));\n    final BitSet parentBits = BitSet.of(parents.iterator(), reader.maxDoc());\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ad30c6a479e764150a3316e57263319775f1df2","date":1463395403,"type":4,"author":"Karl Wright","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/misc/src/test/org/apache/lucene/index/TestBlockJoinSorter#test().mjava","sourceNew":null,"sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    IndexReader indexReader = writer.getReader();\n    writer.close();\n\n    IndexSearcher searcher = newSearcher(indexReader);\n    indexReader = searcher.getIndexReader(); // newSearcher may have wrapped it\n    assertEquals(1, indexReader.leaves().size());\n    final LeafReader reader = indexReader.leaves().get(0).reader();\n    final Query parentsFilter = new TermQuery(new Term(\"parent\", \"true\"));\n\n    final Weight weight = searcher.createNormalizedWeight(parentsFilter, false);\n    final Scorer parents = weight.scorer(indexReader.leaves().get(0));\n    final BitSet parentBits = BitSet.of(parents.iterator(), reader.maxDoc());\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":4,"author":"Noble Paul","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/misc/src/test/org/apache/lucene/index/TestBlockJoinSorter#test().mjava","sourceNew":null,"sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    IndexReader indexReader = writer.getReader();\n    writer.close();\n\n    IndexSearcher searcher = newSearcher(indexReader);\n    indexReader = searcher.getIndexReader(); // newSearcher may have wrapped it\n    assertEquals(1, indexReader.leaves().size());\n    final LeafReader reader = indexReader.leaves().get(0).reader();\n    final Query parentsFilter = new TermQuery(new Term(\"parent\", \"true\"));\n\n    final Weight weight = searcher.createNormalizedWeight(parentsFilter, false);\n    final Scorer parents = weight.scorer(indexReader.leaves().get(0));\n    final BitSet parentBits = BitSet.of(parents.iterator(), reader.maxDoc());\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/misc/src/test/org/apache/lucene/index/TestBlockJoinSorter#test().mjava","sourceNew":null,"sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    IndexReader indexReader = writer.getReader();\n    writer.close();\n\n    IndexSearcher searcher = newSearcher(indexReader);\n    indexReader = searcher.getIndexReader(); // newSearcher may have wrapped it\n    assertEquals(1, indexReader.leaves().size());\n    final LeafReader reader = indexReader.leaves().get(0).reader();\n    final Query parentsFilter = new TermQuery(new Term(\"parent\", \"true\"));\n\n    final Weight weight = searcher.createNormalizedWeight(parentsFilter, false);\n    final Scorer parents = weight.scorer(indexReader.leaves().get(0));\n    final BitSet parentBits = BitSet.of(parents.iterator(), reader.maxDoc());\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"60596f28be69b10c37a56a303c2dbea07b2ca4ba":["82dc51e5dcd7a80b71e40ebe8959b1c43b63f95e"],"0ad30c6a479e764150a3316e57263319775f1df2":["7dd748bb245633a8195281556bb0e68a6ea97d18","3d33e731a93d4b57e662ff094f64f94a745422d4"],"82dc51e5dcd7a80b71e40ebe8959b1c43b63f95e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["7dd748bb245633a8195281556bb0e68a6ea97d18","d470c8182e92b264680e34081b75e70a9f2b3c89"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["7dd748bb245633a8195281556bb0e68a6ea97d18","0ad30c6a479e764150a3316e57263319775f1df2"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["82dc51e5dcd7a80b71e40ebe8959b1c43b63f95e","60596f28be69b10c37a56a303c2dbea07b2ca4ba"],"d52e48927ca4ef3655a261f2230b968b6fdf3608":["60596f28be69b10c37a56a303c2dbea07b2ca4ba"],"78d7843057b09f6de43cf650943dd904affb00c3":["d52e48927ca4ef3655a261f2230b968b6fdf3608"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5e03940e6e9044943de4b7ac08f8581da37a9534":["7dd748bb245633a8195281556bb0e68a6ea97d18"],"7dd748bb245633a8195281556bb0e68a6ea97d18":["d2dce5e63b0228e94e989139c2503dd4018d8b45"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["7dd748bb245633a8195281556bb0e68a6ea97d18","5e03940e6e9044943de4b7ac08f8581da37a9534"],"d2dce5e63b0228e94e989139c2503dd4018d8b45":["78d7843057b09f6de43cf650943dd904affb00c3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d470c8182e92b264680e34081b75e70a9f2b3c89"]},"commit2Childs":{"60596f28be69b10c37a56a303c2dbea07b2ca4ba":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","d52e48927ca4ef3655a261f2230b968b6fdf3608"],"0ad30c6a479e764150a3316e57263319775f1df2":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"82dc51e5dcd7a80b71e40ebe8959b1c43b63f95e":["60596f28be69b10c37a56a303c2dbea07b2ca4ba","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"d470c8182e92b264680e34081b75e70a9f2b3c89":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"d52e48927ca4ef3655a261f2230b968b6fdf3608":["78d7843057b09f6de43cf650943dd904affb00c3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["82dc51e5dcd7a80b71e40ebe8959b1c43b63f95e"],"78d7843057b09f6de43cf650943dd904affb00c3":["d2dce5e63b0228e94e989139c2503dd4018d8b45"],"5e03940e6e9044943de4b7ac08f8581da37a9534":["3d33e731a93d4b57e662ff094f64f94a745422d4"],"7dd748bb245633a8195281556bb0e68a6ea97d18":["0ad30c6a479e764150a3316e57263319775f1df2","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d470c8182e92b264680e34081b75e70a9f2b3c89","5e03940e6e9044943de4b7ac08f8581da37a9534","3d33e731a93d4b57e662ff094f64f94a745422d4"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["0ad30c6a479e764150a3316e57263319775f1df2"],"d2dce5e63b0228e94e989139c2503dd4018d8b45":["7dd748bb245633a8195281556bb0e68a6ea97d18"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}