{"path":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","commits":[{"id":"893269407f5c988a4b2ee13c7ffc01ec43136c1d","date":1268598046,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"/dev/null","sourceNew":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(prefix);\n        startTerm = te.getTermNumber();\n        te.skipTo(prefix + \"\\uffff\\uffff\\uffff\\uffff\");\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompases both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          String label = ft.indexedToReadable(getTermText(te, tnum));\n          res.add(label, c);\n        }\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = ft.indexedToReadable(getTermText(te, i));\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","date":1268599006,"type":4,"author":"Mark Robert Miller","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":null,"sourceOld":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(prefix);\n        startTerm = te.getTermNumber();\n        te.skipTo(prefix + \"\\uffff\\uffff\\uffff\\uffff\");\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompases both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          String label = ft.indexedToReadable(getTermText(te, tnum));\n          res.add(label, c);\n        }\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = ft.indexedToReadable(getTermText(te, i));\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(prefix);\n        startTerm = te.getTermNumber();\n        te.skipTo(prefix + \"\\uffff\\uffff\\uffff\\uffff\");\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompases both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          String label = ft.indexedToReadable(getTermText(te, tnum));\n          res.add(label, c);\n        }\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = ft.indexedToReadable(getTermText(te, i));\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(prefix);\n        startTerm = te.getTermNumber();\n        te.skipTo(prefix + \"\\uffff\\uffff\\uffff\\uffff\");\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompases both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          String label = ft.indexedToReadable(getTermText(te, tnum));\n          res.add(label, c);\n        }\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = ft.indexedToReadable(getTermText(te, i));\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1da8d55113b689b06716246649de6f62430f15c0","date":1453508340,"type":0,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"/dev/null","sourceNew":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(prefix);\n        startTerm = te.getTermNumber();\n        te.skipTo(prefix + \"\\uffff\\uffff\\uffff\\uffff\");\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompases both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          String label = ft.indexedToReadable(getTermText(te, tnum));\n          res.add(label, c);\n        }\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = ft.indexedToReadable(getTermText(te, i));\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"28427ef110c4c5bf5b4057731b83110bd1e13724","date":1276701452,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(new BytesRef(prefix));\n        startTerm = te.getTermNumber();\n        te.skipTo(new BytesRef(prefix + \"\\uffff\\uffff\\uffff\\uffff\"));\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term.utf8ToString())), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompases both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.add(label, c);\n        }\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(prefix);\n        startTerm = te.getTermNumber();\n        te.skipTo(prefix + \"\\uffff\\uffff\\uffff\\uffff\");\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompases both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          String label = ft.indexedToReadable(getTermText(te, tnum));\n          res.add(label, c);\n        }\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = ft.indexedToReadable(getTermText(te, i));\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4f29ba80b723649f5feb7e37afe1a558dd2c1304","date":1278318805,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(new BytesRef(prefix));\n        startTerm = te.getTermNumber();\n        te.skipTo(new BytesRef(prefix + \"\\uffff\\uffff\\uffff\\uffff\"));\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompases both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.add(label, c);\n        }\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(new BytesRef(prefix));\n        startTerm = te.getTermNumber();\n        te.skipTo(new BytesRef(prefix + \"\\uffff\\uffff\\uffff\\uffff\"));\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term.utf8ToString())), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompases both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.add(label, c);\n        }\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(new BytesRef(prefix));\n        startTerm = te.getTermNumber();\n        te.skipTo(new BytesRef(prefix + \"\\uffff\\uffff\\uffff\\uffff\"));\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompases both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.add(label, c);\n        }\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(prefix);\n        startTerm = te.getTermNumber();\n        te.skipTo(prefix + \"\\uffff\\uffff\\uffff\\uffff\");\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompases both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          String label = ft.indexedToReadable(getTermText(te, tnum));\n          res.add(label, c);\n        }\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = ft.indexedToReadable(getTermText(te, i));\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"222ea1c48b8e68b304965aced7ce915aa78588ca","date":1282780459,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(new BytesRef(prefix));\n        startTerm = te.getTermNumber();\n        te.skipTo(new BytesRef(prefix + \"\\uffff\\uffff\\uffff\\uffff\"));\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompasses both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n\n\n        final int[] tnums = new int[Math.min(queue.size()-off, lim)];\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        int tnumCount = 0;\n\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          indirect[tnumCount] = tnumCount;\n          tnums[tnumCount++] = tnum;\n          // String label = ft.indexedToReadable(getTermText(te, tnum));\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(0, tnumCount, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return tnums[a] - tnums[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=0; i<tnumCount; i++) {\n          int idx = indirect[i];\n          int tnum = tnums[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);          \n          res.setName(idx, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(new BytesRef(prefix));\n        startTerm = te.getTermNumber();\n        te.skipTo(new BytesRef(prefix + \"\\uffff\\uffff\\uffff\\uffff\"));\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompases both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.add(label, c);\n        }\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8ee64fe110d7ad3ba967e05a18218d79eeb2bfd9","date":1282860326,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(new BytesRef(prefix));\n        startTerm = te.getTermNumber();\n        te.skipTo(new BytesRef(prefix + \"\\uffff\\uffff\\uffff\\uffff\"));\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompasses both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n\n\n        final int[] tnums = new int[Math.min(Math.max(0, queue.size()-off), lim)];\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= tnums.length;\n        \n        int tnumCount = 0;\n\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          indirect[tnumCount] = tnumCount;\n          tnums[tnumCount++] = tnum;\n          // String label = ft.indexedToReadable(getTermText(te, tnum));\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(0, tnumCount, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return tnums[a] - tnums[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=0; i<tnumCount; i++) {\n          int idx = indirect[i];\n          int tnum = tnums[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);          \n          res.setName(idx, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(new BytesRef(prefix));\n        startTerm = te.getTermNumber();\n        te.skipTo(new BytesRef(prefix + \"\\uffff\\uffff\\uffff\\uffff\"));\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompasses both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n\n\n        final int[] tnums = new int[Math.min(queue.size()-off, lim)];\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        int tnumCount = 0;\n\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          indirect[tnumCount] = tnumCount;\n          tnums[tnumCount++] = tnum;\n          // String label = ft.indexedToReadable(getTermText(te, tnum));\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(0, tnumCount, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return tnums[a] - tnums[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=0; i<tnumCount; i++) {\n          int idx = indirect[i];\n          int tnum = tnums[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);          \n          res.setName(idx, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e789a6ee3a5fa41394cbed2293ff68c3712c32a9","date":1283533946,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(new BytesRef(prefix));\n        startTerm = te.getTermNumber();\n        te.skipTo(new BytesRef(prefix + \"\\uffff\\uffff\\uffff\\uffff\"));\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount < lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(new BytesRef(prefix));\n        startTerm = te.getTermNumber();\n        te.skipTo(new BytesRef(prefix + \"\\uffff\\uffff\\uffff\\uffff\"));\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompasses both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n\n\n        final int[] tnums = new int[Math.min(Math.max(0, queue.size()-off), lim)];\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= tnums.length;\n        \n        int tnumCount = 0;\n\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          indirect[tnumCount] = tnumCount;\n          tnums[tnumCount++] = tnum;\n          // String label = ft.indexedToReadable(getTermText(te, tnum));\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(0, tnumCount, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return tnums[a] - tnums[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=0; i<tnumCount; i++) {\n          int idx = indirect[i];\n          int tnum = tnums[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);          \n          res.setName(idx, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"98807419200815580d853d74ffde5d2af0406d30","date":1283539270,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(new BytesRef(prefix));\n        startTerm = te.getTermNumber();\n        te.skipTo(new BytesRef(prefix + \"\\uffff\\uffff\\uffff\\uffff\"));\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(new BytesRef(prefix));\n        startTerm = te.getTermNumber();\n        te.skipTo(new BytesRef(prefix + \"\\uffff\\uffff\\uffff\\uffff\"));\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount < lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"627ce218a5a68018115c2deb6559b41e3665b8ab","date":1284500689,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        BytesRef prefixBr = new BytesRef(prefix);\n        te.skipTo(prefixBr);\n        startTerm = te.getTermNumber();\n        prefixBr.append(ByteUtils.bigTerm);\n        te.skipTo(prefixBr);\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(new BytesRef(prefix));\n        startTerm = te.getTermNumber();\n        te.skipTo(new BytesRef(prefix + \"\\uffff\\uffff\\uffff\\uffff\"));\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        BytesRef prefixBr = new BytesRef(prefix);\n        te.skipTo(prefixBr);\n        startTerm = te.getTermNumber();\n        prefixBr.append(ByteUtils.bigTerm);\n        te.skipTo(prefixBr);\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      final int[] counts = new int[numTermsInField];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        te.skipTo(new BytesRef(prefix));\n        startTerm = te.getTermNumber();\n        te.skipTo(new BytesRef(prefix + \"\\uffff\\uffff\\uffff\\uffff\"));\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        final BoundedTreeSet<Long> queue = new BoundedTreeSet<Long>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // minimize object creation and speed comparison by creating a long that\n            // encompases both count and term number.\n            // Since smaller values are kept in the TreeSet, make higher counts smaller.\n            //\n            //   for equal counts, lower term numbers\n            // should come first and hence be \"greater\"\n\n            //long pair = (((long)c)<<32) | (0x7fffffff-i) ;   // use if priority queue\n            long pair = (((long)-c)<<32) | i;\n            queue.add(new Long(pair));\n            if (queue.size()>=maxsize) min=-(int)(queue.last().longValue() >>> 32);\n          }\n        }\n        // now select the right page from the results\n        for (Long p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          int c = -(int)(p.longValue() >>> 32);\n          //int tnum = 0x7fffffff - (int)p.longValue();  // use if priority queue\n          int tnum = (int)p.longValue();\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.add(label, c);\n        }\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"755f2f419306d7297c8feee10d1897addf4b2dd0","date":1294442354,"type":3,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        BytesRef prefixBr = new BytesRef(prefix);\n        te.skipTo(prefixBr);\n        startTerm = te.getTermNumber();\n        prefixBr.append(ByteUtils.bigTerm);\n        te.skipTo(prefixBr);\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        BytesRef prefixBr = new BytesRef(prefix);\n        te.skipTo(prefixBr);\n        startTerm = te.getTermNumber();\n        prefixBr.append(ByteUtils.bigTerm);\n        te.skipTo(prefixBr);\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c220849f876de24a79f756f65b3eb045db59f63f","date":1294902803,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        BytesRef prefixBr = new BytesRef(prefix);\n        te.skipTo(prefixBr);\n        startTerm = te.getTermNumber();\n        prefixBr.append(ByteUtils.bigTerm);\n        te.skipTo(prefixBr);\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        BytesRef prefixBr = new BytesRef(prefix);\n        te.skipTo(prefixBr);\n        startTerm = te.getTermNumber();\n        prefixBr.append(ByteUtils.bigTerm);\n        te.skipTo(prefixBr);\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        BytesRef prefixBr = new BytesRef(prefix);\n        te.skipTo(prefixBr);\n        startTerm = te.getTermNumber();\n        prefixBr.append(ByteUtils.bigTerm);\n        te.skipTo(prefixBr);\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        BytesRef prefixBr = new BytesRef(prefix);\n        te.skipTo(prefixBr);\n        startTerm = te.getTermNumber();\n        prefixBr.append(ByteUtils.bigTerm);\n        te.skipTo(prefixBr);\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        BytesRef prefixBr = new BytesRef(prefix);\n        te.skipTo(prefixBr);\n        startTerm = te.getTermNumber();\n        prefixBr.append(ByteUtils.bigTerm);\n        te.skipTo(prefixBr);\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList res = new NamedList();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n      if (prefix != null && prefix.length() > 0) {\n        BytesRef prefixBr = new BytesRef(prefix);\n        te.skipTo(prefixBr);\n        startTerm = te.getTermNumber();\n        prefixBr.append(ByteUtils.bigTerm);\n        te.skipTo(prefixBr);\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"95ae76773bf2b95987d5f9c8f566ab3738953fb4","date":1301758351,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(ByteUtils.bigTerm);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        BytesRef prefixBr = new BytesRef(prefix);\n        te.skipTo(prefixBr);\n        startTerm = te.getTermNumber();\n        prefixBr.append(ByteUtils.bigTerm);\n        te.skipTo(prefixBr);\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"45669a651c970812a680841b97a77cce06af559f","date":1301922222,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(ByteUtils.bigTerm);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        BytesRef prefixBr = new BytesRef(prefix);\n        te.skipTo(prefixBr);\n        startTerm = te.getTermNumber();\n        prefixBr.append(ByteUtils.bigTerm);\n        te.skipTo(prefixBr);\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(ByteUtils.bigTerm);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        BytesRef prefixBr = new BytesRef(prefix);\n        te.skipTo(prefixBr);\n        startTerm = te.getTermNumber();\n        prefixBr.append(ByteUtils.bigTerm);\n        te.skipTo(prefixBr);\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(ByteUtils.bigTerm);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      NumberedTermsEnum te = ti.getEnumerator(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        BytesRef prefixBr = new BytesRef(prefix);\n        te.skipTo(prefixBr);\n        startTerm = te.getTermNumber();\n        prefixBr.append(ByteUtils.bigTerm);\n        te.skipTo(prefixBr);\n        endTerm = te.getTermNumber();\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(ti.field, tt.term)), docs);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n            }\n          } else {\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set as the label\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n\n      te.close();\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a1b3a24d5d9b47345473ff564f5cc127a7b526b4","date":1306277076,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(ByteUtils.bigTerm);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","date":1306767085,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(ByteUtils.bigTerm);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2e10cb22a8bdb44339e282925a29182bb2f3174d","date":1306841137,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(ByteUtils.bigTerm);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n\n      CharArr spare = new CharArr();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          String label = getReadableValue(getTermValue(te, tnum), ft, spare);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          String label = getReadableValue(getTermValue(te, i), ft, spare);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fd9cc9d77712aba3662f24632df7539ab75e3667","date":1309095238,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seek(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":5,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"c26f00b574427b55127e869b935845554afde1fa":["fd9cc9d77712aba3662f24632df7539ab75e3667","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"8ee64fe110d7ad3ba967e05a18218d79eeb2bfd9":["222ea1c48b8e68b304965aced7ce915aa78588ca"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["5f4e87790277826a2aea119328600dfb07761f32","627ce218a5a68018115c2deb6559b41e3665b8ab"],"1da8d55113b689b06716246649de6f62430f15c0":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","ad94625fb8d088209f46650c8097196fec67f00c"],"c220849f876de24a79f756f65b3eb045db59f63f":["755f2f419306d7297c8feee10d1897addf4b2dd0"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["a3776dccca01c11e7046323cfad46a3b4a471233","a1b3a24d5d9b47345473ff564f5cc127a7b526b4"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["627ce218a5a68018115c2deb6559b41e3665b8ab","c220849f876de24a79f756f65b3eb045db59f63f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"28427ef110c4c5bf5b4057731b83110bd1e13724":["1da8d55113b689b06716246649de6f62430f15c0"],"ad94625fb8d088209f46650c8097196fec67f00c":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"fd9cc9d77712aba3662f24632df7539ab75e3667":["a1b3a24d5d9b47345473ff564f5cc127a7b526b4"],"98807419200815580d853d74ffde5d2af0406d30":["e789a6ee3a5fa41394cbed2293ff68c3712c32a9"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":["135621f3a0670a9394eb563224a3b76cc4dddc0f","a1b3a24d5d9b47345473ff564f5cc127a7b526b4"],"627ce218a5a68018115c2deb6559b41e3665b8ab":["98807419200815580d853d74ffde5d2af0406d30"],"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["28427ef110c4c5bf5b4057731b83110bd1e13724"],"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","95ae76773bf2b95987d5f9c8f566ab3738953fb4"],"5f4e87790277826a2aea119328600dfb07761f32":["1da8d55113b689b06716246649de6f62430f15c0","4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"2553b00f699380c64959ccb27991289aae87be2e":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","fd9cc9d77712aba3662f24632df7539ab75e3667"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["a1b3a24d5d9b47345473ff564f5cc127a7b526b4","fd9cc9d77712aba3662f24632df7539ab75e3667"],"222ea1c48b8e68b304965aced7ce915aa78588ca":["4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"e789a6ee3a5fa41394cbed2293ff68c3712c32a9":["8ee64fe110d7ad3ba967e05a18218d79eeb2bfd9"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["2553b00f699380c64959ccb27991289aae87be2e"],"755f2f419306d7297c8feee10d1897addf4b2dd0":["627ce218a5a68018115c2deb6559b41e3665b8ab"],"a3776dccca01c11e7046323cfad46a3b4a471233":["c220849f876de24a79f756f65b3eb045db59f63f","95ae76773bf2b95987d5f9c8f566ab3738953fb4"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["fd9cc9d77712aba3662f24632df7539ab75e3667"],"a1b3a24d5d9b47345473ff564f5cc127a7b526b4":["95ae76773bf2b95987d5f9c8f566ab3738953fb4"],"95ae76773bf2b95987d5f9c8f566ab3738953fb4":["c220849f876de24a79f756f65b3eb045db59f63f"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","c220849f876de24a79f756f65b3eb045db59f63f"],"45669a651c970812a680841b97a77cce06af559f":["868da859b43505d9d2a023bfeae6dd0c795f5295","95ae76773bf2b95987d5f9c8f566ab3738953fb4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c26f00b574427b55127e869b935845554afde1fa"]},"commit2Childs":{"c26f00b574427b55127e869b935845554afde1fa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["ad94625fb8d088209f46650c8097196fec67f00c"],"8ee64fe110d7ad3ba967e05a18218d79eeb2bfd9":["e789a6ee3a5fa41394cbed2293ff68c3712c32a9"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"1da8d55113b689b06716246649de6f62430f15c0":["28427ef110c4c5bf5b4057731b83110bd1e13724","5f4e87790277826a2aea119328600dfb07761f32"],"c220849f876de24a79f756f65b3eb045db59f63f":["29ef99d61cda9641b6250bf9567329a6e65f901d","a3776dccca01c11e7046323cfad46a3b4a471233","95ae76773bf2b95987d5f9c8f566ab3738953fb4","868da859b43505d9d2a023bfeae6dd0c795f5295"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["2553b00f699380c64959ccb27991289aae87be2e"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"28427ef110c4c5bf5b4057731b83110bd1e13724":["4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"ad94625fb8d088209f46650c8097196fec67f00c":["1da8d55113b689b06716246649de6f62430f15c0"],"fd9cc9d77712aba3662f24632df7539ab75e3667":["c26f00b574427b55127e869b935845554afde1fa","2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c","a258fbb26824fd104ed795e5d9033d2d040049ee"],"98807419200815580d853d74ffde5d2af0406d30":["627ce218a5a68018115c2deb6559b41e3665b8ab"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":[],"627ce218a5a68018115c2deb6559b41e3665b8ab":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","29ef99d61cda9641b6250bf9567329a6e65f901d","755f2f419306d7297c8feee10d1897addf4b2dd0"],"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["5f4e87790277826a2aea119328600dfb07761f32","222ea1c48b8e68b304965aced7ce915aa78588ca"],"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["1da8d55113b689b06716246649de6f62430f15c0"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["2e10cb22a8bdb44339e282925a29182bb2f3174d"],"5f4e87790277826a2aea119328600dfb07761f32":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"2553b00f699380c64959ccb27991289aae87be2e":["c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"222ea1c48b8e68b304965aced7ce915aa78588ca":["8ee64fe110d7ad3ba967e05a18218d79eeb2bfd9"],"e789a6ee3a5fa41394cbed2293ff68c3712c32a9":["98807419200815580d853d74ffde5d2af0406d30"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"755f2f419306d7297c8feee10d1897addf4b2dd0":["c220849f876de24a79f756f65b3eb045db59f63f"],"a3776dccca01c11e7046323cfad46a3b4a471233":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"a1b3a24d5d9b47345473ff564f5cc127a7b526b4":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","fd9cc9d77712aba3662f24632df7539ab75e3667","2e10cb22a8bdb44339e282925a29182bb2f3174d","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"95ae76773bf2b95987d5f9c8f566ab3738953fb4":["135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233","a1b3a24d5d9b47345473ff564f5cc127a7b526b4","45669a651c970812a680841b97a77cce06af559f"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["45669a651c970812a680841b97a77cce06af559f"],"45669a651c970812a680841b97a77cce06af559f":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["2e10cb22a8bdb44339e282925a29182bb2f3174d","d083e83f225b11e5fdd900e83d26ddb385b6955c","a258fbb26824fd104ed795e5d9033d2d040049ee","45669a651c970812a680841b97a77cce06af559f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}