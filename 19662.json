{"path":"src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","commits":[{"id":"d34c8b8a760c050d0e5c4b802584b3de2002e431","date":1220117586,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"/dev/null","sourceNew":"    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n      }\n\n      long pos = getFilePointer();\n      while (bb.hasRemaining()) {\n        int i = channel.read(bb, pos);\n        if (i == -1)\n          throw new IOException(\"read past EOF\");\n        pos += i;\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["37cdff042fc21a4f3d9c437c1022deac5d3bab72"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"37cdff042fc21a4f3d9c437c1022deac5d3bab72","date":1247767655,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer();\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          if (i == -1) {\n            throw new IOException(\"read past EOF\");\n          }\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","sourceOld":"    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n      }\n\n      long pos = getFilePointer();\n      while (bb.hasRemaining()) {\n        int i = channel.read(bb, pos);\n        if (i == -1)\n          throw new IOException(\"read past EOF\");\n        pos += i;\n      }\n    }\n\n","bugFix":["d34c8b8a760c050d0e5c4b802584b3de2002e431"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8a9e385641d717e641408d8fbbc62be8fc766357","date":1256746606,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer();\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          if (i == -1) {\n            throw new IOException(\"read past EOF\");\n          }\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","sourceOld":"    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer();\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          if (i == -1) {\n            throw new IOException(\"read past EOF\");\n          }\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer();\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          if (i == -1) {\n            throw new IOException(\"read past EOF\");\n          }\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer();\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          if (i == -1) {\n            throw new IOException(\"read past EOF\");\n          }\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"8a9e385641d717e641408d8fbbc62be8fc766357":["37cdff042fc21a4f3d9c437c1022deac5d3bab72"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"37cdff042fc21a4f3d9c437c1022deac5d3bab72":["d34c8b8a760c050d0e5c4b802584b3de2002e431"],"d34c8b8a760c050d0e5c4b802584b3de2002e431":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["8a9e385641d717e641408d8fbbc62be8fc766357"]},"commit2Childs":{"8a9e385641d717e641408d8fbbc62be8fc766357":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d34c8b8a760c050d0e5c4b802584b3de2002e431"],"37cdff042fc21a4f3d9c437c1022deac5d3bab72":["8a9e385641d717e641408d8fbbc62be8fc766357"],"d34c8b8a760c050d0e5c4b802584b3de2002e431":["37cdff042fc21a4f3d9c437c1022deac5d3bab72"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}