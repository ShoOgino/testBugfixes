{"path":"solr/core/src/java/org/apache/solr/util/SimplePostTool#postWebPages(String[],int,OutputStream).mjava","commits":[{"id":"c2c3a504730329ae644b009dee43024116605d47","date":1345253449,"type":0,"author":"Jan HÃ¸ydahl","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/SimplePostTool#postWebPages(String[],int,OutputStream).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * This method takes as input a list of start URL strings for crawling,\n   * adds each one to a backlog and then starts crawling\n   * @param args the raw input args from main()\n   * @param startIndexInArgs offset for where to start\n   * @param out outputStream to write results to\n   * @return the number of web pages posted\n   */\n  public int postWebPages(String[] args, int startIndexInArgs, OutputStream out) {\n    reset();\n    LinkedHashSet<URL> s = new LinkedHashSet<URL>();\n    for (int j = startIndexInArgs; j < args.length; j++) {\n      try {\n        URL u = new URL(normalizeUrlEnding(args[j]));\n        s.add(u);\n      } catch(MalformedURLException e) {\n        warn(\"Skipping malformed input URL: \"+args[j]);\n      }\n    }\n    // Add URLs to level 0 of the backlog and start recursive crawling\n    backlog.add(s);\n    return webCrawl(0, out);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70fa1c0f4d75735ff2e1485e059d9bc5efa50598","date":1345296911,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/util/SimplePostTool#postWebPages(String[],int,OutputStream).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * This method takes as input a list of start URL strings for crawling,\n   * adds each one to a backlog and then starts crawling\n   * @param args the raw input args from main()\n   * @param startIndexInArgs offset for where to start\n   * @param out outputStream to write results to\n   * @return the number of web pages posted\n   */\n  public int postWebPages(String[] args, int startIndexInArgs, OutputStream out) {\n    reset();\n    LinkedHashSet<URL> s = new LinkedHashSet<URL>();\n    for (int j = startIndexInArgs; j < args.length; j++) {\n      try {\n        URL u = new URL(normalizeUrlEnding(args[j]));\n        s.add(u);\n      } catch(MalformedURLException e) {\n        warn(\"Skipping malformed input URL: \"+args[j]);\n      }\n    }\n    // Add URLs to level 0 of the backlog and start recursive crawling\n    backlog.add(s);\n    return webCrawl(0, out);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"616c1830142ff5c1ddedec1ed898733b73c8e23b","date":1345368925,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/util/SimplePostTool#postWebPages(String[],int,OutputStream).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * This method takes as input a list of start URL strings for crawling,\n   * adds each one to a backlog and then starts crawling\n   * @param args the raw input args from main()\n   * @param startIndexInArgs offset for where to start\n   * @param out outputStream to write results to\n   * @return the number of web pages posted\n   */\n  public int postWebPages(String[] args, int startIndexInArgs, OutputStream out) {\n    reset();\n    LinkedHashSet<URL> s = new LinkedHashSet<URL>();\n    for (int j = startIndexInArgs; j < args.length; j++) {\n      try {\n        URL u = new URL(normalizeUrlEnding(args[j]));\n        s.add(u);\n      } catch(MalformedURLException e) {\n        warn(\"Skipping malformed input URL: \"+args[j]);\n      }\n    }\n    // Add URLs to level 0 of the backlog and start recursive crawling\n    backlog.add(s);\n    return webCrawl(0, out);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/SimplePostTool#postWebPages(String[],int,OutputStream).mjava","pathOld":"solr/core/src/java/org/apache/solr/util/SimplePostTool#postWebPages(String[],int,OutputStream).mjava","sourceNew":"  /**\n   * This method takes as input a list of start URL strings for crawling,\n   * adds each one to a backlog and then starts crawling\n   * @param args the raw input args from main()\n   * @param startIndexInArgs offset for where to start\n   * @param out outputStream to write results to\n   * @return the number of web pages posted\n   */\n  public int postWebPages(String[] args, int startIndexInArgs, OutputStream out) {\n    reset();\n    LinkedHashSet<URL> s = new LinkedHashSet<>();\n    for (int j = startIndexInArgs; j < args.length; j++) {\n      try {\n        URL u = new URL(normalizeUrlEnding(args[j]));\n        s.add(u);\n      } catch(MalformedURLException e) {\n        warn(\"Skipping malformed input URL: \"+args[j]);\n      }\n    }\n    // Add URLs to level 0 of the backlog and start recursive crawling\n    backlog.add(s);\n    return webCrawl(0, out);\n  }\n\n","sourceOld":"  /**\n   * This method takes as input a list of start URL strings for crawling,\n   * adds each one to a backlog and then starts crawling\n   * @param args the raw input args from main()\n   * @param startIndexInArgs offset for where to start\n   * @param out outputStream to write results to\n   * @return the number of web pages posted\n   */\n  public int postWebPages(String[] args, int startIndexInArgs, OutputStream out) {\n    reset();\n    LinkedHashSet<URL> s = new LinkedHashSet<URL>();\n    for (int j = startIndexInArgs; j < args.length; j++) {\n      try {\n        URL u = new URL(normalizeUrlEnding(args[j]));\n        s.add(u);\n      } catch(MalformedURLException e) {\n        warn(\"Skipping malformed input URL: \"+args[j]);\n      }\n    }\n    // Add URLs to level 0 of the backlog and start recursive crawling\n    backlog.add(s);\n    return webCrawl(0, out);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["c2c3a504730329ae644b009dee43024116605d47"],"616c1830142ff5c1ddedec1ed898733b73c8e23b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c2c3a504730329ae644b009dee43024116605d47"],"70fa1c0f4d75735ff2e1485e059d9bc5efa50598":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c2c3a504730329ae644b009dee43024116605d47"],"c2c3a504730329ae644b009dee43024116605d47":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"616c1830142ff5c1ddedec1ed898733b73c8e23b":[],"70fa1c0f4d75735ff2e1485e059d9bc5efa50598":[],"c2c3a504730329ae644b009dee43024116605d47":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","616c1830142ff5c1ddedec1ed898733b73c8e23b","70fa1c0f4d75735ff2e1485e059d9bc5efa50598"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["616c1830142ff5c1ddedec1ed898733b73c8e23b","70fa1c0f4d75735ff2e1485e059d9bc5efa50598","c2c3a504730329ae644b009dee43024116605d47"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["616c1830142ff5c1ddedec1ed898733b73c8e23b","70fa1c0f4d75735ff2e1485e059d9bc5efa50598","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}