{"path":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext,int,SchemaField,DocRouter.Range[],String,HashBasedRouter,AtomicInteger,boolean).mjava","commits":[{"id":"6aafdd2b981170a4d391e52d7cfd3d53a626cc7c","date":1533237435,"type":1,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext,int,SchemaField,DocRouter.Range[],String,HashBasedRouter,AtomicInteger,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext,int,SchemaField,DocRouter.Range[],String,HashBasedRouter,boolean).mjava","sourceNew":"  static FixedBitSet[] split(LeafReaderContext readerContext, int numPieces, SchemaField field, DocRouter.Range[] rangesArr,\n                             String splitKey, HashBasedRouter hashRouter, AtomicInteger currentPartition, boolean delete) throws IOException {\n    LeafReader reader = readerContext.reader();\n    FixedBitSet[] docSets = new FixedBitSet[numPieces];\n    for (int i=0; i<docSets.length; i++) {\n      docSets[i] = new FixedBitSet(reader.maxDoc());\n      if (delete) {\n        docSets[i].set(0, reader.maxDoc());\n      }\n    }\n    Bits liveDocs = reader.getLiveDocs();\n    if (liveDocs != null && delete) {\n      FixedBitSet liveDocsSet = FixedBitSet.copyOf(liveDocs);\n      for (FixedBitSet set : docSets) {\n        set.and(liveDocsSet);\n      }\n    }\n\n    Terms terms = reader.terms(field.getName());\n    TermsEnum termsEnum = terms==null ? null : terms.iterator();\n    if (termsEnum == null) return docSets;\n\n    BytesRef term = null;\n    PostingsEnum postingsEnum = null;\n\n    int[] docsMatchingRanges = null;\n    if (rangesArr != null) {\n      // +1 because documents can belong to *zero*, one, several or all ranges in rangesArr\n      docsMatchingRanges = new int[rangesArr.length+1];\n    }\n\n    CharsRefBuilder idRef = new CharsRefBuilder();\n    for (;;) {\n      term = termsEnum.next();\n      if (term == null) break;\n\n      // figure out the hash for the term\n\n      // FUTURE: if conversion to strings costs too much, we could\n      // specialize and use the hash function that can work over bytes.\n      field.getType().indexedToReadable(term, idRef);\n      String idString = idRef.toString();\n\n      if (splitKey != null) {\n        // todo have composite routers support these kind of things instead\n        String part1 = getRouteKey(idString);\n        if (part1 == null)\n          continue;\n        if (!splitKey.equals(part1))  {\n          continue;\n        }\n      }\n\n      int hash = 0;\n      if (hashRouter != null && rangesArr != null) {\n        hash = hashRouter.sliceHash(idString, null, null, null);\n      }\n\n      postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n      postingsEnum = BitsFilteredPostingsEnum.wrap(postingsEnum, liveDocs);\n      for (;;) {\n        int doc = postingsEnum.nextDoc();\n        if (doc == DocIdSetIterator.NO_MORE_DOCS) break;\n        if (rangesArr == null) {\n          if (delete) {\n            docSets[currentPartition.get()].clear(doc);\n          } else {\n            docSets[currentPartition.get()].set(doc);\n          }\n          currentPartition.set((currentPartition.get() + 1) % numPieces);\n        } else  {\n          int matchingRangesCount = 0;\n          for (int i=0; i < rangesArr.length; i++) {      // inner-loop: use array here for extra speed.\n            if (rangesArr[i].includes(hash)) {\n              if (delete) {\n                docSets[i].clear(doc);\n              } else {\n                docSets[i].set(doc);\n              }\n              ++matchingRangesCount;\n            }\n          }\n          docsMatchingRanges[matchingRangesCount]++;\n        }\n      }\n    }\n\n    if (docsMatchingRanges != null) {\n      for (int ii = 0; ii < docsMatchingRanges.length; ii++) {\n        if (0 == docsMatchingRanges[ii]) continue;\n        switch (ii) {\n          case 0:\n            // document loss\n            log.error(\"Splitting {}: {} documents belong to no shards and will be dropped\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          case 1:\n            // normal case, each document moves to one of the sub-shards\n            log.info(\"Splitting {}: {} documents will move into a sub-shard\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          default:\n            // document duplication\n            log.error(\"Splitting {}: {} documents will be moved to multiple ({}) sub-shards\",\n                reader, docsMatchingRanges[ii], ii);\n            break;\n        }\n      }\n    }\n\n    return docSets;\n  }\n\n","sourceOld":"  static FixedBitSet[] split(LeafReaderContext readerContext, int numPieces, SchemaField field, DocRouter.Range[] rangesArr,\n                             String splitKey, HashBasedRouter hashRouter, boolean delete) throws IOException {\n    LeafReader reader = readerContext.reader();\n    FixedBitSet[] docSets = new FixedBitSet[numPieces];\n    for (int i=0; i<docSets.length; i++) {\n      docSets[i] = new FixedBitSet(reader.maxDoc());\n      if (delete) {\n        docSets[i].set(0, reader.maxDoc());\n      }\n    }\n    Bits liveDocs = reader.getLiveDocs();\n    if (liveDocs != null && delete) {\n      FixedBitSet liveDocsSet = FixedBitSet.copyOf(liveDocs);\n      for (FixedBitSet set : docSets) {\n        set.and(liveDocsSet);\n      }\n    }\n\n    Terms terms = reader.terms(field.getName());\n    TermsEnum termsEnum = terms==null ? null : terms.iterator();\n    if (termsEnum == null) return docSets;\n\n    BytesRef term = null;\n    PostingsEnum postingsEnum = null;\n\n    int[] docsMatchingRanges = null;\n    if (rangesArr != null) {\n      // +1 because documents can belong to *zero*, one, several or all ranges in rangesArr\n      docsMatchingRanges = new int[rangesArr.length+1];\n    }\n\n    int partition = 0;\n    CharsRefBuilder idRef = new CharsRefBuilder();\n    for (;;) {\n      term = termsEnum.next();\n      if (term == null) break;\n\n      // figure out the hash for the term\n\n      // FUTURE: if conversion to strings costs too much, we could\n      // specialize and use the hash function that can work over bytes.\n      field.getType().indexedToReadable(term, idRef);\n      String idString = idRef.toString();\n\n      if (splitKey != null) {\n        // todo have composite routers support these kind of things instead\n        String part1 = getRouteKey(idString);\n        if (part1 == null)\n          continue;\n        if (!splitKey.equals(part1))  {\n          continue;\n        }\n      }\n\n      int hash = 0;\n      if (hashRouter != null) {\n        hash = hashRouter.sliceHash(idString, null, null, null);\n      }\n\n      postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n      postingsEnum = BitsFilteredPostingsEnum.wrap(postingsEnum, liveDocs);\n      for (;;) {\n        int doc = postingsEnum.nextDoc();\n        if (doc == DocIdSetIterator.NO_MORE_DOCS) break;\n        if (rangesArr == null) {\n          if (delete) {\n            docSets[partition].clear(doc);\n          } else {\n            docSets[partition].set(doc);\n          }\n          partition = (partition + 1) % numPieces;\n        } else  {\n          int matchingRangesCount = 0;\n          for (int i=0; i<rangesArr.length; i++) {      // inner-loop: use array here for extra speed.\n            if (rangesArr[i].includes(hash)) {\n              if (delete) {\n                docSets[i].clear(doc);\n              } else {\n                docSets[i].set(doc);\n              }\n              ++matchingRangesCount;\n            }\n          }\n          docsMatchingRanges[matchingRangesCount]++;\n        }\n      }\n    }\n\n    if (docsMatchingRanges != null) {\n      for (int ii = 0; ii < docsMatchingRanges.length; ii++) {\n        if (0 == docsMatchingRanges[ii]) continue;\n        switch (ii) {\n          case 0:\n            // document loss\n            log.error(\"Splitting {}: {} documents belong to no shards and will be dropped\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          case 1:\n            // normal case, each document moves to one of the sub-shards\n            log.info(\"Splitting {}: {} documents will move into a sub-shard\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          default:\n            // document duplication\n            log.error(\"Splitting {}: {} documents will be moved to multiple ({}) sub-shards\",\n                reader, docsMatchingRanges[ii], ii);\n            break;\n        }\n      }\n    }\n\n    return docSets;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"6aafdd2b981170a4d391e52d7cfd3d53a626cc7c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["6aafdd2b981170a4d391e52d7cfd3d53a626cc7c"]},"commit2Childs":{"6aafdd2b981170a4d391e52d7cfd3d53a626cc7c":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["6aafdd2b981170a4d391e52d7cfd3d53a626cc7c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}