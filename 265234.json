{"path":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","commits":[{"id":"01f60198ece724a6e96cd0b45f289cf42ff83d4f","date":1286864103,"type":0,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","pathOld":"/dev/null","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      if(datOut == null)// no data added\n        return;\n      initIndexOut();\n      final int[] sortedEntries = hash.sort(comp);\n      final int count = hash.size();\n      int[] address= new int[count];\n      // first dump bytes data, recording address as we go\n      for(int i=0;i<count;i++) {\n        final int e = sortedEntries[i];\n        final BytesRef bytes = hash.get(e);\n        assert bytes.length == size;\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        address[e] = 1+i;\n      }\n\n      idxOut.writeInt(count);\n\n      // next write index\n      PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount, PackedInts.bitsRequired(count));\n      final int limit;\n      if (docCount > docToEntry.length) {\n        limit = docToEntry.length;\n      } else {\n        limit = docCount;\n      }\n      for(int i=0;i<limit;i++) {\n        final int e = docToEntry[i];\n        if (e == 0) {\n          // null is encoded as zero\n          w.add(0);\n        } else {\n          assert e > 0 && e <= count: \"index must  0 > && <= \" + count + \" was: \" + e;\n          w.add(address[e-1]);\n        }\n      }\n\n      for(int i=limit;i<docCount;i++) {\n        w.add(0);\n      }\n      w.finish();\n\n      super.finish(docCount);\n      bytesUsed.addAndGet((-docToEntry.length)\n          * RamUsageEstimator.NUM_BYTES_INT);\n      docToEntry = null;\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4ecea1664e8617d82eca3b8055a3c37cb4da8511","date":1287578668,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      if(datOut == null)// no data added\n        return;\n      initIndexOut();\n      final int[] sortedEntries = hash.sort(comp);\n      final int count = hash.size();\n      int[] address= new int[count];\n      // first dump bytes data, recording address as we go\n      for(int i=0;i<count;i++) {\n        final int e = sortedEntries[i];\n        final BytesRef bytes = hash.get(e, new BytesRef());\n        assert bytes.length == size;\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        address[e] = 1+i;\n      }\n\n      idxOut.writeInt(count);\n\n      // next write index\n      PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount, PackedInts.bitsRequired(count));\n      final int limit;\n      if (docCount > docToEntry.length) {\n        limit = docToEntry.length;\n      } else {\n        limit = docCount;\n      }\n      for(int i=0;i<limit;i++) {\n        final int e = docToEntry[i];\n        if (e == 0) {\n          // null is encoded as zero\n          w.add(0);\n        } else {\n          assert e > 0 && e <= count: \"index must  0 > && <= \" + count + \" was: \" + e;\n          w.add(address[e-1]);\n        }\n      }\n\n      for(int i=limit;i<docCount;i++) {\n        w.add(0);\n      }\n      w.finish();\n\n      super.finish(docCount);\n      bytesUsed.addAndGet((-docToEntry.length)\n          * RamUsageEstimator.NUM_BYTES_INT);\n      docToEntry = null;\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      if(datOut == null)// no data added\n        return;\n      initIndexOut();\n      final int[] sortedEntries = hash.sort(comp);\n      final int count = hash.size();\n      int[] address= new int[count];\n      // first dump bytes data, recording address as we go\n      for(int i=0;i<count;i++) {\n        final int e = sortedEntries[i];\n        final BytesRef bytes = hash.get(e);\n        assert bytes.length == size;\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        address[e] = 1+i;\n      }\n\n      idxOut.writeInt(count);\n\n      // next write index\n      PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount, PackedInts.bitsRequired(count));\n      final int limit;\n      if (docCount > docToEntry.length) {\n        limit = docToEntry.length;\n      } else {\n        limit = docCount;\n      }\n      for(int i=0;i<limit;i++) {\n        final int e = docToEntry[i];\n        if (e == 0) {\n          // null is encoded as zero\n          w.add(0);\n        } else {\n          assert e > 0 && e <= count: \"index must  0 > && <= \" + count + \" was: \" + e;\n          w.add(address[e-1]);\n        }\n      }\n\n      for(int i=limit;i<docCount;i++) {\n        w.add(0);\n      }\n      w.finish();\n\n      super.finish(docCount);\n      bytesUsed.addAndGet((-docToEntry.length)\n          * RamUsageEstimator.NUM_BYTES_INT);\n      docToEntry = null;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3ae97ad22c2ae646bfc1c09cab424cb07f9474ca","date":1289932456,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      if (datOut == null)// no data added\n        return;\n      initIndexOut();\n      final int[] sortedEntries = hash.sort(comp);\n      final int count = hash.size();\n      int[] address = new int[count];\n      // first dump bytes data, recording address as we go\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        final BytesRef bytes = hash.get(e, new BytesRef());\n        assert bytes.length == size;\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        address[e] = 1 + i;\n      }\n\n      idxOut.writeInt(count);\n\n      // next write index\n      PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount, PackedInts\n          .bitsRequired(count));\n      final int limit;\n      if (docCount > docToEntry.length) {\n        limit = docToEntry.length;\n      } else {\n        limit = docCount;\n      }\n      for (int i = 0; i < limit; i++) {\n        final int e = docToEntry[i];\n        if (e == 0) {\n          // null is encoded as zero\n          w.add(0);\n        } else {\n          assert e > 0 && e <= count : \"index must  0 > && <= \" + count\n              + \" was: \" + e;\n          w.add(address[e - 1]);\n        }\n      }\n\n      for (int i = limit; i < docCount; i++) {\n        w.add(0);\n      }\n      w.finish();\n\n      super.finish(docCount);\n      bytesUsed.addAndGet((-docToEntry.length)\n          * RamUsageEstimator.NUM_BYTES_INT);\n      docToEntry = null;\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      if(datOut == null)// no data added\n        return;\n      initIndexOut();\n      final int[] sortedEntries = hash.sort(comp);\n      final int count = hash.size();\n      int[] address= new int[count];\n      // first dump bytes data, recording address as we go\n      for(int i=0;i<count;i++) {\n        final int e = sortedEntries[i];\n        final BytesRef bytes = hash.get(e, new BytesRef());\n        assert bytes.length == size;\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        address[e] = 1+i;\n      }\n\n      idxOut.writeInt(count);\n\n      // next write index\n      PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount, PackedInts.bitsRequired(count));\n      final int limit;\n      if (docCount > docToEntry.length) {\n        limit = docToEntry.length;\n      } else {\n        limit = docCount;\n      }\n      for(int i=0;i<limit;i++) {\n        final int e = docToEntry[i];\n        if (e == 0) {\n          // null is encoded as zero\n          w.add(0);\n        } else {\n          assert e > 0 && e <= count: \"index must  0 > && <= \" + count + \" was: \" + e;\n          w.add(address[e-1]);\n        }\n      }\n\n      for(int i=limit;i<docCount;i++) {\n        w.add(0);\n      }\n      w.finish();\n\n      super.finish(docCount);\n      bytesUsed.addAndGet((-docToEntry.length)\n          * RamUsageEstimator.NUM_BYTES_INT);\n      docToEntry = null;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a98a65bdb67cd0b27d18a5564d63bd3e944d3f4","date":1291128345,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      if (datOut == null)// no data added\n        return;\n      initIndexOut();\n      final int[] sortedEntries = hash.sort(comp);\n      final int count = hash.size();\n      int[] address = new int[count];\n      // first dump bytes data, recording address as we go\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        final BytesRef bytes = hash.get(e, new BytesRef());\n        assert bytes.length == size;\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        address[e] = 1 + i;\n      }\n\n      idxOut.writeInt(count);\n\n      // next write index\n      PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount, PackedInts\n          .bitsRequired(count));\n      final int limit;\n      if (docCount > docToEntry.length) {\n        limit = docToEntry.length;\n      } else {\n        limit = docCount;\n      }\n      for (int i = 0; i < limit; i++) {\n        final int e = docToEntry[i];\n        if (e == 0) {\n          // null is encoded as zero\n          w.add(0);\n        } else {\n          assert e > 0 && e <= count : \"index must  0 > && <= \" + count\n              + \" was: \" + e;\n          w.add(address[e - 1]);\n        }\n      }\n\n      for (int i = limit; i < docCount; i++) {\n        w.add(0);\n      }\n      w.finish();\n\n      super.finish(docCount);\n      bytesUsed.addAndGet((-docToEntry.length)\n          * RamUsageEstimator.NUM_BYTES_INT);\n      docToEntry = null;\n      hash.close();\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      if (datOut == null)// no data added\n        return;\n      initIndexOut();\n      final int[] sortedEntries = hash.sort(comp);\n      final int count = hash.size();\n      int[] address = new int[count];\n      // first dump bytes data, recording address as we go\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        final BytesRef bytes = hash.get(e, new BytesRef());\n        assert bytes.length == size;\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        address[e] = 1 + i;\n      }\n\n      idxOut.writeInt(count);\n\n      // next write index\n      PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount, PackedInts\n          .bitsRequired(count));\n      final int limit;\n      if (docCount > docToEntry.length) {\n        limit = docToEntry.length;\n      } else {\n        limit = docCount;\n      }\n      for (int i = 0; i < limit; i++) {\n        final int e = docToEntry[i];\n        if (e == 0) {\n          // null is encoded as zero\n          w.add(0);\n        } else {\n          assert e > 0 && e <= count : \"index must  0 > && <= \" + count\n              + \" was: \" + e;\n          w.add(address[e - 1]);\n        }\n      }\n\n      for (int i = limit; i < docCount; i++) {\n        w.add(0);\n      }\n      w.finish();\n\n      super.finish(docCount);\n      bytesUsed.addAndGet((-docToEntry.length)\n          * RamUsageEstimator.NUM_BYTES_INT);\n      docToEntry = null;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      try {\n        if (size == -1) {// no data added\n          datOut.writeInt(size);\n        }\n        final int[] sortedEntries = hash.sort(comp);\n        final int count = hash.size();\n        int[] address = new int[count];\n        // first dump bytes data, recording address as we go\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          assert bytes.length == size;\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          address[e] = 1 + i;\n        }\n\n        idxOut.writeInt(count);\n\n        // next write index\n        PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count));\n        final int limit;\n        if (docCount > docToEntry.length) {\n          limit = docToEntry.length;\n        } else {\n          limit = docCount;\n        }\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          if (e == 0) {\n            // null is encoded as zero\n            w.add(0);\n          } else {\n            assert e > 0 && e <= count : \"index must  0 > && <= \" + count\n                + \" was: \" + e;\n            w.add(address[e - 1]);\n          }\n        }\n\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        super.finish(docCount);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        docToEntry = null;\n        hash.close();\n      }\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      if (datOut == null)// no data added\n        return;\n      initIndexOut();\n      final int[] sortedEntries = hash.sort(comp);\n      final int count = hash.size();\n      int[] address = new int[count];\n      // first dump bytes data, recording address as we go\n      for (int i = 0; i < count; i++) {\n        final int e = sortedEntries[i];\n        final BytesRef bytes = hash.get(e, new BytesRef());\n        assert bytes.length == size;\n        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n        address[e] = 1 + i;\n      }\n\n      idxOut.writeInt(count);\n\n      // next write index\n      PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount, PackedInts\n          .bitsRequired(count));\n      final int limit;\n      if (docCount > docToEntry.length) {\n        limit = docToEntry.length;\n      } else {\n        limit = docCount;\n      }\n      for (int i = 0; i < limit; i++) {\n        final int e = docToEntry[i];\n        if (e == 0) {\n          // null is encoded as zero\n          w.add(0);\n        } else {\n          assert e > 0 && e <= count : \"index must  0 > && <= \" + count\n              + \" was: \" + e;\n          w.add(address[e - 1]);\n        }\n      }\n\n      for (int i = limit; i < docCount; i++) {\n        w.add(0);\n      }\n      w.finish();\n\n      super.finish(docCount);\n      bytesUsed.addAndGet((-docToEntry.length)\n          * RamUsageEstimator.NUM_BYTES_INT);\n      docToEntry = null;\n      hash.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9235b9d4454a46c066cda47fed7ca0a34e614529","date":1304414372,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      try {\n        if (size == -1) {// no data added\n          datOut.writeInt(size);\n        }\n        final int[] sortedEntries = hash.sort(comp);\n        final int count = hash.size();\n        int[] address = new int[count];\n        // first dump bytes data, recording address as we go\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          assert bytes.length == size;\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          address[e] = 1 + i;\n        }\n\n        idxOut.writeInt(count);\n\n        // next write index\n        PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count));\n        final int limit;\n        if (docCount > docToEntry.length) {\n          limit = docToEntry.length;\n        } else {\n          limit = docCount;\n        }\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          if (e == 0) {\n            // null is encoded as zero\n            w.add(0);\n          } else {\n            assert e > 0 && e <= count : \"index must  0 > && <= \" + count\n                + \" was: \" + e;\n            w.add(address[e - 1]);\n          }\n        }\n\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        super.finish(docCount);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        docToEntry = null;\n        hash.close();\n      }\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      try {\n        if (size == -1) {// no data added\n          datOut.writeInt(size);\n        }\n        final int[] sortedEntries = hash.sort(comp);\n        final int count = hash.size();\n        int[] address = new int[count];\n        // first dump bytes data, recording address as we go\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          assert bytes.length == size;\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          address[e] = 1 + i;\n        }\n\n        idxOut.writeInt(count);\n\n        // next write index\n        PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count));\n        final int limit;\n        if (docCount > docToEntry.length) {\n          limit = docToEntry.length;\n        } else {\n          limit = docCount;\n        }\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          if (e == 0) {\n            // null is encoded as zero\n            w.add(0);\n          } else {\n            assert e > 0 && e <= count : \"index must  0 > && <= \" + count\n                + \" was: \" + e;\n            w.add(address[e - 1]);\n          }\n        }\n\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        super.finish(docCount);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        docToEntry = null;\n        hash.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e8d7ba2175f47e280231533f7d3016249cea88b","date":1307711934,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","pathOld":"/dev/null","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      try {\n        if (size == -1) {// no data added\n          datOut.writeInt(size);\n        }\n        final int[] sortedEntries = hash.sort(comp);\n        final int count = hash.size();\n        int[] address = new int[count];\n        // first dump bytes data, recording address as we go\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          assert bytes.length == size;\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          address[e] = 1 + i;\n        }\n\n        idxOut.writeInt(count);\n\n        // next write index\n        PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count));\n        final int limit;\n        if (docCount > docToEntry.length) {\n          limit = docToEntry.length;\n        } else {\n          limit = docCount;\n        }\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          if (e == 0) {\n            // null is encoded as zero\n            w.add(0);\n          } else {\n            assert e > 0 && e <= count : \"index must  0 > && <= \" + count\n                + \" was: \" + e;\n            w.add(address[e - 1]);\n          }\n        }\n\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        super.finish(docCount);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        docToEntry = null;\n        hash.close();\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","date":1307729864,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","pathOld":"/dev/null","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      try {\n        if (size == -1) {// no data added\n          datOut.writeInt(size);\n        }\n        final int[] sortedEntries = hash.sort(comp);\n        final int count = hash.size();\n        int[] address = new int[count];\n        // first dump bytes data, recording address as we go\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          assert bytes.length == size;\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          address[e] = 1 + i;\n        }\n\n        idxOut.writeInt(count);\n\n        // next write index\n        PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count));\n        final int limit;\n        if (docCount > docToEntry.length) {\n          limit = docToEntry.length;\n        } else {\n          limit = docCount;\n        }\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          if (e == 0) {\n            // null is encoded as zero\n            w.add(0);\n          } else {\n            assert e > 0 && e <= count : \"index must  0 > && <= \" + count\n                + \" was: \" + e;\n            w.add(address[e - 1]);\n          }\n        }\n\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        super.finish(docCount);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        docToEntry = null;\n        hash.close();\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3b7a068f550e13e49517c6899cc3b94c8eeb72e5","date":1309354772,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      final IndexOutput datOut = getDataOut();\n      boolean success = false;\n      final int count = hash.size();\n      final int[] address = new int[count];\n\n      try {\n        datOut.writeInt(size);\n        if (size != -1) {\n          final int[] sortedEntries = hash.sort(comp);\n          // first dump bytes data, recording address as we go\n          final BytesRef bytesRef = new BytesRef(size);\n          for (int i = 0; i < count; i++) {\n            final int e = sortedEntries[i];\n            final BytesRef bytes = hash.get(e, bytesRef);\n            assert bytes.length == size;\n            datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n            address[e] = 1 + i;\n          }\n        }\n        success = true;\n      } finally {\n        IOUtils.closeSafely(!success, datOut);\n        hash.close();\n      }\n      final IndexOutput idxOut = getIndexOut();\n      success = false;\n      try {\n        idxOut.writeInt(count);\n        // next write index\n        final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count));\n        final int limit;\n        if (docCount > docToEntry.length) {\n          limit = docToEntry.length;\n        } else {\n          limit = docCount;\n        }\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          if (e == 0) {\n            // null is encoded as zero\n            w.add(0);\n          } else {\n            assert e > 0 && e <= count : \"index must  0 > && <= \" + count\n                + \" was: \" + e;\n            w.add(address[e - 1]);\n          }\n        }\n\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        IOUtils.closeSafely(!success, idxOut);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        docToEntry = null;\n      }\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      try {\n        if (size == -1) {// no data added\n          datOut.writeInt(size);\n        }\n        final int[] sortedEntries = hash.sort(comp);\n        final int count = hash.size();\n        int[] address = new int[count];\n        // first dump bytes data, recording address as we go\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          assert bytes.length == size;\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          address[e] = 1 + i;\n        }\n\n        idxOut.writeInt(count);\n\n        // next write index\n        PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count));\n        final int limit;\n        if (docCount > docToEntry.length) {\n          limit = docToEntry.length;\n        } else {\n          limit = docCount;\n        }\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          if (e == 0) {\n            // null is encoded as zero\n            w.add(0);\n          } else {\n            assert e > 0 && e <= count : \"index must  0 > && <= \" + count\n                + \" was: \" + e;\n            w.add(address[e - 1]);\n          }\n        }\n\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        super.finish(docCount);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        docToEntry = null;\n        hash.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      final IndexOutput datOut = getDataOut();\n      boolean success = false;\n      final int count = hash.size();\n      final int[] address = new int[count];\n\n      try {\n        datOut.writeInt(size);\n        if (size != -1) {\n          final int[] sortedEntries = hash.sort(comp);\n          // first dump bytes data, recording address as we go\n          final BytesRef bytesRef = new BytesRef(size);\n          for (int i = 0; i < count; i++) {\n            final int e = sortedEntries[i];\n            final BytesRef bytes = hash.get(e, bytesRef);\n            assert bytes.length == size;\n            datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n            address[e] = 1 + i;\n          }\n        }\n        success = true;\n      } finally {\n        IOUtils.closeSafely(!success, datOut);\n        hash.close();\n      }\n      final IndexOutput idxOut = getIndexOut();\n      success = false;\n      try {\n        idxOut.writeInt(count);\n        // next write index\n        final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count));\n        final int limit;\n        if (docCount > docToEntry.length) {\n          limit = docToEntry.length;\n        } else {\n          limit = docCount;\n        }\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          if (e == 0) {\n            // null is encoded as zero\n            w.add(0);\n          } else {\n            assert e > 0 && e <= count : \"index must  0 > && <= \" + count\n                + \" was: \" + e;\n            w.add(address[e - 1]);\n          }\n        }\n\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        IOUtils.closeSafely(!success, idxOut);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        docToEntry = null;\n      }\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      try {\n        if (size == -1) {// no data added\n          datOut.writeInt(size);\n        }\n        final int[] sortedEntries = hash.sort(comp);\n        final int count = hash.size();\n        int[] address = new int[count];\n        // first dump bytes data, recording address as we go\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          assert bytes.length == size;\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          address[e] = 1 + i;\n        }\n\n        idxOut.writeInt(count);\n\n        // next write index\n        PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count));\n        final int limit;\n        if (docCount > docToEntry.length) {\n          limit = docToEntry.length;\n        } else {\n          limit = docCount;\n        }\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          if (e == 0) {\n            // null is encoded as zero\n            w.add(0);\n          } else {\n            assert e > 0 && e <= count : \"index must  0 > && <= \" + count\n                + \" was: \" + e;\n            w.add(address[e - 1]);\n          }\n        }\n\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        super.finish(docCount);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        docToEntry = null;\n        hash.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      final IndexOutput datOut = getDataOut();\n      boolean success = false;\n      final int count = hash.size();\n      final int[] address = new int[count];\n\n      try {\n        datOut.writeInt(size);\n        if (size != -1) {\n          final int[] sortedEntries = hash.sort(comp);\n          // first dump bytes data, recording address as we go\n          final BytesRef bytesRef = new BytesRef(size);\n          for (int i = 0; i < count; i++) {\n            final int e = sortedEntries[i];\n            final BytesRef bytes = hash.get(e, bytesRef);\n            assert bytes.length == size;\n            datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n            address[e] = 1 + i;\n          }\n        }\n        success = true;\n      } finally {\n        IOUtils.closeSafely(!success, datOut);\n        hash.close();\n      }\n      final IndexOutput idxOut = getIndexOut();\n      success = false;\n      try {\n        idxOut.writeInt(count);\n        // next write index\n        final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count));\n        final int limit;\n        if (docCount > docToEntry.length) {\n          limit = docToEntry.length;\n        } else {\n          limit = docCount;\n        }\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          if (e == 0) {\n            // null is encoded as zero\n            w.add(0);\n          } else {\n            assert e > 0 && e <= count : \"index must  0 > && <= \" + count\n                + \" was: \" + e;\n            w.add(address[e - 1]);\n          }\n        }\n\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        IOUtils.closeSafely(!success, idxOut);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        docToEntry = null;\n      }\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      try {\n        if (size == -1) {// no data added\n          datOut.writeInt(size);\n        }\n        final int[] sortedEntries = hash.sort(comp);\n        final int count = hash.size();\n        int[] address = new int[count];\n        // first dump bytes data, recording address as we go\n        for (int i = 0; i < count; i++) {\n          final int e = sortedEntries[i];\n          final BytesRef bytes = hash.get(e, new BytesRef());\n          assert bytes.length == size;\n          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n          address[e] = 1 + i;\n        }\n\n        idxOut.writeInt(count);\n\n        // next write index\n        PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count));\n        final int limit;\n        if (docCount > docToEntry.length) {\n          limit = docToEntry.length;\n        } else {\n          limit = docCount;\n        }\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          if (e == 0) {\n            // null is encoded as zero\n            w.add(0);\n          } else {\n            assert e > 0 && e <= count : \"index must  0 > && <= \" + count\n                + \" was: \" + e;\n            w.add(address[e - 1]);\n          }\n        }\n\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        super.finish(docCount);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        docToEntry = null;\n        hash.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24230fe54121f9be9d85f2c2067536296785e421","date":1314462346,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      final IndexOutput datOut = getDataOut();\n      boolean success = false;\n      final int count = hash.size();\n      final int[] address = new int[count];\n\n      try {\n        datOut.writeInt(size);\n        if (size != -1) {\n          final int[] sortedEntries = hash.sort(comp);\n          // first dump bytes data, recording address as we go\n          final BytesRef bytesRef = new BytesRef(size);\n          for (int i = 0; i < count; i++) {\n            final int e = sortedEntries[i];\n            final BytesRef bytes = hash.get(e, bytesRef);\n            assert bytes.length == size;\n            datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n            address[e] = 1 + i;\n          }\n        }\n        success = true;\n      } finally {\n        if (success) {\n          IOUtils.close(datOut);\n        } else {\n          IOUtils.closeWhileHandlingException(datOut);\n        }\n        hash.close();\n      }\n      final IndexOutput idxOut = getIndexOut();\n      success = false;\n      try {\n        idxOut.writeInt(count);\n        // next write index\n        final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count));\n        final int limit;\n        if (docCount > docToEntry.length) {\n          limit = docToEntry.length;\n        } else {\n          limit = docCount;\n        }\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          if (e == 0) {\n            // null is encoded as zero\n            w.add(0);\n          } else {\n            assert e > 0 && e <= count : \"index must  0 > && <= \" + count\n                + \" was: \" + e;\n            w.add(address[e - 1]);\n          }\n        }\n\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        if (success) {\n          IOUtils.close(idxOut);\n        } else {\n          IOUtils.closeWhileHandlingException(idxOut);\n        }\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        docToEntry = null;\n      }\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      final IndexOutput datOut = getDataOut();\n      boolean success = false;\n      final int count = hash.size();\n      final int[] address = new int[count];\n\n      try {\n        datOut.writeInt(size);\n        if (size != -1) {\n          final int[] sortedEntries = hash.sort(comp);\n          // first dump bytes data, recording address as we go\n          final BytesRef bytesRef = new BytesRef(size);\n          for (int i = 0; i < count; i++) {\n            final int e = sortedEntries[i];\n            final BytesRef bytes = hash.get(e, bytesRef);\n            assert bytes.length == size;\n            datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n            address[e] = 1 + i;\n          }\n        }\n        success = true;\n      } finally {\n        IOUtils.closeSafely(!success, datOut);\n        hash.close();\n      }\n      final IndexOutput idxOut = getIndexOut();\n      success = false;\n      try {\n        idxOut.writeInt(count);\n        // next write index\n        final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count));\n        final int limit;\n        if (docCount > docToEntry.length) {\n          limit = docToEntry.length;\n        } else {\n          limit = docCount;\n        }\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          if (e == 0) {\n            // null is encoded as zero\n            w.add(0);\n          } else {\n            assert e > 0 && e <= count : \"index must  0 > && <= \" + count\n                + \" was: \" + e;\n            w.add(address[e - 1]);\n          }\n        }\n\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        IOUtils.closeSafely(!success, idxOut);\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        docToEntry = null;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"85eb75e0c0203e44dcf686f35876cf6080f3a671","date":1317221550,"type":4,"author":"Simon Willnauer","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.Writer#finish(int).mjava","sourceNew":null,"sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      final IndexOutput datOut = getDataOut();\n      boolean success = false;\n      final int count = hash.size();\n      final int[] address = new int[count];\n\n      try {\n        datOut.writeInt(size);\n        if (size != -1) {\n          final int[] sortedEntries = hash.sort(comp);\n          // first dump bytes data, recording address as we go\n          final BytesRef bytesRef = new BytesRef(size);\n          for (int i = 0; i < count; i++) {\n            final int e = sortedEntries[i];\n            final BytesRef bytes = hash.get(e, bytesRef);\n            assert bytes.length == size;\n            datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);\n            address[e] = 1 + i;\n          }\n        }\n        success = true;\n      } finally {\n        if (success) {\n          IOUtils.close(datOut);\n        } else {\n          IOUtils.closeWhileHandlingException(datOut);\n        }\n        hash.close();\n      }\n      final IndexOutput idxOut = getIndexOut();\n      success = false;\n      try {\n        idxOut.writeInt(count);\n        // next write index\n        final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count));\n        final int limit;\n        if (docCount > docToEntry.length) {\n          limit = docToEntry.length;\n        } else {\n          limit = docCount;\n        }\n        for (int i = 0; i < limit; i++) {\n          final int e = docToEntry[i];\n          if (e == 0) {\n            // null is encoded as zero\n            w.add(0);\n          } else {\n            assert e > 0 && e <= count : \"index must  0 > && <= \" + count\n                + \" was: \" + e;\n            w.add(address[e - 1]);\n          }\n        }\n\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        if (success) {\n          IOUtils.close(idxOut);\n        } else {\n          IOUtils.closeWhileHandlingException(idxOut);\n        }\n        bytesUsed.addAndGet((-docToEntry.length)\n            * RamUsageEstimator.NUM_BYTES_INT);\n        docToEntry = null;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"24230fe54121f9be9d85f2c2067536296785e421":["3b7a068f550e13e49517c6899cc3b94c8eeb72e5"],"5a98a65bdb67cd0b27d18a5564d63bd3e944d3f4":["3ae97ad22c2ae646bfc1c09cab424cb07f9474ca"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2e8d7ba2175f47e280231533f7d3016249cea88b"],"3b7a068f550e13e49517c6899cc3b94c8eeb72e5":["2e8d7ba2175f47e280231533f7d3016249cea88b"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["5a98a65bdb67cd0b27d18a5564d63bd3e944d3f4","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"01f60198ece724a6e96cd0b45f289cf42ff83d4f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["2e8d7ba2175f47e280231533f7d3016249cea88b","3b7a068f550e13e49517c6899cc3b94c8eeb72e5"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","3b7a068f550e13e49517c6899cc3b94c8eeb72e5"],"9235b9d4454a46c066cda47fed7ca0a34e614529":["ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"2e8d7ba2175f47e280231533f7d3016249cea88b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","9235b9d4454a46c066cda47fed7ca0a34e614529"],"85eb75e0c0203e44dcf686f35876cf6080f3a671":["24230fe54121f9be9d85f2c2067536296785e421"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4ecea1664e8617d82eca3b8055a3c37cb4da8511":["01f60198ece724a6e96cd0b45f289cf42ff83d4f","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3ae97ad22c2ae646bfc1c09cab424cb07f9474ca":["4ecea1664e8617d82eca3b8055a3c37cb4da8511"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["85eb75e0c0203e44dcf686f35876cf6080f3a671"]},"commit2Childs":{"24230fe54121f9be9d85f2c2067536296785e421":["85eb75e0c0203e44dcf686f35876cf6080f3a671"],"5a98a65bdb67cd0b27d18a5564d63bd3e944d3f4":["ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["817d8435e9135b756f08ce6710ab0baac51bdf88"],"3b7a068f550e13e49517c6899cc3b94c8eeb72e5":["24230fe54121f9be9d85f2c2067536296785e421","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["9235b9d4454a46c066cda47fed7ca0a34e614529"],"01f60198ece724a6e96cd0b45f289cf42ff83d4f":["4ecea1664e8617d82eca3b8055a3c37cb4da8511"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"817d8435e9135b756f08ce6710ab0baac51bdf88":[],"9235b9d4454a46c066cda47fed7ca0a34e614529":["2e8d7ba2175f47e280231533f7d3016249cea88b"],"2e8d7ba2175f47e280231533f7d3016249cea88b":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","3b7a068f550e13e49517c6899cc3b94c8eeb72e5","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","ab5cb6a74aefb78aa0569857970b9151dfe2e787","01f60198ece724a6e96cd0b45f289cf42ff83d4f","2e8d7ba2175f47e280231533f7d3016249cea88b","4ecea1664e8617d82eca3b8055a3c37cb4da8511"],"85eb75e0c0203e44dcf686f35876cf6080f3a671":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4ecea1664e8617d82eca3b8055a3c37cb4da8511":["3ae97ad22c2ae646bfc1c09cab424cb07f9474ca"],"3ae97ad22c2ae646bfc1c09cab424cb07f9474ca":["5a98a65bdb67cd0b27d18a5564d63bd3e944d3f4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}