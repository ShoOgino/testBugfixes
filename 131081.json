{"path":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","commits":[{"id":"52c7e49be259508735752fba88085255014a6ecf","date":1398706273,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    for (IndexableField field : docState.doc.indexableFields()) {\n      IndexableFieldType fieldType = field.fieldType();\n      PerField fp = getOrAddField(field.name(), fieldType, true);\n      boolean first = fp.fieldGen != fieldGen;\n      fp.invert(field, first);\n\n      if (first) {\n        fields[fieldCount++] = fp;\n        fp.fieldGen = fieldGen;\n      }\n    }\n\n    // Finish each field name seen in the document:\n    for (int i=0;i<fieldCount;i++) {\n      fields[i].finish();\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    storedFieldsWriter.startDocument();\n    lastStoredDocID++;\n\n    success = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        final String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n\n        verifyFieldType(fieldName, fieldType);\n\n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n        }\n\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType != null) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n      success = true;\n    } finally {\n      if (success == false) {\n        // We must abort, on the possibility that the\n        // stored fields file is now corrupt:\n        docWriter.setAborting();\n      } else {\n        storedFieldsWriter.finishDocument();\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3394716f52b34ab259ad5247e7595d9f9db6e935","date":1398791921,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    for (IndexableField field : docState.doc.indexableFields()) {\n      IndexableFieldType fieldType = field.fieldType();\n      PerField fp = getOrAddField(field.name(), fieldType, true);\n      boolean first = fp.fieldGen != fieldGen;\n      fp.invert(field, first);\n\n      if (first) {\n        fields[fieldCount++] = fp;\n        fp.fieldGen = fieldGen;\n      }\n    }\n\n    // Finish each field name seen in the document:\n    for (int i=0;i<fieldCount;i++) {\n      fields[i].finish();\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    storedFieldsWriter.startDocument();\n    lastStoredDocID++;\n\n    success = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        final String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n\n        verifyFieldType(fieldName, fieldType);\n\n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n        }\n\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType != null) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n      success = true;\n    } finally {\n      if (success == false) {\n        // We must abort, on the possibility that the\n        // stored fields file is now corrupt:\n        docWriter.setAborting();\n      } else {\n        storedFieldsWriter.finishDocument();\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6d80deb55af16974d9c57440bb1cd937b3366138","date":1398794840,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","sourceNew":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    for (IndexableField field : docState.doc.indexableFields()) {\n      IndexableFieldType fieldType = field.fieldType();\n      PerField fp = getOrAddField(field.name(), fieldType, true);\n      boolean first = fp.fieldGen != fieldGen;\n      fp.invert(field, first);\n\n      if (first) {\n        fields[fieldCount++] = fp;\n        fp.fieldGen = fieldGen;\n      }\n    }\n\n    // Finish each field name seen in the document:\n    for (int i=0;i<fieldCount;i++) {\n      fields[i].finish();\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    storedFieldsWriter.startDocument();\n    lastStoredDocID++;\n\n    success = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        final String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n\n        verifyFieldType(fieldName, fieldType);\n\n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n        }\n\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType != null) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n      storedFieldsWriter.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // We must abort, on the possibility that the\n        // stored fields file is now corrupt:\n        docWriter.setAborting();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    for (IndexableField field : docState.doc.indexableFields()) {\n      IndexableFieldType fieldType = field.fieldType();\n      PerField fp = getOrAddField(field.name(), fieldType, true);\n      boolean first = fp.fieldGen != fieldGen;\n      fp.invert(field, first);\n\n      if (first) {\n        fields[fieldCount++] = fp;\n        fp.fieldGen = fieldGen;\n      }\n    }\n\n    // Finish each field name seen in the document:\n    for (int i=0;i<fieldCount;i++) {\n      fields[i].finish();\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    storedFieldsWriter.startDocument();\n    lastStoredDocID++;\n\n    success = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        final String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n\n        verifyFieldType(fieldName, fieldType);\n\n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n        }\n\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType != null) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n      success = true;\n    } finally {\n      if (success == false) {\n        // We must abort, on the possibility that the\n        // stored fields file is now corrupt:\n        docWriter.setAborting();\n      } else {\n        storedFieldsWriter.finishDocument();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","date":1398844771,"type":0,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    for (IndexableField field : docState.doc.indexableFields()) {\n      IndexableFieldType fieldType = field.fieldType();\n      PerField fp = getOrAddField(field.name(), fieldType, true);\n      boolean first = fp.fieldGen != fieldGen;\n      fp.invert(field, first);\n\n      if (first) {\n        fields[fieldCount++] = fp;\n        fp.fieldGen = fieldGen;\n      }\n    }\n\n    // Finish each field name seen in the document:\n    for (int i=0;i<fieldCount;i++) {\n      fields[i].finish();\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    storedFieldsWriter.startDocument();\n    lastStoredDocID++;\n\n    success = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        final String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n\n        verifyFieldType(fieldName, fieldType);\n\n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n        }\n\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType != null) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n      storedFieldsWriter.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // We must abort, on the possibility that the\n        // stored fields file is now corrupt:\n        docWriter.setAborting();\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1cf5c079ca08bb5521cdc7c9709da6a034c54218","date":1398877974,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","sourceNew":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    storedFieldsWriter.startDocument();\n    lastStoredDocID++;\n\n    success = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        final String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n\n        verifyFieldType(fieldName, fieldType);\n\n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n        }\n\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType != null) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n      storedFieldsWriter.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // We must abort, on the possibility that the\n        // stored fields file is now corrupt:\n        docWriter.setAborting();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    for (IndexableField field : docState.doc.indexableFields()) {\n      IndexableFieldType fieldType = field.fieldType();\n      PerField fp = getOrAddField(field.name(), fieldType, true);\n      boolean first = fp.fieldGen != fieldGen;\n      fp.invert(field, first);\n\n      if (first) {\n        fields[fieldCount++] = fp;\n        fp.fieldGen = fieldGen;\n      }\n    }\n\n    // Finish each field name seen in the document:\n    for (int i=0;i<fieldCount;i++) {\n      fields[i].finish();\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    storedFieldsWriter.startDocument();\n    lastStoredDocID++;\n\n    success = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        final String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n\n        verifyFieldType(fieldName, fieldType);\n\n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n        }\n\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType != null) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n      storedFieldsWriter.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // We must abort, on the possibility that the\n        // stored fields file is now corrupt:\n        docWriter.setAborting();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ca8ea2cdcac8ed9996221e84c4ca65f59e833aa9","date":1398938763,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","sourceNew":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    // TODO: if these hit exc today ->>> corrumption!\n    fillStoredFields(docState.docID);\n    storedFieldsWriter.startDocument();\n    lastStoredDocID++;\n\n    // TODO: clean up this looop, its complicated because dv exceptions are non-aborting,\n    // but storedfields are. Its also bogus that docvalues are treated as stored fields...\n    for (StorableField field : docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType fieldType = field.fieldType();\n      PerField fp = null;\n      \n      success = false;\n      try {\n        // TODO: make this non-aborting and change the test to confirm that!!!\n        verifyFieldType(fieldName, fieldType);\n        \n        fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          docWriter.setAborting();\n        }\n      }\n      \n      success = false;\n      try {\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType != null) {\n          indexDocValue(fp, dvType, field);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          // dv failed: so just try to bail on the current doc by calling finishDocument()...\n          success = false;\n          try {\n            storedFieldsWriter.finishDocument();\n            success = true;\n          } finally {\n            if (!success) {\n              docWriter.setAborting();\n            }\n          }\n        }\n      }\n    }\n    \n    success = false;\n    try {\n      storedFieldsWriter.finishDocument();\n      success = true;\n    } finally {\n      if (!success) {\n        docWriter.setAborting();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    storedFieldsWriter.startDocument();\n    lastStoredDocID++;\n\n    success = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        final String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n\n        verifyFieldType(fieldName, fieldType);\n\n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n        }\n\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType != null) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n      storedFieldsWriter.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // We must abort, on the possibility that the\n        // stored fields file is now corrupt:\n        docWriter.setAborting();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"96eb3a8dfc7a77ec7e482c2c00272d0b428441b6","date":1398951151,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","sourceNew":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    // TODO: if these hit exc today ->>> corrumption!\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's complicated because dv exceptions are non-aborting,\n    // but storedfields are. Its also bogus that docvalues are treated as stored fields...\n    for (StorableField field : docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType fieldType = field.fieldType();\n      PerField fp = null;\n      \n      success = false;\n      try {\n        // TODO: make this non-aborting and change the test to confirm that!!!\n        verifyFieldType(fieldName, fieldType);\n        \n        fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          docWriter.setAborting();\n        }\n      }\n      \n      success = false;\n      try {\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType != null) {\n          indexDocValue(fp, dvType, field);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          // dv failed: so just try to bail on the current doc by calling finishDocument()...\n          finishStoredFields();\n        }\n      }\n    }\n\n    finishStoredFields();\n  }\n\n","sourceOld":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    // TODO: if these hit exc today ->>> corrumption!\n    fillStoredFields(docState.docID);\n    storedFieldsWriter.startDocument();\n    lastStoredDocID++;\n\n    // TODO: clean up this looop, its complicated because dv exceptions are non-aborting,\n    // but storedfields are. Its also bogus that docvalues are treated as stored fields...\n    for (StorableField field : docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType fieldType = field.fieldType();\n      PerField fp = null;\n      \n      success = false;\n      try {\n        // TODO: make this non-aborting and change the test to confirm that!!!\n        verifyFieldType(fieldName, fieldType);\n        \n        fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          docWriter.setAborting();\n        }\n      }\n      \n      success = false;\n      try {\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType != null) {\n          indexDocValue(fp, dvType, field);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          // dv failed: so just try to bail on the current doc by calling finishDocument()...\n          success = false;\n          try {\n            storedFieldsWriter.finishDocument();\n            success = true;\n          } finally {\n            if (!success) {\n              docWriter.setAborting();\n            }\n          }\n        }\n      }\n    }\n    \n    success = false;\n    try {\n      storedFieldsWriter.finishDocument();\n      success = true;\n    } finally {\n      if (!success) {\n        docWriter.setAborting();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fca71501024d125c6671b2fbda9b2920909d3ccf","date":1398961701,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","sourceNew":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's bogus that docvalues are treated as stored fields...\n    boolean abort = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n      \n        verifyFieldType(fieldName, fieldType);\n        \n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          abort = true;\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n          abort = false;\n        }\n\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType != null) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n    } finally {\n      if (abort) {\n        docWriter.setAborting();\n      } else {\n        finishStoredFields();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    // TODO: if these hit exc today ->>> corrumption!\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's complicated because dv exceptions are non-aborting,\n    // but storedfields are. Its also bogus that docvalues are treated as stored fields...\n    for (StorableField field : docState.doc.storableFields()) {\n      final String fieldName = field.name();\n      IndexableFieldType fieldType = field.fieldType();\n      PerField fp = null;\n      \n      success = false;\n      try {\n        // TODO: make this non-aborting and change the test to confirm that!!!\n        verifyFieldType(fieldName, fieldType);\n        \n        fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          docWriter.setAborting();\n        }\n      }\n      \n      success = false;\n      try {\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType != null) {\n          indexDocValue(fp, dvType, field);\n        }\n        success = true;\n      } finally {\n        if (!success) {\n          // dv failed: so just try to bail on the current doc by calling finishDocument()...\n          finishStoredFields();\n        }\n      }\n    }\n\n    finishStoredFields();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bdc1a53703bb3d96d108c76a4321c3fac506b341","date":1400331928,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","sourceNew":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's bogus that docvalues are treated as stored fields...\n    boolean abort = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n      \n        verifyFieldType(fieldName, fieldType);\n        \n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          abort = true;\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n          abort = false;\n        }\n\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType != null) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n    } finally {\n      if (abort) {\n        docWriter.setAborting();\n      } else {\n        finishStoredFields();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's bogus that docvalues are treated as stored fields...\n    boolean abort = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n      \n        verifyFieldType(fieldName, fieldType);\n        \n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          abort = true;\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n          abort = false;\n        }\n\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType != null) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n    } finally {\n      if (abort) {\n        docWriter.setAborting();\n      } else {\n        finishStoredFields();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"56572ec06f1407c066d6b7399413178b33176cd8","date":1400495675,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","sourceNew":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's bogus that docvalues are treated as stored fields...\n    boolean abort = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n      \n        verifyFieldType(fieldName, fieldType);\n        \n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          abort = true;\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n          abort = false;\n        }\n\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType != null) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n    } finally {\n      if (abort) {\n        docWriter.setAborting();\n      } else {\n        finishStoredFields();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's bogus that docvalues are treated as stored fields...\n    boolean abort = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n      \n        verifyFieldType(fieldName, fieldType);\n        \n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          abort = true;\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n          abort = false;\n        }\n\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType != null) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n    } finally {\n      if (abort) {\n        docWriter.setAborting();\n      } else {\n        finishStoredFields();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2bb2842e561df4e8e9ad89010605fc86ac265465","date":1414768208,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","sourceNew":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's bogus that docvalues are treated as stored fields...\n    boolean abort = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n      \n        verifyFieldType(fieldName, fieldType);\n        \n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          abort = true;\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n          abort = false;\n        }\n\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType == null) {\n          throw new NullPointerException(\"docValueType cannot be null (field: \\\"\" + fieldName + \"\\\")\");\n        }\n        if (dvType != DocValuesType.NO) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n    } finally {\n      if (abort) {\n        docWriter.setAborting();\n      } else {\n        finishStoredFields();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's bogus that docvalues are treated as stored fields...\n    boolean abort = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n      \n        verifyFieldType(fieldName, fieldType);\n        \n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          abort = true;\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n          abort = false;\n        }\n\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType != null) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n    } finally {\n      if (abort) {\n        docWriter.setAborting();\n      } else {\n        finishStoredFields();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f95ce1375367b92d411a06175eab3915fe93c6bc","date":1414788502,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","sourceNew":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's bogus that docvalues are treated as stored fields...\n    boolean abort = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n      \n        verifyFieldType(fieldName, fieldType);\n        \n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          abort = true;\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n          abort = false;\n        }\n\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType == null) {\n          throw new NullPointerException(\"docValueType cannot be null (field: \\\"\" + fieldName + \"\\\")\");\n        }\n        if (dvType != DocValuesType.NONE) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n    } finally {\n      if (abort) {\n        docWriter.setAborting();\n      } else {\n        finishStoredFields();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's bogus that docvalues are treated as stored fields...\n    boolean abort = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n      \n        verifyFieldType(fieldName, fieldType);\n        \n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          abort = true;\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n          abort = false;\n        }\n\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType == null) {\n          throw new NullPointerException(\"docValueType cannot be null (field: \\\"\" + fieldName + \"\\\")\");\n        }\n        if (dvType != DocValuesType.NO) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n    } finally {\n      if (abort) {\n        docWriter.setAborting();\n      } else {\n        finishStoredFields();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eac6ccb51c439bec7f67cb0e299d3cb77b62b87e","date":1415435053,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","sourceNew":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's bogus that docvalues are treated as stored fields...\n    boolean abort = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n      \n        verifyFieldType(fieldName, fieldType);\n        \n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          abort = true;\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n          abort = false;\n        }\n\n        DocValuesType dvType = fieldType.docValuesType();\n        if (dvType == null) {\n          throw new NullPointerException(\"docValuesType cannot be null (field: \\\"\" + fieldName + \"\\\")\");\n        }\n        if (dvType != DocValuesType.NONE) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n    } finally {\n      if (abort) {\n        docWriter.setAborting();\n      } else {\n        finishStoredFields();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's bogus that docvalues are treated as stored fields...\n    boolean abort = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n      \n        verifyFieldType(fieldName, fieldType);\n        \n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          abort = true;\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n          abort = false;\n        }\n\n        DocValuesType dvType = fieldType.docValueType();\n        if (dvType == null) {\n          throw new NullPointerException(\"docValueType cannot be null (field: \\\"\" + fieldName + \"\\\")\");\n        }\n        if (dvType != DocValuesType.NONE) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n    } finally {\n      if (abort) {\n        docWriter.setAborting();\n      } else {\n        finishStoredFields();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9299079153fd7895bf3cf6835cf7019af2ba89b3","date":1417813477,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","sourceNew":"  @Override\n  public void processDocument() throws IOException, AbortingException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    try {\n      termsHash.finishDocument();\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      throw new AbortingException(th);\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's bogus that docvalues are treated as stored fields...\n    boolean abort = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n      \n        verifyFieldType(fieldName, fieldType);\n        \n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          try {\n            storedFieldsWriter.writeField(fp.fieldInfo, field);\n          } catch (Throwable th) {\n            abort = true;\n            throw new AbortingException(th);\n          }\n        }\n\n        DocValuesType dvType = fieldType.docValuesType();\n        if (dvType == null) {\n          throw new NullPointerException(\"docValuesType cannot be null (field: \\\"\" + fieldName + \"\\\")\");\n        }\n        if (dvType != DocValuesType.NONE) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n    } finally {\n      if (abort == false) {\n        finishStoredFields();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    boolean success = false;\n    try {\n      termsHash.finishDocument();\n      success = true;\n    } finally {\n      if (success == false) {\n        // Must abort, on the possibility that on-disk term\n        // vectors are now corrupt:\n        docWriter.setAborting();\n      }\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's bogus that docvalues are treated as stored fields...\n    boolean abort = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n      \n        verifyFieldType(fieldName, fieldType);\n        \n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          abort = true;\n          storedFieldsWriter.writeField(fp.fieldInfo, field);\n          abort = false;\n        }\n\n        DocValuesType dvType = fieldType.docValuesType();\n        if (dvType == null) {\n          throw new NullPointerException(\"docValuesType cannot be null (field: \\\"\" + fieldName + \"\\\")\");\n        }\n        if (dvType != DocValuesType.NONE) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n    } finally {\n      if (abort) {\n        docWriter.setAborting();\n      } else {\n        finishStoredFields();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9d2879042d8cb5738afcc7dc5f8f11f9d3006642","date":1418137170,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","sourceNew":"  @Override\n  public void processDocument() throws IOException, AbortingException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    try {\n      termsHash.finishDocument();\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      throw AbortingException.wrap(th);\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's bogus that docvalues are treated as stored fields...\n    boolean abort = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n      \n        verifyFieldType(fieldName, fieldType);\n        \n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          try {\n            storedFieldsWriter.writeField(fp.fieldInfo, field);\n          } catch (Throwable th) {\n            abort = true;\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        DocValuesType dvType = fieldType.docValuesType();\n        if (dvType == null) {\n          throw new NullPointerException(\"docValuesType cannot be null (field: \\\"\" + fieldName + \"\\\")\");\n        }\n        if (dvType != DocValuesType.NONE) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n    } finally {\n      if (abort == false) {\n        finishStoredFields();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument() throws IOException, AbortingException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    try {\n      termsHash.finishDocument();\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      throw new AbortingException(th);\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's bogus that docvalues are treated as stored fields...\n    boolean abort = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n      \n        verifyFieldType(fieldName, fieldType);\n        \n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          try {\n            storedFieldsWriter.writeField(fp.fieldInfo, field);\n          } catch (Throwable th) {\n            abort = true;\n            throw new AbortingException(th);\n          }\n        }\n\n        DocValuesType dvType = fieldType.docValuesType();\n        if (dvType == null) {\n          throw new NullPointerException(\"docValuesType cannot be null (field: \\\"\" + fieldName + \"\\\")\");\n        }\n        if (dvType != DocValuesType.NONE) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n    } finally {\n      if (abort == false) {\n        finishStoredFields();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8881d151c969a46b4293caa77faa7ecd63239100","date":1436824297,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","sourceNew":"  @Override\n  public void processDocument() throws IOException, AbortingException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    try {\n      termsHash.finishDocument();\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      throw AbortingException.wrap(th);\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's bogus that docvalues are treated as stored fields...\n    boolean abort = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n      \n        verifyFieldType(fieldName, fieldType);\n\n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          try {\n            storedFieldsWriter.writeField(fp.fieldInfo, field);\n          } catch (Throwable th) {\n            abort = true;\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        DocValuesType dvType = fieldType.docValuesType();\n        if (dvType == null) {\n          throw new NullPointerException(\"docValuesType cannot be null (field: \\\"\" + fieldName + \"\\\")\");\n        }\n        if (dvType != DocValuesType.NONE) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n    } finally {\n      if (abort == false) {\n        finishStoredFields();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument() throws IOException, AbortingException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    try {\n      termsHash.finishDocument();\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      throw AbortingException.wrap(th);\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's bogus that docvalues are treated as stored fields...\n    boolean abort = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n      \n        verifyFieldType(fieldName, fieldType);\n        \n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          try {\n            storedFieldsWriter.writeField(fp.fieldInfo, field);\n          } catch (Throwable th) {\n            abort = true;\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        DocValuesType dvType = fieldType.docValuesType();\n        if (dvType == null) {\n          throw new NullPointerException(\"docValuesType cannot be null (field: \\\"\" + fieldName + \"\\\")\");\n        }\n        if (dvType != DocValuesType.NONE) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n    } finally {\n      if (abort == false) {\n        finishStoredFields();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ca792c26af46bd6c4a08d81117c60440cf6a7e3d","date":1445938295,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","sourceNew":"  @Override\n  public void processDocument() throws IOException, AbortingException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    try {\n      termsHash.finishDocument();\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      throw AbortingException.wrap(th);\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's bogus that docvalues are treated as stored fields...\n    boolean abort = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n      \n        verifyFieldType(fieldName, fieldType);\n\n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          try {\n            storedFieldsWriter.writeField(fp.fieldInfo, field);\n          } catch (Throwable th) {\n            abort = true;\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        DocValuesType dvType = fieldType.docValuesType();\n        if (dvType == null) {\n          throw new NullPointerException(\"docValuesType cannot be null (field: \\\"\" + fieldName + \"\\\")\");\n        }\n        if (dvType != DocValuesType.NONE) {\n          indexDocValue(fp, dvType, field);\n        }\n        if (fieldType.dimensionCount() != 0) {\n          indexDimensionalValue(fp, field);\n        }\n      }\n    } finally {\n      if (abort == false) {\n        finishStoredFields();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument() throws IOException, AbortingException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    try {\n      termsHash.finishDocument();\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      throw AbortingException.wrap(th);\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's bogus that docvalues are treated as stored fields...\n    boolean abort = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n      \n        verifyFieldType(fieldName, fieldType);\n\n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          try {\n            storedFieldsWriter.writeField(fp.fieldInfo, field);\n          } catch (Throwable th) {\n            abort = true;\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        DocValuesType dvType = fieldType.docValuesType();\n        if (dvType == null) {\n          throw new NullPointerException(\"docValuesType cannot be null (field: \\\"\" + fieldName + \"\\\")\");\n        }\n        if (dvType != DocValuesType.NONE) {\n          indexDocValue(fp, dvType, field);\n        }\n      }\n    } finally {\n      if (abort == false) {\n        finishStoredFields();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","date":1453060490,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","sourceNew":"  @Override\n  public void processDocument() throws IOException, AbortingException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    boolean aborting = false;\n    try {\n      for (IndexableField field : docState.doc) {\n        fieldCount = processField(field, fieldGen, fieldCount);\n      }\n    } catch (AbortingException ae) {\n      aborting = true;\n      throw ae;\n    } finally {\n      if (aborting == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish();\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument();\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      throw AbortingException.wrap(th);\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument() throws IOException, AbortingException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    // Invert indexed fields:\n    try {\n      for (IndexableField field : docState.doc.indexableFields()) {\n        IndexableFieldType fieldType = field.fieldType();\n        \n        // if the field omits norms, the boost cannot be indexed.\n        if (fieldType.omitNorms() && field.boost() != 1.0f) {\n          throw new UnsupportedOperationException(\"You cannot set an index-time boost: norms are omitted for field '\" + field.name() + \"'\");\n        }\n        \n        PerField fp = getOrAddField(field.name(), fieldType, true);\n        boolean first = fp.fieldGen != fieldGen;\n        fp.invert(field, first);\n\n        if (first) {\n          fields[fieldCount++] = fp;\n          fp.fieldGen = fieldGen;\n        }\n      }\n    } finally {\n      // Finish each field name seen in the document:\n      for (int i=0;i<fieldCount;i++) {\n        fields[i].finish();\n      }\n    }\n\n    try {\n      termsHash.finishDocument();\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      throw AbortingException.wrap(th);\n    }\n\n    // Add stored fields:\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    // TODO: clean up this loop, it's bogus that docvalues are treated as stored fields...\n    boolean abort = false;\n    try {\n      for (StorableField field : docState.doc.storableFields()) {\n        String fieldName = field.name();\n        IndexableFieldType fieldType = field.fieldType();\n      \n        verifyFieldType(fieldName, fieldType);\n\n        PerField fp = getOrAddField(fieldName, fieldType, false);\n        if (fieldType.stored()) {\n          try {\n            storedFieldsWriter.writeField(fp.fieldInfo, field);\n          } catch (Throwable th) {\n            abort = true;\n            throw AbortingException.wrap(th);\n          }\n        }\n\n        DocValuesType dvType = fieldType.docValuesType();\n        if (dvType == null) {\n          throw new NullPointerException(\"docValuesType cannot be null (field: \\\"\" + fieldName + \"\\\")\");\n        }\n        if (dvType != DocValuesType.NONE) {\n          indexDocValue(fp, dvType, field);\n        }\n        if (fieldType.dimensionCount() != 0) {\n          indexDimensionalValue(fp, field);\n        }\n      }\n    } finally {\n      if (abort == false) {\n        finishStoredFields();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"86a0a50d2d14aaee1e635bbec914468551f7f9a2","date":1482234306,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","sourceNew":"  @Override\n  public void processDocument() throws IOException, AbortingException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    startStoredFields(docState.docID);\n\n    boolean aborting = false;\n    try {\n      for (IndexableField field : docState.doc) {\n        fieldCount = processField(field, fieldGen, fieldCount);\n      }\n    } catch (AbortingException ae) {\n      aborting = true;\n      throw ae;\n    } finally {\n      if (aborting == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish();\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument();\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      throw AbortingException.wrap(th);\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument() throws IOException, AbortingException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    boolean aborting = false;\n    try {\n      for (IndexableField field : docState.doc) {\n        fieldCount = processField(field, fieldGen, fieldCount);\n      }\n    } catch (AbortingException ae) {\n      aborting = true;\n      throw ae;\n    } finally {\n      if (aborting == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish();\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument();\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      throw AbortingException.wrap(th);\n    }\n  }\n\n","bugFix":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","date":1482251961,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","sourceNew":"  @Override\n  public void processDocument() throws IOException, AbortingException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    startStoredFields(docState.docID);\n\n    boolean aborting = false;\n    try {\n      for (IndexableField field : docState.doc) {\n        fieldCount = processField(field, fieldGen, fieldCount);\n      }\n    } catch (AbortingException ae) {\n      aborting = true;\n      throw ae;\n    } finally {\n      if (aborting == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish();\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument();\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      throw AbortingException.wrap(th);\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument() throws IOException, AbortingException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    fillStoredFields(docState.docID);\n    startStoredFields();\n\n    boolean aborting = false;\n    try {\n      for (IndexableField field : docState.doc) {\n        fieldCount = processField(field, fieldGen, fieldCount);\n      }\n    } catch (AbortingException ae) {\n      aborting = true;\n      throw ae;\n    } finally {\n      if (aborting == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish();\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument();\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      throw AbortingException.wrap(th);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"845b760a99e5f369fcd0a5d723a87b8def6a3f56","date":1521117993,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","sourceNew":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    startStoredFields(docState.docID);\n    try {\n      for (IndexableField field : docState.doc) {\n        fieldCount = processField(field, fieldGen, fieldCount);\n      }\n    } finally {\n      if (docWriter.hasHitAbortingException() == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish();\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument();\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      docWriter.onAbortingException(th);\n      throw th;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument() throws IOException, AbortingException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    startStoredFields(docState.docID);\n\n    boolean aborting = false;\n    try {\n      for (IndexableField field : docState.doc) {\n        fieldCount = processField(field, fieldGen, fieldCount);\n      }\n    } catch (AbortingException ae) {\n      aborting = true;\n      throw ae;\n    } finally {\n      if (aborting == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish();\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument();\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      throw AbortingException.wrap(th);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f97270426d92300e08ac1bd1a4ef499ae02e88b7","date":1592503330,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument(int,Iterable[#-extends-IndexableField]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain#processDocument().mjava","sourceNew":"  @Override\n  public void processDocument(int docID, Iterable<? extends IndexableField> document) throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    startStoredFields(docID);\n    try {\n      for (IndexableField field : document) {\n        fieldCount = processField(docID, field, fieldGen, fieldCount);\n      }\n    } finally {\n      if (docWriter.hasHitAbortingException() == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish(docID);\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument(docID);\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      docWriter.onAbortingException(th);\n      throw th;\n    }\n  }\n\n","sourceOld":"  @Override\n  public void processDocument() throws IOException {\n\n    // How many indexed field names we've seen (collapses\n    // multiple field instances by the same name):\n    int fieldCount = 0;\n\n    long fieldGen = nextFieldGen++;\n\n    // NOTE: we need two passes here, in case there are\n    // multi-valued fields, because we must process all\n    // instances of a given field at once, since the\n    // analyzer is free to reuse TokenStream across fields\n    // (i.e., we cannot have more than one TokenStream\n    // running \"at once\"):\n\n    termsHash.startDocument();\n\n    startStoredFields(docState.docID);\n    try {\n      for (IndexableField field : docState.doc) {\n        fieldCount = processField(field, fieldGen, fieldCount);\n      }\n    } finally {\n      if (docWriter.hasHitAbortingException() == false) {\n        // Finish each indexed field name seen in the document:\n        for (int i=0;i<fieldCount;i++) {\n          fields[i].finish();\n        }\n        finishStoredFields();\n      }\n    }\n\n    try {\n      termsHash.finishDocument();\n    } catch (Throwable th) {\n      // Must abort, on the possibility that on-disk term\n      // vectors are now corrupt:\n      docWriter.onAbortingException(th);\n      throw th;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"bdc1a53703bb3d96d108c76a4321c3fac506b341":["fca71501024d125c6671b2fbda9b2920909d3ccf"],"fca71501024d125c6671b2fbda9b2920909d3ccf":["96eb3a8dfc7a77ec7e482c2c00272d0b428441b6"],"845b760a99e5f369fcd0a5d723a87b8def6a3f56":["86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","6d80deb55af16974d9c57440bb1cd937b3366138"],"96eb3a8dfc7a77ec7e482c2c00272d0b428441b6":["ca8ea2cdcac8ed9996221e84c4ca65f59e833aa9"],"8881d151c969a46b4293caa77faa7ecd63239100":["9d2879042d8cb5738afcc7dc5f8f11f9d3006642"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["bdc1a53703bb3d96d108c76a4321c3fac506b341"],"6d80deb55af16974d9c57440bb1cd937b3366138":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"56572ec06f1407c066d6b7399413178b33176cd8":["fca71501024d125c6671b2fbda9b2920909d3ccf","bdc1a53703bb3d96d108c76a4321c3fac506b341"],"f95ce1375367b92d411a06175eab3915fe93c6bc":["2bb2842e561df4e8e9ad89010605fc86ac265465"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","52c7e49be259508735752fba88085255014a6ecf"],"ca8ea2cdcac8ed9996221e84c4ca65f59e833aa9":["1cf5c079ca08bb5521cdc7c9709da6a034c54218"],"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"9d2879042d8cb5738afcc7dc5f8f11f9d3006642":["9299079153fd7895bf3cf6835cf7019af2ba89b3"],"ca792c26af46bd6c4a08d81117c60440cf6a7e3d":["8881d151c969a46b4293caa77faa7ecd63239100"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9299079153fd7895bf3cf6835cf7019af2ba89b3":["eac6ccb51c439bec7f67cb0e299d3cb77b62b87e"],"eac6ccb51c439bec7f67cb0e299d3cb77b62b87e":["f95ce1375367b92d411a06175eab3915fe93c6bc"],"f97270426d92300e08ac1bd1a4ef499ae02e88b7":["845b760a99e5f369fcd0a5d723a87b8def6a3f56"],"86a0a50d2d14aaee1e635bbec914468551f7f9a2":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"1cf5c079ca08bb5521cdc7c9709da6a034c54218":["6d80deb55af16974d9c57440bb1cd937b3366138"],"52c7e49be259508735752fba88085255014a6ecf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f97270426d92300e08ac1bd1a4ef499ae02e88b7"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["ca792c26af46bd6c4a08d81117c60440cf6a7e3d"]},"commit2Childs":{"bdc1a53703bb3d96d108c76a4321c3fac506b341":["2bb2842e561df4e8e9ad89010605fc86ac265465","56572ec06f1407c066d6b7399413178b33176cd8"],"fca71501024d125c6671b2fbda9b2920909d3ccf":["bdc1a53703bb3d96d108c76a4321c3fac506b341","56572ec06f1407c066d6b7399413178b33176cd8"],"845b760a99e5f369fcd0a5d723a87b8def6a3f56":["f97270426d92300e08ac1bd1a4ef499ae02e88b7"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":[],"96eb3a8dfc7a77ec7e482c2c00272d0b428441b6":["fca71501024d125c6671b2fbda9b2920909d3ccf"],"8881d151c969a46b4293caa77faa7ecd63239100":["ca792c26af46bd6c4a08d81117c60440cf6a7e3d"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["f95ce1375367b92d411a06175eab3915fe93c6bc"],"6d80deb55af16974d9c57440bb1cd937b3366138":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","1cf5c079ca08bb5521cdc7c9709da6a034c54218"],"56572ec06f1407c066d6b7399413178b33176cd8":[],"f95ce1375367b92d411a06175eab3915fe93c6bc":["eac6ccb51c439bec7f67cb0e299d3cb77b62b87e"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["6d80deb55af16974d9c57440bb1cd937b3366138"],"ca8ea2cdcac8ed9996221e84c4ca65f59e833aa9":["96eb3a8dfc7a77ec7e482c2c00272d0b428441b6"],"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf":[],"9d2879042d8cb5738afcc7dc5f8f11f9d3006642":["8881d151c969a46b4293caa77faa7ecd63239100"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","3394716f52b34ab259ad5247e7595d9f9db6e935","52c7e49be259508735752fba88085255014a6ecf"],"ca792c26af46bd6c4a08d81117c60440cf6a7e3d":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"9299079153fd7895bf3cf6835cf7019af2ba89b3":["9d2879042d8cb5738afcc7dc5f8f11f9d3006642"],"eac6ccb51c439bec7f67cb0e299d3cb77b62b87e":["9299079153fd7895bf3cf6835cf7019af2ba89b3"],"86a0a50d2d14aaee1e635bbec914468551f7f9a2":["845b760a99e5f369fcd0a5d723a87b8def6a3f56","5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf"],"52c7e49be259508735752fba88085255014a6ecf":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"1cf5c079ca08bb5521cdc7c9709da6a034c54218":["ca8ea2cdcac8ed9996221e84c4ca65f59e833aa9"],"f97270426d92300e08ac1bd1a4ef499ae02e88b7":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","56572ec06f1407c066d6b7399413178b33176cd8","5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}