{"path":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43NGramTokenFilter#incrementToken().mjava","commits":[{"id":"379db3ad24c4f0214f30a122265a6d6be003a99d","date":1407537768,"type":0,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43NGramTokenFilter#incrementToken().mjava","pathOld":"/dev/null","sourceNew":"  /** Returns the next token in the stream, or null at EOS. */\n  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curCodePointCount = charUtils.codePointCount(termAtt);\n          curGramSize = minGram;\n          curPos = 0;\n          curPosInc = posIncAtt.getPositionIncrement();\n          curPosLen = posLenAtt.getPositionLength();\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          // if length by start + end offsets doesn't match the term text then assume\n          // this is a synonym and don't adjust the offsets.\n          hasIllegalOffsets = (tokStart + curTermLength) != tokEnd;\n        }\n      }\n\n      while (curGramSize <= maxGram) {\n        while (curPos+curGramSize <= curTermLength) {     // while there is input\n          clearAttributes();\n          termAtt.copyBuffer(curTermBuffer, curPos, curGramSize);\n          if (hasIllegalOffsets) {\n            offsetAtt.setOffset(tokStart, tokEnd);\n          } else {\n            offsetAtt.setOffset(tokStart + curPos, tokStart + curPos + curGramSize);\n          }\n          curPos++;\n          return true;\n        }\n        curGramSize++;                         // increase n-gram size\n        curPos = 0;\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"71387d8cb6923eb831b17a8b734608ba2e21c653","date":1414126093,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/Lucene43NGramTokenFilter#incrementToken().mjava","sourceNew":null,"sourceOld":"  /** Returns the next token in the stream, or null at EOS. */\n  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curCodePointCount = charUtils.codePointCount(termAtt);\n          curGramSize = minGram;\n          curPos = 0;\n          curPosInc = posIncAtt.getPositionIncrement();\n          curPosLen = posLenAtt.getPositionLength();\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          // if length by start + end offsets doesn't match the term text then assume\n          // this is a synonym and don't adjust the offsets.\n          hasIllegalOffsets = (tokStart + curTermLength) != tokEnd;\n        }\n      }\n\n      while (curGramSize <= maxGram) {\n        while (curPos+curGramSize <= curTermLength) {     // while there is input\n          clearAttributes();\n          termAtt.copyBuffer(curTermBuffer, curPos, curGramSize);\n          if (hasIllegalOffsets) {\n            offsetAtt.setOffset(tokStart, tokEnd);\n          } else {\n            offsetAtt.setOffset(tokStart + curPos, tokStart + curPos + curGramSize);\n          }\n          curPos++;\n          return true;\n        }\n        curGramSize++;                         // increase n-gram size\n        curPos = 0;\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"379db3ad24c4f0214f30a122265a6d6be003a99d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"71387d8cb6923eb831b17a8b734608ba2e21c653":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["71387d8cb6923eb831b17a8b734608ba2e21c653"]},"commit2Childs":{"379db3ad24c4f0214f30a122265a6d6be003a99d":["71387d8cb6923eb831b17a8b734608ba2e21c653"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"71387d8cb6923eb831b17a8b734608ba2e21c653":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}