{"path":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","commits":[{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","pathOld":"solr/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","sourceNew":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n\n      @SuppressWarnings(\"unchecked\")\n      List<String> excepts = (List<String>)facet_counts.get(\"exception\");\n      fi.addExceptions(excepts);\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_dates as the basis for subsequent shards' data to be merged.\n      // (the \"NOW\" param should ensure consistency)\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_dates = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_dates\");\n      \n      if (facet_dates != null) {\n\n        // go through each facet_date\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_dates) {\n          final String field = entry.getKey();\n          if (fi.dateFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.dateFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field\n\n            SimpleOrderedMap<Object> shardFieldValues \n              = entry.getValue();\n            SimpleOrderedMap<Object> existFieldValues \n              = fi.dateFacets.get(field);\n\n            for (Map.Entry<String,Object> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              if (key.equals(\"gap\") || \n                  key.equals(\"end\") || \n                  key.equals(\"start\")) {\n                // we can skip these, must all be the same across shards\n                continue; \n              }\n              // can be null if inconsistencies in shards responses\n              Integer newValue = (Integer) shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = ((Integer) existPair.getValue());\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n\n      // Distributed facet_ranges\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_ranges as the basis for subsequent shards' data to be merged.\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_ranges = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_ranges\");\n      \n      if (facet_ranges != null) {\n\n        // go through each facet_range\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_ranges) {\n          final String field = entry.getKey();\n          if (fi.rangeFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.rangeFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field counts\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> shardFieldValues \n              = (NamedList<Integer>) entry.getValue().get(\"counts\");\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> existFieldValues \n              = (NamedList<Integer>) fi.rangeFacets.get(field).get(\"counts\");\n\n            for (Map.Entry<String,Integer> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              // can be null if inconsistencies in shards responses\n              Integer newValue = shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = existPair.getValue();\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount == 0) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount == 0 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (!obs.get(sfc.termNum)) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (!obs.get(sfc.termNum) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<String>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n\n      @SuppressWarnings(\"unchecked\")\n      List<String> excepts = (List<String>)facet_counts.get(\"exception\");\n      fi.addExceptions(excepts);\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_dates as the basis for subsequent shards' data to be merged.\n      // (the \"NOW\" param should ensure consistency)\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_dates = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_dates\");\n      \n      if (facet_dates != null) {\n\n        // go through each facet_date\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_dates) {\n          final String field = entry.getKey();\n          if (fi.dateFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.dateFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field\n\n            SimpleOrderedMap<Object> shardFieldValues \n              = entry.getValue();\n            SimpleOrderedMap<Object> existFieldValues \n              = fi.dateFacets.get(field);\n\n            for (Map.Entry<String,Object> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              if (key.equals(\"gap\") || \n                  key.equals(\"end\") || \n                  key.equals(\"start\")) {\n                // we can skip these, must all be the same across shards\n                continue; \n              }\n              // can be null if inconsistencies in shards responses\n              Integer newValue = (Integer) shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = ((Integer) existPair.getValue());\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n\n      // Distributed facet_ranges\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_ranges as the basis for subsequent shards' data to be merged.\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_ranges = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_ranges\");\n      \n      if (facet_ranges != null) {\n\n        // go through each facet_range\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_ranges) {\n          final String field = entry.getKey();\n          if (fi.rangeFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.rangeFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field counts\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> shardFieldValues \n              = (NamedList<Integer>) entry.getValue().get(\"counts\");\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> existFieldValues \n              = (NamedList<Integer>) fi.rangeFacets.get(field).get(\"counts\");\n\n            for (Map.Entry<String,Integer> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              // can be null if inconsistencies in shards responses\n              Integer newValue = shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = existPair.getValue();\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount == 0) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount == 0 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (!obs.get(sfc.termNum)) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (!obs.get(sfc.termNum) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<String>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","pathOld":"solr/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","sourceNew":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n\n      @SuppressWarnings(\"unchecked\")\n      List<String> excepts = (List<String>)facet_counts.get(\"exception\");\n      fi.addExceptions(excepts);\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_dates as the basis for subsequent shards' data to be merged.\n      // (the \"NOW\" param should ensure consistency)\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_dates = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_dates\");\n      \n      if (facet_dates != null) {\n\n        // go through each facet_date\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_dates) {\n          final String field = entry.getKey();\n          if (fi.dateFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.dateFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field\n\n            SimpleOrderedMap<Object> shardFieldValues \n              = entry.getValue();\n            SimpleOrderedMap<Object> existFieldValues \n              = fi.dateFacets.get(field);\n\n            for (Map.Entry<String,Object> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              if (key.equals(\"gap\") || \n                  key.equals(\"end\") || \n                  key.equals(\"start\")) {\n                // we can skip these, must all be the same across shards\n                continue; \n              }\n              // can be null if inconsistencies in shards responses\n              Integer newValue = (Integer) shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = ((Integer) existPair.getValue());\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n\n      // Distributed facet_ranges\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_ranges as the basis for subsequent shards' data to be merged.\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_ranges = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_ranges\");\n      \n      if (facet_ranges != null) {\n\n        // go through each facet_range\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_ranges) {\n          final String field = entry.getKey();\n          if (fi.rangeFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.rangeFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field counts\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> shardFieldValues \n              = (NamedList<Integer>) entry.getValue().get(\"counts\");\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> existFieldValues \n              = (NamedList<Integer>) fi.rangeFacets.get(field).get(\"counts\");\n\n            for (Map.Entry<String,Integer> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              // can be null if inconsistencies in shards responses\n              Integer newValue = shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = existPair.getValue();\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount == 0) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount == 0 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (!obs.get(sfc.termNum)) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (!obs.get(sfc.termNum) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<String>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n\n      @SuppressWarnings(\"unchecked\")\n      List<String> excepts = (List<String>)facet_counts.get(\"exception\");\n      fi.addExceptions(excepts);\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_dates as the basis for subsequent shards' data to be merged.\n      // (the \"NOW\" param should ensure consistency)\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_dates = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_dates\");\n      \n      if (facet_dates != null) {\n\n        // go through each facet_date\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_dates) {\n          final String field = entry.getKey();\n          if (fi.dateFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.dateFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field\n\n            SimpleOrderedMap<Object> shardFieldValues \n              = entry.getValue();\n            SimpleOrderedMap<Object> existFieldValues \n              = fi.dateFacets.get(field);\n\n            for (Map.Entry<String,Object> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              if (key.equals(\"gap\") || \n                  key.equals(\"end\") || \n                  key.equals(\"start\")) {\n                // we can skip these, must all be the same across shards\n                continue; \n              }\n              // can be null if inconsistencies in shards responses\n              Integer newValue = (Integer) shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = ((Integer) existPair.getValue());\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n\n      // Distributed facet_ranges\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_ranges as the basis for subsequent shards' data to be merged.\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_ranges = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_ranges\");\n      \n      if (facet_ranges != null) {\n\n        // go through each facet_range\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_ranges) {\n          final String field = entry.getKey();\n          if (fi.rangeFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.rangeFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field counts\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> shardFieldValues \n              = (NamedList<Integer>) entry.getValue().get(\"counts\");\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> existFieldValues \n              = (NamedList<Integer>) fi.rangeFacets.get(field).get(\"counts\");\n\n            for (Map.Entry<String,Integer> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              // can be null if inconsistencies in shards responses\n              Integer newValue = shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = existPair.getValue();\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount == 0) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount == 0 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (!obs.get(sfc.termNum)) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (!obs.get(sfc.termNum) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<String>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","pathOld":"solr/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","sourceNew":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n\n      @SuppressWarnings(\"unchecked\")\n      List<String> excepts = (List<String>)facet_counts.get(\"exception\");\n      fi.addExceptions(excepts);\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_dates as the basis for subsequent shards' data to be merged.\n      // (the \"NOW\" param should ensure consistency)\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_dates = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_dates\");\n      \n      if (facet_dates != null) {\n\n        // go through each facet_date\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_dates) {\n          final String field = entry.getKey();\n          if (fi.dateFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.dateFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field\n\n            SimpleOrderedMap<Object> shardFieldValues \n              = entry.getValue();\n            SimpleOrderedMap<Object> existFieldValues \n              = fi.dateFacets.get(field);\n\n            for (Map.Entry<String,Object> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              if (key.equals(\"gap\") || \n                  key.equals(\"end\") || \n                  key.equals(\"start\")) {\n                // we can skip these, must all be the same across shards\n                continue; \n              }\n              // can be null if inconsistencies in shards responses\n              Integer newValue = (Integer) shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = ((Integer) existPair.getValue());\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n\n      // Distributed facet_ranges\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_ranges as the basis for subsequent shards' data to be merged.\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_ranges = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_ranges\");\n      \n      if (facet_ranges != null) {\n\n        // go through each facet_range\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_ranges) {\n          final String field = entry.getKey();\n          if (fi.rangeFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.rangeFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field counts\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> shardFieldValues \n              = (NamedList<Integer>) entry.getValue().get(\"counts\");\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> existFieldValues \n              = (NamedList<Integer>) fi.rangeFacets.get(field).get(\"counts\");\n\n            for (Map.Entry<String,Integer> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              // can be null if inconsistencies in shards responses\n              Integer newValue = shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = existPair.getValue();\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount == 0) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount == 0 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (!obs.get(sfc.termNum)) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (!obs.get(sfc.termNum) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<String>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n\n      @SuppressWarnings(\"unchecked\")\n      List<String> excepts = (List<String>)facet_counts.get(\"exception\");\n      fi.addExceptions(excepts);\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_dates as the basis for subsequent shards' data to be merged.\n      // (the \"NOW\" param should ensure consistency)\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_dates = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_dates\");\n      \n      if (facet_dates != null) {\n\n        // go through each facet_date\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_dates) {\n          final String field = entry.getKey();\n          if (fi.dateFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.dateFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field\n\n            SimpleOrderedMap<Object> shardFieldValues \n              = entry.getValue();\n            SimpleOrderedMap<Object> existFieldValues \n              = fi.dateFacets.get(field);\n\n            for (Map.Entry<String,Object> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              if (key.equals(\"gap\") || \n                  key.equals(\"end\") || \n                  key.equals(\"start\")) {\n                // we can skip these, must all be the same across shards\n                continue; \n              }\n              // can be null if inconsistencies in shards responses\n              Integer newValue = (Integer) shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = ((Integer) existPair.getValue());\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n\n      // Distributed facet_ranges\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_ranges as the basis for subsequent shards' data to be merged.\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_ranges = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_ranges\");\n      \n      if (facet_ranges != null) {\n\n        // go through each facet_range\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_ranges) {\n          final String field = entry.getKey();\n          if (fi.rangeFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.rangeFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field counts\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> shardFieldValues \n              = (NamedList<Integer>) entry.getValue().get(\"counts\");\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> existFieldValues \n              = (NamedList<Integer>) fi.rangeFacets.get(field).get(\"counts\");\n\n            for (Map.Entry<String,Integer> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              // can be null if inconsistencies in shards responses\n              Integer newValue = shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = existPair.getValue();\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount == 0) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount == 0 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (!obs.get(sfc.termNum)) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (!obs.get(sfc.termNum) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<String>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2f3eb2c0361adcc3828df1543195800e225f146e","date":1312072219,"type":3,"author":"Koji Sekiguchi","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","sourceNew":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_dates as the basis for subsequent shards' data to be merged.\n      // (the \"NOW\" param should ensure consistency)\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_dates = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_dates\");\n      \n      if (facet_dates != null) {\n\n        // go through each facet_date\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_dates) {\n          final String field = entry.getKey();\n          if (fi.dateFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.dateFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field\n\n            SimpleOrderedMap<Object> shardFieldValues \n              = entry.getValue();\n            SimpleOrderedMap<Object> existFieldValues \n              = fi.dateFacets.get(field);\n\n            for (Map.Entry<String,Object> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              if (key.equals(\"gap\") || \n                  key.equals(\"end\") || \n                  key.equals(\"start\")) {\n                // we can skip these, must all be the same across shards\n                continue; \n              }\n              // can be null if inconsistencies in shards responses\n              Integer newValue = (Integer) shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = ((Integer) existPair.getValue());\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n\n      // Distributed facet_ranges\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_ranges as the basis for subsequent shards' data to be merged.\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_ranges = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_ranges\");\n      \n      if (facet_ranges != null) {\n\n        // go through each facet_range\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_ranges) {\n          final String field = entry.getKey();\n          if (fi.rangeFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.rangeFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field counts\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> shardFieldValues \n              = (NamedList<Integer>) entry.getValue().get(\"counts\");\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> existFieldValues \n              = (NamedList<Integer>) fi.rangeFacets.get(field).get(\"counts\");\n\n            for (Map.Entry<String,Integer> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              // can be null if inconsistencies in shards responses\n              Integer newValue = shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = existPair.getValue();\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount == 0) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount == 0 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (!obs.get(sfc.termNum)) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (!obs.get(sfc.termNum) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<String>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n\n      @SuppressWarnings(\"unchecked\")\n      List<String> excepts = (List<String>)facet_counts.get(\"exception\");\n      fi.addExceptions(excepts);\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_dates as the basis for subsequent shards' data to be merged.\n      // (the \"NOW\" param should ensure consistency)\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_dates = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_dates\");\n      \n      if (facet_dates != null) {\n\n        // go through each facet_date\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_dates) {\n          final String field = entry.getKey();\n          if (fi.dateFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.dateFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field\n\n            SimpleOrderedMap<Object> shardFieldValues \n              = entry.getValue();\n            SimpleOrderedMap<Object> existFieldValues \n              = fi.dateFacets.get(field);\n\n            for (Map.Entry<String,Object> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              if (key.equals(\"gap\") || \n                  key.equals(\"end\") || \n                  key.equals(\"start\")) {\n                // we can skip these, must all be the same across shards\n                continue; \n              }\n              // can be null if inconsistencies in shards responses\n              Integer newValue = (Integer) shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = ((Integer) existPair.getValue());\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n\n      // Distributed facet_ranges\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_ranges as the basis for subsequent shards' data to be merged.\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_ranges = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_ranges\");\n      \n      if (facet_ranges != null) {\n\n        // go through each facet_range\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_ranges) {\n          final String field = entry.getKey();\n          if (fi.rangeFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.rangeFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field counts\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> shardFieldValues \n              = (NamedList<Integer>) entry.getValue().get(\"counts\");\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> existFieldValues \n              = (NamedList<Integer>) fi.rangeFacets.get(field).get(\"counts\");\n\n            for (Map.Entry<String,Integer> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              // can be null if inconsistencies in shards responses\n              Integer newValue = shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = existPair.getValue();\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount == 0) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount == 0 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (!obs.get(sfc.termNum)) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (!obs.get(sfc.termNum) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<String>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"88dcbe1d36c9d50525f58425024e87783c4005ab","date":1340729054,"type":3,"author":"Ryan McKinley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","sourceNew":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      }\n      catch(Exception ex) {\n        if(rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to read facet info for shard: \"+srsp.getShard(), ex);\n      }\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_dates as the basis for subsequent shards' data to be merged.\n      // (the \"NOW\" param should ensure consistency)\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_dates = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_dates\");\n      \n      if (facet_dates != null) {\n\n        // go through each facet_date\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_dates) {\n          final String field = entry.getKey();\n          if (fi.dateFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.dateFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field\n\n            SimpleOrderedMap<Object> shardFieldValues \n              = entry.getValue();\n            SimpleOrderedMap<Object> existFieldValues \n              = fi.dateFacets.get(field);\n\n            for (Map.Entry<String,Object> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              if (key.equals(\"gap\") || \n                  key.equals(\"end\") || \n                  key.equals(\"start\")) {\n                // we can skip these, must all be the same across shards\n                continue; \n              }\n              // can be null if inconsistencies in shards responses\n              Integer newValue = (Integer) shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = ((Integer) existPair.getValue());\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n\n      // Distributed facet_ranges\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_ranges as the basis for subsequent shards' data to be merged.\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_ranges = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_ranges\");\n      \n      if (facet_ranges != null) {\n\n        // go through each facet_range\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_ranges) {\n          final String field = entry.getKey();\n          if (fi.rangeFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.rangeFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field counts\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> shardFieldValues \n              = (NamedList<Integer>) entry.getValue().get(\"counts\");\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> existFieldValues \n              = (NamedList<Integer>) fi.rangeFacets.get(field).get(\"counts\");\n\n            for (Map.Entry<String,Integer> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              // can be null if inconsistencies in shards responses\n              Integer newValue = shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = existPair.getValue();\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount == 0) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount == 0 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (obs!=null && !obs.get(sfc.termNum)) {  // obs can be null if a shard request failed\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if(obs!=null && !obs.get(sfc.termNum) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<String>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_dates as the basis for subsequent shards' data to be merged.\n      // (the \"NOW\" param should ensure consistency)\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_dates = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_dates\");\n      \n      if (facet_dates != null) {\n\n        // go through each facet_date\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_dates) {\n          final String field = entry.getKey();\n          if (fi.dateFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.dateFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field\n\n            SimpleOrderedMap<Object> shardFieldValues \n              = entry.getValue();\n            SimpleOrderedMap<Object> existFieldValues \n              = fi.dateFacets.get(field);\n\n            for (Map.Entry<String,Object> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              if (key.equals(\"gap\") || \n                  key.equals(\"end\") || \n                  key.equals(\"start\")) {\n                // we can skip these, must all be the same across shards\n                continue; \n              }\n              // can be null if inconsistencies in shards responses\n              Integer newValue = (Integer) shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = ((Integer) existPair.getValue());\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n\n      // Distributed facet_ranges\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_ranges as the basis for subsequent shards' data to be merged.\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_ranges = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_ranges\");\n      \n      if (facet_ranges != null) {\n\n        // go through each facet_range\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_ranges) {\n          final String field = entry.getKey();\n          if (fi.rangeFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.rangeFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field counts\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> shardFieldValues \n              = (NamedList<Integer>) entry.getValue().get(\"counts\");\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> existFieldValues \n              = (NamedList<Integer>) fi.rangeFacets.get(field).get(\"counts\");\n\n            for (Map.Entry<String,Integer> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              // can be null if inconsistencies in shards responses\n              Integer newValue = shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = existPair.getValue();\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount == 0) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount == 0 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (!obs.get(sfc.termNum)) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (!obs.get(sfc.termNum) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<String>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","sourceNew":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      }\n      catch(Exception ex) {\n        if(rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to read facet info for shard: \"+srsp.getShard(), ex);\n      }\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_dates as the basis for subsequent shards' data to be merged.\n      // (the \"NOW\" param should ensure consistency)\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_dates = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_dates\");\n      \n      if (facet_dates != null) {\n\n        // go through each facet_date\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_dates) {\n          final String field = entry.getKey();\n          if (fi.dateFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.dateFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field\n\n            SimpleOrderedMap<Object> shardFieldValues \n              = entry.getValue();\n            SimpleOrderedMap<Object> existFieldValues \n              = fi.dateFacets.get(field);\n\n            for (Map.Entry<String,Object> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              if (key.equals(\"gap\") || \n                  key.equals(\"end\") || \n                  key.equals(\"start\")) {\n                // we can skip these, must all be the same across shards\n                continue; \n              }\n              // can be null if inconsistencies in shards responses\n              Integer newValue = (Integer) shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = ((Integer) existPair.getValue());\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n\n      // Distributed facet_ranges\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_ranges as the basis for subsequent shards' data to be merged.\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_ranges = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_ranges\");\n      \n      if (facet_ranges != null) {\n\n        // go through each facet_range\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_ranges) {\n          final String field = entry.getKey();\n          if (fi.rangeFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.rangeFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field counts\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> shardFieldValues \n              = (NamedList<Integer>) entry.getValue().get(\"counts\");\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> existFieldValues \n              = (NamedList<Integer>) fi.rangeFacets.get(field).get(\"counts\");\n\n            for (Map.Entry<String,Integer> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              // can be null if inconsistencies in shards responses\n              Integer newValue = shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = existPair.getValue();\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount == 0) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount == 0 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (obs!=null && !obs.get(sfc.termNum)) {  // obs can be null if a shard request failed\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if(obs!=null && !obs.get(sfc.termNum) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<String>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_dates as the basis for subsequent shards' data to be merged.\n      // (the \"NOW\" param should ensure consistency)\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_dates = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_dates\");\n      \n      if (facet_dates != null) {\n\n        // go through each facet_date\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_dates) {\n          final String field = entry.getKey();\n          if (fi.dateFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.dateFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field\n\n            SimpleOrderedMap<Object> shardFieldValues \n              = entry.getValue();\n            SimpleOrderedMap<Object> existFieldValues \n              = fi.dateFacets.get(field);\n\n            for (Map.Entry<String,Object> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              if (key.equals(\"gap\") || \n                  key.equals(\"end\") || \n                  key.equals(\"start\")) {\n                // we can skip these, must all be the same across shards\n                continue; \n              }\n              // can be null if inconsistencies in shards responses\n              Integer newValue = (Integer) shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = ((Integer) existPair.getValue());\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n\n      // Distributed facet_ranges\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_ranges as the basis for subsequent shards' data to be merged.\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_ranges = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_ranges\");\n      \n      if (facet_ranges != null) {\n\n        // go through each facet_range\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_ranges) {\n          final String field = entry.getKey();\n          if (fi.rangeFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.rangeFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field counts\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> shardFieldValues \n              = (NamedList<Integer>) entry.getValue().get(\"counts\");\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> existFieldValues \n              = (NamedList<Integer>) fi.rangeFacets.get(field).get(\"counts\");\n\n            for (Map.Entry<String,Integer> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              // can be null if inconsistencies in shards responses\n              Integer newValue = shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = existPair.getValue();\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount == 0) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount == 0 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (!obs.get(sfc.termNum)) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (!obs.get(sfc.termNum) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<String>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9742dc77a3dbf4efdfcddc9be43eea63e62e5e7a","date":1360141694,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","sourceNew":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      }\n      catch(Exception ex) {\n        if(rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to read facet info for shard: \"+srsp.getShard(), ex);\n      }\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_dates as the basis for subsequent shards' data to be merged.\n      // (the \"NOW\" param should ensure consistency)\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_dates = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_dates\");\n      \n      if (facet_dates != null) {\n\n        // go through each facet_date\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_dates) {\n          final String field = entry.getKey();\n          if (fi.dateFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.dateFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field\n\n            SimpleOrderedMap<Object> shardFieldValues \n              = entry.getValue();\n            SimpleOrderedMap<Object> existFieldValues \n              = fi.dateFacets.get(field);\n\n            for (Map.Entry<String,Object> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              if (key.equals(\"gap\") || \n                  key.equals(\"end\") || \n                  key.equals(\"start\")) {\n                // we can skip these, must all be the same across shards\n                continue; \n              }\n              // can be null if inconsistencies in shards responses\n              Integer newValue = (Integer) shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = ((Integer) existPair.getValue());\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n\n      // Distributed facet_ranges\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_ranges as the basis for subsequent shards' data to be merged.\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_ranges = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_ranges\");\n      \n      if (facet_ranges != null) {\n\n        // go through each facet_range\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_ranges) {\n          final String field = entry.getKey();\n          if (fi.rangeFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.rangeFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field counts\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> shardFieldValues \n              = (NamedList<Integer>) entry.getValue().get(\"counts\");\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> existFieldValues \n              = (NamedList<Integer>) fi.rangeFacets.get(field).get(\"counts\");\n\n            for (Map.Entry<String,Integer> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              // can be null if inconsistencies in shards responses\n              Integer newValue = shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = existPair.getValue();\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (obs!=null && !obs.get(sfc.termNum)) {  // obs can be null if a shard request failed\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if(obs!=null && !obs.get(sfc.termNum) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<String>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      }\n      catch(Exception ex) {\n        if(rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to read facet info for shard: \"+srsp.getShard(), ex);\n      }\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_dates as the basis for subsequent shards' data to be merged.\n      // (the \"NOW\" param should ensure consistency)\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_dates = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_dates\");\n      \n      if (facet_dates != null) {\n\n        // go through each facet_date\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_dates) {\n          final String field = entry.getKey();\n          if (fi.dateFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.dateFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field\n\n            SimpleOrderedMap<Object> shardFieldValues \n              = entry.getValue();\n            SimpleOrderedMap<Object> existFieldValues \n              = fi.dateFacets.get(field);\n\n            for (Map.Entry<String,Object> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              if (key.equals(\"gap\") || \n                  key.equals(\"end\") || \n                  key.equals(\"start\")) {\n                // we can skip these, must all be the same across shards\n                continue; \n              }\n              // can be null if inconsistencies in shards responses\n              Integer newValue = (Integer) shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = ((Integer) existPair.getValue());\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n\n      // Distributed facet_ranges\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_ranges as the basis for subsequent shards' data to be merged.\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_ranges = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_ranges\");\n      \n      if (facet_ranges != null) {\n\n        // go through each facet_range\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_ranges) {\n          final String field = entry.getKey();\n          if (fi.rangeFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.rangeFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field counts\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> shardFieldValues \n              = (NamedList<Integer>) entry.getValue().get(\"counts\");\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> existFieldValues \n              = (NamedList<Integer>) fi.rangeFacets.get(field).get(\"counts\");\n\n            for (Map.Entry<String,Integer> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              // can be null if inconsistencies in shards responses\n              Integer newValue = shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = existPair.getValue();\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount == 0) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount == 0 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (obs!=null && !obs.get(sfc.termNum)) {  // obs can be null if a shard request failed\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if(obs!=null && !obs.get(sfc.termNum) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<String>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a69cf7f1b4cac5d5b1363402b565cd535f13e6a1","date":1392536197,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","sourceNew":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      }\n      catch(Exception ex) {\n        if(rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to read facet info for shard: \"+srsp.getShard(), ex);\n      }\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_dates as the basis for subsequent shards' data to be merged.\n      // (the \"NOW\" param should ensure consistency)\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_dates = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_dates\");\n      \n      if (facet_dates != null) {\n\n        // go through each facet_date\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_dates) {\n          final String field = entry.getKey();\n          if (fi.dateFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.dateFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field\n\n            SimpleOrderedMap<Object> shardFieldValues \n              = entry.getValue();\n            SimpleOrderedMap<Object> existFieldValues \n              = fi.dateFacets.get(field);\n\n            for (Map.Entry<String,Object> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              if (key.equals(\"gap\") || \n                  key.equals(\"end\") || \n                  key.equals(\"start\")) {\n                // we can skip these, must all be the same across shards\n                continue; \n              }\n              // can be null if inconsistencies in shards responses\n              Integer newValue = (Integer) shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = ((Integer) existPair.getValue());\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n\n      // Distributed facet_ranges\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_ranges as the basis for subsequent shards' data to be merged.\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_ranges = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_ranges\");\n      \n      if (facet_ranges != null) {\n\n        // go through each facet_range\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_ranges) {\n          final String field = entry.getKey();\n          if (fi.rangeFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.rangeFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field counts\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> shardFieldValues \n              = (NamedList<Integer>) entry.getValue().get(\"counts\");\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> existFieldValues \n              = (NamedList<Integer>) fi.rangeFacets.get(field).get(\"counts\");\n\n            for (Map.Entry<String,Integer> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              // can be null if inconsistencies in shards responses\n              Integer newValue = shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = existPair.getValue();\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            if (fbs!=null && !fbs.get(sfc.termNum)) {  // fbs can be null if a shard request failed\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            if(fbs!=null && !fbs.get(sfc.termNum) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<String>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      }\n      catch(Exception ex) {\n        if(rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to read facet info for shard: \"+srsp.getShard(), ex);\n      }\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_dates as the basis for subsequent shards' data to be merged.\n      // (the \"NOW\" param should ensure consistency)\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_dates = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_dates\");\n      \n      if (facet_dates != null) {\n\n        // go through each facet_date\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_dates) {\n          final String field = entry.getKey();\n          if (fi.dateFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.dateFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field\n\n            SimpleOrderedMap<Object> shardFieldValues \n              = entry.getValue();\n            SimpleOrderedMap<Object> existFieldValues \n              = fi.dateFacets.get(field);\n\n            for (Map.Entry<String,Object> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              if (key.equals(\"gap\") || \n                  key.equals(\"end\") || \n                  key.equals(\"start\")) {\n                // we can skip these, must all be the same across shards\n                continue; \n              }\n              // can be null if inconsistencies in shards responses\n              Integer newValue = (Integer) shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = ((Integer) existPair.getValue());\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n\n      // Distributed facet_ranges\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_ranges as the basis for subsequent shards' data to be merged.\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_ranges = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_ranges\");\n      \n      if (facet_ranges != null) {\n\n        // go through each facet_range\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_ranges) {\n          final String field = entry.getKey();\n          if (fi.rangeFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.rangeFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field counts\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> shardFieldValues \n              = (NamedList<Integer>) entry.getValue().get(\"counts\");\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> existFieldValues \n              = (NamedList<Integer>) fi.rangeFacets.get(field).get(\"counts\");\n\n            for (Map.Entry<String,Integer> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              // can be null if inconsistencies in shards responses\n              Integer newValue = shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = existPair.getValue();\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if (obs!=null && !obs.get(sfc.termNum)) {  // obs can be null if a shard request failed\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            OpenBitSet obs = dff.counted[shardNum];\n            if(obs!=null && !obs.get(sfc.termNum) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<String>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"008ce2e6bd88e213b7888377901d5a70286479c0","date":1392583048,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","sourceNew":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      }\n      catch(Exception ex) {\n        if(rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to read facet info for shard: \"+srsp.getShard(), ex);\n      }\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_dates as the basis for subsequent shards' data to be merged.\n      // (the \"NOW\" param should ensure consistency)\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_dates = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_dates\");\n      \n      if (facet_dates != null) {\n\n        // go through each facet_date\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_dates) {\n          final String field = entry.getKey();\n          if (fi.dateFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.dateFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field\n\n            SimpleOrderedMap<Object> shardFieldValues \n              = entry.getValue();\n            SimpleOrderedMap<Object> existFieldValues \n              = fi.dateFacets.get(field);\n\n            for (Map.Entry<String,Object> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              if (key.equals(\"gap\") || \n                  key.equals(\"end\") || \n                  key.equals(\"start\")) {\n                // we can skip these, must all be the same across shards\n                continue; \n              }\n              // can be null if inconsistencies in shards responses\n              Integer newValue = (Integer) shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = ((Integer) existPair.getValue());\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n\n      // Distributed facet_ranges\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_ranges as the basis for subsequent shards' data to be merged.\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_ranges = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_ranges\");\n      \n      if (facet_ranges != null) {\n\n        // go through each facet_range\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_ranges) {\n          final String field = entry.getKey();\n          if (fi.rangeFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.rangeFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field counts\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> shardFieldValues \n              = (NamedList<Integer>) entry.getValue().get(\"counts\");\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> existFieldValues \n              = (NamedList<Integer>) fi.rangeFacets.get(field).get(\"counts\");\n\n            for (Map.Entry<String,Integer> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              // can be null if inconsistencies in shards responses\n              Integer newValue = shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = existPair.getValue();\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            if (fbs!=null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {  // fbs can be null if a shard request failed\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            if(fbs!=null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<String>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      }\n      catch(Exception ex) {\n        if(rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to read facet info for shard: \"+srsp.getShard(), ex);\n      }\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_dates as the basis for subsequent shards' data to be merged.\n      // (the \"NOW\" param should ensure consistency)\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_dates = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_dates\");\n      \n      if (facet_dates != null) {\n\n        // go through each facet_date\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_dates) {\n          final String field = entry.getKey();\n          if (fi.dateFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.dateFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field\n\n            SimpleOrderedMap<Object> shardFieldValues \n              = entry.getValue();\n            SimpleOrderedMap<Object> existFieldValues \n              = fi.dateFacets.get(field);\n\n            for (Map.Entry<String,Object> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              if (key.equals(\"gap\") || \n                  key.equals(\"end\") || \n                  key.equals(\"start\")) {\n                // we can skip these, must all be the same across shards\n                continue; \n              }\n              // can be null if inconsistencies in shards responses\n              Integer newValue = (Integer) shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = ((Integer) existPair.getValue());\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n\n      // Distributed facet_ranges\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_ranges as the basis for subsequent shards' data to be merged.\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_ranges = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_ranges\");\n      \n      if (facet_ranges != null) {\n\n        // go through each facet_range\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_ranges) {\n          final String field = entry.getKey();\n          if (fi.rangeFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.rangeFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field counts\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> shardFieldValues \n              = (NamedList<Integer>) entry.getValue().get(\"counts\");\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> existFieldValues \n              = (NamedList<Integer>) fi.rangeFacets.get(field).get(\"counts\");\n\n            for (Map.Entry<String,Integer> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              // can be null if inconsistencies in shards responses\n              Integer newValue = shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = existPair.getValue();\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            if (fbs!=null && !fbs.get(sfc.termNum)) {  // fbs can be null if a shard request failed\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            if(fbs!=null && !fbs.get(sfc.termNum) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<String>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","sourceNew":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      }\n      catch(Exception ex) {\n        if(rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to read facet info for shard: \"+srsp.getShard(), ex);\n      }\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_dates as the basis for subsequent shards' data to be merged.\n      // (the \"NOW\" param should ensure consistency)\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_dates = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_dates\");\n      \n      if (facet_dates != null) {\n\n        // go through each facet_date\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_dates) {\n          final String field = entry.getKey();\n          if (fi.dateFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.dateFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field\n\n            SimpleOrderedMap<Object> shardFieldValues \n              = entry.getValue();\n            SimpleOrderedMap<Object> existFieldValues \n              = fi.dateFacets.get(field);\n\n            for (Map.Entry<String,Object> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              if (key.equals(\"gap\") || \n                  key.equals(\"end\") || \n                  key.equals(\"start\")) {\n                // we can skip these, must all be the same across shards\n                continue; \n              }\n              // can be null if inconsistencies in shards responses\n              Integer newValue = (Integer) shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = ((Integer) existPair.getValue());\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n\n      // Distributed facet_ranges\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_ranges as the basis for subsequent shards' data to be merged.\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_ranges = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_ranges\");\n      \n      if (facet_ranges != null) {\n\n        // go through each facet_range\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_ranges) {\n          final String field = entry.getKey();\n          if (fi.rangeFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.rangeFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field counts\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> shardFieldValues \n              = (NamedList<Integer>) entry.getValue().get(\"counts\");\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> existFieldValues \n              = (NamedList<Integer>) fi.rangeFacets.get(field).get(\"counts\");\n\n            for (Map.Entry<String,Integer> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              // can be null if inconsistencies in shards responses\n              Integer newValue = shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = existPair.getValue();\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            if (fbs!=null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {  // fbs can be null if a shard request failed\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            if(fbs!=null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      }\n      catch(Exception ex) {\n        if(rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to read facet info for shard: \"+srsp.getShard(), ex);\n      }\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_dates as the basis for subsequent shards' data to be merged.\n      // (the \"NOW\" param should ensure consistency)\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_dates = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_dates\");\n      \n      if (facet_dates != null) {\n\n        // go through each facet_date\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_dates) {\n          final String field = entry.getKey();\n          if (fi.dateFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.dateFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field\n\n            SimpleOrderedMap<Object> shardFieldValues \n              = entry.getValue();\n            SimpleOrderedMap<Object> existFieldValues \n              = fi.dateFacets.get(field);\n\n            for (Map.Entry<String,Object> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              if (key.equals(\"gap\") || \n                  key.equals(\"end\") || \n                  key.equals(\"start\")) {\n                // we can skip these, must all be the same across shards\n                continue; \n              }\n              // can be null if inconsistencies in shards responses\n              Integer newValue = (Integer) shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = ((Integer) existPair.getValue());\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n\n      // Distributed facet_ranges\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_ranges as the basis for subsequent shards' data to be merged.\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_ranges = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_ranges\");\n      \n      if (facet_ranges != null) {\n\n        // go through each facet_range\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_ranges) {\n          final String field = entry.getKey();\n          if (fi.rangeFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.rangeFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field counts\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> shardFieldValues \n              = (NamedList<Integer>) entry.getValue().get(\"counts\");\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> existFieldValues \n              = (NamedList<Integer>) fi.rangeFacets.get(field).get(\"counts\");\n\n            for (Map.Entry<String,Integer> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              // can be null if inconsistencies in shards responses\n              Integer newValue = shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = existPair.getValue();\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            if (fbs!=null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {  // fbs can be null if a shard request failed\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            if(fbs!=null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<String>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"84b24f47dd3dfa4e2396cd6f819a35445b0a53fd","date":1406137403,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","sourceNew":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      }\n      catch(Exception ex) {\n        if(rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to read facet info for shard: \"+srsp.getShard(), ex);\n      }\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      doDistribDates(fi, facet_counts);\n\n\n      // Distributed facet_ranges\n      doDistribRanges(fi, facet_counts);\n\n\n      // Distributed facet_intervals\n      doDistribIntervals(fi, facet_counts);\n\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            if (fbs!=null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {  // fbs can be null if a shard request failed\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            if(fbs!=null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      }\n      catch(Exception ex) {\n        if(rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to read facet info for shard: \"+srsp.getShard(), ex);\n      }\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_dates as the basis for subsequent shards' data to be merged.\n      // (the \"NOW\" param should ensure consistency)\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_dates = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_dates\");\n      \n      if (facet_dates != null) {\n\n        // go through each facet_date\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_dates) {\n          final String field = entry.getKey();\n          if (fi.dateFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.dateFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field\n\n            SimpleOrderedMap<Object> shardFieldValues \n              = entry.getValue();\n            SimpleOrderedMap<Object> existFieldValues \n              = fi.dateFacets.get(field);\n\n            for (Map.Entry<String,Object> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              if (key.equals(\"gap\") || \n                  key.equals(\"end\") || \n                  key.equals(\"start\")) {\n                // we can skip these, must all be the same across shards\n                continue; \n              }\n              // can be null if inconsistencies in shards responses\n              Integer newValue = (Integer) shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = ((Integer) existPair.getValue());\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n\n      // Distributed facet_ranges\n      //\n      // The implementation below uses the first encountered shard's \n      // facet_ranges as the basis for subsequent shards' data to be merged.\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> facet_ranges = \n        (SimpleOrderedMap<SimpleOrderedMap<Object>>) \n        facet_counts.get(\"facet_ranges\");\n      \n      if (facet_ranges != null) {\n\n        // go through each facet_range\n        for (Map.Entry<String,SimpleOrderedMap<Object>> entry : facet_ranges) {\n          final String field = entry.getKey();\n          if (fi.rangeFacets.get(field) == null) { \n            // first time we've seen this field, no merging\n            fi.rangeFacets.add(field, entry.getValue());\n\n          } else { \n            // not the first time, merge current field counts\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> shardFieldValues \n              = (NamedList<Integer>) entry.getValue().get(\"counts\");\n\n            @SuppressWarnings(\"unchecked\")\n            NamedList<Integer> existFieldValues \n              = (NamedList<Integer>) fi.rangeFacets.get(field).get(\"counts\");\n\n            for (Map.Entry<String,Integer> existPair : existFieldValues) {\n              final String key = existPair.getKey();\n              // can be null if inconsistencies in shards responses\n              Integer newValue = shardFieldValues.get(key);\n              if  (null != newValue) {\n                Integer oldValue = existPair.getValue();\n                existPair.setValue(oldValue + newValue);\n              }\n            }\n          }\n        }\n      }\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            if (fbs!=null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {  // fbs can be null if a shard request failed\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            if(fbs!=null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"92751ba9273251eab6a2e379ec42a1697a32ff96","date":1407954233,"type":3,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","sourceNew":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n    \n    for (ShardResponse srsp : sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList) srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      } catch (Exception ex) {\n        if (rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Unable to read facet info for shard: \" + srsp.getShard(), ex);\n      }\n      \n      // handle facet queries\n      NamedList facet_queries = (NamedList) facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i = 0; i < facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number) facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n      \n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList) facet_counts.get(\"facet_fields\");\n      \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList) facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n      \n      // Distributed facet_dates\n      doDistribDates(fi, facet_counts);\n\n      // Distributed facet_ranges\n      doDistribRanges(fi, facet_counts);\n\n      // Distributed facet_intervals\n      doDistribIntervals(fi, facet_counts);\n      \n      // Distributed facet_pivots - this is just the per shard collection,\n      // refinement reqs still needed (below) once we've considered every shard\n      doDistribPivots(rb, shardNum, facet_counts);\n\n    } // end for-each-response-in-shard-request...\n    \n    // refine each pivot based on the new shard data\n    for (Entry<String,PivotFacet> pivotFacet : fi.pivotFacets) {\n      pivotFacet.getValue().queuePivotRefinementRequests();\n    }\n    \n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n    for (DistribFieldFacet dff : fi.facets.values()) {\n      // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n      \n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n      \n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n      \n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, \n                          dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop - 1].count;\n      \n      for (int i = 0; i < counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n        \n        if (i < ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n          \n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc, shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n        \n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null && \n                (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) && \n                dff.maxPossible(sfc, shardNum) > 0) {\n\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n\n    for (ShardResponse srsp: sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList)srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      }\n      catch(Exception ex) {\n        if(rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR, \"Unable to read facet info for shard: \"+srsp.getShard(), ex);\n      }\n\n      // handle facet queries\n      NamedList facet_queries = (NamedList)facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i=0; i<facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number)facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList)facet_counts.get(\"facet_fields\");\n    \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList)facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      doDistribDates(fi, facet_counts);\n\n\n      // Distributed facet_ranges\n      doDistribRanges(fi, facet_counts);\n\n\n      // Distributed facet_intervals\n      doDistribIntervals(fi, facet_counts);\n\n    }\n\n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n\n    for (DistribFieldFacet dff : fi.facets.values()) {\n       // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop-1].count;\n\n      for (int i=0; i<counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i<ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n\n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            if (fbs!=null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {  // fbs can be null if a shard request failed\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc,shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum=0; shardNum<rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            if(fbs!=null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) && dff.maxPossible(sfc,shardNum)>0) {\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["fa16b8f2a7bb28ece5a9fdc471357e89de17bc57"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8ff4734b6c86245e852fe8b6a286716d5e59d415","date":1410194063,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","sourceNew":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n    \n    for (ShardResponse srsp : sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList) srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      } catch (Exception ex) {\n        if (rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Unable to read facet info for shard: \" + srsp.getShard(), ex);\n      }\n      \n      // handle facet queries\n      NamedList facet_queries = (NamedList) facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i = 0; i < facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number) facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n      \n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList) facet_counts.get(\"facet_fields\");\n      \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList) facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      doDistribDates(fi, facet_counts);\n\n      // Distributed facet_ranges\n      doDistribRanges(fi, facet_counts);\n\n      // Distributed facet_intervals\n      doDistribIntervals(fi, facet_counts);\n      \n      // Distributed facet_pivots - this is just the per shard collection,\n      // refinement reqs still needed (below) once we've considered every shard\n      doDistribPivots(rb, shardNum, facet_counts);\n\n    } // end for-each-response-in-shard-request...\n    \n    // refine each pivot based on the new shard data\n    for (Entry<String,PivotFacet> pivotFacet : fi.pivotFacets) {\n      pivotFacet.getValue().queuePivotRefinementRequests();\n    }\n    \n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n    for (DistribFieldFacet dff : fi.facets.values()) {\n      // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, \n                          dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop - 1].count;\n      \n      for (int i = 0; i < counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i < ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n          \n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc, shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null &&\n                (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) &&\n                dff.maxPossible(sfc, shardNum) > 0) {\n\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n    removeFieldFacetsUnderLimits(rb);\n    removeRangeFacetsUnderLimits(rb);\n    removeQueryFacetsUnderLimits(rb);\n\n  }\n\n","sourceOld":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n    \n    for (ShardResponse srsp : sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList) srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      } catch (Exception ex) {\n        if (rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Unable to read facet info for shard: \" + srsp.getShard(), ex);\n      }\n      \n      // handle facet queries\n      NamedList facet_queries = (NamedList) facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i = 0; i < facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number) facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n      \n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList) facet_counts.get(\"facet_fields\");\n      \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList) facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n      \n      // Distributed facet_dates\n      doDistribDates(fi, facet_counts);\n\n      // Distributed facet_ranges\n      doDistribRanges(fi, facet_counts);\n\n      // Distributed facet_intervals\n      doDistribIntervals(fi, facet_counts);\n      \n      // Distributed facet_pivots - this is just the per shard collection,\n      // refinement reqs still needed (below) once we've considered every shard\n      doDistribPivots(rb, shardNum, facet_counts);\n\n    } // end for-each-response-in-shard-request...\n    \n    // refine each pivot based on the new shard data\n    for (Entry<String,PivotFacet> pivotFacet : fi.pivotFacets) {\n      pivotFacet.getValue().queuePivotRefinementRequests();\n    }\n    \n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n    for (DistribFieldFacet dff : fi.facets.values()) {\n      // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n      \n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n      \n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n      \n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, \n                          dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop - 1].count;\n      \n      for (int i = 0; i < counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n        \n        if (i < ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n          \n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc, shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n        \n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null && \n                (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) && \n                dff.maxPossible(sfc, shardNum) > 0) {\n\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"59d82b0be40ecfcc2c94c776b324e0903a62b844","date":1423535462,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","sourceNew":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n    \n    for (ShardResponse srsp : sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList) srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      } catch (Exception ex) {\n        if (rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Unable to read facet info for shard: \" + srsp.getShard(), ex);\n      }\n      \n      // handle facet queries\n      NamedList facet_queries = (NamedList) facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i = 0; i < facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number) facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n      \n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList) facet_counts.get(\"facet_fields\");\n      \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList) facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      doDistribDates(fi, facet_counts);\n\n      // Distributed facet_ranges\n      doDistribRanges(fi, facet_counts);\n\n      // Distributed facet_intervals\n      doDistribIntervals(fi, facet_counts);\n      \n      // Distributed facet_pivots - this is just the per shard collection,\n      // refinement reqs still needed (below) once we've considered every shard\n      doDistribPivots(rb, shardNum, facet_counts);\n\n      // Distributed facet_heatmaps\n      SpatialHeatmapFacets.distribHandleResponse(fi.heatmapFacets, facet_counts);\n\n    } // end for-each-response-in-shard-request...\n    \n    // refine each pivot based on the new shard data\n    for (Entry<String,PivotFacet> pivotFacet : fi.pivotFacets) {\n      pivotFacet.getValue().queuePivotRefinementRequests();\n    }\n    \n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n    for (DistribFieldFacet dff : fi.facets.values()) {\n      // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, \n                          dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop - 1].count;\n      \n      for (int i = 0; i < counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i < ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n          \n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc, shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null &&\n                (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) &&\n                dff.maxPossible(sfc, shardNum) > 0) {\n\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n    removeFieldFacetsUnderLimits(rb);\n    removeRangeFacetsUnderLimits(rb);\n    removeQueryFacetsUnderLimits(rb);\n\n  }\n\n","sourceOld":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n    \n    for (ShardResponse srsp : sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList) srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      } catch (Exception ex) {\n        if (rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Unable to read facet info for shard: \" + srsp.getShard(), ex);\n      }\n      \n      // handle facet queries\n      NamedList facet_queries = (NamedList) facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i = 0; i < facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number) facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n      \n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList) facet_counts.get(\"facet_fields\");\n      \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList) facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      doDistribDates(fi, facet_counts);\n\n      // Distributed facet_ranges\n      doDistribRanges(fi, facet_counts);\n\n      // Distributed facet_intervals\n      doDistribIntervals(fi, facet_counts);\n      \n      // Distributed facet_pivots - this is just the per shard collection,\n      // refinement reqs still needed (below) once we've considered every shard\n      doDistribPivots(rb, shardNum, facet_counts);\n\n    } // end for-each-response-in-shard-request...\n    \n    // refine each pivot based on the new shard data\n    for (Entry<String,PivotFacet> pivotFacet : fi.pivotFacets) {\n      pivotFacet.getValue().queuePivotRefinementRequests();\n    }\n    \n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n    for (DistribFieldFacet dff : fi.facets.values()) {\n      // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, \n                          dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop - 1].count;\n      \n      for (int i = 0; i < counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i < ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n          \n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc, shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null &&\n                (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) &&\n                dff.maxPossible(sfc, shardNum) > 0) {\n\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n    removeFieldFacetsUnderLimits(rb);\n    removeRangeFacetsUnderLimits(rb);\n    removeQueryFacetsUnderLimits(rb);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fa16b8f2a7bb28ece5a9fdc471357e89de17bc57","date":1423733077,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","sourceNew":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n    \n    for (ShardResponse srsp : sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList) srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      } catch (Exception ex) {\n        if (rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Unable to read facet info for shard: \" + srsp.getShard(), ex);\n      }\n      \n      // handle facet queries\n      NamedList facet_queries = (NamedList) facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i = 0; i < facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number) facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n      \n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList) facet_counts.get(\"facet_fields\");\n      \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList) facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      doDistribDates(fi, facet_counts);\n\n      // Distributed facet_ranges\n      doDistribRanges(fi, facet_counts);\n\n      // Distributed facet_intervals\n      doDistribIntervals(fi, facet_counts);\n      \n      // Distributed facet_pivots - this is just the per shard collection,\n      // refinement reqs still needed (below) once we've considered every shard\n      doDistribPivots(rb, shardNum, facet_counts);\n\n      // Distributed facet_heatmaps\n      SpatialHeatmapFacets.distribHandleResponse(fi.heatmapFacets, facet_counts);\n\n    } // end for-each-response-in-shard-request...\n    \n    // refine each pivot based on the new shard data\n    for (Entry<String,PivotFacet> pivotFacet : fi.pivotFacets) {\n      pivotFacet.getValue().queuePivotRefinementRequests();\n    }\n    \n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n    for (DistribFieldFacet dff : fi.facets.values()) {\n      // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, \n                          dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop - 1].count;\n      \n      for (int i = 0; i < counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i < ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n          \n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null &&\n                (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) &&\n                dff.maxPossible(shardNum) > 0) {\n\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n    removeFieldFacetsUnderLimits(rb);\n    removeRangeFacetsUnderLimits(rb);\n    removeQueryFacetsUnderLimits(rb);\n\n  }\n\n","sourceOld":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n    \n    for (ShardResponse srsp : sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList) srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      } catch (Exception ex) {\n        if (rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Unable to read facet info for shard: \" + srsp.getShard(), ex);\n      }\n      \n      // handle facet queries\n      NamedList facet_queries = (NamedList) facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i = 0; i < facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number) facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n      \n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList) facet_counts.get(\"facet_fields\");\n      \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList) facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      doDistribDates(fi, facet_counts);\n\n      // Distributed facet_ranges\n      doDistribRanges(fi, facet_counts);\n\n      // Distributed facet_intervals\n      doDistribIntervals(fi, facet_counts);\n      \n      // Distributed facet_pivots - this is just the per shard collection,\n      // refinement reqs still needed (below) once we've considered every shard\n      doDistribPivots(rb, shardNum, facet_counts);\n\n      // Distributed facet_heatmaps\n      SpatialHeatmapFacets.distribHandleResponse(fi.heatmapFacets, facet_counts);\n\n    } // end for-each-response-in-shard-request...\n    \n    // refine each pivot based on the new shard data\n    for (Entry<String,PivotFacet> pivotFacet : fi.pivotFacets) {\n      pivotFacet.getValue().queuePivotRefinementRequests();\n    }\n    \n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n    for (DistribFieldFacet dff : fi.facets.values()) {\n      // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, \n                          dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop - 1].count;\n      \n      for (int i = 0; i < counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i < ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n          \n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(sfc, shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null &&\n                (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) &&\n                dff.maxPossible(sfc, shardNum) > 0) {\n\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n    removeFieldFacetsUnderLimits(rb);\n    removeRangeFacetsUnderLimits(rb);\n    removeQueryFacetsUnderLimits(rb);\n\n  }\n\n","bugFix":["92751ba9273251eab6a2e379ec42a1697a32ff96"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"22d0a81a05eba47d5e18976f17d88306b218cc22","date":1436341569,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","sourceNew":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n    \n    for (ShardResponse srsp : sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList) srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      } catch (Exception ex) {\n        if (rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Unable to read facet info for shard: \" + srsp.getShard(), ex);\n      }\n      \n      // handle facet queries\n      NamedList facet_queries = (NamedList) facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i = 0; i < facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number) facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList) facet_counts.get(\"facet_fields\");\n      \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList) facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      doDistribDates(fi, facet_counts);\n\n      // Distributed facet_ranges\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> rangesFromShard = (SimpleOrderedMap<SimpleOrderedMap<Object>>)\n          facet_counts.get(\"facet_ranges\");\n      if (rangesFromShard != null)  {\n        RangeFacetRequest.DistribRangeFacet.mergeFacetRangesFromShardResponse(fi.rangeFacets, rangesFromShard);\n      }\n\n      // Distributed facet_intervals\n      doDistribIntervals(fi, facet_counts);\n      \n      // Distributed facet_pivots - this is just the per shard collection,\n      // refinement reqs still needed (below) once we've considered every shard\n      doDistribPivots(rb, shardNum, facet_counts);\n\n      // Distributed facet_heatmaps\n      SpatialHeatmapFacets.distribHandleResponse(fi.heatmapFacets, facet_counts);\n\n    } // end for-each-response-in-shard-request...\n    \n    // refine each pivot based on the new shard data\n    for (Entry<String,PivotFacet> pivotFacet : fi.pivotFacets) {\n      pivotFacet.getValue().queuePivotRefinementRequests();\n    }\n    \n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n    for (DistribFieldFacet dff : fi.facets.values()) {\n      // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, \n                          dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop - 1].count;\n      \n      for (int i = 0; i < counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i < ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n          \n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null &&\n                (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) &&\n                dff.maxPossible(shardNum) > 0) {\n\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n    removeFieldFacetsUnderLimits(rb);\n    removeRangeFacetsUnderLimits(rb);\n    removeQueryFacetsUnderLimits(rb);\n\n  }\n\n","sourceOld":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n    \n    for (ShardResponse srsp : sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList) srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      } catch (Exception ex) {\n        if (rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Unable to read facet info for shard: \" + srsp.getShard(), ex);\n      }\n      \n      // handle facet queries\n      NamedList facet_queries = (NamedList) facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i = 0; i < facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number) facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n      \n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList) facet_counts.get(\"facet_fields\");\n      \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList) facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      doDistribDates(fi, facet_counts);\n\n      // Distributed facet_ranges\n      doDistribRanges(fi, facet_counts);\n\n      // Distributed facet_intervals\n      doDistribIntervals(fi, facet_counts);\n      \n      // Distributed facet_pivots - this is just the per shard collection,\n      // refinement reqs still needed (below) once we've considered every shard\n      doDistribPivots(rb, shardNum, facet_counts);\n\n      // Distributed facet_heatmaps\n      SpatialHeatmapFacets.distribHandleResponse(fi.heatmapFacets, facet_counts);\n\n    } // end for-each-response-in-shard-request...\n    \n    // refine each pivot based on the new shard data\n    for (Entry<String,PivotFacet> pivotFacet : fi.pivotFacets) {\n      pivotFacet.getValue().queuePivotRefinementRequests();\n    }\n    \n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n    for (DistribFieldFacet dff : fi.facets.values()) {\n      // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, \n                          dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop - 1].count;\n      \n      for (int i = 0; i < counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i < ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n          \n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null &&\n                (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) &&\n                dff.maxPossible(shardNum) > 0) {\n\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n    removeFieldFacetsUnderLimits(rb);\n    removeRangeFacetsUnderLimits(rb);\n    removeQueryFacetsUnderLimits(rb);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"15cb24c5fa70ba12290e43b3aa0feab5582863ee","date":1457033685,"type":3,"author":"Steve Rowe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","sourceNew":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n    \n    for (ShardResponse srsp : sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList) srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      } catch (Exception ex) {\n        if (rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Unable to read facet info for shard: \" + srsp.getShard(), ex);\n      }\n      \n      // handle facet queries\n      NamedList facet_queries = (NamedList) facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i = 0; i < facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number) facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList) facet_counts.get(\"facet_fields\");\n      \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList) facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_ranges\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> rangesFromShard = (SimpleOrderedMap<SimpleOrderedMap<Object>>)\n          facet_counts.get(\"facet_ranges\");\n      if (rangesFromShard != null)  {\n        RangeFacetRequest.DistribRangeFacet.mergeFacetRangesFromShardResponse(fi.rangeFacets, rangesFromShard);\n      }\n\n      // Distributed facet_intervals\n      doDistribIntervals(fi, facet_counts);\n      \n      // Distributed facet_pivots - this is just the per shard collection,\n      // refinement reqs still needed (below) once we've considered every shard\n      doDistribPivots(rb, shardNum, facet_counts);\n\n      // Distributed facet_heatmaps\n      SpatialHeatmapFacets.distribHandleResponse(fi.heatmapFacets, facet_counts);\n\n    } // end for-each-response-in-shard-request...\n    \n    // refine each pivot based on the new shard data\n    for (Entry<String,PivotFacet> pivotFacet : fi.pivotFacets) {\n      pivotFacet.getValue().queuePivotRefinementRequests();\n    }\n    \n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n    for (DistribFieldFacet dff : fi.facets.values()) {\n      // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, \n                          dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop - 1].count;\n      \n      for (int i = 0; i < counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i < ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n          \n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null &&\n                (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) &&\n                dff.maxPossible(shardNum) > 0) {\n\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n    removeFieldFacetsUnderLimits(rb);\n    removeRangeFacetsUnderLimits(rb);\n    removeQueryFacetsUnderLimits(rb);\n\n  }\n\n","sourceOld":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n    \n    for (ShardResponse srsp : sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList) srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      } catch (Exception ex) {\n        if (rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Unable to read facet info for shard: \" + srsp.getShard(), ex);\n      }\n      \n      // handle facet queries\n      NamedList facet_queries = (NamedList) facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i = 0; i < facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number) facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList) facet_counts.get(\"facet_fields\");\n      \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList) facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      doDistribDates(fi, facet_counts);\n\n      // Distributed facet_ranges\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> rangesFromShard = (SimpleOrderedMap<SimpleOrderedMap<Object>>)\n          facet_counts.get(\"facet_ranges\");\n      if (rangesFromShard != null)  {\n        RangeFacetRequest.DistribRangeFacet.mergeFacetRangesFromShardResponse(fi.rangeFacets, rangesFromShard);\n      }\n\n      // Distributed facet_intervals\n      doDistribIntervals(fi, facet_counts);\n      \n      // Distributed facet_pivots - this is just the per shard collection,\n      // refinement reqs still needed (below) once we've considered every shard\n      doDistribPivots(rb, shardNum, facet_counts);\n\n      // Distributed facet_heatmaps\n      SpatialHeatmapFacets.distribHandleResponse(fi.heatmapFacets, facet_counts);\n\n    } // end for-each-response-in-shard-request...\n    \n    // refine each pivot based on the new shard data\n    for (Entry<String,PivotFacet> pivotFacet : fi.pivotFacets) {\n      pivotFacet.getValue().queuePivotRefinementRequests();\n    }\n    \n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n    for (DistribFieldFacet dff : fi.facets.values()) {\n      // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, \n                          dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop - 1].count;\n      \n      for (int i = 0; i < counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i < ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n          \n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null &&\n                (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) &&\n                dff.maxPossible(shardNum) > 0) {\n\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n    removeFieldFacetsUnderLimits(rb);\n    removeRangeFacetsUnderLimits(rb);\n    removeQueryFacetsUnderLimits(rb);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cf1a614098b46c9c22afebd7b898ae4d1d2fc273","date":1457088850,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","sourceNew":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n    \n    for (ShardResponse srsp : sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList) srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      } catch (Exception ex) {\n        if (rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Unable to read facet info for shard: \" + srsp.getShard(), ex);\n      }\n      \n      // handle facet queries\n      NamedList facet_queries = (NamedList) facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i = 0; i < facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number) facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList) facet_counts.get(\"facet_fields\");\n      \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList) facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_ranges\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> rangesFromShard = (SimpleOrderedMap<SimpleOrderedMap<Object>>)\n          facet_counts.get(\"facet_ranges\");\n      if (rangesFromShard != null)  {\n        RangeFacetRequest.DistribRangeFacet.mergeFacetRangesFromShardResponse(fi.rangeFacets, rangesFromShard);\n      }\n\n      // Distributed facet_intervals\n      doDistribIntervals(fi, facet_counts);\n      \n      // Distributed facet_pivots - this is just the per shard collection,\n      // refinement reqs still needed (below) once we've considered every shard\n      doDistribPivots(rb, shardNum, facet_counts);\n\n      // Distributed facet_heatmaps\n      SpatialHeatmapFacets.distribHandleResponse(fi.heatmapFacets, facet_counts);\n\n    } // end for-each-response-in-shard-request...\n    \n    // refine each pivot based on the new shard data\n    for (Entry<String,PivotFacet> pivotFacet : fi.pivotFacets) {\n      pivotFacet.getValue().queuePivotRefinementRequests();\n    }\n    \n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n    for (DistribFieldFacet dff : fi.facets.values()) {\n      // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, \n                          dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop - 1].count;\n      \n      for (int i = 0; i < counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i < ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n          \n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null &&\n                (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) &&\n                dff.maxPossible(shardNum) > 0) {\n\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n    removeFieldFacetsUnderLimits(rb);\n    removeRangeFacetsUnderLimits(rb);\n    removeQueryFacetsUnderLimits(rb);\n\n  }\n\n","sourceOld":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n    \n    for (ShardResponse srsp : sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList) srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      } catch (Exception ex) {\n        if (rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Unable to read facet info for shard: \" + srsp.getShard(), ex);\n      }\n      \n      // handle facet queries\n      NamedList facet_queries = (NamedList) facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i = 0; i < facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number) facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList) facet_counts.get(\"facet_fields\");\n      \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList) facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_dates\n      doDistribDates(fi, facet_counts);\n\n      // Distributed facet_ranges\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> rangesFromShard = (SimpleOrderedMap<SimpleOrderedMap<Object>>)\n          facet_counts.get(\"facet_ranges\");\n      if (rangesFromShard != null)  {\n        RangeFacetRequest.DistribRangeFacet.mergeFacetRangesFromShardResponse(fi.rangeFacets, rangesFromShard);\n      }\n\n      // Distributed facet_intervals\n      doDistribIntervals(fi, facet_counts);\n      \n      // Distributed facet_pivots - this is just the per shard collection,\n      // refinement reqs still needed (below) once we've considered every shard\n      doDistribPivots(rb, shardNum, facet_counts);\n\n      // Distributed facet_heatmaps\n      SpatialHeatmapFacets.distribHandleResponse(fi.heatmapFacets, facet_counts);\n\n    } // end for-each-response-in-shard-request...\n    \n    // refine each pivot based on the new shard data\n    for (Entry<String,PivotFacet> pivotFacet : fi.pivotFacets) {\n      pivotFacet.getValue().queuePivotRefinementRequests();\n    }\n    \n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n    for (DistribFieldFacet dff : fi.facets.values()) {\n      // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, \n                          dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop - 1].count;\n      \n      for (int i = 0; i < counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i < ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n          \n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null &&\n                (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) &&\n                dff.maxPossible(shardNum) > 0) {\n\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n    removeFieldFacetsUnderLimits(rb);\n    removeRangeFacetsUnderLimits(rb);\n    removeQueryFacetsUnderLimits(rb);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bd22dcd3ba035a1626face7319c94be45ae07172","date":1527224634,"type":3,"author":"Steve Rowe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","sourceNew":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n    \n    for (ShardResponse srsp : sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList) srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      } catch (Exception ex) {\n        if (ShardParams.getShardsTolerantAsBool(rb.req.getParams())) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Unable to read facet info for shard: \" + srsp.getShard(), ex);\n      }\n      \n      // handle facet queries\n      NamedList facet_queries = (NamedList) facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i = 0; i < facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number) facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList) facet_counts.get(\"facet_fields\");\n      \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList) facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_ranges\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> rangesFromShard = (SimpleOrderedMap<SimpleOrderedMap<Object>>)\n          facet_counts.get(\"facet_ranges\");\n      if (rangesFromShard != null)  {\n        RangeFacetRequest.DistribRangeFacet.mergeFacetRangesFromShardResponse(fi.rangeFacets, rangesFromShard);\n      }\n\n      // Distributed facet_intervals\n      doDistribIntervals(fi, facet_counts);\n      \n      // Distributed facet_pivots - this is just the per shard collection,\n      // refinement reqs still needed (below) once we've considered every shard\n      doDistribPivots(rb, shardNum, facet_counts);\n\n      // Distributed facet_heatmaps\n      SpatialHeatmapFacets.distribHandleResponse(fi.heatmapFacets, facet_counts);\n\n    } // end for-each-response-in-shard-request...\n    \n    // refine each pivot based on the new shard data\n    for (Entry<String,PivotFacet> pivotFacet : fi.pivotFacets) {\n      pivotFacet.getValue().queuePivotRefinementRequests();\n    }\n    \n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n    for (DistribFieldFacet dff : fi.facets.values()) {\n      // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, \n                          dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop - 1].count;\n      \n      for (int i = 0; i < counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i < ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n          \n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null &&\n                (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) &&\n                dff.maxPossible(shardNum) > 0) {\n\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n    removeFieldFacetsUnderLimits(rb);\n    removeRangeFacetsUnderLimits(rb);\n    removeQueryFacetsUnderLimits(rb);\n\n  }\n\n","sourceOld":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n    \n    for (ShardResponse srsp : sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList) srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      } catch (Exception ex) {\n        if (rb.req.getParams().getBool(ShardParams.SHARDS_TOLERANT, false)) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Unable to read facet info for shard: \" + srsp.getShard(), ex);\n      }\n      \n      // handle facet queries\n      NamedList facet_queries = (NamedList) facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i = 0; i < facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number) facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList) facet_counts.get(\"facet_fields\");\n      \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList) facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_ranges\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> rangesFromShard = (SimpleOrderedMap<SimpleOrderedMap<Object>>)\n          facet_counts.get(\"facet_ranges\");\n      if (rangesFromShard != null)  {\n        RangeFacetRequest.DistribRangeFacet.mergeFacetRangesFromShardResponse(fi.rangeFacets, rangesFromShard);\n      }\n\n      // Distributed facet_intervals\n      doDistribIntervals(fi, facet_counts);\n      \n      // Distributed facet_pivots - this is just the per shard collection,\n      // refinement reqs still needed (below) once we've considered every shard\n      doDistribPivots(rb, shardNum, facet_counts);\n\n      // Distributed facet_heatmaps\n      SpatialHeatmapFacets.distribHandleResponse(fi.heatmapFacets, facet_counts);\n\n    } // end for-each-response-in-shard-request...\n    \n    // refine each pivot based on the new shard data\n    for (Entry<String,PivotFacet> pivotFacet : fi.pivotFacets) {\n      pivotFacet.getValue().queuePivotRefinementRequests();\n    }\n    \n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n    for (DistribFieldFacet dff : fi.facets.values()) {\n      // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, \n                          dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop - 1].count;\n      \n      for (int i = 0; i < counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i < ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n          \n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null &&\n                (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) &&\n                dff.maxPossible(shardNum) > 0) {\n\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n    removeFieldFacetsUnderLimits(rb);\n    removeRangeFacetsUnderLimits(rb);\n    removeQueryFacetsUnderLimits(rb);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3584d3db8b472772e3329d9d95d584b68ae997e","date":1551710517,"type":3,"author":"Mikhail Khludnev","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","sourceNew":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n    \n    for (ShardResponse srsp : sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList) srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n        if (facet_counts==null) {\n          NamedList<?> responseHeader = (NamedList<?>)srsp.getSolrResponse().getResponse().get(\"responseHeader\");\n          if (Boolean.TRUE.equals(responseHeader.getBooleanArg(SolrQueryResponse.RESPONSE_HEADER_PARTIAL_RESULTS_KEY))) {\n            continue;\n          } else {\n            log.warn(\"corrupted response on \"+srsp.getShardRequest()+\": \"+srsp.getSolrResponse());\n            throw new SolrException(ErrorCode.SERVER_ERROR,\n                \"facet_counts is absent in response from \" + srsp.getNodeName() +\n                \", but \"+SolrQueryResponse.RESPONSE_HEADER_PARTIAL_RESULTS_KEY+\" hasn't been responded\");\n          }\n        }\n      } catch (Exception ex) {\n        if (ShardParams.getShardsTolerantAsBool(rb.req.getParams())) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Unable to read facet info for shard: \" + srsp.getShard(), ex);\n      }\n      \n      // handle facet queries\n      NamedList facet_queries = (NamedList) facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i = 0; i < facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number) facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList) facet_counts.get(\"facet_fields\");\n      \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList) facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_ranges\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> rangesFromShard = (SimpleOrderedMap<SimpleOrderedMap<Object>>)\n          facet_counts.get(\"facet_ranges\");\n      if (rangesFromShard != null)  {\n        RangeFacetRequest.DistribRangeFacet.mergeFacetRangesFromShardResponse(fi.rangeFacets, rangesFromShard);\n      }\n\n      // Distributed facet_intervals\n      doDistribIntervals(fi, facet_counts);\n      \n      // Distributed facet_pivots - this is just the per shard collection,\n      // refinement reqs still needed (below) once we've considered every shard\n      doDistribPivots(rb, shardNum, facet_counts);\n\n      // Distributed facet_heatmaps\n      SpatialHeatmapFacets.distribHandleResponse(fi.heatmapFacets, facet_counts);\n\n    } // end for-each-response-in-shard-request...\n    \n    // refine each pivot based on the new shard data\n    for (Entry<String,PivotFacet> pivotFacet : fi.pivotFacets) {\n      pivotFacet.getValue().queuePivotRefinementRequests();\n    }\n    \n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n    for (DistribFieldFacet dff : fi.facets.values()) {\n      // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, \n                          dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop - 1].count;\n      \n      for (int i = 0; i < counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i < ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n          \n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null &&\n                (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) &&\n                dff.maxPossible(shardNum) > 0) {\n\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n    removeFieldFacetsUnderLimits(rb);\n    removeRangeFacetsUnderLimits(rb);\n    removeQueryFacetsUnderLimits(rb);\n\n  }\n\n","sourceOld":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n    \n    for (ShardResponse srsp : sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList) srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n      } catch (Exception ex) {\n        if (ShardParams.getShardsTolerantAsBool(rb.req.getParams())) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Unable to read facet info for shard: \" + srsp.getShard(), ex);\n      }\n      \n      // handle facet queries\n      NamedList facet_queries = (NamedList) facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i = 0; i < facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number) facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList) facet_counts.get(\"facet_fields\");\n      \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList) facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_ranges\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> rangesFromShard = (SimpleOrderedMap<SimpleOrderedMap<Object>>)\n          facet_counts.get(\"facet_ranges\");\n      if (rangesFromShard != null)  {\n        RangeFacetRequest.DistribRangeFacet.mergeFacetRangesFromShardResponse(fi.rangeFacets, rangesFromShard);\n      }\n\n      // Distributed facet_intervals\n      doDistribIntervals(fi, facet_counts);\n      \n      // Distributed facet_pivots - this is just the per shard collection,\n      // refinement reqs still needed (below) once we've considered every shard\n      doDistribPivots(rb, shardNum, facet_counts);\n\n      // Distributed facet_heatmaps\n      SpatialHeatmapFacets.distribHandleResponse(fi.heatmapFacets, facet_counts);\n\n    } // end for-each-response-in-shard-request...\n    \n    // refine each pivot based on the new shard data\n    for (Entry<String,PivotFacet> pivotFacet : fi.pivotFacets) {\n      pivotFacet.getValue().queuePivotRefinementRequests();\n    }\n    \n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n    for (DistribFieldFacet dff : fi.facets.values()) {\n      // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, \n                          dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop - 1].count;\n      \n      for (int i = 0; i < counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i < ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n          \n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null &&\n                (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) &&\n                dff.maxPossible(shardNum) > 0) {\n\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n    removeFieldFacetsUnderLimits(rb);\n    removeRangeFacetsUnderLimits(rb);\n    removeQueryFacetsUnderLimits(rb);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"575e66bd4b2349209027f6801184da7fc3cba13f","date":1587609169,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/FacetComponent#countFacets(ResponseBuilder,ShardRequest).mjava","sourceNew":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n    \n    for (ShardResponse srsp : sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList) srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n        if (facet_counts==null) {\n          NamedList<?> responseHeader = (NamedList<?>)srsp.getSolrResponse().getResponse().get(\"responseHeader\");\n          if (Boolean.TRUE.equals(responseHeader.getBooleanArg(SolrQueryResponse.RESPONSE_HEADER_PARTIAL_RESULTS_KEY))) {\n            continue;\n          } else {\n            log.warn(\"corrupted response on {} : {}\", srsp.getShardRequest(), srsp.getSolrResponse());\n            throw new SolrException(ErrorCode.SERVER_ERROR,\n                \"facet_counts is absent in response from \" + srsp.getNodeName() +\n                \", but \"+SolrQueryResponse.RESPONSE_HEADER_PARTIAL_RESULTS_KEY+\" hasn't been responded\");\n          }\n        }\n      } catch (Exception ex) {\n        if (ShardParams.getShardsTolerantAsBool(rb.req.getParams())) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Unable to read facet info for shard: \" + srsp.getShard(), ex);\n      }\n      \n      // handle facet queries\n      NamedList facet_queries = (NamedList) facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i = 0; i < facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number) facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList) facet_counts.get(\"facet_fields\");\n      \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList) facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_ranges\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> rangesFromShard = (SimpleOrderedMap<SimpleOrderedMap<Object>>)\n          facet_counts.get(\"facet_ranges\");\n      if (rangesFromShard != null)  {\n        RangeFacetRequest.DistribRangeFacet.mergeFacetRangesFromShardResponse(fi.rangeFacets, rangesFromShard);\n      }\n\n      // Distributed facet_intervals\n      doDistribIntervals(fi, facet_counts);\n      \n      // Distributed facet_pivots - this is just the per shard collection,\n      // refinement reqs still needed (below) once we've considered every shard\n      doDistribPivots(rb, shardNum, facet_counts);\n\n      // Distributed facet_heatmaps\n      SpatialHeatmapFacets.distribHandleResponse(fi.heatmapFacets, facet_counts);\n\n    } // end for-each-response-in-shard-request...\n    \n    // refine each pivot based on the new shard data\n    for (Entry<String,PivotFacet> pivotFacet : fi.pivotFacets) {\n      pivotFacet.getValue().queuePivotRefinementRequests();\n    }\n    \n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n    for (DistribFieldFacet dff : fi.facets.values()) {\n      // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, \n                          dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop - 1].count;\n      \n      for (int i = 0; i < counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i < ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n          \n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null &&\n                (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) &&\n                dff.maxPossible(shardNum) > 0) {\n\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n    removeFieldFacetsUnderLimits(rb);\n    removeRangeFacetsUnderLimits(rb);\n    removeQueryFacetsUnderLimits(rb);\n\n  }\n\n","sourceOld":"  private void countFacets(ResponseBuilder rb, ShardRequest sreq) {\n    FacetInfo fi = rb._facetInfo;\n    \n    for (ShardResponse srsp : sreq.responses) {\n      int shardNum = rb.getShardNum(srsp.getShard());\n      NamedList facet_counts = null;\n      try {\n        facet_counts = (NamedList) srsp.getSolrResponse().getResponse().get(\"facet_counts\");\n        if (facet_counts==null) {\n          NamedList<?> responseHeader = (NamedList<?>)srsp.getSolrResponse().getResponse().get(\"responseHeader\");\n          if (Boolean.TRUE.equals(responseHeader.getBooleanArg(SolrQueryResponse.RESPONSE_HEADER_PARTIAL_RESULTS_KEY))) {\n            continue;\n          } else {\n            log.warn(\"corrupted response on \"+srsp.getShardRequest()+\": \"+srsp.getSolrResponse());\n            throw new SolrException(ErrorCode.SERVER_ERROR,\n                \"facet_counts is absent in response from \" + srsp.getNodeName() +\n                \", but \"+SolrQueryResponse.RESPONSE_HEADER_PARTIAL_RESULTS_KEY+\" hasn't been responded\");\n          }\n        }\n      } catch (Exception ex) {\n        if (ShardParams.getShardsTolerantAsBool(rb.req.getParams())) {\n          continue; // looks like a shard did not return anything\n        }\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n            \"Unable to read facet info for shard: \" + srsp.getShard(), ex);\n      }\n      \n      // handle facet queries\n      NamedList facet_queries = (NamedList) facet_counts.get(\"facet_queries\");\n      if (facet_queries != null) {\n        for (int i = 0; i < facet_queries.size(); i++) {\n          String returnedKey = facet_queries.getName(i);\n          long count = ((Number) facet_queries.getVal(i)).longValue();\n          QueryFacet qf = fi.queryFacets.get(returnedKey);\n          qf.count += count;\n        }\n      }\n\n      // step through each facet.field, adding results from this shard\n      NamedList facet_fields = (NamedList) facet_counts.get(\"facet_fields\");\n      \n      if (facet_fields != null) {\n        for (DistribFieldFacet dff : fi.facets.values()) {\n          dff.add(shardNum, (NamedList) facet_fields.get(dff.getKey()), dff.initialLimit);\n        }\n      }\n\n      // Distributed facet_ranges\n      @SuppressWarnings(\"unchecked\")\n      SimpleOrderedMap<SimpleOrderedMap<Object>> rangesFromShard = (SimpleOrderedMap<SimpleOrderedMap<Object>>)\n          facet_counts.get(\"facet_ranges\");\n      if (rangesFromShard != null)  {\n        RangeFacetRequest.DistribRangeFacet.mergeFacetRangesFromShardResponse(fi.rangeFacets, rangesFromShard);\n      }\n\n      // Distributed facet_intervals\n      doDistribIntervals(fi, facet_counts);\n      \n      // Distributed facet_pivots - this is just the per shard collection,\n      // refinement reqs still needed (below) once we've considered every shard\n      doDistribPivots(rb, shardNum, facet_counts);\n\n      // Distributed facet_heatmaps\n      SpatialHeatmapFacets.distribHandleResponse(fi.heatmapFacets, facet_counts);\n\n    } // end for-each-response-in-shard-request...\n    \n    // refine each pivot based on the new shard data\n    for (Entry<String,PivotFacet> pivotFacet : fi.pivotFacets) {\n      pivotFacet.getValue().queuePivotRefinementRequests();\n    }\n    \n    //\n    // This code currently assumes that there will be only a single\n    // request ((with responses from all shards) sent out to get facets...\n    // otherwise we would need to wait until all facet responses were received.\n    //\n    for (DistribFieldFacet dff : fi.facets.values()) {\n      // no need to check these facets for refinement\n      if (dff.initialLimit <= 0 && dff.initialMincount <= 1) continue;\n\n      // only other case where index-sort doesn't need refinement is if minCount==0\n      if (dff.minCount <= 1 && dff.sort.equals(FacetParams.FACET_SORT_INDEX)) continue;\n\n      @SuppressWarnings(\"unchecked\") // generic array's are annoying\n      List<String>[] tmp = (List<String>[]) new List[rb.shards.length];\n      dff._toRefine = tmp;\n\n      ShardFacetCount[] counts = dff.getCountSorted();\n      int ntop = Math.min(counts.length, \n                          dff.limit >= 0 ? dff.offset + dff.limit : Integer.MAX_VALUE);\n      long smallestCount = counts.length == 0 ? 0 : counts[ntop - 1].count;\n      \n      for (int i = 0; i < counts.length; i++) {\n        ShardFacetCount sfc = counts[i];\n        boolean needRefinement = false;\n\n        if (i < ntop) {\n          // automatically flag the top values for refinement\n          // this should always be true for facet.sort=index\n          needRefinement = true;\n        } else {\n          // this logic should only be invoked for facet.sort=index (for now)\n          \n          // calculate the maximum value that this term may have\n          // and if it is >= smallestCount, then flag for refinement\n          long maxCount = sfc.count;\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null && (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum))) {\n              // if missing from this shard, add the max it could be\n              maxCount += dff.maxPossible(shardNum);\n            }\n          }\n          if (maxCount >= smallestCount) {\n            // TODO: on a tie, we could check the term values\n            needRefinement = true;\n          }\n        }\n\n        if (needRefinement) {\n          // add a query for each shard missing the term that needs refinement\n          for (int shardNum = 0; shardNum < rb.shards.length; shardNum++) {\n            FixedBitSet fbs = dff.counted[shardNum];\n            // fbs can be null if a shard request failed\n            if (fbs != null &&\n                (sfc.termNum >= fbs.length() || !fbs.get(sfc.termNum)) &&\n                dff.maxPossible(shardNum) > 0) {\n\n              dff.needRefinements = true;\n              List<String> lst = dff._toRefine[shardNum];\n              if (lst == null) {\n                lst = dff._toRefine[shardNum] = new ArrayList<>();\n              }\n              lst.add(sfc.name);\n            }\n          }\n        }\n      }\n    }\n    removeFieldFacetsUnderLimits(rb);\n    removeRangeFacetsUnderLimits(rb);\n    removeQueryFacetsUnderLimits(rb);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"59d82b0be40ecfcc2c94c776b324e0903a62b844":["8ff4734b6c86245e852fe8b6a286716d5e59d415"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["008ce2e6bd88e213b7888377901d5a70286479c0"],"92751ba9273251eab6a2e379ec42a1697a32ff96":["84b24f47dd3dfa4e2396cd6f819a35445b0a53fd"],"c26f00b574427b55127e869b935845554afde1fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"575e66bd4b2349209027f6801184da7fc3cba13f":["a3584d3db8b472772e3329d9d95d584b68ae997e"],"a3584d3db8b472772e3329d9d95d584b68ae997e":["bd22dcd3ba035a1626face7319c94be45ae07172"],"008ce2e6bd88e213b7888377901d5a70286479c0":["a69cf7f1b4cac5d5b1363402b565cd535f13e6a1"],"84b24f47dd3dfa4e2396cd6f819a35445b0a53fd":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"fa16b8f2a7bb28ece5a9fdc471357e89de17bc57":["59d82b0be40ecfcc2c94c776b324e0903a62b844"],"88dcbe1d36c9d50525f58425024e87783c4005ab":["2f3eb2c0361adcc3828df1543195800e225f146e"],"15cb24c5fa70ba12290e43b3aa0feab5582863ee":["22d0a81a05eba47d5e18976f17d88306b218cc22"],"a69cf7f1b4cac5d5b1363402b565cd535f13e6a1":["9742dc77a3dbf4efdfcddc9be43eea63e62e5e7a"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["2f3eb2c0361adcc3828df1543195800e225f146e","88dcbe1d36c9d50525f58425024e87783c4005ab"],"bd22dcd3ba035a1626face7319c94be45ae07172":["cf1a614098b46c9c22afebd7b898ae4d1d2fc273"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"22d0a81a05eba47d5e18976f17d88306b218cc22":["fa16b8f2a7bb28ece5a9fdc471357e89de17bc57"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cf1a614098b46c9c22afebd7b898ae4d1d2fc273":["22d0a81a05eba47d5e18976f17d88306b218cc22","15cb24c5fa70ba12290e43b3aa0feab5582863ee"],"8ff4734b6c86245e852fe8b6a286716d5e59d415":["92751ba9273251eab6a2e379ec42a1697a32ff96"],"9742dc77a3dbf4efdfcddc9be43eea63e62e5e7a":["88dcbe1d36c9d50525f58425024e87783c4005ab"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["575e66bd4b2349209027f6801184da7fc3cba13f"],"2f3eb2c0361adcc3828df1543195800e225f146e":["c26f00b574427b55127e869b935845554afde1fa"]},"commit2Childs":{"59d82b0be40ecfcc2c94c776b324e0903a62b844":["fa16b8f2a7bb28ece5a9fdc471357e89de17bc57"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["84b24f47dd3dfa4e2396cd6f819a35445b0a53fd"],"92751ba9273251eab6a2e379ec42a1697a32ff96":["8ff4734b6c86245e852fe8b6a286716d5e59d415"],"c26f00b574427b55127e869b935845554afde1fa":["2f3eb2c0361adcc3828df1543195800e225f146e"],"008ce2e6bd88e213b7888377901d5a70286479c0":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"575e66bd4b2349209027f6801184da7fc3cba13f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a3584d3db8b472772e3329d9d95d584b68ae997e":["575e66bd4b2349209027f6801184da7fc3cba13f"],"84b24f47dd3dfa4e2396cd6f819a35445b0a53fd":["92751ba9273251eab6a2e379ec42a1697a32ff96"],"fa16b8f2a7bb28ece5a9fdc471357e89de17bc57":["22d0a81a05eba47d5e18976f17d88306b218cc22"],"88dcbe1d36c9d50525f58425024e87783c4005ab":["fe33227f6805edab2036cbb80645cc4e2d1fa424","9742dc77a3dbf4efdfcddc9be43eea63e62e5e7a"],"15cb24c5fa70ba12290e43b3aa0feab5582863ee":["cf1a614098b46c9c22afebd7b898ae4d1d2fc273"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"a69cf7f1b4cac5d5b1363402b565cd535f13e6a1":["008ce2e6bd88e213b7888377901d5a70286479c0"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c26f00b574427b55127e869b935845554afde1fa","c903c3d15906a3da96b8c0c2fb704491005fdbdb","a258fbb26824fd104ed795e5d9033d2d040049ee"],"bd22dcd3ba035a1626face7319c94be45ae07172":["a3584d3db8b472772e3329d9d95d584b68ae997e"],"22d0a81a05eba47d5e18976f17d88306b218cc22":["15cb24c5fa70ba12290e43b3aa0feab5582863ee","cf1a614098b46c9c22afebd7b898ae4d1d2fc273"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"cf1a614098b46c9c22afebd7b898ae4d1d2fc273":["bd22dcd3ba035a1626face7319c94be45ae07172"],"8ff4734b6c86245e852fe8b6a286716d5e59d415":["59d82b0be40ecfcc2c94c776b324e0903a62b844"],"9742dc77a3dbf4efdfcddc9be43eea63e62e5e7a":["a69cf7f1b4cac5d5b1363402b565cd535f13e6a1"],"2f3eb2c0361adcc3828df1543195800e225f146e":["88dcbe1d36c9d50525f58425024e87783c4005ab","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["fe33227f6805edab2036cbb80645cc4e2d1fa424","a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}