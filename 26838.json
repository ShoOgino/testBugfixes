{"path":"src/java/org/apache/solr/analysis/PatternTokenizerFactory#create(Reader).mjava","commits":[{"id":"5222a966794b33e0bc95cdeb0fe615e0328f3457","date":1177539820,"type":0,"author":"Ryan McKinley","isMerge":false,"pathNew":"src/java/org/apache/solr/analysis/PatternTokenizerFactory#create(Reader).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Split the input using configured pattern\n   */\n  public TokenStream create(Reader input) {\n    try {\n      // Read the input into a single string\n      String str = IOUtils.toString( input );\n      \n      Matcher matcher = pattern.matcher( str );\n      List<Token> tokens = (group < 0 ) \n        ? split( matcher, str )\n        : group( matcher, str, group );\n        \n      final Iterator<Token> iter = tokens.iterator();\n      return new TokenStream() {\n        @Override\n        public Token next() throws IOException {\n          if( iter.hasNext() ) {\n            return iter.next();\n          }\n          return null;\n        }\n      };\n    }\n    catch( IOException ex ) {\n      throw new SolrException( 500, ex );\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c4abe53aaee39b5f2f41dd9a0b905c1ddf880996","date":1180477701,"type":3,"author":"Ryan McKinley","isMerge":false,"pathNew":"src/java/org/apache/solr/analysis/PatternTokenizerFactory#create(Reader).mjava","pathOld":"src/java/org/apache/solr/analysis/PatternTokenizerFactory#create(Reader).mjava","sourceNew":"  /**\n   * Split the input using configured pattern\n   */\n  public TokenStream create(Reader input) {\n    try {\n      // Read the input into a single string\n      String str = IOUtils.toString( input );\n      \n      Matcher matcher = pattern.matcher( str );\n      List<Token> tokens = (group < 0 ) \n        ? split( matcher, str )\n        : group( matcher, str, group );\n        \n      final Iterator<Token> iter = tokens.iterator();\n      return new TokenStream() {\n        @Override\n        public Token next() throws IOException {\n          if( iter.hasNext() ) {\n            return iter.next();\n          }\n          return null;\n        }\n      };\n    }\n    catch( IOException ex ) {\n      throw new SolrException( SolrException.ErrorCode.SERVER_ERROR, ex );\n    }\n  }\n\n","sourceOld":"  /**\n   * Split the input using configured pattern\n   */\n  public TokenStream create(Reader input) {\n    try {\n      // Read the input into a single string\n      String str = IOUtils.toString( input );\n      \n      Matcher matcher = pattern.matcher( str );\n      List<Token> tokens = (group < 0 ) \n        ? split( matcher, str )\n        : group( matcher, str, group );\n        \n      final Iterator<Token> iter = tokens.iterator();\n      return new TokenStream() {\n        @Override\n        public Token next() throws IOException {\n          if( iter.hasNext() ) {\n            return iter.next();\n          }\n          return null;\n        }\n      };\n    }\n    catch( IOException ex ) {\n      throw new SolrException( 500, ex );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"be29e0e2cef1fd569147732e48caf8538790339b","date":1250443738,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/analysis/PatternTokenizerFactory#create(Reader).mjava","pathOld":"src/java/org/apache/solr/analysis/PatternTokenizerFactory#create(Reader).mjava","sourceNew":"  /**\n   * Split the input using configured pattern\n   */\n  public TokenStream create(Reader input) {\n    try {\n      // Read the input into a single string\n      String str = IOUtils.toString( input );\n      \n      Matcher matcher = pattern.matcher( str );\n      List<Token> tokens = (group < 0 ) \n        ? split( matcher, str )\n        : group( matcher, str, group );\n        \n      final Iterator<Token> iter = tokens.iterator();\n      return new TokenStream() {\n        @Override\n        public boolean incrementToken() throws IOException {\n          return super.incrementToken();\n        }\n\n        @Override\n        public void end() throws IOException {\n          super.end();\n        }\n\n        @Override\n        public Token next(Token reusableToken) throws IOException {\n          return super.next(reusableToken);\n        }\n\n        @Override\n        public void reset() throws IOException {\n          super.reset();\n        }\n\n        @Override\n        public void close() throws IOException {\n          super.close();\n        }\n\n        @Override\n        public Token next() throws IOException {\n          if( iter.hasNext() ) {\n            return iter.next();\n          }\n          return null;\n        }\n      };\n    }\n    catch( IOException ex ) {\n      throw new SolrException( SolrException.ErrorCode.SERVER_ERROR, ex );\n    }\n  }\n\n","sourceOld":"  /**\n   * Split the input using configured pattern\n   */\n  public TokenStream create(Reader input) {\n    try {\n      // Read the input into a single string\n      String str = IOUtils.toString( input );\n      \n      Matcher matcher = pattern.matcher( str );\n      List<Token> tokens = (group < 0 ) \n        ? split( matcher, str )\n        : group( matcher, str, group );\n        \n      final Iterator<Token> iter = tokens.iterator();\n      return new TokenStream() {\n        @Override\n        public Token next() throws IOException {\n          if( iter.hasNext() ) {\n            return iter.next();\n          }\n          return null;\n        }\n      };\n    }\n    catch( IOException ex ) {\n      throw new SolrException( SolrException.ErrorCode.SERVER_ERROR, ex );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55044da960047bf158eb7323b7e38952ae067e7a","date":1251140302,"type":3,"author":"Ryan McKinley","isMerge":false,"pathNew":"src/java/org/apache/solr/analysis/PatternTokenizerFactory#create(Reader).mjava","pathOld":"src/java/org/apache/solr/analysis/PatternTokenizerFactory#create(Reader).mjava","sourceNew":"  /**\n   * Split the input using configured pattern\n   */\n  public Tokenizer create(final Reader in) {\n    try {\n      return new Tokenizer(in) {\n        {init();}\n\n        List<Token> tokens;\n        Iterator<Token> iter;\n\n        void init() throws IOException {\n          // Read the input into a single string\n          String str = IOUtils.toString( input );\n\n          Matcher matcher = pattern.matcher( str );\n          tokens = (group < 0 )\n                  ? split( matcher, str )\n                  : group( matcher, str, group );\n          iter = tokens.iterator();\n        }\n\n//        @Override\n//        public boolean incrementToken() throws IOException {\n//          return super.incrementToken();\n//        }\n\n        @Override\n        public void end() throws IOException {\n          super.end();\n        }\n\n//        @Override\n//        public Token next(Token reusableToken) throws IOException {\n//          return super.next(reusableToken);\n//        }\n\n        @Override\n        public void reset(Reader input) throws IOException {\n          super.reset(input);\n          init();\n        }\n\n        @Override\n        public Token next() throws IOException {\n          if( iter.hasNext() ) {\n            return iter.next();\n          }\n          return null;\n        }\n      };\n    }\n    catch( IOException ex ) {\n      throw new SolrException( SolrException.ErrorCode.SERVER_ERROR, ex );\n    }\n  }\n\n","sourceOld":"  /**\n   * Split the input using configured pattern\n   */\n  public TokenStream create(Reader input) {\n    try {\n      // Read the input into a single string\n      String str = IOUtils.toString( input );\n      \n      Matcher matcher = pattern.matcher( str );\n      List<Token> tokens = (group < 0 ) \n        ? split( matcher, str )\n        : group( matcher, str, group );\n        \n      final Iterator<Token> iter = tokens.iterator();\n      return new TokenStream() {\n        @Override\n        public boolean incrementToken() throws IOException {\n          return super.incrementToken();\n        }\n\n        @Override\n        public void end() throws IOException {\n          super.end();\n        }\n\n        @Override\n        public Token next(Token reusableToken) throws IOException {\n          return super.next(reusableToken);\n        }\n\n        @Override\n        public void reset() throws IOException {\n          super.reset();\n        }\n\n        @Override\n        public void close() throws IOException {\n          super.close();\n        }\n\n        @Override\n        public Token next() throws IOException {\n          if( iter.hasNext() ) {\n            return iter.next();\n          }\n          return null;\n        }\n      };\n    }\n    catch( IOException ex ) {\n      throw new SolrException( SolrException.ErrorCode.SERVER_ERROR, ex );\n    }\n  }\n\n","bugFix":null,"bugIntro":["98b77b6a0a7c8d189695f0da38c58b5008a9863c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"98b77b6a0a7c8d189695f0da38c58b5008a9863c","date":1252208742,"type":3,"author":"Koji Sekiguchi","isMerge":false,"pathNew":"src/java/org/apache/solr/analysis/PatternTokenizerFactory#create(Reader).mjava","pathOld":"src/java/org/apache/solr/analysis/PatternTokenizerFactory#create(Reader).mjava","sourceNew":"  /**\n   * Split the input using configured pattern\n   */\n  public Tokenizer create(final Reader in) {\n    try {\n      return new Tokenizer(in) {\n        {init();}\n\n        List<Token> tokens;\n        Iterator<Token> iter;\n\n        void init() throws IOException {\n          // Read the input into a single string\n          String str = IOUtils.toString( input );\n\n          Matcher matcher = pattern.matcher( str );\n          tokens = (group < 0 )\n                  ? split( matcher, str, input )\n                  : group( matcher, str, group, input );\n          iter = tokens.iterator();\n        }\n\n//        @Override\n//        public boolean incrementToken() throws IOException {\n//          return super.incrementToken();\n//        }\n\n        @Override\n        public void end() throws IOException {\n          super.end();\n        }\n\n//        @Override\n//        public Token next(Token reusableToken) throws IOException {\n//          return super.next(reusableToken);\n//        }\n\n        @Override\n        public void reset(Reader input) throws IOException {\n          super.reset(input);\n          init();\n        }\n\n        @Override\n        public Token next() throws IOException {\n          if( iter.hasNext() ) {\n            return iter.next();\n          }\n          return null;\n        }\n      };\n    }\n    catch( IOException ex ) {\n      throw new SolrException( SolrException.ErrorCode.SERVER_ERROR, ex );\n    }\n  }\n\n","sourceOld":"  /**\n   * Split the input using configured pattern\n   */\n  public Tokenizer create(final Reader in) {\n    try {\n      return new Tokenizer(in) {\n        {init();}\n\n        List<Token> tokens;\n        Iterator<Token> iter;\n\n        void init() throws IOException {\n          // Read the input into a single string\n          String str = IOUtils.toString( input );\n\n          Matcher matcher = pattern.matcher( str );\n          tokens = (group < 0 )\n                  ? split( matcher, str )\n                  : group( matcher, str, group );\n          iter = tokens.iterator();\n        }\n\n//        @Override\n//        public boolean incrementToken() throws IOException {\n//          return super.incrementToken();\n//        }\n\n        @Override\n        public void end() throws IOException {\n          super.end();\n        }\n\n//        @Override\n//        public Token next(Token reusableToken) throws IOException {\n//          return super.next(reusableToken);\n//        }\n\n        @Override\n        public void reset(Reader input) throws IOException {\n          super.reset(input);\n          init();\n        }\n\n        @Override\n        public Token next() throws IOException {\n          if( iter.hasNext() ) {\n            return iter.next();\n          }\n          return null;\n        }\n      };\n    }\n    catch( IOException ex ) {\n      throw new SolrException( SolrException.ErrorCode.SERVER_ERROR, ex );\n    }\n  }\n\n","bugFix":["55044da960047bf158eb7323b7e38952ae067e7a"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28e349236232860728fc91596fa4a1ec2c64bde6","date":1253259442,"type":3,"author":"Koji Sekiguchi","isMerge":false,"pathNew":"src/java/org/apache/solr/analysis/PatternTokenizerFactory#create(Reader).mjava","pathOld":"src/java/org/apache/solr/analysis/PatternTokenizerFactory#create(Reader).mjava","sourceNew":"  /**\n   * Split the input using configured pattern\n   */\n  public Tokenizer create(final Reader in) {\n    try {\n      return new PatternTokenizer(in, pattern, group);\n    } catch( IOException ex ) {\n      throw new SolrException( SolrException.ErrorCode.SERVER_ERROR, ex );\n    }\n  }\n\n","sourceOld":"  /**\n   * Split the input using configured pattern\n   */\n  public Tokenizer create(final Reader in) {\n    try {\n      return new Tokenizer(in) {\n        {init();}\n\n        List<Token> tokens;\n        Iterator<Token> iter;\n\n        void init() throws IOException {\n          // Read the input into a single string\n          String str = IOUtils.toString( input );\n\n          Matcher matcher = pattern.matcher( str );\n          tokens = (group < 0 )\n                  ? split( matcher, str, input )\n                  : group( matcher, str, group, input );\n          iter = tokens.iterator();\n        }\n\n//        @Override\n//        public boolean incrementToken() throws IOException {\n//          return super.incrementToken();\n//        }\n\n        @Override\n        public void end() throws IOException {\n          super.end();\n        }\n\n//        @Override\n//        public Token next(Token reusableToken) throws IOException {\n//          return super.next(reusableToken);\n//        }\n\n        @Override\n        public void reset(Reader input) throws IOException {\n          super.reset(input);\n          init();\n        }\n\n        @Override\n        public Token next() throws IOException {\n          if( iter.hasNext() ) {\n            return iter.next();\n          }\n          return null;\n        }\n      };\n    }\n    catch( IOException ex ) {\n      throw new SolrException( SolrException.ErrorCode.SERVER_ERROR, ex );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/analysis/PatternTokenizerFactory#create(Reader).mjava","pathOld":"src/java/org/apache/solr/analysis/PatternTokenizerFactory#create(Reader).mjava","sourceNew":"  /**\n   * Split the input using configured pattern\n   */\n  public Tokenizer create(final Reader in) {\n    try {\n      return new PatternTokenizer(in, pattern, group);\n    } catch( IOException ex ) {\n      throw new SolrException( SolrException.ErrorCode.SERVER_ERROR, ex );\n    }\n  }\n\n","sourceOld":"  /**\n   * Split the input using configured pattern\n   */\n  public Tokenizer create(final Reader in) {\n    try {\n      return new PatternTokenizer(in, pattern, group);\n    } catch( IOException ex ) {\n      throw new SolrException( SolrException.ErrorCode.SERVER_ERROR, ex );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"28e349236232860728fc91596fa4a1ec2c64bde6":["98b77b6a0a7c8d189695f0da38c58b5008a9863c"],"98b77b6a0a7c8d189695f0da38c58b5008a9863c":["55044da960047bf158eb7323b7e38952ae067e7a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5222a966794b33e0bc95cdeb0fe615e0328f3457":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"be29e0e2cef1fd569147732e48caf8538790339b":["c4abe53aaee39b5f2f41dd9a0b905c1ddf880996"],"ad94625fb8d088209f46650c8097196fec67f00c":["28e349236232860728fc91596fa4a1ec2c64bde6"],"c4abe53aaee39b5f2f41dd9a0b905c1ddf880996":["5222a966794b33e0bc95cdeb0fe615e0328f3457"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"55044da960047bf158eb7323b7e38952ae067e7a":["be29e0e2cef1fd569147732e48caf8538790339b"]},"commit2Childs":{"28e349236232860728fc91596fa4a1ec2c64bde6":["ad94625fb8d088209f46650c8097196fec67f00c"],"98b77b6a0a7c8d189695f0da38c58b5008a9863c":["28e349236232860728fc91596fa4a1ec2c64bde6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5222a966794b33e0bc95cdeb0fe615e0328f3457":["c4abe53aaee39b5f2f41dd9a0b905c1ddf880996"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["5222a966794b33e0bc95cdeb0fe615e0328f3457"],"be29e0e2cef1fd569147732e48caf8538790339b":["55044da960047bf158eb7323b7e38952ae067e7a"],"ad94625fb8d088209f46650c8097196fec67f00c":[],"c4abe53aaee39b5f2f41dd9a0b905c1ddf880996":["be29e0e2cef1fd569147732e48caf8538790339b"],"55044da960047bf158eb7323b7e38952ae067e7a":["98b77b6a0a7c8d189695f0da38c58b5008a9863c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ad94625fb8d088209f46650c8097196fec67f00c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"pathCommit":null}