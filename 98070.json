{"path":"modules/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer#testNonIdeographicNonLetter().mjava","commits":[{"id":"0f080986da691a3bba7b757f43ab72cdc82b57ce","date":1273069619,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer#testNonIdeographicNonLetter().mjava","pathOld":"lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer#testNonIdeographicNonLetter().mjava","sourceNew":"  /*\n   * Non-english text with nonletters (non-spacing marks,etc) is treated as C1C2 C2C3,\n   * except for words are split around non-letters.\n   */\n  public void testNonIdeographicNonLetter() throws Exception {\n    String str = \"\\u4e00 رُوبرت موير\";\n    TestToken[] out_tokens = {\n        newToken(\"\\u4e00\", 0, 1, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"ر\", 2, 3, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"وب\", 4, 6, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"بر\", 5, 7, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"رت\", 6, 8, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"مو\", 9, 11, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"وي\", 10, 12, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"ير\", 11, 13, CJKTokenizer.DOUBLE_TOKEN_TYPE)\n    };\n    checkCJKToken(str, out_tokens);\n  }\n\n","sourceOld":"  /*\n   * Non-english text with nonletters (non-spacing marks,etc) is treated as C1C2 C2C3,\n   * except for words are split around non-letters.\n   */\n  public void testNonIdeographicNonLetter() throws Exception {\n    String str = \"\\u4e00 رُوبرت موير\";\n    TestToken[] out_tokens = {\n        newToken(\"\\u4e00\", 0, 1, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"ر\", 2, 3, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"وب\", 4, 6, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"بر\", 5, 7, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"رت\", 6, 8, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"مو\", 9, 11, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"وي\", 10, 12, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"ير\", 11, 13, CJKTokenizer.DOUBLE_TOKEN_TYPE)\n    };\n    checkCJKToken(str, out_tokens);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer#testNonIdeographicNonLetter().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer#testNonIdeographicNonLetter().mjava","sourceNew":"  /*\n   * Non-english text with nonletters (non-spacing marks,etc) is treated as C1C2 C2C3,\n   * except for words are split around non-letters.\n   */\n  public void testNonIdeographicNonLetter() throws Exception {\n    String str = \"\\u4e00 رُوبرت موير\";\n    TestToken[] out_tokens = {\n        newToken(\"\\u4e00\", 0, 1, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"ر\", 2, 3, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"وب\", 4, 6, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"بر\", 5, 7, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"رت\", 6, 8, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"مو\", 9, 11, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"وي\", 10, 12, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"ير\", 11, 13, CJKTokenizer.DOUBLE_TOKEN_TYPE)\n    };\n    checkCJKToken(str, out_tokens);\n  }\n\n","sourceOld":"  /*\n   * Non-english text with nonletters (non-spacing marks,etc) is treated as C1C2 C2C3,\n   * except for words are split around non-letters.\n   */\n  public void testNonIdeographicNonLetter() throws Exception {\n    String str = \"\\u4e00 رُوبرت موير\";\n    TestToken[] out_tokens = {\n        newToken(\"\\u4e00\", 0, 1, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"ر\", 2, 3, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"وب\", 4, 6, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"بر\", 5, 7, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"رت\", 6, 8, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"مو\", 9, 11, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"وي\", 10, 12, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"ير\", 11, 13, CJKTokenizer.DOUBLE_TOKEN_TYPE)\n    };\n    checkCJKToken(str, out_tokens);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["0f080986da691a3bba7b757f43ab72cdc82b57ce"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0f080986da691a3bba7b757f43ab72cdc82b57ce"],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}