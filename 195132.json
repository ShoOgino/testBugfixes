{"path":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,Term[],int[],List[AtomicReaderContext],int).mjava","commits":[{"id":"4b3d16cba9355e2e97962eb1c441bbd0b6735c15","date":1357426290,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,Term[],int[],List[AtomicReaderContext],int).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,Term[],int[],List[AtomicReaderContext],int).mjava","sourceNew":"  private Map<Integer,String> highlightField(String field, String contents[], BreakIterator bi, Term terms[], int[] docids, List<AtomicReaderContext> leaves, int maxPassages) throws IOException {  \n    Map<Integer,String> highlights = new HashMap<Integer,String>();\n    \n    // reuse in the real sense... for docs in same segment we just advance our old enum\n    DocsAndPositionsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      AtomicReaderContext subContext = leaves.get(leaf);\n      AtomicReader r = subContext.reader();\n      Terms t = r.terms(field);\n      if (t == null) {\n        continue; // nothing to do\n      }\n      if (leaf != lastLeaf) {\n        termsEnum = t.iterator(null);\n        postings = new DocsAndPositionsEnum[terms.length];\n      }\n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      if (passages.length > 0) {\n        // otherwise a null snippet\n        highlights.put(doc, formatter.format(passages, content));\n      }\n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","sourceOld":"  private Map<Integer,String> highlightField(String field, String contents[], BreakIterator bi, Term terms[], int[] docids, List<AtomicReaderContext> leaves, int maxPassages) throws IOException {  \n    Map<Integer,String> highlights = new HashMap<Integer,String>();\n    \n    // reuse in the real sense... for docs in same segment we just advance our old enum\n    DocsAndPositionsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      AtomicReaderContext subContext = leaves.get(leaf);\n      AtomicReader r = subContext.reader();\n      Terms t = r.terms(field);\n      if (t == null) {\n        continue; // nothing to do\n      }\n      if (leaf != lastLeaf) {\n        termsEnum = t.iterator(null);\n        postings = new DocsAndPositionsEnum[terms.length];\n      }\n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      if (passages.length > 0) {\n        // otherwise a null snippet\n        highlights.put(doc, formatter.format(passages, content));\n      }\n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,Term[],int[],List[AtomicReaderContext],int).mjava","pathOld":"/dev/null","sourceNew":"  private Map<Integer,String> highlightField(String field, String contents[], BreakIterator bi, Term terms[], int[] docids, List<AtomicReaderContext> leaves, int maxPassages) throws IOException {  \n    Map<Integer,String> highlights = new HashMap<Integer,String>();\n    \n    // reuse in the real sense... for docs in same segment we just advance our old enum\n    DocsAndPositionsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      AtomicReaderContext subContext = leaves.get(leaf);\n      AtomicReader r = subContext.reader();\n      Terms t = r.terms(field);\n      if (t == null) {\n        continue; // nothing to do\n      }\n      if (leaf != lastLeaf) {\n        termsEnum = t.iterator(null);\n        postings = new DocsAndPositionsEnum[terms.length];\n      }\n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      if (passages.length > 0) {\n        // otherwise a null snippet\n        highlights.put(doc, formatter.format(passages, content));\n      }\n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"77fc0eb4b8857a9f5235049cdfe6f678a3ddae55","date":1363791725,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,Term[],int[],List[AtomicReaderContext],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,Term[],int[],List[AtomicReaderContext],int).mjava","sourceNew":"  private Map<Integer,String> highlightField(String field, String contents[], BreakIterator bi, Term terms[], int[] docids, List<AtomicReaderContext> leaves, int maxPassages) throws IOException {  \n    Map<Integer,String> highlights = new HashMap<Integer,String>();\n    \n    // reuse in the real sense... for docs in same segment we just advance our old enum\n    DocsAndPositionsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n\n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      AtomicReaderContext subContext = leaves.get(leaf);\n      AtomicReader r = subContext.reader();\n      Terms t = r.terms(field);\n      if (t == null) {\n        continue; // nothing to do\n      }\n      if (leaf != lastLeaf) {\n        termsEnum = t.iterator(null);\n        postings = new DocsAndPositionsEnum[terms.length];\n      }\n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      if (passages.length == 0) {\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n      if (passages.length > 0) {\n        // otherwise a null snippet (eg if field is missing\n        // entirely from the doc)\n        highlights.put(doc, formatter.format(passages, content));\n      }\n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","sourceOld":"  private Map<Integer,String> highlightField(String field, String contents[], BreakIterator bi, Term terms[], int[] docids, List<AtomicReaderContext> leaves, int maxPassages) throws IOException {  \n    Map<Integer,String> highlights = new HashMap<Integer,String>();\n    \n    // reuse in the real sense... for docs in same segment we just advance our old enum\n    DocsAndPositionsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      AtomicReaderContext subContext = leaves.get(leaf);\n      AtomicReader r = subContext.reader();\n      Terms t = r.terms(field);\n      if (t == null) {\n        continue; // nothing to do\n      }\n      if (leaf != lastLeaf) {\n        termsEnum = t.iterator(null);\n        postings = new DocsAndPositionsEnum[terms.length];\n      }\n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      if (passages.length > 0) {\n        // otherwise a null snippet\n        highlights.put(doc, formatter.format(passages, content));\n      }\n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a385683d8ce32386bb71e8c427cb78573debda2b","date":1363792009,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,Term[],int[],List[AtomicReaderContext],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,Term[],int[],List[AtomicReaderContext],int).mjava","sourceNew":"  private Map<Integer,String> highlightField(String field, String contents[], BreakIterator bi, Term terms[], int[] docids, List<AtomicReaderContext> leaves, int maxPassages) throws IOException {  \n    Map<Integer,String> highlights = new HashMap<Integer,String>();\n    \n    // reuse in the real sense... for docs in same segment we just advance our old enum\n    DocsAndPositionsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      AtomicReaderContext subContext = leaves.get(leaf);\n      AtomicReader r = subContext.reader();\n      Terms t = r.terms(field);\n      if (t == null) {\n        continue; // nothing to do\n      }\n      if (leaf != lastLeaf) {\n        termsEnum = t.iterator(null);\n        postings = new DocsAndPositionsEnum[terms.length];\n      }\n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      if (passages.length > 0) {\n        // otherwise a null snippet\n        highlights.put(doc, formatter.format(passages, content));\n      }\n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","sourceOld":"  private Map<Integer,String> highlightField(String field, String contents[], BreakIterator bi, Term terms[], int[] docids, List<AtomicReaderContext> leaves, int maxPassages) throws IOException {  \n    Map<Integer,String> highlights = new HashMap<Integer,String>();\n    \n    // reuse in the real sense... for docs in same segment we just advance our old enum\n    DocsAndPositionsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n\n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      AtomicReaderContext subContext = leaves.get(leaf);\n      AtomicReader r = subContext.reader();\n      Terms t = r.terms(field);\n      if (t == null) {\n        continue; // nothing to do\n      }\n      if (leaf != lastLeaf) {\n        termsEnum = t.iterator(null);\n        postings = new DocsAndPositionsEnum[terms.length];\n      }\n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      if (passages.length == 0) {\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n      if (passages.length > 0) {\n        // otherwise a null snippet (eg if field is missing\n        // entirely from the doc)\n        highlights.put(doc, formatter.format(passages, content));\n      }\n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4ca2025fc6d81ec43c276473ba49e4fbcb15ccb1","date":1363793774,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,Term[],int[],List[AtomicReaderContext],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,Term[],int[],List[AtomicReaderContext],int).mjava","sourceNew":"  private Map<Integer,String> highlightField(String field, String contents[], BreakIterator bi, Term terms[], int[] docids, List<AtomicReaderContext> leaves, int maxPassages) throws IOException {  \n    Map<Integer,String> highlights = new HashMap<Integer,String>();\n    \n    // reuse in the real sense... for docs in same segment we just advance our old enum\n    DocsAndPositionsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n\n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      AtomicReaderContext subContext = leaves.get(leaf);\n      AtomicReader r = subContext.reader();\n      Terms t = r.terms(field);\n      if (t == null) {\n        continue; // nothing to do\n      }\n      if (leaf != lastLeaf) {\n        termsEnum = t.iterator(null);\n        postings = new DocsAndPositionsEnum[terms.length];\n      }\n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      if (passages.length == 0) {\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n      if (passages.length > 0) {\n        // otherwise a null snippet (eg if field is missing\n        // entirely from the doc)\n        highlights.put(doc, formatter.format(passages, content));\n      }\n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","sourceOld":"  private Map<Integer,String> highlightField(String field, String contents[], BreakIterator bi, Term terms[], int[] docids, List<AtomicReaderContext> leaves, int maxPassages) throws IOException {  \n    Map<Integer,String> highlights = new HashMap<Integer,String>();\n    \n    // reuse in the real sense... for docs in same segment we just advance our old enum\n    DocsAndPositionsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n    \n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      AtomicReaderContext subContext = leaves.get(leaf);\n      AtomicReader r = subContext.reader();\n      Terms t = r.terms(field);\n      if (t == null) {\n        continue; // nothing to do\n      }\n      if (leaf != lastLeaf) {\n        termsEnum = t.iterator(null);\n        postings = new DocsAndPositionsEnum[terms.length];\n      }\n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      if (passages.length > 0) {\n        // otherwise a null snippet\n        highlights.put(doc, formatter.format(passages, content));\n      }\n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4831dd345148fcd7c33877b449ade21fc45459d8","date":1363963811,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,BytesRef[],int[],List[AtomicReaderContext],int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter#highlightField(String,String[],BreakIterator,Term[],int[],List[AtomicReaderContext],int).mjava","sourceNew":"  private Map<Integer,String> highlightField(String field, String contents[], BreakIterator bi, BytesRef terms[], int[] docids, List<AtomicReaderContext> leaves, int maxPassages) throws IOException {  \n    Map<Integer,String> highlights = new HashMap<Integer,String>();\n    \n    // reuse in the real sense... for docs in same segment we just advance our old enum\n    DocsAndPositionsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n\n    PassageFormatter fieldFormatter = getFormatter(field);\n    if (fieldFormatter == null) {\n      throw new NullPointerException(\"PassageFormatter cannot be null\");\n    }\n\n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      AtomicReaderContext subContext = leaves.get(leaf);\n      AtomicReader r = subContext.reader();\n      Terms t = r.terms(field);\n      if (t == null) {\n        continue; // nothing to do\n      }\n      if (leaf != lastLeaf) {\n        termsEnum = t.iterator(null);\n        postings = new DocsAndPositionsEnum[terms.length];\n      }\n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      if (passages.length == 0) {\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n      if (passages.length > 0) {\n        // otherwise a null snippet (eg if field is missing\n        // entirely from the doc)\n        highlights.put(doc, fieldFormatter.format(passages, content));\n      }\n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","sourceOld":"  private Map<Integer,String> highlightField(String field, String contents[], BreakIterator bi, Term terms[], int[] docids, List<AtomicReaderContext> leaves, int maxPassages) throws IOException {  \n    Map<Integer,String> highlights = new HashMap<Integer,String>();\n    \n    // reuse in the real sense... for docs in same segment we just advance our old enum\n    DocsAndPositionsEnum postings[] = null;\n    TermsEnum termsEnum = null;\n    int lastLeaf = -1;\n\n    for (int i = 0; i < docids.length; i++) {\n      String content = contents[i];\n      if (content.length() == 0) {\n        continue; // nothing to do\n      }\n      bi.setText(content);\n      int doc = docids[i];\n      int leaf = ReaderUtil.subIndex(doc, leaves);\n      AtomicReaderContext subContext = leaves.get(leaf);\n      AtomicReader r = subContext.reader();\n      Terms t = r.terms(field);\n      if (t == null) {\n        continue; // nothing to do\n      }\n      if (leaf != lastLeaf) {\n        termsEnum = t.iterator(null);\n        postings = new DocsAndPositionsEnum[terms.length];\n      }\n      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);\n      if (passages.length == 0) {\n        passages = getEmptyHighlight(field, bi, maxPassages);\n      }\n      if (passages.length > 0) {\n        // otherwise a null snippet (eg if field is missing\n        // entirely from the doc)\n        highlights.put(doc, formatter.format(passages, content));\n      }\n      lastLeaf = leaf;\n    }\n    \n    return highlights;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","4b3d16cba9355e2e97962eb1c441bbd0b6735c15"],"4b3d16cba9355e2e97962eb1c441bbd0b6735c15":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a385683d8ce32386bb71e8c427cb78573debda2b":["77fc0eb4b8857a9f5235049cdfe6f678a3ddae55"],"4831dd345148fcd7c33877b449ade21fc45459d8":["4ca2025fc6d81ec43c276473ba49e4fbcb15ccb1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"77fc0eb4b8857a9f5235049cdfe6f678a3ddae55":["4b3d16cba9355e2e97962eb1c441bbd0b6735c15"],"4ca2025fc6d81ec43c276473ba49e4fbcb15ccb1":["a385683d8ce32386bb71e8c427cb78573debda2b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4831dd345148fcd7c33877b449ade21fc45459d8"]},"commit2Childs":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"4b3d16cba9355e2e97962eb1c441bbd0b6735c15":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","77fc0eb4b8857a9f5235049cdfe6f678a3ddae55"],"a385683d8ce32386bb71e8c427cb78573debda2b":["4ca2025fc6d81ec43c276473ba49e4fbcb15ccb1"],"4831dd345148fcd7c33877b449ade21fc45459d8":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","4b3d16cba9355e2e97962eb1c441bbd0b6735c15"],"77fc0eb4b8857a9f5235049cdfe6f678a3ddae55":["a385683d8ce32386bb71e8c427cb78573debda2b"],"4ca2025fc6d81ec43c276473ba49e4fbcb15ccb1":["4831dd345148fcd7c33877b449ade21fc45459d8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}