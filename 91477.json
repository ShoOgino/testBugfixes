{"path":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    assert !postingsCompacted;\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n\n    // Get the text of this term.\n    final char[] tokenText = termAtt.termBuffer();\n    final int tokenTextLen = termAtt.termLength();\n\n    // Compute hashcode & replace any invalid UTF16 sequences\n    int downto = tokenTextLen;\n    int code = 0;\n    while (downto > 0) {\n      char ch = tokenText[--downto];\n\n      if (ch >= UnicodeUtil.UNI_SUR_LOW_START && ch <= UnicodeUtil.UNI_SUR_LOW_END) {\n        if (0 == downto) {\n          // Unpaired\n          ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n        } else {\n          final char ch2 = tokenText[downto-1];\n          if (ch2 >= UnicodeUtil.UNI_SUR_HIGH_START && ch2 <= UnicodeUtil.UNI_SUR_HIGH_END) {\n            // OK: high followed by low.  This is a valid\n            // surrogate pair.\n            code = ((code*31) + ch)*31+ch2;\n            downto--;\n            continue;\n          } else {\n            // Unpaired\n            ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n          }            \n        }\n      } else if (ch >= UnicodeUtil.UNI_SUR_HIGH_START && (ch <= UnicodeUtil.UNI_SUR_HIGH_END ||\n                                                          ch == 0xffff)) {\n        // Unpaired or 0xffff\n        ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n      }\n\n      code = (code*31) + ch;\n    }\n\n    int hashPos = code & postingsHashMask;\n\n    // Locate RawPostingList in hash\n    p = postingsHash[hashPos];\n\n    if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        p = postingsHash[hashPos];\n      } while (p != null && !postingEquals(tokenText, tokenTextLen));\n    }\n\n    if (p == null) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      final int textLen1 = 1+tokenTextLen;\n      if (textLen1 + charPool.charUpto > DocumentsWriter.CHAR_BLOCK_SIZE) {\n        if (textLen1 > DocumentsWriter.CHAR_BLOCK_SIZE) {\n          // Just skip this term, to remain as robust as\n          // possible during indexing.  A TokenFilter\n          // can be inserted into the analyzer chain if\n          // other behavior is wanted (pruning the term\n          // to a prefix, throwing an exception, etc).\n\n          if (docState.maxTermPrefix == null)\n            docState.maxTermPrefix = new String(tokenText, 0, 30);\n\n          consumer.skippingLongTerm();\n          return;\n        }\n        charPool.nextBuffer();\n      }\n\n      // Refill?\n      if (0 == perThread.freePostingsCount)\n        perThread.morePostings();\n\n      // Pull next free RawPostingList from free list\n      p = perThread.freePostings[--perThread.freePostingsCount];\n      assert p != null;\n\n      final char[] text = charPool.buffer;\n      final int textUpto = charPool.charUpto;\n      p.textStart = textUpto + charPool.charOffset;\n      charPool.charUpto += textLen1;\n      System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n      text[textUpto+tokenTextLen] = 0xffff;\n          \n      assert postingsHash[hashPos] == null;\n      postingsHash[hashPos] = p;\n      numPostings++;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      p.intStart = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      p.byteStart = intUptos[intUptoStart];\n\n      consumer.newTerm(p);\n\n    } else {\n      intUptos = intPool.buffers[p.intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = p.intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(p);\n    }\n\n    if (doNextCall)\n      nextPerField.add(p.textStart);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    assert !postingsCompacted;\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n\n    // Get the text of this term.\n    final char[] tokenText = termAtt.termBuffer();\n    final int tokenTextLen = termAtt.termLength();\n\n    // Compute hashcode & replace any invalid UTF16 sequences\n    int downto = tokenTextLen;\n    int code = 0;\n    while (downto > 0) {\n      char ch = tokenText[--downto];\n\n      if (ch >= UnicodeUtil.UNI_SUR_LOW_START && ch <= UnicodeUtil.UNI_SUR_LOW_END) {\n        if (0 == downto) {\n          // Unpaired\n          ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n        } else {\n          final char ch2 = tokenText[downto-1];\n          if (ch2 >= UnicodeUtil.UNI_SUR_HIGH_START && ch2 <= UnicodeUtil.UNI_SUR_HIGH_END) {\n            // OK: high followed by low.  This is a valid\n            // surrogate pair.\n            code = ((code*31) + ch)*31+ch2;\n            downto--;\n            continue;\n          } else {\n            // Unpaired\n            ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n          }            \n        }\n      } else if (ch >= UnicodeUtil.UNI_SUR_HIGH_START && (ch <= UnicodeUtil.UNI_SUR_HIGH_END ||\n                                                          ch == 0xffff)) {\n        // Unpaired or 0xffff\n        ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n      }\n\n      code = (code*31) + ch;\n    }\n\n    int hashPos = code & postingsHashMask;\n\n    // Locate RawPostingList in hash\n    p = postingsHash[hashPos];\n\n    if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        p = postingsHash[hashPos];\n      } while (p != null && !postingEquals(tokenText, tokenTextLen));\n    }\n\n    if (p == null) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      final int textLen1 = 1+tokenTextLen;\n      if (textLen1 + charPool.charUpto > DocumentsWriter.CHAR_BLOCK_SIZE) {\n        if (textLen1 > DocumentsWriter.CHAR_BLOCK_SIZE) {\n          // Just skip this term, to remain as robust as\n          // possible during indexing.  A TokenFilter\n          // can be inserted into the analyzer chain if\n          // other behavior is wanted (pruning the term\n          // to a prefix, throwing an exception, etc).\n\n          if (docState.maxTermPrefix == null)\n            docState.maxTermPrefix = new String(tokenText, 0, 30);\n\n          consumer.skippingLongTerm();\n          return;\n        }\n        charPool.nextBuffer();\n      }\n\n      // Refill?\n      if (0 == perThread.freePostingsCount)\n        perThread.morePostings();\n\n      // Pull next free RawPostingList from free list\n      p = perThread.freePostings[--perThread.freePostingsCount];\n      assert p != null;\n\n      final char[] text = charPool.buffer;\n      final int textUpto = charPool.charUpto;\n      p.textStart = textUpto + charPool.charOffset;\n      charPool.charUpto += textLen1;\n      System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n      text[textUpto+tokenTextLen] = 0xffff;\n          \n      assert postingsHash[hashPos] == null;\n      postingsHash[hashPos] = p;\n      numPostings++;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      p.intStart = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      p.byteStart = intUptos[intUptoStart];\n\n      consumer.newTerm(p);\n\n    } else {\n      intUptos = intPool.buffers[p.intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = p.intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(p);\n    }\n\n    if (doNextCall)\n      nextPerField.add(p.textStart);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"741ed634ca00f7fcf06280bd2bf3f7eb9b605cc9","date":1269379515,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    assert !postingsCompacted;\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n\n    // Get the text of this term.\n    final char[] tokenText = termAtt.termBuffer();\n    final int tokenTextLen = termAtt.termLength();\n\n    // Compute hashcode & replace any invalid UTF16 sequences\n    int downto = tokenTextLen;\n    int code = 0;\n    while (downto > 0) {\n      char ch = tokenText[--downto];\n\n      if (ch >= UnicodeUtil.UNI_SUR_LOW_START && ch <= UnicodeUtil.UNI_SUR_LOW_END) {\n        if (0 == downto) {\n          // Unpaired\n          ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n        } else {\n          final char ch2 = tokenText[downto-1];\n          if (ch2 >= UnicodeUtil.UNI_SUR_HIGH_START && ch2 <= UnicodeUtil.UNI_SUR_HIGH_END) {\n            // OK: high followed by low.  This is a valid\n            // surrogate pair.\n            code = ((code*31) + ch)*31+ch2;\n            downto--;\n            continue;\n          } else {\n            // Unpaired\n            ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n          }            \n        }\n      } else if (ch >= UnicodeUtil.UNI_SUR_HIGH_START && (ch <= UnicodeUtil.UNI_SUR_HIGH_END ||\n                                                          ch == 0xffff)) {\n        // Unpaired or 0xffff\n        ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n      }\n\n      code = (code*31) + ch;\n    }\n\n    int hashPos = code & postingsHashMask;\n\n    // Locate RawPostingList in hash\n    int termID = postingsHash[hashPos];\n\n    if (termID != -1 && !postingEquals(termID, tokenText, tokenTextLen)) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        termID = postingsHash[hashPos];\n      } while (termID != -1 && !postingEquals(termID, tokenText, tokenTextLen));\n    }\n\n    if (termID == -1) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      final int textLen1 = 1+tokenTextLen;\n      if (textLen1 + charPool.charUpto > DocumentsWriter.CHAR_BLOCK_SIZE) {\n        if (textLen1 > DocumentsWriter.CHAR_BLOCK_SIZE) {\n          // Just skip this term, to remain as robust as\n          // possible during indexing.  A TokenFilter\n          // can be inserted into the analyzer chain if\n          // other behavior is wanted (pruning the term\n          // to a prefix, throwing an exception, etc).\n\n          if (docState.maxTermPrefix == null)\n            docState.maxTermPrefix = new String(tokenText, 0, 30);\n\n          consumer.skippingLongTerm();\n          return;\n        }\n        charPool.nextBuffer();\n      }\n\n      // New posting\n      termID = numPostings++;\n      if (termID >= postingsArray.textStarts.length) {\n        growParallelPostingsArray();\n      }\n      if (perThread.termsHash.trackAllocations) {\n        perThread.termsHash.docWriter.bytesUsed(bytesPerPosting);\n      }\n\n      assert termID != -1;\n\n      final char[] text = charPool.buffer;\n      final int textUpto = charPool.charUpto;\n      postingsArray.textStarts[termID] = textUpto + charPool.charOffset;\n      charPool.charUpto += textLen1;\n      System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n      text[textUpto+tokenTextLen] = 0xffff;\n          \n      assert postingsHash[hashPos] == -1;\n      postingsHash[hashPos] = termID;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    assert !postingsCompacted;\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n\n    // Get the text of this term.\n    final char[] tokenText = termAtt.termBuffer();\n    final int tokenTextLen = termAtt.termLength();\n\n    // Compute hashcode & replace any invalid UTF16 sequences\n    int downto = tokenTextLen;\n    int code = 0;\n    while (downto > 0) {\n      char ch = tokenText[--downto];\n\n      if (ch >= UnicodeUtil.UNI_SUR_LOW_START && ch <= UnicodeUtil.UNI_SUR_LOW_END) {\n        if (0 == downto) {\n          // Unpaired\n          ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n        } else {\n          final char ch2 = tokenText[downto-1];\n          if (ch2 >= UnicodeUtil.UNI_SUR_HIGH_START && ch2 <= UnicodeUtil.UNI_SUR_HIGH_END) {\n            // OK: high followed by low.  This is a valid\n            // surrogate pair.\n            code = ((code*31) + ch)*31+ch2;\n            downto--;\n            continue;\n          } else {\n            // Unpaired\n            ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n          }            \n        }\n      } else if (ch >= UnicodeUtil.UNI_SUR_HIGH_START && (ch <= UnicodeUtil.UNI_SUR_HIGH_END ||\n                                                          ch == 0xffff)) {\n        // Unpaired or 0xffff\n        ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n      }\n\n      code = (code*31) + ch;\n    }\n\n    int hashPos = code & postingsHashMask;\n\n    // Locate RawPostingList in hash\n    p = postingsHash[hashPos];\n\n    if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        p = postingsHash[hashPos];\n      } while (p != null && !postingEquals(tokenText, tokenTextLen));\n    }\n\n    if (p == null) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      final int textLen1 = 1+tokenTextLen;\n      if (textLen1 + charPool.charUpto > DocumentsWriter.CHAR_BLOCK_SIZE) {\n        if (textLen1 > DocumentsWriter.CHAR_BLOCK_SIZE) {\n          // Just skip this term, to remain as robust as\n          // possible during indexing.  A TokenFilter\n          // can be inserted into the analyzer chain if\n          // other behavior is wanted (pruning the term\n          // to a prefix, throwing an exception, etc).\n\n          if (docState.maxTermPrefix == null)\n            docState.maxTermPrefix = new String(tokenText, 0, 30);\n\n          consumer.skippingLongTerm();\n          return;\n        }\n        charPool.nextBuffer();\n      }\n\n      // Refill?\n      if (0 == perThread.freePostingsCount)\n        perThread.morePostings();\n\n      // Pull next free RawPostingList from free list\n      p = perThread.freePostings[--perThread.freePostingsCount];\n      assert p != null;\n\n      final char[] text = charPool.buffer;\n      final int textUpto = charPool.charUpto;\n      p.textStart = textUpto + charPool.charOffset;\n      charPool.charUpto += textLen1;\n      System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n      text[textUpto+tokenTextLen] = 0xffff;\n          \n      assert postingsHash[hashPos] == null;\n      postingsHash[hashPos] = p;\n      numPostings++;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      p.intStart = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      p.byteStart = intUptos[intUptoStart];\n\n      consumer.newTerm(p);\n\n    } else {\n      intUptos = intPool.buffers[p.intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = p.intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(p);\n    }\n\n    if (doNextCall)\n      nextPerField.add(p.textStart);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    assert !postingsCompacted;\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n\n    // Get the text & hash of this term.\n    int code = termAtt.toBytesRef(utf8);\n\n    int hashPos = code & postingsHashMask;\n\n    // Locate RawPostingList in hash\n    int termID = postingsHash[hashPos];\n\n    if (termID != -1 && !postingEquals(termID)) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        termID = postingsHash[hashPos];\n      } while (termID != -1 && !postingEquals(termID));\n    }\n\n    if (termID == -1) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      final int textLen2 = 2+utf8.length;\n      if (textLen2 + bytePool.byteUpto > DocumentsWriter.BYTE_BLOCK_SIZE) {\n        // Not enough room in current block\n\n        if (utf8.length > DocumentsWriter.MAX_TERM_LENGTH_UTF8) {\n          // Just skip this term, to remain as robust as\n          // possible during indexing.  A TokenFilter\n          // can be inserted into the analyzer chain if\n          // other behavior is wanted (pruning the term\n          // to a prefix, throwing an exception, etc).\n          if (docState.maxTermPrefix == null) {\n            final int saved = utf8.length;\n            try {\n              utf8.length = Math.min(30, DocumentsWriter.MAX_TERM_LENGTH_UTF8);\n              docState.maxTermPrefix = utf8.toString();\n            } finally {\n              utf8.length = saved;\n            }\n          }\n\n          consumer.skippingLongTerm();\n          return;\n        }\n        bytePool.nextBuffer();\n      }\n\n      // New posting\n      termID = numPostings++;\n      if (termID >= postingsArray.size) {\n        growParallelPostingsArray();\n      }\n\n      assert termID != -1;\n      assert postingsHash[hashPos] == -1;\n\n      postingsHash[hashPos] = termID;\n\n      final byte[] text = bytePool.buffer;\n      final int textUpto = bytePool.byteUpto;\n      postingsArray.textStarts[termID] = textUpto + bytePool.byteOffset;\n\n      // We first encode the length, followed by the UTF8\n      // bytes.  Length is encoded as vInt, but will consume\n      // 1 or 2 bytes at most (we reject too-long terms,\n      // above).\n\n      // encode length @ start of bytes\n      if (utf8.length < 128) {\n        // 1 byte to store length\n        text[textUpto] = (byte) utf8.length;\n        bytePool.byteUpto += utf8.length + 1;\n        System.arraycopy(utf8.bytes, 0, text, textUpto+1, utf8.length);\n      } else {\n        // 2 byte to store length\n        text[textUpto] = (byte) (0x80 | (utf8.length & 0x7f));\n        text[textUpto+1] = (byte) ((utf8.length>>7) & 0xff);\n        bytePool.byteUpto += utf8.length + 2;\n        System.arraycopy(utf8.bytes, 0, text, textUpto+2, utf8.length);\n      }\n\n      if (numPostings == postingsHashHalfSize) {\n        rehashPostings(2*postingsHashSize);\n        bytesUsed(2*numPostings * RamUsageEstimator.NUM_BYTES_INT);\n      }\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    assert !postingsCompacted;\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n\n    // Get the text of this term.\n    final char[] tokenText = termAtt.termBuffer();\n    final int tokenTextLen = termAtt.termLength();\n\n    // Compute hashcode & replace any invalid UTF16 sequences\n    int downto = tokenTextLen;\n    int code = 0;\n    while (downto > 0) {\n      char ch = tokenText[--downto];\n\n      if (ch >= UnicodeUtil.UNI_SUR_LOW_START && ch <= UnicodeUtil.UNI_SUR_LOW_END) {\n        if (0 == downto) {\n          // Unpaired\n          ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n        } else {\n          final char ch2 = tokenText[downto-1];\n          if (ch2 >= UnicodeUtil.UNI_SUR_HIGH_START && ch2 <= UnicodeUtil.UNI_SUR_HIGH_END) {\n            // OK: high followed by low.  This is a valid\n            // surrogate pair.\n            code = ((code*31) + ch)*31+ch2;\n            downto--;\n            continue;\n          } else {\n            // Unpaired\n            ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n          }            \n        }\n      } else if (ch >= UnicodeUtil.UNI_SUR_HIGH_START && (ch <= UnicodeUtil.UNI_SUR_HIGH_END ||\n                                                          ch == 0xffff)) {\n        // Unpaired or 0xffff\n        ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n      }\n\n      code = (code*31) + ch;\n    }\n\n    int hashPos = code & postingsHashMask;\n\n    // Locate RawPostingList in hash\n    int termID = postingsHash[hashPos];\n\n    if (termID != -1 && !postingEquals(termID, tokenText, tokenTextLen)) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        termID = postingsHash[hashPos];\n      } while (termID != -1 && !postingEquals(termID, tokenText, tokenTextLen));\n    }\n\n    if (termID == -1) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      final int textLen1 = 1+tokenTextLen;\n      if (textLen1 + charPool.charUpto > DocumentsWriter.CHAR_BLOCK_SIZE) {\n        if (textLen1 > DocumentsWriter.CHAR_BLOCK_SIZE) {\n          // Just skip this term, to remain as robust as\n          // possible during indexing.  A TokenFilter\n          // can be inserted into the analyzer chain if\n          // other behavior is wanted (pruning the term\n          // to a prefix, throwing an exception, etc).\n\n          if (docState.maxTermPrefix == null)\n            docState.maxTermPrefix = new String(tokenText, 0, 30);\n\n          consumer.skippingLongTerm();\n          return;\n        }\n        charPool.nextBuffer();\n      }\n\n      // New posting\n      termID = numPostings++;\n      if (termID >= postingsArray.textStarts.length) {\n        growParallelPostingsArray();\n      }\n      if (perThread.termsHash.trackAllocations) {\n        perThread.termsHash.docWriter.bytesUsed(bytesPerPosting);\n      }\n\n      assert termID != -1;\n\n      final char[] text = charPool.buffer;\n      final int textUpto = charPool.charUpto;\n      postingsArray.textStarts[termID] = textUpto + charPool.charOffset;\n      charPool.charUpto += textLen1;\n      System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n      text[textUpto+tokenTextLen] = 0xffff;\n          \n      assert postingsHash[hashPos] == -1;\n      postingsHash[hashPos] = termID;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6c18273ea5b3974d2f30117f46f1ae416c28f727","date":1279708040,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    assert !postingsCompacted;\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n\n    // Get the text & hash of this term.\n    int code = termAtt.toBytesRef(utf8);\n\n    int hashPos = code & postingsHashMask;\n\n    // Locate RawPostingList in hash\n    int termID = postingsHash[hashPos];\n\n    if (termID != -1 && !postingEquals(termID)) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        termID = postingsHash[hashPos];\n      } while (termID != -1 && !postingEquals(termID));\n    }\n\n    if (termID == -1) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      final int textLen2 = 2+utf8.length;\n      if (textLen2 + bytePool.byteUpto > DocumentsWriterRAMAllocator.BYTE_BLOCK_SIZE) {\n        // Not enough room in current block\n\n        if (utf8.length > DocumentsWriterRAMAllocator.MAX_TERM_LENGTH_UTF8) {\n          // Just skip this term, to remain as robust as\n          // possible during indexing.  A TokenFilter\n          // can be inserted into the analyzer chain if\n          // other behavior is wanted (pruning the term\n          // to a prefix, throwing an exception, etc).\n          if (docState.maxTermPrefix == null) {\n            final int saved = utf8.length;\n            try {\n              utf8.length = Math.min(30, DocumentsWriterRAMAllocator.MAX_TERM_LENGTH_UTF8);\n              docState.maxTermPrefix = utf8.toString();\n            } finally {\n              utf8.length = saved;\n            }\n          }\n\n          consumer.skippingLongTerm();\n          return;\n        }\n        bytePool.nextBuffer();\n      }\n\n      // New posting\n      termID = numPostings++;\n      if (termID >= postingsArray.size) {\n        growParallelPostingsArray();\n      }\n\n      assert termID != -1;\n      assert postingsHash[hashPos] == -1;\n\n      postingsHash[hashPos] = termID;\n\n      final byte[] text = bytePool.buffer;\n      final int textUpto = bytePool.byteUpto;\n      postingsArray.textStarts[termID] = textUpto + bytePool.byteOffset;\n\n      // We first encode the length, followed by the UTF8\n      // bytes.  Length is encoded as vInt, but will consume\n      // 1 or 2 bytes at most (we reject too-long terms,\n      // above).\n\n      // encode length @ start of bytes\n      if (utf8.length < 128) {\n        // 1 byte to store length\n        text[textUpto] = (byte) utf8.length;\n        bytePool.byteUpto += utf8.length + 1;\n        System.arraycopy(utf8.bytes, 0, text, textUpto+1, utf8.length);\n      } else {\n        // 2 byte to store length\n        text[textUpto] = (byte) (0x80 | (utf8.length & 0x7f));\n        text[textUpto+1] = (byte) ((utf8.length>>7) & 0xff);\n        bytePool.byteUpto += utf8.length + 2;\n        System.arraycopy(utf8.bytes, 0, text, textUpto+2, utf8.length);\n      }\n\n      if (numPostings == postingsHashHalfSize) {\n        rehashPostings(2*postingsHashSize);\n        bytesUsed(2*numPostings * RamUsageEstimator.NUM_BYTES_INT);\n      }\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterRAMAllocator.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (DocumentsWriterRAMAllocator.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterRAMAllocator.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterRAMAllocator.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    assert !postingsCompacted;\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n\n    // Get the text & hash of this term.\n    int code = termAtt.toBytesRef(utf8);\n\n    int hashPos = code & postingsHashMask;\n\n    // Locate RawPostingList in hash\n    int termID = postingsHash[hashPos];\n\n    if (termID != -1 && !postingEquals(termID)) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        termID = postingsHash[hashPos];\n      } while (termID != -1 && !postingEquals(termID));\n    }\n\n    if (termID == -1) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      final int textLen2 = 2+utf8.length;\n      if (textLen2 + bytePool.byteUpto > DocumentsWriter.BYTE_BLOCK_SIZE) {\n        // Not enough room in current block\n\n        if (utf8.length > DocumentsWriter.MAX_TERM_LENGTH_UTF8) {\n          // Just skip this term, to remain as robust as\n          // possible during indexing.  A TokenFilter\n          // can be inserted into the analyzer chain if\n          // other behavior is wanted (pruning the term\n          // to a prefix, throwing an exception, etc).\n          if (docState.maxTermPrefix == null) {\n            final int saved = utf8.length;\n            try {\n              utf8.length = Math.min(30, DocumentsWriter.MAX_TERM_LENGTH_UTF8);\n              docState.maxTermPrefix = utf8.toString();\n            } finally {\n              utf8.length = saved;\n            }\n          }\n\n          consumer.skippingLongTerm();\n          return;\n        }\n        bytePool.nextBuffer();\n      }\n\n      // New posting\n      termID = numPostings++;\n      if (termID >= postingsArray.size) {\n        growParallelPostingsArray();\n      }\n\n      assert termID != -1;\n      assert postingsHash[hashPos] == -1;\n\n      postingsHash[hashPos] = termID;\n\n      final byte[] text = bytePool.buffer;\n      final int textUpto = bytePool.byteUpto;\n      postingsArray.textStarts[termID] = textUpto + bytePool.byteOffset;\n\n      // We first encode the length, followed by the UTF8\n      // bytes.  Length is encoded as vInt, but will consume\n      // 1 or 2 bytes at most (we reject too-long terms,\n      // above).\n\n      // encode length @ start of bytes\n      if (utf8.length < 128) {\n        // 1 byte to store length\n        text[textUpto] = (byte) utf8.length;\n        bytePool.byteUpto += utf8.length + 1;\n        System.arraycopy(utf8.bytes, 0, text, textUpto+1, utf8.length);\n      } else {\n        // 2 byte to store length\n        text[textUpto] = (byte) (0x80 | (utf8.length & 0x7f));\n        text[textUpto+1] = (byte) ((utf8.length>>7) & 0xff);\n        bytePool.byteUpto += utf8.length + 2;\n        System.arraycopy(utf8.bytes, 0, text, textUpto+2, utf8.length);\n      }\n\n      if (numPostings == postingsHashHalfSize) {\n        rehashPostings(2*postingsHashSize);\n        bytesUsed(2*numPostings * RamUsageEstimator.NUM_BYTES_INT);\n      }\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5d6c52f55ea3ba9a5b1d5a6dd17f79bc7d308392","date":1286023472,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try{\n       termID = bytesHash.add(termBytesRef, termAtt.toBytesRef(termBytesRef));\n    }catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriter.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    assert !postingsCompacted;\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n\n    // Get the text & hash of this term.\n    int code = termAtt.toBytesRef(utf8);\n\n    int hashPos = code & postingsHashMask;\n\n    // Locate RawPostingList in hash\n    int termID = postingsHash[hashPos];\n\n    if (termID != -1 && !postingEquals(termID)) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        termID = postingsHash[hashPos];\n      } while (termID != -1 && !postingEquals(termID));\n    }\n\n    if (termID == -1) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      final int textLen2 = 2+utf8.length;\n      if (textLen2 + bytePool.byteUpto > DocumentsWriter.BYTE_BLOCK_SIZE) {\n        // Not enough room in current block\n\n        if (utf8.length > DocumentsWriter.MAX_TERM_LENGTH_UTF8) {\n          // Just skip this term, to remain as robust as\n          // possible during indexing.  A TokenFilter\n          // can be inserted into the analyzer chain if\n          // other behavior is wanted (pruning the term\n          // to a prefix, throwing an exception, etc).\n          if (docState.maxTermPrefix == null) {\n            final int saved = utf8.length;\n            try {\n              utf8.length = Math.min(30, DocumentsWriter.MAX_TERM_LENGTH_UTF8);\n              docState.maxTermPrefix = utf8.toString();\n            } finally {\n              utf8.length = saved;\n            }\n          }\n\n          consumer.skippingLongTerm();\n          return;\n        }\n        bytePool.nextBuffer();\n      }\n\n      // New posting\n      termID = numPostings++;\n      if (termID >= postingsArray.size) {\n        growParallelPostingsArray();\n      }\n\n      assert termID != -1;\n      assert postingsHash[hashPos] == -1;\n\n      postingsHash[hashPos] = termID;\n\n      final byte[] text = bytePool.buffer;\n      final int textUpto = bytePool.byteUpto;\n      postingsArray.textStarts[termID] = textUpto + bytePool.byteOffset;\n\n      // We first encode the length, followed by the UTF8\n      // bytes.  Length is encoded as vInt, but will consume\n      // 1 or 2 bytes at most (we reject too-long terms,\n      // above).\n\n      // encode length @ start of bytes\n      if (utf8.length < 128) {\n        // 1 byte to store length\n        text[textUpto] = (byte) utf8.length;\n        bytePool.byteUpto += utf8.length + 1;\n        System.arraycopy(utf8.bytes, 0, text, textUpto+1, utf8.length);\n      } else {\n        // 2 byte to store length\n        text[textUpto] = (byte) (0x80 | (utf8.length & 0x7f));\n        text[textUpto+1] = (byte) ((utf8.length>>7) & 0xff);\n        bytePool.byteUpto += utf8.length + 2;\n        System.arraycopy(utf8.bytes, 0, text, textUpto+2, utf8.length);\n      }\n\n      if (numPostings == postingsHashHalfSize) {\n        rehashPostings(2*postingsHashSize);\n        bytesUsed(2*numPostings * RamUsageEstimator.NUM_BYTES_INT);\n      }\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try{\n       termID = bytesHash.add(termBytesRef, termAtt.toBytesRef(termBytesRef));\n    }catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    assert !postingsCompacted;\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n\n    // Get the text & hash of this term.\n    int code = termAtt.toBytesRef(utf8);\n\n    int hashPos = code & postingsHashMask;\n\n    // Locate RawPostingList in hash\n    int termID = postingsHash[hashPos];\n\n    if (termID != -1 && !postingEquals(termID)) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        termID = postingsHash[hashPos];\n      } while (termID != -1 && !postingEquals(termID));\n    }\n\n    if (termID == -1) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      final int textLen2 = 2+utf8.length;\n      if (textLen2 + bytePool.byteUpto > DocumentsWriterRAMAllocator.BYTE_BLOCK_SIZE) {\n        // Not enough room in current block\n\n        if (utf8.length > DocumentsWriterRAMAllocator.MAX_TERM_LENGTH_UTF8) {\n          // Just skip this term, to remain as robust as\n          // possible during indexing.  A TokenFilter\n          // can be inserted into the analyzer chain if\n          // other behavior is wanted (pruning the term\n          // to a prefix, throwing an exception, etc).\n          if (docState.maxTermPrefix == null) {\n            final int saved = utf8.length;\n            try {\n              utf8.length = Math.min(30, DocumentsWriterRAMAllocator.MAX_TERM_LENGTH_UTF8);\n              docState.maxTermPrefix = utf8.toString();\n            } finally {\n              utf8.length = saved;\n            }\n          }\n\n          consumer.skippingLongTerm();\n          return;\n        }\n        bytePool.nextBuffer();\n      }\n\n      // New posting\n      termID = numPostings++;\n      if (termID >= postingsArray.size) {\n        growParallelPostingsArray();\n      }\n\n      assert termID != -1;\n      assert postingsHash[hashPos] == -1;\n\n      postingsHash[hashPos] = termID;\n\n      final byte[] text = bytePool.buffer;\n      final int textUpto = bytePool.byteUpto;\n      postingsArray.textStarts[termID] = textUpto + bytePool.byteOffset;\n\n      // We first encode the length, followed by the UTF8\n      // bytes.  Length is encoded as vInt, but will consume\n      // 1 or 2 bytes at most (we reject too-long terms,\n      // above).\n\n      // encode length @ start of bytes\n      if (utf8.length < 128) {\n        // 1 byte to store length\n        text[textUpto] = (byte) utf8.length;\n        bytePool.byteUpto += utf8.length + 1;\n        System.arraycopy(utf8.bytes, 0, text, textUpto+1, utf8.length);\n      } else {\n        // 2 byte to store length\n        text[textUpto] = (byte) (0x80 | (utf8.length & 0x7f));\n        text[textUpto+1] = (byte) ((utf8.length>>7) & 0xff);\n        bytePool.byteUpto += utf8.length + 2;\n        System.arraycopy(utf8.bytes, 0, text, textUpto+2, utf8.length);\n      }\n\n      if (numPostings == postingsHashHalfSize) {\n        rehashPostings(2*postingsHashSize);\n        bytesUsed(2*numPostings * RamUsageEstimator.NUM_BYTES_INT);\n      }\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterRAMAllocator.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (DocumentsWriterRAMAllocator.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterRAMAllocator.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterRAMAllocator.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3d07f1ae3b58102f36f3393c397d78ba4e547a4","date":1300715535,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try{\n       termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    }catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriter.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try{\n       termID = bytesHash.add(termBytesRef, termAtt.toBytesRef(termBytesRef));\n    }catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriter.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try{\n       termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    }catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriter.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try{\n       termID = bytesHash.add(termBytesRef, termAtt.toBytesRef(termBytesRef));\n    }catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriter.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0ef0193974807e4bddf5432a6b0287fe4d6c9df","date":1301476645,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try{\n       termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    }catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try{\n       termID = bytesHash.add(termBytesRef, termAtt.toBytesRef(termBytesRef));\n    }catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try{\n       termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    }catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try{\n       termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    }catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriter.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try{\n       termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    }catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try{\n       termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    }catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriter.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try{\n       termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    }catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try{\n       termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    }catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriter.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6b861c0fdfa4d005c70848c9121655e9dc704f96","date":1307129511,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try {\n      termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    } catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try{\n       termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    }catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c03daa6ddcb4768a702115ec63799cab5fff3d92","date":1307140842,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try {\n      termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    } catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try{\n       termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    }catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1e7c99bd45fa88a3d93a03fdd773053bef72268e","date":1307218088,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try {\n      termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    } catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try{\n       termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    }catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try {\n      termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    } catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try {\n      termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    } catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"6b861c0fdfa4d005c70848c9121655e9dc704f96":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"5d6c52f55ea3ba9a5b1d5a6dd17f79bc7d308392":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"6c18273ea5b3974d2f30117f46f1ae416c28f727":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["6b861c0fdfa4d005c70848c9121655e9dc704f96"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["d619839baa8ce5503e496b94a9e42ad6f079293f","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"741ed634ca00f7fcf06280bd2bf3f7eb9b605cc9":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["741ed634ca00f7fcf06280bd2bf3f7eb9b605cc9"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["b3d07f1ae3b58102f36f3393c397d78ba4e547a4","c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["6c18273ea5b3974d2f30117f46f1ae416c28f727","5d6c52f55ea3ba9a5b1d5a6dd17f79bc7d308392"],"1e7c99bd45fa88a3d93a03fdd773053bef72268e":["a3776dccca01c11e7046323cfad46a3b4a471233","6b861c0fdfa4d005c70848c9121655e9dc704f96"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["5d6c52f55ea3ba9a5b1d5a6dd17f79bc7d308392","b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"a3776dccca01c11e7046323cfad46a3b4a471233":["b3d07f1ae3b58102f36f3393c397d78ba4e547a4","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b3d07f1ae3b58102f36f3393c397d78ba4e547a4":["5d6c52f55ea3ba9a5b1d5a6dd17f79bc7d308392"],"c03daa6ddcb4768a702115ec63799cab5fff3d92":["135621f3a0670a9394eb563224a3b76cc4dddc0f","6b861c0fdfa4d005c70848c9121655e9dc704f96"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"6b861c0fdfa4d005c70848c9121655e9dc704f96":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","1e7c99bd45fa88a3d93a03fdd773053bef72268e","c03daa6ddcb4768a702115ec63799cab5fff3d92"],"5d6c52f55ea3ba9a5b1d5a6dd17f79bc7d308392":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","d619839baa8ce5503e496b94a9e42ad6f079293f","b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"6c18273ea5b3974d2f30117f46f1ae416c28f727":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["c03daa6ddcb4768a702115ec63799cab5fff3d92"],"741ed634ca00f7fcf06280bd2bf3f7eb9b605cc9":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["6b861c0fdfa4d005c70848c9121655e9dc704f96","135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["5d6c52f55ea3ba9a5b1d5a6dd17f79bc7d308392","6c18273ea5b3974d2f30117f46f1ae416c28f727"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"1e7c99bd45fa88a3d93a03fdd773053bef72268e":[],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a3776dccca01c11e7046323cfad46a3b4a471233":["1e7c99bd45fa88a3d93a03fdd773053bef72268e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"b3d07f1ae3b58102f36f3393c397d78ba4e547a4":["b3e06be49006ecac364d39d12b9c9f74882f9b9f","c0ef0193974807e4bddf5432a6b0287fe4d6c9df","d619839baa8ce5503e496b94a9e42ad6f079293f","a3776dccca01c11e7046323cfad46a3b4a471233"],"c03daa6ddcb4768a702115ec63799cab5fff3d92":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["741ed634ca00f7fcf06280bd2bf3f7eb9b605cc9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["1e7c99bd45fa88a3d93a03fdd773053bef72268e","c03daa6ddcb4768a702115ec63799cab5fff3d92","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}