{"path":"lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.Writer#finish(int).mjava","commits":[{"id":"01f60198ece724a6e96cd0b45f289cf42ff83d4f","date":1286864103,"type":0,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.Writer#finish(int).mjava","pathOld":"/dev/null","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      if (datOut == null) // no added data\n        return;\n      initIndexOut();\n      final int count = 1+hash.size();\n      idxOut.writeInt(count - 1);\n      // write index\n      final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n          PackedInts.bitsRequired(count - 1));\n      final int limit = docCount > docToID.length ? docToID.length : docCount;\n      for (int i = 0; i < limit; i++) {\n        w.add(docToID[i]);\n      }\n      // fill up remaining doc with zeros\n      for (int i = limit; i < docCount; i++) {\n        w.add(0);\n      }\n      w.finish();\n      hash.clear();\n\n      super.finish(docCount);\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2d84736c4614acce6720851cbd2c823d7cd516e4","date":1290551630,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      if (datOut == null) // no added data\n        return;\n      initIndexOut();\n      final int count = 1 + hash.size();\n      idxOut.writeInt(count - 1);\n      // write index\n      final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n          PackedInts.bitsRequired(count - 1));\n      final int limit = docCount > docToID.length ? docToID.length : docCount;\n      for (int i = 0; i < limit; i++) {\n        w.add(docToID[i]);\n      }\n      // fill up remaining doc with zeros\n      for (int i = limit; i < docCount; i++) {\n        w.add(0);\n      }\n      w.finish();\n      hash.clear();\n\n      super.finish(docCount);\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      if (datOut == null) // no added data\n        return;\n      initIndexOut();\n      final int count = 1+hash.size();\n      idxOut.writeInt(count - 1);\n      // write index\n      final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n          PackedInts.bitsRequired(count - 1));\n      final int limit = docCount > docToID.length ? docToID.length : docCount;\n      for (int i = 0; i < limit; i++) {\n        w.add(docToID[i]);\n      }\n      // fill up remaining doc with zeros\n      for (int i = limit; i < docCount; i++) {\n        w.add(0);\n      }\n      w.finish();\n      hash.clear();\n\n      super.finish(docCount);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a98a65bdb67cd0b27d18a5564d63bd3e944d3f4","date":1291128345,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      if (datOut == null) // no added data\n        return;\n      initIndexOut();\n      final int count = 1 + hash.size();\n      idxOut.writeInt(count - 1);\n      // write index\n      final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n          PackedInts.bitsRequired(count - 1));\n      final int limit = docCount > docToID.length ? docToID.length : docCount;\n      for (int i = 0; i < limit; i++) {\n        w.add(docToID[i]);\n      }\n      // fill up remaining doc with zeros\n      for (int i = limit; i < docCount; i++) {\n        w.add(0);\n      }\n      w.finish();\n      hash.close();\n      super.finish(docCount);\n      bytesUsed.addAndGet((-docToID.length)\n          * RamUsageEstimator.NUM_BYTES_INT);\n      docToID = null;\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      if (datOut == null) // no added data\n        return;\n      initIndexOut();\n      final int count = 1 + hash.size();\n      idxOut.writeInt(count - 1);\n      // write index\n      final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n          PackedInts.bitsRequired(count - 1));\n      final int limit = docCount > docToID.length ? docToID.length : docCount;\n      for (int i = 0; i < limit; i++) {\n        w.add(docToID[i]);\n      }\n      // fill up remaining doc with zeros\n      for (int i = limit; i < docCount; i++) {\n        w.add(0);\n      }\n      w.finish();\n      hash.clear();\n\n      super.finish(docCount);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      try {\n        if (size == -1) {\n          datOut.writeInt(size);\n        }\n        final int count = 1 + hash.size();\n        idxOut.writeInt(count - 1);\n        // write index\n        final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count - 1));\n        final int limit = docCount > docToID.length ? docToID.length : docCount;\n        for (int i = 0; i < limit; i++) {\n          w.add(docToID[i]);\n        }\n        // fill up remaining doc with zeros\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        hash.close();\n        super.finish(docCount);\n        bytesUsed\n            .addAndGet((-docToID.length) * RamUsageEstimator.NUM_BYTES_INT);\n        docToID = null;\n      }\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      if (datOut == null) // no added data\n        return;\n      initIndexOut();\n      final int count = 1 + hash.size();\n      idxOut.writeInt(count - 1);\n      // write index\n      final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n          PackedInts.bitsRequired(count - 1));\n      final int limit = docCount > docToID.length ? docToID.length : docCount;\n      for (int i = 0; i < limit; i++) {\n        w.add(docToID[i]);\n      }\n      // fill up remaining doc with zeros\n      for (int i = limit; i < docCount; i++) {\n        w.add(0);\n      }\n      w.finish();\n      hash.close();\n      super.finish(docCount);\n      bytesUsed.addAndGet((-docToID.length)\n          * RamUsageEstimator.NUM_BYTES_INT);\n      docToID = null;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9235b9d4454a46c066cda47fed7ca0a34e614529","date":1304414372,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      try {\n        if (size == -1) {\n          datOut.writeInt(size);\n        }\n        final int count = 1 + hash.size();\n        idxOut.writeInt(count - 1);\n        // write index\n        final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count - 1));\n        final int limit = docCount > docToID.length ? docToID.length : docCount;\n        for (int i = 0; i < limit; i++) {\n          w.add(docToID[i]);\n        }\n        // fill up remaining doc with zeros\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        hash.close();\n        super.finish(docCount);\n        bytesUsed\n            .addAndGet((-docToID.length) * RamUsageEstimator.NUM_BYTES_INT);\n        docToID = null;\n      }\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    synchronized public void finish(int docCount) throws IOException {\n      try {\n        if (size == -1) {\n          datOut.writeInt(size);\n        }\n        final int count = 1 + hash.size();\n        idxOut.writeInt(count - 1);\n        // write index\n        final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count - 1));\n        final int limit = docCount > docToID.length ? docToID.length : docCount;\n        for (int i = 0; i < limit; i++) {\n          w.add(docToID[i]);\n        }\n        // fill up remaining doc with zeros\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        hash.close();\n        super.finish(docCount);\n        bytesUsed\n            .addAndGet((-docToID.length) * RamUsageEstimator.NUM_BYTES_INT);\n        docToID = null;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e8d7ba2175f47e280231533f7d3016249cea88b","date":1307711934,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.Writer#finish(int).mjava","pathOld":"/dev/null","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      try {\n        if (size == -1) {\n          datOut.writeInt(size);\n        }\n        final int count = 1 + hash.size();\n        idxOut.writeInt(count - 1);\n        // write index\n        final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count - 1));\n        final int limit = docCount > docToID.length ? docToID.length : docCount;\n        for (int i = 0; i < limit; i++) {\n          w.add(docToID[i]);\n        }\n        // fill up remaining doc with zeros\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        hash.close();\n        super.finish(docCount);\n        bytesUsed\n            .addAndGet((-docToID.length) * RamUsageEstimator.NUM_BYTES_INT);\n        docToID = null;\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","date":1307729864,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.Writer#finish(int).mjava","pathOld":"/dev/null","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      try {\n        if (size == -1) {\n          datOut.writeInt(size);\n        }\n        final int count = 1 + hash.size();\n        idxOut.writeInt(count - 1);\n        // write index\n        final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count - 1));\n        final int limit = docCount > docToID.length ? docToID.length : docCount;\n        for (int i = 0; i < limit; i++) {\n          w.add(docToID[i]);\n        }\n        // fill up remaining doc with zeros\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        hash.close();\n        super.finish(docCount);\n        bytesUsed\n            .addAndGet((-docToID.length) * RamUsageEstimator.NUM_BYTES_INT);\n        docToID = null;\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3b7a068f550e13e49517c6899cc3b94c8eeb72e5","date":1309354772,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      boolean success = false;\n      final int numValues = hash.size();\n      final IndexOutput datOut = getDataOut();\n      try {\n        datOut.writeInt(size);\n        if (size != -1) {\n          final BytesRef bytesRef = new BytesRef(size);\n          for (int i = 0; i < numValues; i++) {\n            hash.get(i, bytesRef);\n            datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);\n          }\n        }\n        success = true;\n      } finally {\n        IOUtils.closeSafely(!success, datOut);\n        hash.close();\n      }\n      success = false;\n      final IndexOutput idxOut = getIndexOut();\n      try {\n        final int count = 1 + numValues;\n        idxOut.writeInt(count - 1);\n        // write index\n        final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count - 1));\n        final int limit = docCount > docToID.length ? docToID.length : docCount;\n        for (int i = 0; i < limit; i++) {\n          w.add(docToID[i]);\n        }\n        // fill up remaining doc with zeros\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n        success = true;\n      } finally {\n        IOUtils.closeSafely(!success, idxOut);\n        bytesUsed\n            .addAndGet((-docToID.length) * RamUsageEstimator.NUM_BYTES_INT);\n        docToID = null;\n      }\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      try {\n        if (size == -1) {\n          datOut.writeInt(size);\n        }\n        final int count = 1 + hash.size();\n        idxOut.writeInt(count - 1);\n        // write index\n        final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count - 1));\n        final int limit = docCount > docToID.length ? docToID.length : docCount;\n        for (int i = 0; i < limit; i++) {\n          w.add(docToID[i]);\n        }\n        // fill up remaining doc with zeros\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        hash.close();\n        super.finish(docCount);\n        bytesUsed\n            .addAndGet((-docToID.length) * RamUsageEstimator.NUM_BYTES_INT);\n        docToID = null;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      boolean success = false;\n      final int numValues = hash.size();\n      final IndexOutput datOut = getDataOut();\n      try {\n        datOut.writeInt(size);\n        if (size != -1) {\n          final BytesRef bytesRef = new BytesRef(size);\n          for (int i = 0; i < numValues; i++) {\n            hash.get(i, bytesRef);\n            datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);\n          }\n        }\n        success = true;\n      } finally {\n        IOUtils.closeSafely(!success, datOut);\n        hash.close();\n      }\n      success = false;\n      final IndexOutput idxOut = getIndexOut();\n      try {\n        final int count = 1 + numValues;\n        idxOut.writeInt(count - 1);\n        // write index\n        final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count - 1));\n        final int limit = docCount > docToID.length ? docToID.length : docCount;\n        for (int i = 0; i < limit; i++) {\n          w.add(docToID[i]);\n        }\n        // fill up remaining doc with zeros\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n        success = true;\n      } finally {\n        IOUtils.closeSafely(!success, idxOut);\n        bytesUsed\n            .addAndGet((-docToID.length) * RamUsageEstimator.NUM_BYTES_INT);\n        docToID = null;\n      }\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      try {\n        if (size == -1) {\n          datOut.writeInt(size);\n        }\n        final int count = 1 + hash.size();\n        idxOut.writeInt(count - 1);\n        // write index\n        final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count - 1));\n        final int limit = docCount > docToID.length ? docToID.length : docCount;\n        for (int i = 0; i < limit; i++) {\n          w.add(docToID[i]);\n        }\n        // fill up remaining doc with zeros\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        hash.close();\n        super.finish(docCount);\n        bytesUsed\n            .addAndGet((-docToID.length) * RamUsageEstimator.NUM_BYTES_INT);\n        docToID = null;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      boolean success = false;\n      final int numValues = hash.size();\n      final IndexOutput datOut = getDataOut();\n      try {\n        datOut.writeInt(size);\n        if (size != -1) {\n          final BytesRef bytesRef = new BytesRef(size);\n          for (int i = 0; i < numValues; i++) {\n            hash.get(i, bytesRef);\n            datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);\n          }\n        }\n        success = true;\n      } finally {\n        IOUtils.closeSafely(!success, datOut);\n        hash.close();\n      }\n      success = false;\n      final IndexOutput idxOut = getIndexOut();\n      try {\n        final int count = 1 + numValues;\n        idxOut.writeInt(count - 1);\n        // write index\n        final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count - 1));\n        final int limit = docCount > docToID.length ? docToID.length : docCount;\n        for (int i = 0; i < limit; i++) {\n          w.add(docToID[i]);\n        }\n        // fill up remaining doc with zeros\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n        success = true;\n      } finally {\n        IOUtils.closeSafely(!success, idxOut);\n        bytesUsed\n            .addAndGet((-docToID.length) * RamUsageEstimator.NUM_BYTES_INT);\n        docToID = null;\n      }\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      try {\n        if (size == -1) {\n          datOut.writeInt(size);\n        }\n        final int count = 1 + hash.size();\n        idxOut.writeInt(count - 1);\n        // write index\n        final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count - 1));\n        final int limit = docCount > docToID.length ? docToID.length : docCount;\n        for (int i = 0; i < limit; i++) {\n          w.add(docToID[i]);\n        }\n        // fill up remaining doc with zeros\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n      } finally {\n        hash.close();\n        super.finish(docCount);\n        bytesUsed\n            .addAndGet((-docToID.length) * RamUsageEstimator.NUM_BYTES_INT);\n        docToID = null;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24230fe54121f9be9d85f2c2067536296785e421","date":1314462346,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.Writer#finish(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.Writer#finish(int).mjava","sourceNew":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      boolean success = false;\n      final int numValues = hash.size();\n      final IndexOutput datOut = getDataOut();\n      try {\n        datOut.writeInt(size);\n        if (size != -1) {\n          final BytesRef bytesRef = new BytesRef(size);\n          for (int i = 0; i < numValues; i++) {\n            hash.get(i, bytesRef);\n            datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);\n          }\n        }\n        success = true;\n      } finally {\n        if (success) {\n          IOUtils.close(datOut);\n        } else {\n          IOUtils.closeWhileHandlingException(datOut);\n        }\n        hash.close();\n      }\n      success = false;\n      final IndexOutput idxOut = getIndexOut();\n      try {\n        final int count = 1 + numValues;\n        idxOut.writeInt(count - 1);\n        // write index\n        final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count - 1));\n        final int limit = docCount > docToID.length ? docToID.length : docCount;\n        for (int i = 0; i < limit; i++) {\n          w.add(docToID[i]);\n        }\n        // fill up remaining doc with zeros\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n        success = true;\n      } finally {\n        if (success) {\n          IOUtils.close(idxOut);\n        } else {\n          IOUtils.closeWhileHandlingException(idxOut);\n        }\n        bytesUsed\n            .addAndGet((-docToID.length) * RamUsageEstimator.NUM_BYTES_INT);\n        docToID = null;\n      }\n    }\n\n","sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      boolean success = false;\n      final int numValues = hash.size();\n      final IndexOutput datOut = getDataOut();\n      try {\n        datOut.writeInt(size);\n        if (size != -1) {\n          final BytesRef bytesRef = new BytesRef(size);\n          for (int i = 0; i < numValues; i++) {\n            hash.get(i, bytesRef);\n            datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);\n          }\n        }\n        success = true;\n      } finally {\n        IOUtils.closeSafely(!success, datOut);\n        hash.close();\n      }\n      success = false;\n      final IndexOutput idxOut = getIndexOut();\n      try {\n        final int count = 1 + numValues;\n        idxOut.writeInt(count - 1);\n        // write index\n        final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count - 1));\n        final int limit = docCount > docToID.length ? docToID.length : docCount;\n        for (int i = 0; i < limit; i++) {\n          w.add(docToID[i]);\n        }\n        // fill up remaining doc with zeros\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n        success = true;\n      } finally {\n        IOUtils.closeSafely(!success, idxOut);\n        bytesUsed\n            .addAndGet((-docToID.length) * RamUsageEstimator.NUM_BYTES_INT);\n        docToID = null;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"85eb75e0c0203e44dcf686f35876cf6080f3a671","date":1317221550,"type":4,"author":"Simon Willnauer","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.Writer#finish(int).mjava","sourceNew":null,"sourceOld":"    // Important that we get docCount, in case there were\n    // some last docs that we didn't see\n    @Override\n    public void finish(int docCount) throws IOException {\n      boolean success = false;\n      final int numValues = hash.size();\n      final IndexOutput datOut = getDataOut();\n      try {\n        datOut.writeInt(size);\n        if (size != -1) {\n          final BytesRef bytesRef = new BytesRef(size);\n          for (int i = 0; i < numValues; i++) {\n            hash.get(i, bytesRef);\n            datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);\n          }\n        }\n        success = true;\n      } finally {\n        if (success) {\n          IOUtils.close(datOut);\n        } else {\n          IOUtils.closeWhileHandlingException(datOut);\n        }\n        hash.close();\n      }\n      success = false;\n      final IndexOutput idxOut = getIndexOut();\n      try {\n        final int count = 1 + numValues;\n        idxOut.writeInt(count - 1);\n        // write index\n        final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,\n            PackedInts.bitsRequired(count - 1));\n        final int limit = docCount > docToID.length ? docToID.length : docCount;\n        for (int i = 0; i < limit; i++) {\n          w.add(docToID[i]);\n        }\n        // fill up remaining doc with zeros\n        for (int i = limit; i < docCount; i++) {\n          w.add(0);\n        }\n        w.finish();\n        success = true;\n      } finally {\n        if (success) {\n          IOUtils.close(idxOut);\n        } else {\n          IOUtils.closeWhileHandlingException(idxOut);\n        }\n        bytesUsed\n            .addAndGet((-docToID.length) * RamUsageEstimator.NUM_BYTES_INT);\n        docToID = null;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"24230fe54121f9be9d85f2c2067536296785e421":["3b7a068f550e13e49517c6899cc3b94c8eeb72e5"],"5a98a65bdb67cd0b27d18a5564d63bd3e944d3f4":["2d84736c4614acce6720851cbd2c823d7cd516e4"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2e8d7ba2175f47e280231533f7d3016249cea88b"],"3b7a068f550e13e49517c6899cc3b94c8eeb72e5":["2e8d7ba2175f47e280231533f7d3016249cea88b"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["5a98a65bdb67cd0b27d18a5564d63bd3e944d3f4","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"01f60198ece724a6e96cd0b45f289cf42ff83d4f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["2e8d7ba2175f47e280231533f7d3016249cea88b","3b7a068f550e13e49517c6899cc3b94c8eeb72e5"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","3b7a068f550e13e49517c6899cc3b94c8eeb72e5"],"9235b9d4454a46c066cda47fed7ca0a34e614529":["ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"2d84736c4614acce6720851cbd2c823d7cd516e4":["01f60198ece724a6e96cd0b45f289cf42ff83d4f"],"2e8d7ba2175f47e280231533f7d3016249cea88b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","9235b9d4454a46c066cda47fed7ca0a34e614529"],"85eb75e0c0203e44dcf686f35876cf6080f3a671":["24230fe54121f9be9d85f2c2067536296785e421"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["85eb75e0c0203e44dcf686f35876cf6080f3a671"]},"commit2Childs":{"24230fe54121f9be9d85f2c2067536296785e421":["85eb75e0c0203e44dcf686f35876cf6080f3a671"],"5a98a65bdb67cd0b27d18a5564d63bd3e944d3f4":["ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["817d8435e9135b756f08ce6710ab0baac51bdf88"],"3b7a068f550e13e49517c6899cc3b94c8eeb72e5":["24230fe54121f9be9d85f2c2067536296785e421","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["9235b9d4454a46c066cda47fed7ca0a34e614529"],"01f60198ece724a6e96cd0b45f289cf42ff83d4f":["2d84736c4614acce6720851cbd2c823d7cd516e4"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"817d8435e9135b756f08ce6710ab0baac51bdf88":[],"2d84736c4614acce6720851cbd2c823d7cd516e4":["5a98a65bdb67cd0b27d18a5564d63bd3e944d3f4"],"9235b9d4454a46c066cda47fed7ca0a34e614529":["2e8d7ba2175f47e280231533f7d3016249cea88b"],"2e8d7ba2175f47e280231533f7d3016249cea88b":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","3b7a068f550e13e49517c6899cc3b94c8eeb72e5","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","ab5cb6a74aefb78aa0569857970b9151dfe2e787","01f60198ece724a6e96cd0b45f289cf42ff83d4f","2e8d7ba2175f47e280231533f7d3016249cea88b"],"85eb75e0c0203e44dcf686f35876cf6080f3a671":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}