{"path":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","commits":[{"id":"683d3f90dda2bbb999c3ce855706d74563a53680","date":1285654576,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getDeletedDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getDeletedDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n    StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getDeletedDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","date":1309960478,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getDeletedDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getDeletedDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getDeletedDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9f2f1c6050eb49fa5cb22fbdf977c76e65ea3caf","date":1323050351,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3615ce4a1f785ae1b779244de52c6a7d99227e60","date":1323422019,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","date":1323437438,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31f025ae60076ae95274433f3fe8e6ace2857a87","date":1326669465,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"),\n                                                                false);\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f08557cdb6c60ac7b88a9342c983a20cd236e74f","date":1330954480,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"),\n                                                                false);\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"),\n                                                                false);\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","date":1331075828,"type":3,"author":"Ryan McKinley","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"),\n                                                                false);\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"),\n                                                                false);\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"),\n                                                                false);\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new TextField(\"content\", contents));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new TextField(\"content\", \"abc bbb ccc\"));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getLiveDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"),\n                                                                false);\n    assertTrue(tps.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new TextField(\"content\", bigTerm));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"683d3f90dda2bbb999c3ce855706d74563a53680":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":["31f025ae60076ae95274433f3fe8e6ace2857a87","f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":["1509f151d7692d84fae414b2b799ac06ba60fcb4","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"f08557cdb6c60ac7b88a9342c983a20cd236e74f":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["683d3f90dda2bbb999c3ce855706d74563a53680"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","683d3f90dda2bbb999c3ce855706d74563a53680"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["683d3f90dda2bbb999c3ce855706d74563a53680","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["683d3f90dda2bbb999c3ce855706d74563a53680","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"9f2f1c6050eb49fa5cb22fbdf977c76e65ea3caf":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["1509f151d7692d84fae414b2b799ac06ba60fcb4","9f2f1c6050eb49fa5cb22fbdf977c76e65ea3caf"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["3615ce4a1f785ae1b779244de52c6a7d99227e60"]},"commit2Childs":{"683d3f90dda2bbb999c3ce855706d74563a53680":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":[],"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":[],"f08557cdb6c60ac7b88a9342c983a20cd236e74f":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","b89678825b68eccaf09e6ab71675fc0b0af1e099"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","1509f151d7692d84fae414b2b799ac06ba60fcb4"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":[],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"817d8435e9135b756f08ce6710ab0baac51bdf88":[],"9f2f1c6050eb49fa5cb22fbdf977c76e65ea3caf":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["683d3f90dda2bbb999c3ce855706d74563a53680","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","31f025ae60076ae95274433f3fe8e6ace2857a87"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","9f2f1c6050eb49fa5cb22fbdf977c76e65ea3caf","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}