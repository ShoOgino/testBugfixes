{"path":"lucene/core/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","commits":[{"id":"fb0345a2d45479f891041f8b3ce351bc975e64ac","date":1462708700,"type":1,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    // nocommit:\n    iwc.setCodec(Codec.forName(\"SimpleText\"));\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","sourceOld":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    // nocommit:\n    iwc.setCodec(Codec.forName(\"SimpleText\"));\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5e03940e6e9044943de4b7ac08f8581da37a9534","date":1462870173,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","sourceOld":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    // nocommit:\n    iwc.setCodec(Codec.forName(\"SimpleText\"));\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3d33e731a93d4b57e662ff094f64f94a745422d4","date":1463128289,"type":1,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","sourceOld":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    mergePolicy = TestSortingMergePolicy.newSortingMergePolicy(sort);\n    iwc.setMergePolicy(mergePolicy);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ad30c6a479e764150a3316e57263319775f1df2","date":1463395403,"type":1,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","sourceOld":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    mergePolicy = TestSortingMergePolicy.newSortingMergePolicy(sort);\n    iwc.setMergePolicy(mergePolicy);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":1,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","sourceOld":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    mergePolicy = TestSortingMergePolicy.newSortingMergePolicy(sort);\n    iwc.setMergePolicy(mergePolicy);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","pathOld":"/dev/null","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5de502b5478255493125e7e801411ba17a6682ec","date":1490974101,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(FORCE_MERGE_MAX_SEGMENT_COUNT);\n    }\n    reader = iw.getReader();\n  }\n\n","sourceOld":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6f20fd35e3055a0c5b387df0b986a68d65d86441","date":1491045405,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(FORCE_MERGE_MAX_SEGMENT_COUNT);\n    }\n    reader = iw.getReader();\n  }\n\n","sourceOld":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(forceMergeMaxSegmentCount);\n    }\n    reader = iw.getReader();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"404d1ab7f6f396235047017c88d545fec15dafb7","date":1511975378,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTopFieldCollectorEarlyTermination#createRandomIndex(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(FORCE_MERGE_MAX_SEGMENT_COUNT);\n    }\n    reader = iw.getReader();\n  }\n\n","sourceOld":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(FORCE_MERGE_MAX_SEGMENT_COUNT);\n    }\n    reader = iw.getReader();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1aad05eeff7818b0833c02ac6b743aa72054963b","date":1512093122,"type":5,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTopFieldCollectorEarlyTermination#createRandomIndex(boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestEarlyTerminatingSortingCollector#createRandomIndex(boolean).mjava","sourceNew":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(FORCE_MERGE_MAX_SEGMENT_COUNT);\n    }\n    reader = iw.getReader();\n    if (reader.maxDoc() == 0) {\n      iw.addDocument(new Document());\n      reader.close();\n      reader = iw.getReader();\n    }\n  }\n\n","sourceOld":"  private void createRandomIndex(boolean singleSortedSegment) throws IOException {\n    dir = newDirectory();\n    numDocs = atLeast(150);\n    final int numTerms = TestUtil.nextInt(random(), 1, numDocs / 5);\n    Set<String> randomTerms = new HashSet<>();\n    while (randomTerms.size() < numTerms) {\n      randomTerms.add(TestUtil.randomSimpleString(random()));\n    }\n    terms = new ArrayList<>(randomTerms);\n    final long seed = random().nextLong();\n    final IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(new Random(seed)));\n    if (iwc.getMergePolicy() instanceof MockRandomMergePolicy) {\n      // MockRandomMP randomly wraps the leaf readers which makes merging angry\n      iwc.setMergePolicy(newTieredMergePolicy());\n    }\n    iwc.setMergeScheduler(new SerialMergeScheduler()); // for reproducible tests\n    iwc.setIndexSort(sort);\n    iw = new RandomIndexWriter(new Random(seed), dir, iwc);\n    iw.setDoRandomForceMerge(false); // don't do this, it may happen anyway with MockRandomMP\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = randomDocument();\n      iw.addDocument(doc);\n      if (i == numDocs / 2 || (i != numDocs - 1 && random().nextInt(8) == 0)) {\n        iw.commit();\n      }\n      if (random().nextInt(15) == 0) {\n        final String term = RandomPicks.randomFrom(random(), terms);\n        iw.deleteDocuments(new Term(\"s\", term));\n      }\n    }\n    if (singleSortedSegment) {\n      // because of deletions, there might still be a single flush segment in\n      // the index, although want want a sorted segment so it needs to be merged\n      iw.getReader().close(); // refresh\n      iw.addDocument(new Document());\n      iw.commit();\n      iw.addDocument(new Document());\n      iw.forceMerge(1);\n    }\n    else if (random().nextBoolean()) {\n      iw.forceMerge(FORCE_MERGE_MAX_SEGMENT_COUNT);\n    }\n    reader = iw.getReader();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1aad05eeff7818b0833c02ac6b743aa72054963b":["5de502b5478255493125e7e801411ba17a6682ec","404d1ab7f6f396235047017c88d545fec15dafb7"],"5de502b5478255493125e7e801411ba17a6682ec":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"404d1ab7f6f396235047017c88d545fec15dafb7":["5de502b5478255493125e7e801411ba17a6682ec"],"6f20fd35e3055a0c5b387df0b986a68d65d86441":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0ad30c6a479e764150a3316e57263319775f1df2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3d33e731a93d4b57e662ff094f64f94a745422d4"],"5e03940e6e9044943de4b7ac08f8581da37a9534":["fb0345a2d45479f891041f8b3ce351bc975e64ac"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","5e03940e6e9044943de4b7ac08f8581da37a9534"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","d470c8182e92b264680e34081b75e70a9f2b3c89"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["1aad05eeff7818b0833c02ac6b743aa72054963b"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","0ad30c6a479e764150a3316e57263319775f1df2"],"fb0345a2d45479f891041f8b3ce351bc975e64ac":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"1aad05eeff7818b0833c02ac6b743aa72054963b":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5de502b5478255493125e7e801411ba17a6682ec":["1aad05eeff7818b0833c02ac6b743aa72054963b","404d1ab7f6f396235047017c88d545fec15dafb7"],"404d1ab7f6f396235047017c88d545fec15dafb7":["1aad05eeff7818b0833c02ac6b743aa72054963b"],"6f20fd35e3055a0c5b387df0b986a68d65d86441":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0ad30c6a479e764150a3316e57263319775f1df2","3d33e731a93d4b57e662ff094f64f94a745422d4","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d470c8182e92b264680e34081b75e70a9f2b3c89","fb0345a2d45479f891041f8b3ce351bc975e64ac"],"0ad30c6a479e764150a3316e57263319775f1df2":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"5e03940e6e9044943de4b7ac08f8581da37a9534":["3d33e731a93d4b57e662ff094f64f94a745422d4"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["0ad30c6a479e764150a3316e57263319775f1df2"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"d470c8182e92b264680e34081b75e70a9f2b3c89":["5de502b5478255493125e7e801411ba17a6682ec","6f20fd35e3055a0c5b387df0b986a68d65d86441","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"fb0345a2d45479f891041f8b3ce351bc975e64ac":["5e03940e6e9044943de4b7ac08f8581da37a9534"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["6f20fd35e3055a0c5b387df0b986a68d65d86441","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}