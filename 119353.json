{"path":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","commits":[{"id":"70f91c8322fbffe3a3a897ef20ea19119cac10cd","date":1386170124,"type":1,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","pathOld":"solr/contrib/solr-mr/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","sourceNew":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Merging into dstDir: \" + workDir + \", srcDirs: {}\", shards);\n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        IndexWriterConfig writerConfig = new IndexWriterConfig(Version.LUCENE_CURRENT, null)\n            .setOpenMode(OpenMode.CREATE)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        long start = System.currentTimeMillis();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n        \n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        float secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Logical merge took {} secs\", secs);        \n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        start = System.currentTimeMillis();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {} secs\", maxSegments, secs);\n        \n        start = System.currentTimeMillis();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Optimizing Solr: Done closing index writer in {} secs\", secs);\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }    \n\n","sourceOld":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Merging into dstDir: \" + workDir + \", srcDirs: {}\", shards);\n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        IndexWriterConfig writerConfig = new IndexWriterConfig(Version.LUCENE_CURRENT, null)\n            .setOpenMode(OpenMode.CREATE)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        long start = System.currentTimeMillis();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n        \n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        float secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Logical merge took {} secs\", secs);        \n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        start = System.currentTimeMillis();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {} secs\", maxSegments, secs);\n        \n        start = System.currentTimeMillis();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Optimizing Solr: Done closing index writer in {} secs\", secs);\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"42d384b06aa87eae925b668b65f3246154f0b0fa","date":1386181725,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","sourceNew":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(Version.LUCENE_CURRENT, null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        long start = System.currentTimeMillis();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n        \n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        float secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Logical merge took {} secs\", secs);        \n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        start = System.currentTimeMillis();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {} secs\", maxSegments, secs);\n        \n        start = System.currentTimeMillis();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Optimizing Solr: Done closing index writer in {} secs\", secs);\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","sourceOld":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Merging into dstDir: \" + workDir + \", srcDirs: {}\", shards);\n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        IndexWriterConfig writerConfig = new IndexWriterConfig(Version.LUCENE_CURRENT, null)\n            .setOpenMode(OpenMode.CREATE)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        long start = System.currentTimeMillis();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n        \n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        float secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Logical merge took {} secs\", secs);        \n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        start = System.currentTimeMillis();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {} secs\", maxSegments, secs);\n        \n        start = System.currentTimeMillis();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Optimizing Solr: Done closing index writer in {} secs\", secs);\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"74f45af4339b0daf7a95c820ab88c1aea74fbce0","date":1387475327,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","pathOld":"/dev/null","sourceNew":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(Version.LUCENE_CURRENT, null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        long start = System.currentTimeMillis();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n        \n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        float secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Logical merge took {} secs\", secs);        \n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        start = System.currentTimeMillis();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {} secs\", maxSegments, secs);\n        \n        start = System.currentTimeMillis();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Optimizing Solr: Done closing index writer in {} secs\", secs);\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fd5bc858b8426d40bbe90b94120ead37c77d7954","date":1393812525,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","sourceNew":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(Version.LUCENE_CURRENT, null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        long start = System.nanoTime();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n        \n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        float secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Logical merge took {} secs\", secs);        \n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        start = System.nanoTime();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {} secs\", maxSegments, secs);\n        \n        start = System.nanoTime();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Optimizing Solr: Done closing index writer in {} secs\", secs);\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","sourceOld":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(Version.LUCENE_CURRENT, null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        long start = System.currentTimeMillis();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n        \n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        float secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Logical merge took {} secs\", secs);        \n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        start = System.currentTimeMillis();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {} secs\", maxSegments, secs);\n        \n        start = System.currentTimeMillis();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        secs = (System.currentTimeMillis() - start) / 1000.0f;\n        LOG.info(\"Optimizing Solr: Done closing index writer in {} secs\", secs);\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","sourceNew":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(Version.LUCENE_CURRENT, null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        long start = System.nanoTime();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n        \n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        float secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Logical merge took {} secs\", secs);        \n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        start = System.nanoTime();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {} secs\", maxSegments, secs);\n        \n        start = System.nanoTime();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.shutdown();\n        secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Optimizing Solr: Done closing index writer in {} secs\", secs);\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","sourceOld":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(Version.LUCENE_CURRENT, null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        long start = System.nanoTime();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n        \n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        float secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Logical merge took {} secs\", secs);        \n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        start = System.nanoTime();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {} secs\", maxSegments, secs);\n        \n        start = System.nanoTime();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Optimizing Solr: Done closing index writer in {} secs\", secs);\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","sourceNew":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        long start = System.nanoTime();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n        \n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        float secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Logical merge took {} secs\", secs);        \n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        start = System.nanoTime();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {} secs\", maxSegments, secs);\n        \n        start = System.nanoTime();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Optimizing Solr: Done closing index writer in {} secs\", secs);\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","sourceOld":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(Version.LUCENE_CURRENT, null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        long start = System.nanoTime();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n        \n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        float secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Logical merge took {} secs\", secs);        \n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        start = System.nanoTime();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {} secs\", maxSegments, secs);\n        \n        start = System.nanoTime();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.shutdown();\n        secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Optimizing Solr: Done closing index writer in {} secs\", secs);\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0a506fe165b26e024afa1aec8a4a7d758e837ff","date":1410971446,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","sourceNew":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, NoLockFactory.getNoLockFactory(), context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), NoLockFactory.getNoLockFactory(), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        long start = System.nanoTime();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n        \n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        float secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Logical merge took {} secs\", secs);        \n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        start = System.nanoTime();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {} secs\", maxSegments, secs);\n        \n        start = System.nanoTime();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Optimizing Solr: Done closing index writer in {} secs\", secs);\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","sourceOld":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        long start = System.nanoTime();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n        \n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        float secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Logical merge took {} secs\", secs);        \n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        start = System.nanoTime();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {} secs\", maxSegments, secs);\n        \n        start = System.nanoTime();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Optimizing Solr: Done closing index writer in {} secs\", secs);\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"285cdc737de75b7cc7c284a156b20214deb67bca","date":1415535483,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","sourceNew":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, NoLockFactory.INSTANCE, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), NoLockFactory.INSTANCE, context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        long start = System.nanoTime();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n        \n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        float secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Logical merge took {} secs\", secs);        \n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        start = System.nanoTime();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {} secs\", maxSegments, secs);\n        \n        start = System.nanoTime();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Optimizing Solr: Done closing index writer in {} secs\", secs);\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","sourceOld":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, NoLockFactory.getNoLockFactory(), context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), NoLockFactory.getNoLockFactory(), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        long start = System.nanoTime();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n        \n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        float secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Logical merge took {} secs\", secs);        \n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        start = System.nanoTime();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {} secs\", maxSegments, secs);\n        \n        start = System.nanoTime();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Optimizing Solr: Done closing index writer in {} secs\", secs);\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bcf9886c8ff537aafde14de48ebf744f5673f08b","date":1439041198,"type":3,"author":"Ramkumar Aiyengar","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","sourceNew":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, NoLockFactory.INSTANCE, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), NoLockFactory.INSTANCE, context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        RTimer timer = new RTimer();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Logical merge took {}ms\", timer.getTime());\n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        timer = new RTimer();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {}ms\", maxSegments, timer.getTime());\n\n        timer = new RTimer();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        LOG.info(\"Optimizing Solr: Done closing index writer in {}ms\", timer.getTime());\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","sourceOld":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, NoLockFactory.INSTANCE, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), NoLockFactory.INSTANCE, context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        long start = System.nanoTime();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n        \n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        float secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Logical merge took {} secs\", secs);        \n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        start = System.nanoTime();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment(System.currentTimeMillis() - start);\n        }\n        secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {} secs\", maxSegments, secs);\n        \n        start = System.nanoTime();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        secs = (System.nanoTime() - start) / (float)(10^9);\n        LOG.info(\"Optimizing Solr: Done closing index writer in {} secs\", secs);\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"61c6fb105d0ce095d846502a1d7a2a4bbf8fdecd","date":1466528770,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","sourceNew":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        RTimer timer = new RTimer();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Logical merge took {}ms\", timer.getTime());\n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        timer = new RTimer();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {}ms\", maxSegments, timer.getTime());\n\n        timer = new RTimer();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        LOG.info(\"Optimizing Solr: Done closing index writer in {}ms\", timer.getTime());\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","sourceOld":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, NoLockFactory.INSTANCE, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), NoLockFactory.INSTANCE, context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        RTimer timer = new RTimer();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Logical merge took {}ms\", timer.getTime());\n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        timer = new RTimer();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {}ms\", maxSegments, timer.getTime());\n\n        timer = new RTimer();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        LOG.info(\"Optimizing Solr: Done closing index writer in {}ms\", timer.getTime());\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"13734b36bfd631ed6a46b961df376f679e8a3f57","date":1473743967,"type":3,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","sourceNew":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        RTimer timer = new RTimer();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Logical merge took {}ms\", timer.getTime());\n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        timer = new RTimer();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments);\n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {}ms\", maxSegments, timer.getTime());\n\n        // Set Solr's commit data so the created index is usable by SolrCloud. E.g. Currently SolrCloud relies on\n        // commitTimeMSec in the commit data to do replication.\n        SolrIndexWriter.setCommitData(writer);\n\n        timer = new RTimer();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        LOG.info(\"Optimizing Solr: Done closing index writer in {}ms\", timer.getTime());\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","sourceOld":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        RTimer timer = new RTimer();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Logical merge took {}ms\", timer.getTime());\n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        timer = new RTimer();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {}ms\", maxSegments, timer.getTime());\n\n        timer = new RTimer();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        LOG.info(\"Optimizing Solr: Done closing index writer in {}ms\", timer.getTime());\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","bugFix":["d6e604e9030fb0cabf0c5a85ae6039921a81419c"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"89424def13674ea17829b41c5883c54ecc31a132","date":1473767373,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","sourceNew":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        RTimer timer = new RTimer();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Logical merge took {}ms\", timer.getTime());\n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        timer = new RTimer();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments);\n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {}ms\", maxSegments, timer.getTime());\n\n        // Set Solr's commit data so the created index is usable by SolrCloud. E.g. Currently SolrCloud relies on\n        // commitTimeMSec in the commit data to do replication.\n        SolrIndexWriter.setCommitData(writer);\n\n        timer = new RTimer();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        LOG.info(\"Optimizing Solr: Done closing index writer in {}ms\", timer.getTime());\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","sourceOld":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        RTimer timer = new RTimer();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Logical merge took {}ms\", timer.getTime());\n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        timer = new RTimer();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {}ms\", maxSegments, timer.getTime());\n\n        timer = new RTimer();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        LOG.info(\"Optimizing Solr: Done closing index writer in {}ms\", timer.getTime());\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","sourceNew":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        RTimer timer = new RTimer();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Logical merge took {}ms\", timer.getTime());\n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        timer = new RTimer();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments);\n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {}ms\", maxSegments, timer.getTime());\n\n        // Set Solr's commit data so the created index is usable by SolrCloud. E.g. Currently SolrCloud relies on\n        // commitTimeMSec in the commit data to do replication.\n        SolrIndexWriter.setCommitData(writer);\n\n        timer = new RTimer();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        LOG.info(\"Optimizing Solr: Done closing index writer in {}ms\", timer.getTime());\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","sourceOld":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        RTimer timer = new RTimer();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Logical merge took {}ms\", timer.getTime());\n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        timer = new RTimer();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {}ms\", maxSegments, timer.getTime());\n\n        timer = new RTimer();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        LOG.info(\"Optimizing Solr: Done closing index writer in {}ms\", timer.getTime());\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","sourceNew":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        RTimer timer = new RTimer();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Logical merge took {}ms\", timer.getTime());\n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        timer = new RTimer();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments);\n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {}ms\", maxSegments, timer.getTime());\n\n        // Set Solr's commit data so the created index is usable by SolrCloud. E.g. Currently SolrCloud relies on\n        // commitTimeMSec in the commit data to do replication.\n        SolrIndexWriter.setCommitData(writer);\n\n        timer = new RTimer();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        LOG.info(\"Optimizing Solr: Done closing index writer in {}ms\", timer.getTime());\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","sourceOld":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, NoLockFactory.INSTANCE, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), NoLockFactory.INSTANCE, context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        RTimer timer = new RTimer();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Logical merge took {}ms\", timer.getTime());\n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        timer = new RTimer();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments); \n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {}ms\", maxSegments, timer.getTime());\n\n        timer = new RTimer();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        LOG.info(\"Optimizing Solr: Done closing index writer in {}ms\", timer.getTime());\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"be320990bdc77e643388fa801e75017f19289c42","date":1489477067,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","sourceNew":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        RTimer timer = new RTimer();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Logical merge took {}ms\", timer.getTime());\n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        timer = new RTimer();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments);\n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {}ms\", maxSegments, timer.getTime());\n\n        // Set Solr's commit data so the created index is usable by SolrCloud. E.g. Currently SolrCloud relies on\n        // commitTimeMSec in the commit data to do replication.\n        //TODO no commitUpdateCommand\n        SolrIndexWriter.setCommitData(writer, -1);\n\n        timer = new RTimer();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        LOG.info(\"Optimizing Solr: Done closing index writer in {}ms\", timer.getTime());\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","sourceOld":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        RTimer timer = new RTimer();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Logical merge took {}ms\", timer.getTime());\n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        timer = new RTimer();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments);\n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {}ms\", maxSegments, timer.getTime());\n\n        // Set Solr's commit data so the created index is usable by SolrCloud. E.g. Currently SolrCloud relies on\n        // commitTimeMSec in the commit data to do replication.\n        SolrIndexWriter.setCommitData(writer);\n\n        timer = new RTimer();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        LOG.info(\"Optimizing Solr: Done closing index writer in {}ms\", timer.getTime());\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f996f8177b9204bdc92f7164460c6cefad9ac99a","date":1489482690,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","sourceNew":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        RTimer timer = new RTimer();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Logical merge took {}ms\", timer.getTime());\n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        timer = new RTimer();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments);\n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {}ms\", maxSegments, timer.getTime());\n\n        // Set Solr's commit data so the created index is usable by SolrCloud. E.g. Currently SolrCloud relies on\n        // commitTimeMSec in the commit data to do replication.\n        //TODO no commitUpdateCommand\n        SolrIndexWriter.setCommitData(writer, -1);\n\n        timer = new RTimer();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        LOG.info(\"Optimizing Solr: Done closing index writer in {}ms\", timer.getTime());\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","sourceOld":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        RTimer timer = new RTimer();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Logical merge took {}ms\", timer.getTime());\n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        timer = new RTimer();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments);\n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {}ms\", maxSegments, timer.getTime());\n\n        // Set Solr's commit data so the created index is usable by SolrCloud. E.g. Currently SolrCloud relies on\n        // commitTimeMSec in the commit data to do replication.\n        SolrIndexWriter.setCommitData(writer);\n\n        timer = new RTimer();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        LOG.info(\"Optimizing Solr: Done closing index writer in {}ms\", timer.getTime());\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab68488225b6a6c357dda72ed11dedca9914a192","date":1490013111,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","sourceNew":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        RTimer timer = new RTimer();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Logical merge took {}ms\", timer.getTime());\n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        timer = new RTimer();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments);\n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {}ms\", maxSegments, timer.getTime());\n\n        // Set Solr's commit data so the created index is usable by SolrCloud. E.g. Currently SolrCloud relies on\n        // commitTimeMSec in the commit data to do replication.\n        //TODO no commitUpdateCommand\n        SolrIndexWriter.setCommitData(writer, -1);\n\n        timer = new RTimer();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        LOG.info(\"Optimizing Solr: Done closing index writer in {}ms\", timer.getTime());\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","sourceOld":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        RTimer timer = new RTimer();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Logical merge took {}ms\", timer.getTime());\n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        timer = new RTimer();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments);\n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {}ms\", maxSegments, timer.getTime());\n\n        // Set Solr's commit data so the created index is usable by SolrCloud. E.g. Currently SolrCloud relies on\n        // commitTimeMSec in the commit data to do replication.\n        SolrIndexWriter.setCommitData(writer);\n\n        timer = new RTimer();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        LOG.info(\"Optimizing Solr: Done closing index writer in {}ms\", timer.getTime());\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"12109b652e9210b8d58fca47f6c4a725d058a58e","date":1490373076,"type":4,"author":"Steve Rowe","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","sourceNew":null,"sourceOld":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        RTimer timer = new RTimer();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Logical merge took {}ms\", timer.getTime());\n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        timer = new RTimer();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments);\n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {}ms\", maxSegments, timer.getTime());\n\n        // Set Solr's commit data so the created index is usable by SolrCloud. E.g. Currently SolrCloud relies on\n        // commitTimeMSec in the commit data to do replication.\n        //TODO no commitUpdateCommand\n        SolrIndexWriter.setCommitData(writer, -1);\n\n        timer = new RTimer();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        LOG.info(\"Optimizing Solr: Done closing index writer in {}ms\", timer.getTime());\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe1c4aa9af769a38e878f608070f672efbeac27f","date":1490594650,"type":4,"author":"Steve Rowe","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/TreeMergeOutputFormat.TreeMergeRecordWriter#close(TaskAttemptContext).mjava","sourceNew":null,"sourceOld":"    @Override\n    public void close(TaskAttemptContext context) throws IOException {\n      LOG.debug(\"Task \" + context.getTaskAttemptID() + \" merging into dstDir: \" + workDir + \", srcDirs: \" + shards);\n      writeShardNumberFile(context);      \n      heartBeater.needHeartBeat();\n      try {\n        Directory mergedIndex = new HdfsDirectory(workDir, context.getConfiguration());\n        \n        // TODO: shouldn't we pull the Version from the solrconfig.xml?\n        IndexWriterConfig writerConfig = new IndexWriterConfig(null)\n            .setOpenMode(OpenMode.CREATE).setUseCompoundFile(false)\n            //.setMergePolicy(mergePolicy) // TODO: grab tuned MergePolicy from solrconfig.xml?\n            //.setMergeScheduler(...) // TODO: grab tuned MergeScheduler from solrconfig.xml?\n            ;\n          \n        if (LOG.isDebugEnabled()) {\n          writerConfig.setInfoStream(System.out);\n        }\n//        writerConfig.setRAMBufferSizeMB(100); // improve performance\n//        writerConfig.setMaxThreadStates(1);\n        \n        // disable compound file to improve performance\n        // also see http://lucene.472066.n3.nabble.com/Questions-on-compound-file-format-td489105.html\n        // also see defaults in SolrIndexConfig\n        MergePolicy mergePolicy = writerConfig.getMergePolicy();\n        LOG.debug(\"mergePolicy was: {}\", mergePolicy);\n        if (mergePolicy instanceof TieredMergePolicy) {\n          ((TieredMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnceExplicit(10000);          \n//          ((TieredMergePolicy) mergePolicy).setMaxMergeAtOnce(10000);       \n//          ((TieredMergePolicy) mergePolicy).setSegmentsPerTier(10000);\n        } else if (mergePolicy instanceof LogMergePolicy) {\n          ((LogMergePolicy) mergePolicy).setNoCFSRatio(0.0);\n        }\n        LOG.info(\"Using mergePolicy: {}\", mergePolicy);\n        \n        IndexWriter writer = new IndexWriter(mergedIndex, writerConfig);\n        \n        Directory[] indexes = new Directory[shards.size()];\n        for (int i = 0; i < shards.size(); i++) {\n          indexes[i] = new HdfsDirectory(shards.get(i), context.getConfiguration());\n        }\n\n        context.setStatus(\"Logically merging \" + shards.size() + \" shards into one shard\");\n        LOG.info(\"Logically merging \" + shards.size() + \" shards into one shard: \" + workDir);\n        RTimer timer = new RTimer();\n        \n        writer.addIndexes(indexes); \n        // TODO: avoid intermediate copying of files into dst directory; rename the files into the dir instead (cp -> rename) \n        // This can improve performance and turns this phase into a true \"logical\" merge, completing in constant time.\n        // See https://issues.apache.org/jira/browse/LUCENE-4746\n\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.LOGICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Logical merge took {}ms\", timer.getTime());\n        int maxSegments = context.getConfiguration().getInt(TreeMergeMapper.MAX_SEGMENTS_ON_TREE_MERGE, Integer.MAX_VALUE);\n        context.setStatus(\"Optimizing Solr: forcing mtree merge down to \" + maxSegments + \" segments\");\n        LOG.info(\"Optimizing Solr: forcing tree merge down to {} segments\", maxSegments);\n        timer = new RTimer();\n        if (maxSegments < Integer.MAX_VALUE) {\n          writer.forceMerge(maxSegments);\n          // TODO: consider perf enhancement for no-deletes merges: bulk-copy the postings data \n          // see http://lucene.472066.n3.nabble.com/Experience-with-large-merge-factors-tp1637832p1647046.html\n        }\n        timer.stop();\n        if (LOG.isDebugEnabled()) {\n          context.getCounter(SolrCounters.class.getName(), SolrCounters.PHYSICAL_TREE_MERGE_TIME.toString()).increment((long) timer.getTime());\n        }\n        LOG.info(\"Optimizing Solr: done forcing tree merge down to {} segments in {}ms\", maxSegments, timer.getTime());\n\n        // Set Solr's commit data so the created index is usable by SolrCloud. E.g. Currently SolrCloud relies on\n        // commitTimeMSec in the commit data to do replication.\n        //TODO no commitUpdateCommand\n        SolrIndexWriter.setCommitData(writer, -1);\n\n        timer = new RTimer();\n        LOG.info(\"Optimizing Solr: Closing index writer\");\n        writer.close();\n        LOG.info(\"Optimizing Solr: Done closing index writer in {}ms\", timer.getTime());\n        context.setStatus(\"Done\");\n      } finally {\n        heartBeater.cancelHeartBeat();\n        heartBeater.close();\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0a506fe165b26e024afa1aec8a4a7d758e837ff":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"13734b36bfd631ed6a46b961df376f679e8a3f57":["61c6fb105d0ce095d846502a1d7a2a4bbf8fdecd"],"12109b652e9210b8d58fca47f6c4a725d058a58e":["be320990bdc77e643388fa801e75017f19289c42"],"285cdc737de75b7cc7c284a156b20214deb67bca":["a0a506fe165b26e024afa1aec8a4a7d758e837ff"],"61c6fb105d0ce095d846502a1d7a2a4bbf8fdecd":["bcf9886c8ff537aafde14de48ebf744f5673f08b"],"fe1c4aa9af769a38e878f608070f672efbeac27f":["ab68488225b6a6c357dda72ed11dedca9914a192"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["bcf9886c8ff537aafde14de48ebf744f5673f08b","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"fd5bc858b8426d40bbe90b94120ead37c77d7954":["42d384b06aa87eae925b668b65f3246154f0b0fa"],"42d384b06aa87eae925b668b65f3246154f0b0fa":["70f91c8322fbffe3a3a897ef20ea19119cac10cd"],"89424def13674ea17829b41c5883c54ecc31a132":["61c6fb105d0ce095d846502a1d7a2a4bbf8fdecd","13734b36bfd631ed6a46b961df376f679e8a3f57"],"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"f996f8177b9204bdc92f7164460c6cefad9ac99a":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["61c6fb105d0ce095d846502a1d7a2a4bbf8fdecd","89424def13674ea17829b41c5883c54ecc31a132"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","42d384b06aa87eae925b668b65f3246154f0b0fa"],"bcf9886c8ff537aafde14de48ebf744f5673f08b":["285cdc737de75b7cc7c284a156b20214deb67bca"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["fd5bc858b8426d40bbe90b94120ead37c77d7954"],"be320990bdc77e643388fa801e75017f19289c42":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"ab68488225b6a6c357dda72ed11dedca9914a192":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","f996f8177b9204bdc92f7164460c6cefad9ac99a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["12109b652e9210b8d58fca47f6c4a725d058a58e"]},"commit2Childs":{"a0a506fe165b26e024afa1aec8a4a7d758e837ff":["285cdc737de75b7cc7c284a156b20214deb67bca"],"13734b36bfd631ed6a46b961df376f679e8a3f57":["89424def13674ea17829b41c5883c54ecc31a132"],"12109b652e9210b8d58fca47f6c4a725d058a58e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"285cdc737de75b7cc7c284a156b20214deb67bca":["bcf9886c8ff537aafde14de48ebf744f5673f08b"],"61c6fb105d0ce095d846502a1d7a2a4bbf8fdecd":["13734b36bfd631ed6a46b961df376f679e8a3f57","89424def13674ea17829b41c5883c54ecc31a132","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"fe1c4aa9af769a38e878f608070f672efbeac27f":[],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"fd5bc858b8426d40bbe90b94120ead37c77d7954":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"42d384b06aa87eae925b668b65f3246154f0b0fa":["fd5bc858b8426d40bbe90b94120ead37c77d7954","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"89424def13674ea17829b41c5883c54ecc31a132":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["42d384b06aa87eae925b668b65f3246154f0b0fa"],"f996f8177b9204bdc92f7164460c6cefad9ac99a":["ab68488225b6a6c357dda72ed11dedca9914a192"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","f996f8177b9204bdc92f7164460c6cefad9ac99a","be320990bdc77e643388fa801e75017f19289c42","ab68488225b6a6c357dda72ed11dedca9914a192"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["70f91c8322fbffe3a3a897ef20ea19119cac10cd","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["a0a506fe165b26e024afa1aec8a4a7d758e837ff"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":[],"bcf9886c8ff537aafde14de48ebf744f5673f08b":["61c6fb105d0ce095d846502a1d7a2a4bbf8fdecd","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"be320990bdc77e643388fa801e75017f19289c42":["12109b652e9210b8d58fca47f6c4a725d058a58e"],"ab68488225b6a6c357dda72ed11dedca9914a192":["fe1c4aa9af769a38e878f608070f672efbeac27f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["fe1c4aa9af769a38e878f608070f672efbeac27f","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","74f45af4339b0daf7a95c820ab88c1aea74fbce0","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}