{"path":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","commits":[{"id":"6e1dc1767a334250071c4115941a7af8938063cc","date":1291471683,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String).mjava","sourceNew":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    Searcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","sourceOld":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    Searcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, \"EACGI\");\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, \"BJDFH\");\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, \"BJDHF\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d8c4bb144102e537495ae5b321f77a9898f7b0b8","date":1291834816,"type":1,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String).mjava","sourceNew":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    Searcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","sourceOld":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    Searcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, \"EACGI\");\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, \"BJDFH\");\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, \"BJDHF\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","pathOld":"/dev/null","sourceNew":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    Searcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8b241ea5e635d896cc0af83cd96ffd0322e0aba7","date":1294226200,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","sourceNew":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","sourceOld":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    Searcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70ad682703b8585f5d0a637efec044d57ec05efb","date":1294259117,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","sourceNew":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","sourceOld":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    Searcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","sourceNew":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","sourceOld":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    Searcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","sourceNew":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","sourceOld":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","sourceNew":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","sourceOld":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","sourceNew":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","sourceOld":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","sourceNew":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","sourceOld":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"090a0320e4de4a3674376aef96b9701f47564f86","date":1308707325,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","sourceNew":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.Type.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.Type.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.Type.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.Type.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","sourceOld":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","sourceNew":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.Type.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.Type.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.Type.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.Type.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","sourceOld":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","sourceNew":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.Type.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.Type.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.Type.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.Type.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","sourceOld":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d17d4fe0503a62f6522b1dd15204dd25cd231edf","date":1313599393,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#testCollationKeySort(Analyzer,Analyzer,Analyzer,Analyzer,String,String,String,String).mjava","sourceNew":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.Type.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.Type.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.Type.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.Type.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","sourceOld":"  // Test using various international locales with accented characters (which\n  // sort differently depending on locale)\n  //\n  // Copied (and slightly modified) from \n  // org.apache.lucene.search.TestSort.testInternationalSort()\n  //  \n  // TODO: this test is really fragile. there are already 3 different cases,\n  // depending upon unicode version.\n  public void testCollationKeySort(Analyzer usAnalyzer,\n                                   Analyzer franceAnalyzer,\n                                   Analyzer swedenAnalyzer,\n                                   Analyzer denmarkAnalyzer,\n                                   String usResult,\n                                   String frResult,\n                                   String svResult,\n                                   String dkResult) throws Exception {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n\n    // document data:\n    // the tracer field is used to determine which document was hit\n    String[][] sortData = new String[][] {\n      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)\n      {  \"A\",   \"x\",     \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\",      \"p\\u00EAche\"      },\n      {  \"B\",   \"y\",     \"HAT\",             \"HAT\",             \"HAT\",             \"HAT\"             },\n      {  \"C\",   \"x\",     \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\", \"p\\u00E9ch\\u00E9\" },\n      {  \"D\",   \"y\",     \"HUT\",             \"HUT\",             \"HUT\",             \"HUT\"             },\n      {  \"E\",   \"x\",     \"peach\",           \"peach\",           \"peach\",           \"peach\"           },\n      {  \"F\",   \"y\",     \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\",        \"H\\u00C5T\"        },\n      {  \"G\",   \"x\",     \"sin\",             \"sin\",             \"sin\",             \"sin\"             },\n      {  \"H\",   \"y\",     \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\",        \"H\\u00D8T\"        },\n      {  \"I\",   \"x\",     \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\",        \"s\\u00EDn\"        },\n      {  \"J\",   \"y\",     \"HOT\",             \"HOT\",             \"HOT\",             \"HOT\"             },\n    };\n\n    for (int i = 0 ; i < sortData.length ; ++i) {\n      Document doc = new Document();\n      doc.add(new Field(\"tracer\", sortData[i][0], \n                        Field.Store.YES, Field.Index.NO));\n      doc.add(new Field(\"contents\", sortData[i][1], \n                        Field.Store.NO, Field.Index.ANALYZED));\n      if (sortData[i][2] != null) \n        doc.add(new Field(\"US\", usAnalyzer.reusableTokenStream(\"US\", new StringReader(sortData[i][2]))));\n      if (sortData[i][3] != null) \n        doc.add(new Field(\"France\", franceAnalyzer.reusableTokenStream(\"France\", new StringReader(sortData[i][3]))));\n      if (sortData[i][4] != null)\n        doc.add(new Field(\"Sweden\", swedenAnalyzer.reusableTokenStream(\"Sweden\", new StringReader(sortData[i][4]))));\n      if (sortData[i][5] != null) \n        doc.add(new Field(\"Denmark\", denmarkAnalyzer.reusableTokenStream(\"Denmark\", new StringReader(sortData[i][5]))));\n      writer.addDocument(doc);\n    }\n    writer.optimize();\n    writer.close();\n    IndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n    Sort sort = new Sort();\n    Query queryX = new TermQuery(new Term (\"contents\", \"x\"));\n    Query queryY = new TermQuery(new Term (\"contents\", \"y\"));\n    \n    sort.setSort(new SortField(\"US\", SortField.Type.STRING));\n    assertMatches(searcher, queryY, sort, usResult);\n\n    sort.setSort(new SortField(\"France\", SortField.Type.STRING));\n    assertMatches(searcher, queryX, sort, frResult);\n\n    sort.setSort(new SortField(\"Sweden\", SortField.Type.STRING));\n    assertMatches(searcher, queryY, sort, svResult);\n\n    sort.setSort(new SortField(\"Denmark\", SortField.Type.STRING));\n    assertMatches(searcher, queryY, sort, dkResult);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"70ad682703b8585f5d0a637efec044d57ec05efb":["d8c4bb144102e537495ae5b321f77a9898f7b0b8","8b241ea5e635d896cc0af83cd96ffd0322e0aba7"],"d17d4fe0503a62f6522b1dd15204dd25cd231edf":["090a0320e4de4a3674376aef96b9701f47564f86"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["70ad682703b8585f5d0a637efec044d57ec05efb","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"962d04139994fce5193143ef35615499a9a96d78":["868da859b43505d9d2a023bfeae6dd0c795f5295","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"2553b00f699380c64959ccb27991289aae87be2e":["a3776dccca01c11e7046323cfad46a3b4a471233","090a0320e4de4a3674376aef96b9701f47564f86"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","6e1dc1767a334250071c4115941a7af8938063cc"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["f2c5f0cb44df114db4228c8f77861714b5cabaea","090a0320e4de4a3674376aef96b9701f47564f86"],"090a0320e4de4a3674376aef96b9701f47564f86":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["8b241ea5e635d896cc0af83cd96ffd0322e0aba7"],"8b241ea5e635d896cc0af83cd96ffd0322e0aba7":["6e1dc1767a334250071c4115941a7af8938063cc"],"a3776dccca01c11e7046323cfad46a3b4a471233":["8b241ea5e635d896cc0af83cd96ffd0322e0aba7","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"6e1dc1767a334250071c4115941a7af8938063cc":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d8c4bb144102e537495ae5b321f77a9898f7b0b8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","6e1dc1767a334250071c4115941a7af8938063cc"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","8b241ea5e635d896cc0af83cd96ffd0322e0aba7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d17d4fe0503a62f6522b1dd15204dd25cd231edf"]},"commit2Childs":{"70ad682703b8585f5d0a637efec044d57ec05efb":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"d17d4fe0503a62f6522b1dd15204dd25cd231edf":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"962d04139994fce5193143ef35615499a9a96d78":[],"2553b00f699380c64959ccb27991289aae87be2e":[],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"090a0320e4de4a3674376aef96b9701f47564f86":["d17d4fe0503a62f6522b1dd15204dd25cd231edf","2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"8b241ea5e635d896cc0af83cd96ffd0322e0aba7":["70ad682703b8585f5d0a637efec044d57ec05efb","f2c5f0cb44df114db4228c8f77861714b5cabaea","a3776dccca01c11e7046323cfad46a3b4a471233","868da859b43505d9d2a023bfeae6dd0c795f5295"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","d083e83f225b11e5fdd900e83d26ddb385b6955c","090a0320e4de4a3674376aef96b9701f47564f86","a3776dccca01c11e7046323cfad46a3b4a471233"],"a3776dccca01c11e7046323cfad46a3b4a471233":["2553b00f699380c64959ccb27991289aae87be2e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","6e1dc1767a334250071c4115941a7af8938063cc","d8c4bb144102e537495ae5b321f77a9898f7b0b8"],"6e1dc1767a334250071c4115941a7af8938063cc":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","8b241ea5e635d896cc0af83cd96ffd0322e0aba7","d8c4bb144102e537495ae5b321f77a9898f7b0b8"],"d8c4bb144102e537495ae5b321f77a9898f7b0b8":["70ad682703b8585f5d0a637efec044d57ec05efb"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["962d04139994fce5193143ef35615499a9a96d78"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}