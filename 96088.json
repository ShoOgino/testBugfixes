{"path":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","commits":[{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    final BytesRef bytes = bytesAtt.getBytesRef();\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(new BytesRef(bytes));\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    final BytesRef bytes = bytesAtt.getBytesRef();\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(new BytesRef(bytes));\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    final BytesRef bytes = bytesAtt.getBytesRef();\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(new BytesRef(bytes));\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    final BytesRef bytes = bytesAtt.getBytesRef();\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(new BytesRef(bytes));\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    final BytesRef bytes = bytesAtt.getBytesRef();\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(new BytesRef(bytes));\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    final BytesRef bytes = bytesAtt.getBytesRef();\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(new BytesRef(bytes));\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"60c9885566d6f83ba835be67d76ecbf214685052","date":1317096458,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    try {\n      final Set<BytesRef> tokens = new HashSet<BytesRef>();\n      final TokenStream tokenStream = analyzer.reusableTokenStream(\"\", new StringReader(query));\n      final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n      final BytesRef bytes = bytesAtt.getBytesRef();\n\n      tokenStream.reset();\n\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(new BytesRef(bytes));\n      }\n\n      tokenStream.end();\n      tokenStream.close();\n      return tokens;\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    final Set<BytesRef> tokens = new HashSet<BytesRef>();\n    final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n    final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    final BytesRef bytes = bytesAtt.getBytesRef();\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(new BytesRef(bytes));\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":["99c9d8533c954f661481ae44273622957dbf572f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"69e043c521d4e8db770cc140c63f5ef51f03426a","date":1317187614,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    try {\n      final Set<BytesRef> tokens = new HashSet<BytesRef>();\n      final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n      final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n      final BytesRef bytes = bytesAtt.getBytesRef();\n\n      tokenStream.reset();\n\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(new BytesRef(bytes));\n      }\n\n      tokenStream.end();\n      tokenStream.close();\n      return tokens;\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    try {\n      final Set<BytesRef> tokens = new HashSet<BytesRef>();\n      final TokenStream tokenStream = analyzer.reusableTokenStream(\"\", new StringReader(query));\n      final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n      final BytesRef bytes = bytesAtt.getBytesRef();\n\n      tokenStream.reset();\n\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(new BytesRef(bytes));\n      }\n\n      tokenStream.end();\n      tokenStream.close();\n      return tokens;\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n  }\n\n","bugFix":null,"bugIntro":["c83d6c4335f31cae14f625a222bc842f20073dcd"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e6e919043fa85ee891123768dd655a98edbbf63c","date":1322225413,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    try {\n      final Set<BytesRef> tokens = new HashSet<BytesRef>();\n      final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n      final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n      final BytesRef bytes = bytesAtt.getBytesRef();\n\n      tokenStream.reset();\n\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(BytesRef.deepCopyOf(bytes));\n      }\n\n      tokenStream.end();\n      tokenStream.close();\n      return tokens;\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    try {\n      final Set<BytesRef> tokens = new HashSet<BytesRef>();\n      final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n      final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n      final BytesRef bytes = bytesAtt.getBytesRef();\n\n      tokenStream.reset();\n\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(new BytesRef(bytes));\n      }\n\n      tokenStream.end();\n      tokenStream.close();\n      return tokens;\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c83d6c4335f31cae14f625a222bc842f20073dcd","date":1373306148,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    try {\n      final Set<BytesRef> tokens = new HashSet<BytesRef>();\n      final TokenStream tokenStream = analyzer.tokenStream(\"\", query);\n      final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n      final BytesRef bytes = bytesAtt.getBytesRef();\n\n      tokenStream.reset();\n\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(BytesRef.deepCopyOf(bytes));\n      }\n\n      tokenStream.end();\n      tokenStream.close();\n      return tokens;\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    try {\n      final Set<BytesRef> tokens = new HashSet<BytesRef>();\n      final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n      final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n      final BytesRef bytes = bytesAtt.getBytesRef();\n\n      tokenStream.reset();\n\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(BytesRef.deepCopyOf(bytes));\n      }\n\n      tokenStream.end();\n      tokenStream.close();\n      return tokens;\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n  }\n\n","bugFix":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"bugIntro":["99c9d8533c954f661481ae44273622957dbf572f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    try {\n      final Set<BytesRef> tokens = new HashSet<BytesRef>();\n      final TokenStream tokenStream = analyzer.tokenStream(\"\", query);\n      final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n      final BytesRef bytes = bytesAtt.getBytesRef();\n\n      tokenStream.reset();\n\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(BytesRef.deepCopyOf(bytes));\n      }\n\n      tokenStream.end();\n      tokenStream.close();\n      return tokens;\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    try {\n      final Set<BytesRef> tokens = new HashSet<BytesRef>();\n      final TokenStream tokenStream = analyzer.tokenStream(\"\", new StringReader(query));\n      final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n      final BytesRef bytes = bytesAtt.getBytesRef();\n\n      tokenStream.reset();\n\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(BytesRef.deepCopyOf(bytes));\n      }\n\n      tokenStream.end();\n      tokenStream.close();\n      return tokens;\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"99c9d8533c954f661481ae44273622957dbf572f","date":1380991288,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    TokenStream tokenStream = null;\n    try {\n      final Set<BytesRef> tokens = new HashSet<BytesRef>();\n      tokenStream = analyzer.tokenStream(\"\", query);\n      final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n      final BytesRef bytes = bytesAtt.getBytesRef();\n\n      tokenStream.reset();\n\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(BytesRef.deepCopyOf(bytes));\n      }\n\n      tokenStream.end();\n      return tokens;\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    } finally {\n      IOUtils.closeWhileHandlingException(tokenStream);\n    }\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    try {\n      final Set<BytesRef> tokens = new HashSet<BytesRef>();\n      final TokenStream tokenStream = analyzer.tokenStream(\"\", query);\n      final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n      final BytesRef bytes = bytesAtt.getBytesRef();\n\n      tokenStream.reset();\n\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(BytesRef.deepCopyOf(bytes));\n      }\n\n      tokenStream.end();\n      tokenStream.close();\n      return tokens;\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n  }\n\n","bugFix":["c83d6c4335f31cae14f625a222bc842f20073dcd","60c9885566d6f83ba835be67d76ecbf214685052"],"bugIntro":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"782ed6a4b4ba50ec19734fc8db4e570ee193d627","date":1381127065,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    try (TokenStream tokenStream = analyzer.tokenStream(\"\", query)){\n      final Set<BytesRef> tokens = new HashSet<BytesRef>();\n      final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n      final BytesRef bytes = bytesAtt.getBytesRef();\n\n      tokenStream.reset();\n\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(BytesRef.deepCopyOf(bytes));\n      }\n\n      tokenStream.end();\n      return tokens;\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    TokenStream tokenStream = null;\n    try {\n      final Set<BytesRef> tokens = new HashSet<BytesRef>();\n      tokenStream = analyzer.tokenStream(\"\", query);\n      final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n      final BytesRef bytes = bytesAtt.getBytesRef();\n\n      tokenStream.reset();\n\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(BytesRef.deepCopyOf(bytes));\n      }\n\n      tokenStream.end();\n      return tokens;\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    } finally {\n      IOUtils.closeWhileHandlingException(tokenStream);\n    }\n  }\n\n","bugFix":["99c9d8533c954f661481ae44273622957dbf572f","ec58fb7921964848d01bea54f8ec4a2ac813eaeb"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    try (TokenStream tokenStream = analyzer.tokenStream(\"\", query)){\n      final Set<BytesRef> tokens = new HashSet<>();\n      final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n      final BytesRef bytes = bytesAtt.getBytesRef();\n\n      tokenStream.reset();\n\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(BytesRef.deepCopyOf(bytes));\n      }\n\n      tokenStream.end();\n      return tokens;\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    try (TokenStream tokenStream = analyzer.tokenStream(\"\", query)){\n      final Set<BytesRef> tokens = new HashSet<BytesRef>();\n      final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n      final BytesRef bytes = bytesAtt.getBytesRef();\n\n      tokenStream.reset();\n\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(BytesRef.deepCopyOf(bytes));\n      }\n\n      tokenStream.end();\n      return tokens;\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"804b857d1066ab5185b3b9101bde41b0b71426ec","date":1435846169,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    try (TokenStream tokenStream = analyzer.tokenStream(\"\", query)){\n      final Set<BytesRef> tokens = new HashSet<>();\n      final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n\n      tokenStream.reset();\n\n      while (tokenStream.incrementToken()) {\n        tokens.add(BytesRef.deepCopyOf(bytesAtt.getBytesRef()));\n      }\n\n      tokenStream.end();\n      return tokens;\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    try (TokenStream tokenStream = analyzer.tokenStream(\"\", query)){\n      final Set<BytesRef> tokens = new HashSet<>();\n      final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n      final BytesRef bytes = bytesAtt.getBytesRef();\n\n      tokenStream.reset();\n\n      while (tokenStream.incrementToken()) {\n        bytesAtt.fillBytesRef();\n        tokens.add(BytesRef.deepCopyOf(bytes));\n      }\n\n      tokenStream.end();\n      return tokens;\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d669cb2b7fbc6ceb7f966e63a1c625e6400c7d2a","date":1550036130,"type":3,"author":"Bruno P. Kinoshita","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#getQueryTokenSet(String,Analyzer).mjava","sourceNew":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    try (TokenStream tokenStream = analyzer.tokenStream(\"\", query)){\n      final Set<BytesRef> tokens = new HashSet<>();\n      final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n\n      tokenStream.reset();\n\n      while (tokenStream.incrementToken()) {\n        tokens.add(BytesRef.deepCopyOf(bytesAtt.getBytesRef()));\n      }\n\n      tokenStream.end();\n      return tokens;\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occurred while iterating over tokenstream\", ioe);\n    }\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given text using the given analyzer and returns the produced tokens.\n   *\n   * @param query    The query to analyze.\n   * @param analyzer The analyzer to use.\n   */\n  protected Set<BytesRef> getQueryTokenSet(String query, Analyzer analyzer) {\n    try (TokenStream tokenStream = analyzer.tokenStream(\"\", query)){\n      final Set<BytesRef> tokens = new HashSet<>();\n      final TermToBytesRefAttribute bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n\n      tokenStream.reset();\n\n      while (tokenStream.incrementToken()) {\n        tokens.add(BytesRef.deepCopyOf(bytesAtt.getBytesRef()));\n      }\n\n      tokenStream.end();\n      return tokens;\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"804b857d1066ab5185b3b9101bde41b0b71426ec":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"99c9d8533c954f661481ae44273622957dbf572f":["c83d6c4335f31cae14f625a222bc842f20073dcd"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"60c9885566d6f83ba835be67d76ecbf214685052":["c26f00b574427b55127e869b935845554afde1fa"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["e6e919043fa85ee891123768dd655a98edbbf63c","c83d6c4335f31cae14f625a222bc842f20073dcd"],"c26f00b574427b55127e869b935845554afde1fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"782ed6a4b4ba50ec19734fc8db4e570ee193d627":["99c9d8533c954f661481ae44273622957dbf572f"],"d669cb2b7fbc6ceb7f966e63a1c625e6400c7d2a":["804b857d1066ab5185b3b9101bde41b0b71426ec"],"c83d6c4335f31cae14f625a222bc842f20073dcd":["e6e919043fa85ee891123768dd655a98edbbf63c"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a258fbb26824fd104ed795e5d9033d2d040049ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"e6e919043fa85ee891123768dd655a98edbbf63c":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"69e043c521d4e8db770cc140c63f5ef51f03426a":["60c9885566d6f83ba835be67d76ecbf214685052"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d669cb2b7fbc6ceb7f966e63a1c625e6400c7d2a"]},"commit2Childs":{"804b857d1066ab5185b3b9101bde41b0b71426ec":["d669cb2b7fbc6ceb7f966e63a1c625e6400c7d2a"],"99c9d8533c954f661481ae44273622957dbf572f":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["804b857d1066ab5185b3b9101bde41b0b71426ec"],"60c9885566d6f83ba835be67d76ecbf214685052":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"c26f00b574427b55127e869b935845554afde1fa":["60c9885566d6f83ba835be67d76ecbf214685052"],"782ed6a4b4ba50ec19734fc8db4e570ee193d627":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"d669cb2b7fbc6ceb7f966e63a1c625e6400c7d2a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c83d6c4335f31cae14f625a222bc842f20073dcd":["99c9d8533c954f661481ae44273622957dbf572f","37a0f60745e53927c4c876cfe5b5a58170f0646c"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c26f00b574427b55127e869b935845554afde1fa","c903c3d15906a3da96b8c0c2fb704491005fdbdb","a258fbb26824fd104ed795e5d9033d2d040049ee"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"e6e919043fa85ee891123768dd655a98edbbf63c":["37a0f60745e53927c4c876cfe5b5a58170f0646c","c83d6c4335f31cae14f625a222bc842f20073dcd"],"69e043c521d4e8db770cc140c63f5ef51f03426a":["e6e919043fa85ee891123768dd655a98edbbf63c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["37a0f60745e53927c4c876cfe5b5a58170f0646c","a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}