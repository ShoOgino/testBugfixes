{"path":"src/java/org/apache/lucene/index/IndexWriter#applyDeletes(SegmentInfo).mjava","commits":[{"id":"e82780afe6097066eb5befb86e9432f077667e3d","date":1202756169,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#applyDeletes(SegmentInfo).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#applyDeletes(boolean).mjava","sourceNew":"  // Called during flush to apply any buffered deletes.  If\n  // flushedNewSegment is true then a new segment was just\n  // created and flushed from the ram segments, so we will\n  // selectively apply the deletes to that new segment.\n  private final void applyDeletes(SegmentInfo newSegment) throws CorruptIndexException, IOException {\n\n    final HashMap bufferedDeleteTerms = docWriter.getBufferedDeleteTerms();\n    final List bufferedDeleteDocIDs = docWriter.getBufferedDeleteDocIDs();\n\n    if (infoStream != null)\n      message(\"flush \" + docWriter.getNumBufferedDeleteTerms() + \" buffered deleted terms and \" +\n              bufferedDeleteDocIDs.size() + \" deleted docIDs on \"\n              + segmentInfos.size() + \" segments.\");\n\n    if (newSegment != null) {\n      IndexReader reader = null;\n      try {\n        // Open readers w/o opening the stored fields /\n        // vectors because these files may still be held\n        // open for writing by docWriter\n        reader = SegmentReader.get(newSegment, false);\n\n        // Apply delete terms to the segment just flushed from ram\n        // apply appropriately so that a delete term is only applied to\n        // the documents buffered before it, not those buffered after it.\n        applyDeletesSelectively(bufferedDeleteTerms, bufferedDeleteDocIDs, reader);\n      } finally {\n        if (reader != null) {\n          try {\n            reader.doCommit();\n          } finally {\n            reader.doClose();\n          }\n        }\n      }\n    }\n\n    final int infosEnd = segmentInfos.size();\n\n    for (int i = 0; i < infosEnd; i++) {\n      IndexReader reader = null;\n      try {\n        reader = SegmentReader.get(segmentInfos.info(i), false);\n\n        // Apply delete terms to disk segments\n        // except the one just flushed from ram.\n        applyDeletes(bufferedDeleteTerms, reader);\n      } finally {\n        if (reader != null) {\n          try {\n            reader.doCommit();\n          } finally {\n            reader.doClose();\n          }\n        }\n      }\n    }\n  }\n\n","sourceOld":"  // Called during flush to apply any buffered deletes.  If\n  // flushedNewSegment is true then a new segment was just\n  // created and flushed from the ram segments, so we will\n  // selectively apply the deletes to that new segment.\n  private final void applyDeletes(boolean flushedNewSegment) throws CorruptIndexException, IOException {\n\n    final HashMap bufferedDeleteTerms = docWriter.getBufferedDeleteTerms();\n    final List bufferedDeleteDocIDs = docWriter.getBufferedDeleteDocIDs();\n\n    if (infoStream != null)\n      message(\"flush \" + docWriter.getNumBufferedDeleteTerms() + \" buffered deleted terms and \" +\n              bufferedDeleteDocIDs.size() + \" deleted docIDs on \"\n              + segmentInfos.size() + \" segments.\");\n\n    if (flushedNewSegment) {\n      IndexReader reader = null;\n      try {\n        // Open readers w/o opening the stored fields /\n        // vectors because these files may still be held\n        // open for writing by docWriter\n        reader = SegmentReader.get(segmentInfos.info(segmentInfos.size() - 1), false);\n\n        // Apply delete terms to the segment just flushed from ram\n        // apply appropriately so that a delete term is only applied to\n        // the documents buffered before it, not those buffered after it.\n        applyDeletesSelectively(bufferedDeleteTerms, bufferedDeleteDocIDs, reader);\n      } finally {\n        if (reader != null) {\n          try {\n            reader.doCommit();\n          } finally {\n            reader.doClose();\n          }\n        }\n      }\n    }\n\n    int infosEnd = segmentInfos.size();\n    if (flushedNewSegment) {\n      infosEnd--;\n    }\n\n    for (int i = 0; i < infosEnd; i++) {\n      IndexReader reader = null;\n      try {\n        reader = SegmentReader.get(segmentInfos.info(i), false);\n\n        // Apply delete terms to disk segments\n        // except the one just flushed from ram.\n        applyDeletes(bufferedDeleteTerms, reader);\n      } finally {\n        if (reader != null) {\n          try {\n            reader.doCommit();\n          } finally {\n            reader.doClose();\n          }\n        }\n      }\n    }\n\n    // Clean up bufferedDeleteTerms.\n    docWriter.clearBufferedDeletes();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be","date":1204801324,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"src/java/org/apache/lucene/index/IndexWriter#applyDeletes(SegmentInfo).mjava","sourceNew":null,"sourceOld":"  // Called during flush to apply any buffered deletes.  If\n  // flushedNewSegment is true then a new segment was just\n  // created and flushed from the ram segments, so we will\n  // selectively apply the deletes to that new segment.\n  private final void applyDeletes(SegmentInfo newSegment) throws CorruptIndexException, IOException {\n\n    final HashMap bufferedDeleteTerms = docWriter.getBufferedDeleteTerms();\n    final List bufferedDeleteDocIDs = docWriter.getBufferedDeleteDocIDs();\n\n    if (infoStream != null)\n      message(\"flush \" + docWriter.getNumBufferedDeleteTerms() + \" buffered deleted terms and \" +\n              bufferedDeleteDocIDs.size() + \" deleted docIDs on \"\n              + segmentInfos.size() + \" segments.\");\n\n    if (newSegment != null) {\n      IndexReader reader = null;\n      try {\n        // Open readers w/o opening the stored fields /\n        // vectors because these files may still be held\n        // open for writing by docWriter\n        reader = SegmentReader.get(newSegment, false);\n\n        // Apply delete terms to the segment just flushed from ram\n        // apply appropriately so that a delete term is only applied to\n        // the documents buffered before it, not those buffered after it.\n        applyDeletesSelectively(bufferedDeleteTerms, bufferedDeleteDocIDs, reader);\n      } finally {\n        if (reader != null) {\n          try {\n            reader.doCommit();\n          } finally {\n            reader.doClose();\n          }\n        }\n      }\n    }\n\n    final int infosEnd = segmentInfos.size();\n\n    for (int i = 0; i < infosEnd; i++) {\n      IndexReader reader = null;\n      try {\n        reader = SegmentReader.get(segmentInfos.info(i), false);\n\n        // Apply delete terms to disk segments\n        // except the one just flushed from ram.\n        applyDeletes(bufferedDeleteTerms, reader);\n      } finally {\n        if (reader != null) {\n          try {\n            reader.doCommit();\n          } finally {\n            reader.doClose();\n          }\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be":["e82780afe6097066eb5befb86e9432f077667e3d"],"e82780afe6097066eb5befb86e9432f077667e3d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["e82780afe6097066eb5befb86e9432f077667e3d"],"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e82780afe6097066eb5befb86e9432f077667e3d":["a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}