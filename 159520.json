{"path":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testRandom().mjava","commits":[{"id":"defc703c2a146fe90a612765e6ca29b9187a8fec","date":1425369902,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testRandom().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom() throws IOException {\n    Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TextField f = new TextField(\"foo\", \"foo\", Store.NO);\n    doc.add(f);\n    w.addDocument(doc);\n    IndexReader reader = w.getReader();\n\n    final int maxSize = TestUtil.nextInt(random(), 1, 10000);\n    final long maxRamBytesUsed = TestUtil.nextLong(random(), 1, 5000000);\n    final LRUQueryCache queryCache = new LRUQueryCache(maxSize, maxRamBytesUsed);\n    IndexSearcher uncachedSearcher = null;\n    IndexSearcher cachedSearcher = null;\n\n    final int iters = atLeast(20000);\n    for (int i = 0; i < iters; ++i) {\n      if (i == 0 || random().nextInt(100) == 1) {\n        reader.close();\n        f.setStringValue(RandomPicks.randomFrom(random(), Arrays.asList(\"foo\", \"bar\", \"bar baz\")));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.deleteDocuments(buildRandomQuery(0));\n        }\n        reader = w.getReader();\n        uncachedSearcher = newSearcher(reader);\n        uncachedSearcher.setQueryCache(null);\n        cachedSearcher = newSearcher(reader);\n        cachedSearcher.setQueryCache(queryCache);\n        cachedSearcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n      }\n      final Query q = buildRandomQuery(0);\n      assertEquals(uncachedSearcher.count(q), cachedSearcher.count(q));\n      if (rarely()) {\n        queryCache.assertConsistent();\n      }\n    }\n    queryCache.assertConsistent();\n    w.close();\n    reader.close();\n    dir.close();\n    queryCache.assertConsistent();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":0,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testRandom().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom() throws IOException {\n    Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TextField f = new TextField(\"foo\", \"foo\", Store.NO);\n    doc.add(f);\n    w.addDocument(doc);\n    IndexReader reader = w.getReader();\n\n    final int maxSize = TestUtil.nextInt(random(), 1, 10000);\n    final long maxRamBytesUsed = TestUtil.nextLong(random(), 1, 5000000);\n    final LRUQueryCache queryCache = new LRUQueryCache(maxSize, maxRamBytesUsed);\n    IndexSearcher uncachedSearcher = null;\n    IndexSearcher cachedSearcher = null;\n\n    final int iters = atLeast(20000);\n    for (int i = 0; i < iters; ++i) {\n      if (i == 0 || random().nextInt(100) == 1) {\n        reader.close();\n        f.setStringValue(RandomPicks.randomFrom(random(), Arrays.asList(\"foo\", \"bar\", \"bar baz\")));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.deleteDocuments(buildRandomQuery(0));\n        }\n        reader = w.getReader();\n        uncachedSearcher = newSearcher(reader);\n        uncachedSearcher.setQueryCache(null);\n        cachedSearcher = newSearcher(reader);\n        cachedSearcher.setQueryCache(queryCache);\n        cachedSearcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n      }\n      final Query q = buildRandomQuery(0);\n      assertEquals(uncachedSearcher.count(q), cachedSearcher.count(q));\n      if (rarely()) {\n        queryCache.assertConsistent();\n      }\n    }\n    queryCache.assertConsistent();\n    w.close();\n    reader.close();\n    dir.close();\n    queryCache.assertConsistent();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7ac93f1b4f66dba87b555cd5815ecbfe0060dae","date":1429483620,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testRandom().mjava","sourceNew":"  public void testRandom() throws IOException {\n    Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TextField f = new TextField(\"foo\", \"foo\", Store.NO);\n    doc.add(f);\n    w.addDocument(doc);\n    IndexReader reader = w.getReader();\n    \n    final int maxSize;\n    final long maxRamBytesUsed;\n    final int iters;\n    \n    if (TEST_NIGHTLY) {\n      maxSize = TestUtil.nextInt(random(), 1, 10000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 5000000);\n      iters = atLeast(20000);\n    } else {\n      maxSize = TestUtil.nextInt(random(), 1, 1000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 500000);\n      iters = atLeast(2000);\n    }\n\n    final LRUQueryCache queryCache = new LRUQueryCache(maxSize, maxRamBytesUsed);\n    IndexSearcher uncachedSearcher = null;\n    IndexSearcher cachedSearcher = null;\n\n    for (int i = 0; i < iters; ++i) {\n      if (i == 0 || random().nextInt(100) == 1) {\n        reader.close();\n        f.setStringValue(RandomPicks.randomFrom(random(), Arrays.asList(\"foo\", \"bar\", \"bar baz\")));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.deleteDocuments(buildRandomQuery(0));\n        }\n        reader = w.getReader();\n        uncachedSearcher = newSearcher(reader);\n        uncachedSearcher.setQueryCache(null);\n        cachedSearcher = newSearcher(reader);\n        cachedSearcher.setQueryCache(queryCache);\n        cachedSearcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n      }\n      final Query q = buildRandomQuery(0);\n      assertEquals(uncachedSearcher.count(q), cachedSearcher.count(q));\n      if (rarely()) {\n        queryCache.assertConsistent();\n      }\n    }\n    queryCache.assertConsistent();\n    w.close();\n    reader.close();\n    dir.close();\n    queryCache.assertConsistent();\n  }\n\n","sourceOld":"  public void testRandom() throws IOException {\n    Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TextField f = new TextField(\"foo\", \"foo\", Store.NO);\n    doc.add(f);\n    w.addDocument(doc);\n    IndexReader reader = w.getReader();\n\n    final int maxSize = TestUtil.nextInt(random(), 1, 10000);\n    final long maxRamBytesUsed = TestUtil.nextLong(random(), 1, 5000000);\n    final LRUQueryCache queryCache = new LRUQueryCache(maxSize, maxRamBytesUsed);\n    IndexSearcher uncachedSearcher = null;\n    IndexSearcher cachedSearcher = null;\n\n    final int iters = atLeast(20000);\n    for (int i = 0; i < iters; ++i) {\n      if (i == 0 || random().nextInt(100) == 1) {\n        reader.close();\n        f.setStringValue(RandomPicks.randomFrom(random(), Arrays.asList(\"foo\", \"bar\", \"bar baz\")));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.deleteDocuments(buildRandomQuery(0));\n        }\n        reader = w.getReader();\n        uncachedSearcher = newSearcher(reader);\n        uncachedSearcher.setQueryCache(null);\n        cachedSearcher = newSearcher(reader);\n        cachedSearcher.setQueryCache(queryCache);\n        cachedSearcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n      }\n      final Query q = buildRandomQuery(0);\n      assertEquals(uncachedSearcher.count(q), cachedSearcher.count(q));\n      if (rarely()) {\n        queryCache.assertConsistent();\n      }\n    }\n    queryCache.assertConsistent();\n    w.close();\n    reader.close();\n    dir.close();\n    queryCache.assertConsistent();\n  }\n\n","bugFix":null,"bugIntro":["631e24c389c59f74b6d125a2a4cb909d6fbfa356"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"631e24c389c59f74b6d125a2a4cb909d6fbfa356","date":1445957240,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testRandom().mjava","sourceNew":"  public void testRandom() throws IOException {\n    Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TextField f = new TextField(\"foo\", \"foo\", Store.NO);\n    doc.add(f);\n    w.addDocument(doc);\n    IndexReader reader = w.getReader();\n\n    final int maxSize;\n    final long maxRamBytesUsed;\n    final int iters;\n\n    if (TEST_NIGHTLY) {\n      maxSize = TestUtil.nextInt(random(), 1, 10000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 5000000);\n      iters = atLeast(20000);\n    } else {\n      maxSize = TestUtil.nextInt(random(), 1, 1000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 500000);\n      iters = atLeast(2000);\n    }\n\n    final LRUQueryCache queryCache = new LRUQueryCache(maxSize, maxRamBytesUsed);\n    IndexSearcher uncachedSearcher = null;\n    IndexSearcher cachedSearcher = null;\n\n    for (int i = 0; i < iters; ++i) {\n      if (i == 0 || random().nextInt(100) == 1) {\n        reader.close();\n        f.setStringValue(RandomPicks.randomFrom(random(), Arrays.asList(\"foo\", \"bar\", \"bar baz\")));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.deleteDocuments(buildRandomQuery(0));\n        }\n        reader = w.getReader();\n        uncachedSearcher = newSearcher(reader);\n        uncachedSearcher.setQueryCache(null);\n        cachedSearcher = newSearcher(reader);\n        cachedSearcher.setQueryCache(queryCache);\n        cachedSearcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n      }\n      final Query q = buildRandomQuery(0);\n      assertEquals(uncachedSearcher.count(q), cachedSearcher.count(q));\n      if (rarely()) {\n        queryCache.assertConsistent();\n      }\n    }\n    queryCache.assertConsistent();\n    w.close();\n    reader.close();\n    dir.close();\n    queryCache.assertConsistent();\n  }\n\n","sourceOld":"  public void testRandom() throws IOException {\n    Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TextField f = new TextField(\"foo\", \"foo\", Store.NO);\n    doc.add(f);\n    w.addDocument(doc);\n    IndexReader reader = w.getReader();\n    \n    final int maxSize;\n    final long maxRamBytesUsed;\n    final int iters;\n    \n    if (TEST_NIGHTLY) {\n      maxSize = TestUtil.nextInt(random(), 1, 10000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 5000000);\n      iters = atLeast(20000);\n    } else {\n      maxSize = TestUtil.nextInt(random(), 1, 1000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 500000);\n      iters = atLeast(2000);\n    }\n\n    final LRUQueryCache queryCache = new LRUQueryCache(maxSize, maxRamBytesUsed);\n    IndexSearcher uncachedSearcher = null;\n    IndexSearcher cachedSearcher = null;\n\n    for (int i = 0; i < iters; ++i) {\n      if (i == 0 || random().nextInt(100) == 1) {\n        reader.close();\n        f.setStringValue(RandomPicks.randomFrom(random(), Arrays.asList(\"foo\", \"bar\", \"bar baz\")));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.deleteDocuments(buildRandomQuery(0));\n        }\n        reader = w.getReader();\n        uncachedSearcher = newSearcher(reader);\n        uncachedSearcher.setQueryCache(null);\n        cachedSearcher = newSearcher(reader);\n        cachedSearcher.setQueryCache(queryCache);\n        cachedSearcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n      }\n      final Query q = buildRandomQuery(0);\n      assertEquals(uncachedSearcher.count(q), cachedSearcher.count(q));\n      if (rarely()) {\n        queryCache.assertConsistent();\n      }\n    }\n    queryCache.assertConsistent();\n    w.close();\n    reader.close();\n    dir.close();\n    queryCache.assertConsistent();\n  }\n\n","bugFix":["e7ac93f1b4f66dba87b555cd5815ecbfe0060dae"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aac61ee5b4492f174e60bd54939aba9539906edf","date":1461245473,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testRandom().mjava","sourceNew":"  public void testRandom() throws IOException {\n    Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TextField f = new TextField(\"foo\", \"foo\", Store.NO);\n    doc.add(f);\n    w.addDocument(doc);\n    IndexReader reader = w.getReader();\n\n    final int maxSize;\n    final long maxRamBytesUsed;\n    final int iters;\n\n    if (TEST_NIGHTLY) {\n      maxSize = TestUtil.nextInt(random(), 1, 10000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 5000000);\n      iters = atLeast(20000);\n    } else {\n      maxSize = TestUtil.nextInt(random(), 1, 1000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 500000);\n      iters = atLeast(2000);\n    }\n\n    final LRUQueryCache queryCache = new LRUQueryCache(maxSize, maxRamBytesUsed, context -> random().nextBoolean());\n    IndexSearcher uncachedSearcher = null;\n    IndexSearcher cachedSearcher = null;\n\n    for (int i = 0; i < iters; ++i) {\n      if (i == 0 || random().nextInt(100) == 1) {\n        reader.close();\n        f.setStringValue(RandomPicks.randomFrom(random(), Arrays.asList(\"foo\", \"bar\", \"bar baz\")));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.deleteDocuments(buildRandomQuery(0));\n        }\n        reader = w.getReader();\n        uncachedSearcher = newSearcher(reader);\n        uncachedSearcher.setQueryCache(null);\n        cachedSearcher = newSearcher(reader);\n        cachedSearcher.setQueryCache(queryCache);\n        cachedSearcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n      }\n      final Query q = buildRandomQuery(0);\n      assertEquals(uncachedSearcher.count(q), cachedSearcher.count(q));\n      if (rarely()) {\n        queryCache.assertConsistent();\n      }\n    }\n    queryCache.assertConsistent();\n    w.close();\n    reader.close();\n    dir.close();\n    queryCache.assertConsistent();\n  }\n\n","sourceOld":"  public void testRandom() throws IOException {\n    Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TextField f = new TextField(\"foo\", \"foo\", Store.NO);\n    doc.add(f);\n    w.addDocument(doc);\n    IndexReader reader = w.getReader();\n\n    final int maxSize;\n    final long maxRamBytesUsed;\n    final int iters;\n\n    if (TEST_NIGHTLY) {\n      maxSize = TestUtil.nextInt(random(), 1, 10000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 5000000);\n      iters = atLeast(20000);\n    } else {\n      maxSize = TestUtil.nextInt(random(), 1, 1000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 500000);\n      iters = atLeast(2000);\n    }\n\n    final LRUQueryCache queryCache = new LRUQueryCache(maxSize, maxRamBytesUsed);\n    IndexSearcher uncachedSearcher = null;\n    IndexSearcher cachedSearcher = null;\n\n    for (int i = 0; i < iters; ++i) {\n      if (i == 0 || random().nextInt(100) == 1) {\n        reader.close();\n        f.setStringValue(RandomPicks.randomFrom(random(), Arrays.asList(\"foo\", \"bar\", \"bar baz\")));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.deleteDocuments(buildRandomQuery(0));\n        }\n        reader = w.getReader();\n        uncachedSearcher = newSearcher(reader);\n        uncachedSearcher.setQueryCache(null);\n        cachedSearcher = newSearcher(reader);\n        cachedSearcher.setQueryCache(queryCache);\n        cachedSearcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n      }\n      final Query q = buildRandomQuery(0);\n      assertEquals(uncachedSearcher.count(q), cachedSearcher.count(q));\n      if (rarely()) {\n        queryCache.assertConsistent();\n      }\n    }\n    queryCache.assertConsistent();\n    w.close();\n    reader.close();\n    dir.close();\n    queryCache.assertConsistent();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c7732a106554be0db3e03ac5211e46f6e0c285b8","date":1511975378,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testRandom().mjava","sourceNew":"  public void testRandom() throws IOException {\n    Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TextField f = new TextField(\"foo\", \"foo\", Store.NO);\n    doc.add(f);\n    w.addDocument(doc);\n    IndexReader reader = w.getReader();\n\n    final int maxSize;\n    final long maxRamBytesUsed;\n    final int iters;\n\n    if (TEST_NIGHTLY) {\n      maxSize = TestUtil.nextInt(random(), 1, 10000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 5000000);\n      iters = atLeast(20000);\n    } else {\n      maxSize = TestUtil.nextInt(random(), 1, 1000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 500000);\n      iters = atLeast(2000);\n    }\n\n    final LRUQueryCache queryCache = new LRUQueryCache(maxSize, maxRamBytesUsed, context -> random().nextBoolean(), Float.POSITIVE_INFINITY);\n    IndexSearcher uncachedSearcher = null;\n    IndexSearcher cachedSearcher = null;\n\n    for (int i = 0; i < iters; ++i) {\n      if (i == 0 || random().nextInt(100) == 1) {\n        reader.close();\n        f.setStringValue(RandomPicks.randomFrom(random(), Arrays.asList(\"foo\", \"bar\", \"bar baz\")));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.deleteDocuments(buildRandomQuery(0));\n        }\n        reader = w.getReader();\n        uncachedSearcher = newSearcher(reader);\n        uncachedSearcher.setQueryCache(null);\n        cachedSearcher = newSearcher(reader);\n        cachedSearcher.setQueryCache(queryCache);\n        cachedSearcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n      }\n      final Query q = buildRandomQuery(0);\n      assertEquals(uncachedSearcher.count(q), cachedSearcher.count(q));\n      if (rarely()) {\n        queryCache.assertConsistent();\n      }\n    }\n    queryCache.assertConsistent();\n    w.close();\n    reader.close();\n    dir.close();\n    queryCache.assertConsistent();\n  }\n\n","sourceOld":"  public void testRandom() throws IOException {\n    Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TextField f = new TextField(\"foo\", \"foo\", Store.NO);\n    doc.add(f);\n    w.addDocument(doc);\n    IndexReader reader = w.getReader();\n\n    final int maxSize;\n    final long maxRamBytesUsed;\n    final int iters;\n\n    if (TEST_NIGHTLY) {\n      maxSize = TestUtil.nextInt(random(), 1, 10000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 5000000);\n      iters = atLeast(20000);\n    } else {\n      maxSize = TestUtil.nextInt(random(), 1, 1000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 500000);\n      iters = atLeast(2000);\n    }\n\n    final LRUQueryCache queryCache = new LRUQueryCache(maxSize, maxRamBytesUsed, context -> random().nextBoolean());\n    IndexSearcher uncachedSearcher = null;\n    IndexSearcher cachedSearcher = null;\n\n    for (int i = 0; i < iters; ++i) {\n      if (i == 0 || random().nextInt(100) == 1) {\n        reader.close();\n        f.setStringValue(RandomPicks.randomFrom(random(), Arrays.asList(\"foo\", \"bar\", \"bar baz\")));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.deleteDocuments(buildRandomQuery(0));\n        }\n        reader = w.getReader();\n        uncachedSearcher = newSearcher(reader);\n        uncachedSearcher.setQueryCache(null);\n        cachedSearcher = newSearcher(reader);\n        cachedSearcher.setQueryCache(queryCache);\n        cachedSearcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n      }\n      final Query q = buildRandomQuery(0);\n      assertEquals(uncachedSearcher.count(q), cachedSearcher.count(q));\n      if (rarely()) {\n        queryCache.assertConsistent();\n      }\n    }\n    queryCache.assertConsistent();\n    w.close();\n    reader.close();\n    dir.close();\n    queryCache.assertConsistent();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1aad05eeff7818b0833c02ac6b743aa72054963b","date":1512093122,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testRandom().mjava","sourceNew":"  public void testRandom() throws IOException {\n    Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TextField f = new TextField(\"foo\", \"foo\", Store.NO);\n    doc.add(f);\n    w.addDocument(doc);\n    IndexReader reader = w.getReader();\n\n    final int maxSize;\n    final long maxRamBytesUsed;\n    final int iters;\n\n    if (TEST_NIGHTLY) {\n      maxSize = TestUtil.nextInt(random(), 1, 10000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 5000000);\n      iters = atLeast(20000);\n    } else {\n      maxSize = TestUtil.nextInt(random(), 1, 1000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 500000);\n      iters = atLeast(2000);\n    }\n\n    final LRUQueryCache queryCache = new LRUQueryCache(maxSize, maxRamBytesUsed, context -> random().nextBoolean(), Float.POSITIVE_INFINITY);\n    IndexSearcher uncachedSearcher = null;\n    IndexSearcher cachedSearcher = null;\n\n    for (int i = 0; i < iters; ++i) {\n      if (i == 0 || random().nextInt(100) == 1) {\n        reader.close();\n        f.setStringValue(RandomPicks.randomFrom(random(), Arrays.asList(\"foo\", \"bar\", \"bar baz\")));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.deleteDocuments(buildRandomQuery(0));\n        }\n        reader = w.getReader();\n        uncachedSearcher = newSearcher(reader);\n        uncachedSearcher.setQueryCache(null);\n        cachedSearcher = newSearcher(reader);\n        cachedSearcher.setQueryCache(queryCache);\n        cachedSearcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n      }\n      final Query q = buildRandomQuery(0);\n      assertEquals(uncachedSearcher.count(q), cachedSearcher.count(q));\n      if (rarely()) {\n        queryCache.assertConsistent();\n      }\n    }\n    queryCache.assertConsistent();\n    w.close();\n    reader.close();\n    dir.close();\n    queryCache.assertConsistent();\n  }\n\n","sourceOld":"  public void testRandom() throws IOException {\n    Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TextField f = new TextField(\"foo\", \"foo\", Store.NO);\n    doc.add(f);\n    w.addDocument(doc);\n    IndexReader reader = w.getReader();\n\n    final int maxSize;\n    final long maxRamBytesUsed;\n    final int iters;\n\n    if (TEST_NIGHTLY) {\n      maxSize = TestUtil.nextInt(random(), 1, 10000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 5000000);\n      iters = atLeast(20000);\n    } else {\n      maxSize = TestUtil.nextInt(random(), 1, 1000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 500000);\n      iters = atLeast(2000);\n    }\n\n    final LRUQueryCache queryCache = new LRUQueryCache(maxSize, maxRamBytesUsed, context -> random().nextBoolean());\n    IndexSearcher uncachedSearcher = null;\n    IndexSearcher cachedSearcher = null;\n\n    for (int i = 0; i < iters; ++i) {\n      if (i == 0 || random().nextInt(100) == 1) {\n        reader.close();\n        f.setStringValue(RandomPicks.randomFrom(random(), Arrays.asList(\"foo\", \"bar\", \"bar baz\")));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.deleteDocuments(buildRandomQuery(0));\n        }\n        reader = w.getReader();\n        uncachedSearcher = newSearcher(reader);\n        uncachedSearcher.setQueryCache(null);\n        cachedSearcher = newSearcher(reader);\n        cachedSearcher.setQueryCache(queryCache);\n        cachedSearcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n      }\n      final Query q = buildRandomQuery(0);\n      assertEquals(uncachedSearcher.count(q), cachedSearcher.count(q));\n      if (rarely()) {\n        queryCache.assertConsistent();\n      }\n    }\n    queryCache.assertConsistent();\n    w.close();\n    reader.close();\n    dir.close();\n    queryCache.assertConsistent();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"789fb338d3c53b4478938723d60f6623e764ca38","date":1521535944,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testRandom().mjava","sourceNew":"  public void testRandom() throws IOException {\n    Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TextField f = new TextField(\"foo\", \"foo\", Store.NO);\n    doc.add(f);\n    w.addDocument(doc);\n    IndexReader reader = w.getReader();\n\n    final int maxSize;\n    final long maxRamBytesUsed;\n    final int iters;\n\n    if (TEST_NIGHTLY) {\n      maxSize = TestUtil.nextInt(random(), 1, 10000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 5000000);\n      iters = atLeast(20000);\n    } else {\n      maxSize = TestUtil.nextInt(random(), 1, 1000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 500000);\n      iters = atLeast(2000);\n    }\n\n    final LRUQueryCache queryCache = new LRUQueryCache(maxSize, maxRamBytesUsed, context -> random().nextBoolean());\n    IndexSearcher uncachedSearcher = null;\n    IndexSearcher cachedSearcher = null;\n\n    for (int i = 0; i < iters; ++i) {\n      if (i == 0 || random().nextInt(100) == 1) {\n        reader.close();\n        f.setStringValue(RandomPicks.randomFrom(random(), Arrays.asList(\"foo\", \"bar\", \"bar baz\")));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.deleteDocuments(buildRandomQuery(0));\n        }\n        reader = w.getReader();\n        uncachedSearcher = newSearcher(reader);\n        uncachedSearcher.setQueryCache(null);\n        cachedSearcher = newSearcher(reader);\n        cachedSearcher.setQueryCache(queryCache);\n        cachedSearcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n      }\n      final Query q = buildRandomQuery(0);\n      assertEquals(uncachedSearcher.count(q), cachedSearcher.count(q));\n      if (rarely()) {\n        queryCache.assertConsistent();\n      }\n    }\n    queryCache.assertConsistent();\n    w.close();\n    reader.close();\n    dir.close();\n    queryCache.assertConsistent();\n  }\n\n","sourceOld":"  public void testRandom() throws IOException {\n    Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TextField f = new TextField(\"foo\", \"foo\", Store.NO);\n    doc.add(f);\n    w.addDocument(doc);\n    IndexReader reader = w.getReader();\n\n    final int maxSize;\n    final long maxRamBytesUsed;\n    final int iters;\n\n    if (TEST_NIGHTLY) {\n      maxSize = TestUtil.nextInt(random(), 1, 10000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 5000000);\n      iters = atLeast(20000);\n    } else {\n      maxSize = TestUtil.nextInt(random(), 1, 1000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 500000);\n      iters = atLeast(2000);\n    }\n\n    final LRUQueryCache queryCache = new LRUQueryCache(maxSize, maxRamBytesUsed, context -> random().nextBoolean(), Float.POSITIVE_INFINITY);\n    IndexSearcher uncachedSearcher = null;\n    IndexSearcher cachedSearcher = null;\n\n    for (int i = 0; i < iters; ++i) {\n      if (i == 0 || random().nextInt(100) == 1) {\n        reader.close();\n        f.setStringValue(RandomPicks.randomFrom(random(), Arrays.asList(\"foo\", \"bar\", \"bar baz\")));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.deleteDocuments(buildRandomQuery(0));\n        }\n        reader = w.getReader();\n        uncachedSearcher = newSearcher(reader);\n        uncachedSearcher.setQueryCache(null);\n        cachedSearcher = newSearcher(reader);\n        cachedSearcher.setQueryCache(queryCache);\n        cachedSearcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n      }\n      final Query q = buildRandomQuery(0);\n      assertEquals(uncachedSearcher.count(q), cachedSearcher.count(q));\n      if (rarely()) {\n        queryCache.assertConsistent();\n      }\n    }\n    queryCache.assertConsistent();\n    w.close();\n    reader.close();\n    dir.close();\n    queryCache.assertConsistent();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6815b5b5d6334b2245dd7be2f8b6cca949bf7f43","date":1521731438,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testRandom().mjava","sourceNew":"  public void testRandom() throws IOException {\n    Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TextField f = new TextField(\"foo\", \"foo\", Store.NO);\n    doc.add(f);\n    w.addDocument(doc);\n    IndexReader reader = w.getReader();\n\n    final int maxSize;\n    final long maxRamBytesUsed;\n    final int iters;\n\n    if (TEST_NIGHTLY) {\n      maxSize = TestUtil.nextInt(random(), 1, 10000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 5000000);\n      iters = atLeast(20000);\n    } else {\n      maxSize = TestUtil.nextInt(random(), 1, 1000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 500000);\n      iters = atLeast(2000);\n    }\n\n    final LRUQueryCache queryCache = new LRUQueryCache(maxSize, maxRamBytesUsed, context -> random().nextBoolean());\n    IndexSearcher uncachedSearcher = null;\n    IndexSearcher cachedSearcher = null;\n\n    for (int i = 0; i < iters; ++i) {\n      if (i == 0 || random().nextInt(100) == 1) {\n        reader.close();\n        f.setStringValue(RandomPicks.randomFrom(random(), Arrays.asList(\"foo\", \"bar\", \"bar baz\")));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.deleteDocuments(buildRandomQuery(0));\n        }\n        reader = w.getReader();\n        uncachedSearcher = newSearcher(reader);\n        uncachedSearcher.setQueryCache(null);\n        cachedSearcher = newSearcher(reader);\n        cachedSearcher.setQueryCache(queryCache);\n        cachedSearcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n      }\n      final Query q = buildRandomQuery(0);\n      assertEquals(uncachedSearcher.count(q), cachedSearcher.count(q));\n      if (rarely()) {\n        queryCache.assertConsistent();\n      }\n    }\n    queryCache.assertConsistent();\n    w.close();\n    reader.close();\n    dir.close();\n    queryCache.assertConsistent();\n  }\n\n","sourceOld":"  public void testRandom() throws IOException {\n    Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TextField f = new TextField(\"foo\", \"foo\", Store.NO);\n    doc.add(f);\n    w.addDocument(doc);\n    IndexReader reader = w.getReader();\n\n    final int maxSize;\n    final long maxRamBytesUsed;\n    final int iters;\n\n    if (TEST_NIGHTLY) {\n      maxSize = TestUtil.nextInt(random(), 1, 10000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 5000000);\n      iters = atLeast(20000);\n    } else {\n      maxSize = TestUtil.nextInt(random(), 1, 1000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 500000);\n      iters = atLeast(2000);\n    }\n\n    final LRUQueryCache queryCache = new LRUQueryCache(maxSize, maxRamBytesUsed, context -> random().nextBoolean(), Float.POSITIVE_INFINITY);\n    IndexSearcher uncachedSearcher = null;\n    IndexSearcher cachedSearcher = null;\n\n    for (int i = 0; i < iters; ++i) {\n      if (i == 0 || random().nextInt(100) == 1) {\n        reader.close();\n        f.setStringValue(RandomPicks.randomFrom(random(), Arrays.asList(\"foo\", \"bar\", \"bar baz\")));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.deleteDocuments(buildRandomQuery(0));\n        }\n        reader = w.getReader();\n        uncachedSearcher = newSearcher(reader);\n        uncachedSearcher.setQueryCache(null);\n        cachedSearcher = newSearcher(reader);\n        cachedSearcher.setQueryCache(queryCache);\n        cachedSearcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n      }\n      final Query q = buildRandomQuery(0);\n      assertEquals(uncachedSearcher.count(q), cachedSearcher.count(q));\n      if (rarely()) {\n        queryCache.assertConsistent();\n      }\n    }\n    queryCache.assertConsistent();\n    w.close();\n    reader.close();\n    dir.close();\n    queryCache.assertConsistent();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6c9d00c591703058371b3dc36f4957a6f24ca302","date":1527233410,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testRandom().mjava","sourceNew":"  public void testRandom() throws IOException {\n    Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TextField f = new TextField(\"foo\", \"foo\", Store.NO);\n    doc.add(f);\n    w.addDocument(doc);\n    IndexReader reader = w.getReader();\n\n    final int maxSize;\n    final long maxRamBytesUsed;\n    final int iters;\n\n    if (TEST_NIGHTLY) {\n      maxSize = TestUtil.nextInt(random(), 1, 10000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 5000000);\n      iters = atLeast(20000);\n    } else {\n      maxSize = TestUtil.nextInt(random(), 1, 1000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 500000);\n      iters = atLeast(2000);\n    }\n\n    final LRUQueryCache queryCache = new LRUQueryCache(maxSize, maxRamBytesUsed, context -> random().nextBoolean());\n    IndexSearcher uncachedSearcher = null;\n    IndexSearcher cachedSearcher = null;\n\n    for (int i = 0; i < iters; ++i) {\n      if (i == 0 || random().nextInt(100) == 1) {\n        reader.close();\n        f.setStringValue(RandomPicks.randomFrom(random(), Arrays.asList(\"foo\", \"bar\", \"bar baz\")));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.deleteDocuments(buildRandomQuery(0));\n        }\n        reader = w.getReader();\n        uncachedSearcher = newSearcher(reader);\n        uncachedSearcher.setQueryCache(null);\n        cachedSearcher = newSearcher(reader);\n        cachedSearcher.setQueryCache(queryCache);\n        cachedSearcher.setQueryCachingPolicy(ALWAYS_CACHE);\n      }\n      final Query q = buildRandomQuery(0);\n      assertEquals(uncachedSearcher.count(q), cachedSearcher.count(q));\n      if (rarely()) {\n        queryCache.assertConsistent();\n      }\n    }\n    queryCache.assertConsistent();\n    w.close();\n    reader.close();\n    dir.close();\n    queryCache.assertConsistent();\n  }\n\n","sourceOld":"  public void testRandom() throws IOException {\n    Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TextField f = new TextField(\"foo\", \"foo\", Store.NO);\n    doc.add(f);\n    w.addDocument(doc);\n    IndexReader reader = w.getReader();\n\n    final int maxSize;\n    final long maxRamBytesUsed;\n    final int iters;\n\n    if (TEST_NIGHTLY) {\n      maxSize = TestUtil.nextInt(random(), 1, 10000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 5000000);\n      iters = atLeast(20000);\n    } else {\n      maxSize = TestUtil.nextInt(random(), 1, 1000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 500000);\n      iters = atLeast(2000);\n    }\n\n    final LRUQueryCache queryCache = new LRUQueryCache(maxSize, maxRamBytesUsed, context -> random().nextBoolean());\n    IndexSearcher uncachedSearcher = null;\n    IndexSearcher cachedSearcher = null;\n\n    for (int i = 0; i < iters; ++i) {\n      if (i == 0 || random().nextInt(100) == 1) {\n        reader.close();\n        f.setStringValue(RandomPicks.randomFrom(random(), Arrays.asList(\"foo\", \"bar\", \"bar baz\")));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.deleteDocuments(buildRandomQuery(0));\n        }\n        reader = w.getReader();\n        uncachedSearcher = newSearcher(reader);\n        uncachedSearcher.setQueryCache(null);\n        cachedSearcher = newSearcher(reader);\n        cachedSearcher.setQueryCache(queryCache);\n        cachedSearcher.setQueryCachingPolicy(QueryCachingPolicy.ALWAYS_CACHE);\n      }\n      final Query q = buildRandomQuery(0);\n      assertEquals(uncachedSearcher.count(q), cachedSearcher.count(q));\n      if (rarely()) {\n        queryCache.assertConsistent();\n      }\n    }\n    queryCache.assertConsistent();\n    w.close();\n    reader.close();\n    dir.close();\n    queryCache.assertConsistent();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b5754bd6f04f13b67e9575f8b226a0303c31c7d5","date":1573506453,"type":3,"author":"ginger","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testRandom().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestLRUQueryCache#testRandom().mjava","sourceNew":"  public void testRandom() throws IOException {\n    Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TextField f = new TextField(\"foo\", \"foo\", Store.NO);\n    doc.add(f);\n    w.addDocument(doc);\n    IndexReader reader = w.getReader();\n\n    final int maxSize;\n    final long maxRamBytesUsed;\n    final int iters;\n\n    if (TEST_NIGHTLY) {\n      maxSize = TestUtil.nextInt(random(), 1, 10000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 5000000);\n      iters = atLeast(20000);\n    } else {\n      maxSize = TestUtil.nextInt(random(), 1, 1000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 500000);\n      iters = atLeast(2000);\n    }\n\n    final LRUQueryCache queryCache = new LRUQueryCache(maxSize, maxRamBytesUsed, context -> random().nextBoolean(), Float.POSITIVE_INFINITY);\n    IndexSearcher uncachedSearcher = null;\n    IndexSearcher cachedSearcher = null;\n\n    for (int i = 0; i < iters; ++i) {\n      if (i == 0 || random().nextInt(100) == 1) {\n        reader.close();\n        f.setStringValue(RandomPicks.randomFrom(random(), Arrays.asList(\"foo\", \"bar\", \"bar baz\")));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.deleteDocuments(buildRandomQuery(0));\n        }\n        reader = w.getReader();\n        uncachedSearcher = newSearcher(reader);\n        uncachedSearcher.setQueryCache(null);\n        cachedSearcher = newSearcher(reader);\n        cachedSearcher.setQueryCache(queryCache);\n        cachedSearcher.setQueryCachingPolicy(ALWAYS_CACHE);\n      }\n      final Query q = buildRandomQuery(0);\n      assertEquals(uncachedSearcher.count(q), cachedSearcher.count(q));\n      if (rarely()) {\n        queryCache.assertConsistent();\n      }\n    }\n    queryCache.assertConsistent();\n    w.close();\n    reader.close();\n    dir.close();\n    queryCache.assertConsistent();\n  }\n\n","sourceOld":"  public void testRandom() throws IOException {\n    Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    Document doc = new Document();\n    TextField f = new TextField(\"foo\", \"foo\", Store.NO);\n    doc.add(f);\n    w.addDocument(doc);\n    IndexReader reader = w.getReader();\n\n    final int maxSize;\n    final long maxRamBytesUsed;\n    final int iters;\n\n    if (TEST_NIGHTLY) {\n      maxSize = TestUtil.nextInt(random(), 1, 10000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 5000000);\n      iters = atLeast(20000);\n    } else {\n      maxSize = TestUtil.nextInt(random(), 1, 1000);\n      maxRamBytesUsed = TestUtil.nextLong(random(), 1, 500000);\n      iters = atLeast(2000);\n    }\n\n    final LRUQueryCache queryCache = new LRUQueryCache(maxSize, maxRamBytesUsed, context -> random().nextBoolean());\n    IndexSearcher uncachedSearcher = null;\n    IndexSearcher cachedSearcher = null;\n\n    for (int i = 0; i < iters; ++i) {\n      if (i == 0 || random().nextInt(100) == 1) {\n        reader.close();\n        f.setStringValue(RandomPicks.randomFrom(random(), Arrays.asList(\"foo\", \"bar\", \"bar baz\")));\n        w.addDocument(doc);\n        if (random().nextBoolean()) {\n          w.deleteDocuments(buildRandomQuery(0));\n        }\n        reader = w.getReader();\n        uncachedSearcher = newSearcher(reader);\n        uncachedSearcher.setQueryCache(null);\n        cachedSearcher = newSearcher(reader);\n        cachedSearcher.setQueryCache(queryCache);\n        cachedSearcher.setQueryCachingPolicy(ALWAYS_CACHE);\n      }\n      final Query q = buildRandomQuery(0);\n      assertEquals(uncachedSearcher.count(q), cachedSearcher.count(q));\n      if (rarely()) {\n        queryCache.assertConsistent();\n      }\n    }\n    queryCache.assertConsistent();\n    w.close();\n    reader.close();\n    dir.close();\n    queryCache.assertConsistent();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"6815b5b5d6334b2245dd7be2f8b6cca949bf7f43":["1aad05eeff7818b0833c02ac6b743aa72054963b","789fb338d3c53b4478938723d60f6623e764ca38"],"aac61ee5b4492f174e60bd54939aba9539906edf":["631e24c389c59f74b6d125a2a4cb909d6fbfa356"],"e7ac93f1b4f66dba87b555cd5815ecbfe0060dae":["defc703c2a146fe90a612765e6ca29b9187a8fec"],"6c9d00c591703058371b3dc36f4957a6f24ca302":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43"],"defc703c2a146fe90a612765e6ca29b9187a8fec":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"b5754bd6f04f13b67e9575f8b226a0303c31c7d5":["6c9d00c591703058371b3dc36f4957a6f24ca302"],"c7732a106554be0db3e03ac5211e46f6e0c285b8":["aac61ee5b4492f174e60bd54939aba9539906edf"],"1aad05eeff7818b0833c02ac6b743aa72054963b":["aac61ee5b4492f174e60bd54939aba9539906edf","c7732a106554be0db3e03ac5211e46f6e0c285b8"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","defc703c2a146fe90a612765e6ca29b9187a8fec"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"789fb338d3c53b4478938723d60f6623e764ca38":["1aad05eeff7818b0833c02ac6b743aa72054963b"],"631e24c389c59f74b6d125a2a4cb909d6fbfa356":["e7ac93f1b4f66dba87b555cd5815ecbfe0060dae"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b5754bd6f04f13b67e9575f8b226a0303c31c7d5"]},"commit2Childs":{"6815b5b5d6334b2245dd7be2f8b6cca949bf7f43":["6c9d00c591703058371b3dc36f4957a6f24ca302"],"aac61ee5b4492f174e60bd54939aba9539906edf":["c7732a106554be0db3e03ac5211e46f6e0c285b8","1aad05eeff7818b0833c02ac6b743aa72054963b"],"e7ac93f1b4f66dba87b555cd5815ecbfe0060dae":["631e24c389c59f74b6d125a2a4cb909d6fbfa356"],"6c9d00c591703058371b3dc36f4957a6f24ca302":["b5754bd6f04f13b67e9575f8b226a0303c31c7d5"],"defc703c2a146fe90a612765e6ca29b9187a8fec":["e7ac93f1b4f66dba87b555cd5815ecbfe0060dae","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"b5754bd6f04f13b67e9575f8b226a0303c31c7d5":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c7732a106554be0db3e03ac5211e46f6e0c285b8":["1aad05eeff7818b0833c02ac6b743aa72054963b"],"1aad05eeff7818b0833c02ac6b743aa72054963b":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43","789fb338d3c53b4478938723d60f6623e764ca38"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"789fb338d3c53b4478938723d60f6623e764ca38":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["defc703c2a146fe90a612765e6ca29b9187a8fec","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"631e24c389c59f74b6d125a2a4cb909d6fbfa356":["aac61ee5b4492f174e60bd54939aba9539906edf"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}