{"path":"src/java/org/apache/solr/search/ExtendedAnalyzer[ExtendedDismaxQParserPlugin]#tokenStream(String,Reader).mjava","commits":[{"id":"20349324eb18d1565d301e59be543989f38743d3","date":1258494398,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/ExtendedAnalyzer[ExtendedDismaxQParserPlugin]#tokenStream(String,Reader).mjava","pathOld":"/dev/null","sourceNew":"  public TokenStream tokenStream(String fieldName, Reader reader) {\n    if (!removeStopFilter) {\n      return queryAnalyzer.tokenStream(fieldName, reader);\n    }\n    \n    Analyzer a = map.get(fieldName);\n    if (a != null) {\n      return a.tokenStream(fieldName, reader);\n    }\n\n    FieldType ft = parser.getReq().getSchema().getFieldType(fieldName);\n    Analyzer qa = ft.getQueryAnalyzer();\n    if (!(qa instanceof TokenizerChain)) {\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n    TokenizerChain tcq = (TokenizerChain)qa;\n    Analyzer ia = ft.getAnalyzer();\n    if (ia == qa || !(ia instanceof TokenizerChain)) {\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n    TokenizerChain tci = (TokenizerChain)ia;\n\n    // make sure that there isn't a stop filter in the indexer\n    for (TokenFilterFactory tf : tci.getTokenFilterFactories()) {\n      if (tf instanceof StopFilterFactory) {\n        map.put(fieldName, qa);\n        return qa.tokenStream(fieldName, reader);\n      }\n    }\n\n    // now if there is a stop filter in the query analyzer, remove it\n    int stopIdx = -1;\n    TokenFilterFactory[] facs = tcq.getTokenFilterFactories();\n\n    for (int i=0; i<facs.length; i++) {\n      TokenFilterFactory tf = facs[i];\n      if (tf instanceof StopFilterFactory) {\n        stopIdx = i;\n        break;\n      }\n    }\n\n    if (stopIdx == -1) {\n      // no stop filter exists\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n\n    TokenFilterFactory[] newtf = new TokenFilterFactory[facs.length-1];\n    for (int i=0,j=0; i<facs.length; i++) {\n      if (i==stopIdx) continue;\n      newtf[j++] = facs[i];\n    }\n\n    TokenizerChain newa = new TokenizerChain(tcq.getTokenizerFactory(), newtf);\n    newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));\n\n    map.put(fieldName, newa);\n    return newa.tokenStream(fieldName, reader);        \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/ExtendedAnalyzer[ExtendedDismaxQParserPlugin]#tokenStream(String,Reader).mjava","pathOld":"src/java/org/apache/solr/search/ExtendedAnalyzer[ExtendedDismaxQParserPlugin]#tokenStream(String,Reader).mjava","sourceNew":"  public TokenStream tokenStream(String fieldName, Reader reader) {\n    if (!removeStopFilter) {\n      return queryAnalyzer.tokenStream(fieldName, reader);\n    }\n    \n    Analyzer a = map.get(fieldName);\n    if (a != null) {\n      return a.tokenStream(fieldName, reader);\n    }\n\n    FieldType ft = parser.getReq().getSchema().getFieldType(fieldName);\n    Analyzer qa = ft.getQueryAnalyzer();\n    if (!(qa instanceof TokenizerChain)) {\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n    TokenizerChain tcq = (TokenizerChain)qa;\n    Analyzer ia = ft.getAnalyzer();\n    if (ia == qa || !(ia instanceof TokenizerChain)) {\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n    TokenizerChain tci = (TokenizerChain)ia;\n\n    // make sure that there isn't a stop filter in the indexer\n    for (TokenFilterFactory tf : tci.getTokenFilterFactories()) {\n      if (tf instanceof StopFilterFactory) {\n        map.put(fieldName, qa);\n        return qa.tokenStream(fieldName, reader);\n      }\n    }\n\n    // now if there is a stop filter in the query analyzer, remove it\n    int stopIdx = -1;\n    TokenFilterFactory[] facs = tcq.getTokenFilterFactories();\n\n    for (int i=0; i<facs.length; i++) {\n      TokenFilterFactory tf = facs[i];\n      if (tf instanceof StopFilterFactory) {\n        stopIdx = i;\n        break;\n      }\n    }\n\n    if (stopIdx == -1) {\n      // no stop filter exists\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n\n    TokenFilterFactory[] newtf = new TokenFilterFactory[facs.length-1];\n    for (int i=0,j=0; i<facs.length; i++) {\n      if (i==stopIdx) continue;\n      newtf[j++] = facs[i];\n    }\n\n    TokenizerChain newa = new TokenizerChain(tcq.getTokenizerFactory(), newtf);\n    newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));\n\n    map.put(fieldName, newa);\n    return newa.tokenStream(fieldName, reader);        \n  }\n\n","sourceOld":"  public TokenStream tokenStream(String fieldName, Reader reader) {\n    if (!removeStopFilter) {\n      return queryAnalyzer.tokenStream(fieldName, reader);\n    }\n    \n    Analyzer a = map.get(fieldName);\n    if (a != null) {\n      return a.tokenStream(fieldName, reader);\n    }\n\n    FieldType ft = parser.getReq().getSchema().getFieldType(fieldName);\n    Analyzer qa = ft.getQueryAnalyzer();\n    if (!(qa instanceof TokenizerChain)) {\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n    TokenizerChain tcq = (TokenizerChain)qa;\n    Analyzer ia = ft.getAnalyzer();\n    if (ia == qa || !(ia instanceof TokenizerChain)) {\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n    TokenizerChain tci = (TokenizerChain)ia;\n\n    // make sure that there isn't a stop filter in the indexer\n    for (TokenFilterFactory tf : tci.getTokenFilterFactories()) {\n      if (tf instanceof StopFilterFactory) {\n        map.put(fieldName, qa);\n        return qa.tokenStream(fieldName, reader);\n      }\n    }\n\n    // now if there is a stop filter in the query analyzer, remove it\n    int stopIdx = -1;\n    TokenFilterFactory[] facs = tcq.getTokenFilterFactories();\n\n    for (int i=0; i<facs.length; i++) {\n      TokenFilterFactory tf = facs[i];\n      if (tf instanceof StopFilterFactory) {\n        stopIdx = i;\n        break;\n      }\n    }\n\n    if (stopIdx == -1) {\n      // no stop filter exists\n      map.put(fieldName, qa);\n      return qa.tokenStream(fieldName, reader);\n    }\n\n    TokenFilterFactory[] newtf = new TokenFilterFactory[facs.length-1];\n    for (int i=0,j=0; i<facs.length; i++) {\n      if (i==stopIdx) continue;\n      newtf[j++] = facs[i];\n    }\n\n    TokenizerChain newa = new TokenizerChain(tcq.getTokenizerFactory(), newtf);\n    newa.setPositionIncrementGap(tcq.getPositionIncrementGap(fieldName));\n\n    map.put(fieldName, newa);\n    return newa.tokenStream(fieldName, reader);        \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"20349324eb18d1565d301e59be543989f38743d3":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"ad94625fb8d088209f46650c8097196fec67f00c":["20349324eb18d1565d301e59be543989f38743d3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"20349324eb18d1565d301e59be543989f38743d3":["ad94625fb8d088209f46650c8097196fec67f00c"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["20349324eb18d1565d301e59be543989f38743d3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ad94625fb8d088209f46650c8097196fec67f00c":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ad94625fb8d088209f46650c8097196fec67f00c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}