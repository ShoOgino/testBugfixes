{"path":"lucene/contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor#build(IndexReader,String,TermVectorMapper,int).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor#build(IndexReader,String,TermVectorMapper,int).mjava","pathOld":"contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor#build(IndexReader,String,TermVectorMapper,int).mjava","sourceNew":"  /**\n   * Populates the mapper with terms available for the given field in a document\n   * by resolving the inverted index.\n   *\n   * @param indexReader\n   * @param field interned field name\n   * @param mapper\n   * @param documentNumber\n   * @throws IOException\n   */\n  private void build(IndexReader indexReader, String field, TermVectorMapper mapper, int documentNumber) throws IOException {\n\n    if (tokens == null) {\n      tokens = new ArrayList<String>(500);\n      positions = new ArrayList<int[]>(500);\n      frequencies = new ArrayList<Integer>(500);\n    } else {\n      tokens.clear();\n      frequencies.clear();\n      positions.clear();\n    }\n\n    TermEnum termEnum = indexReader.terms(new Term(field, \"\"));\n    if (termEnum.term() != null) {\n      while (termEnum.term().field() == field) {\n        TermPositions termPositions = indexReader.termPositions(termEnum.term());\n        if (termPositions.skipTo(documentNumber)) {\n  \n          frequencies.add(Integer.valueOf(termPositions.freq()));\n          tokens.add(termEnum.term().text());\n  \n  \n          if (!mapper.isIgnoringPositions()) {\n            int[] positions = new int[termPositions.freq()];\n            for (int i = 0; i < positions.length; i++) {\n              positions[i] = termPositions.nextPosition();\n            }\n            this.positions.add(positions);\n          } else {\n            positions.add(null);\n          }\n        }\n        termPositions.close();\n        if (!termEnum.next()) {\n          break;\n        }\n      }\n      mapper.setDocumentNumber(documentNumber);\n      mapper.setExpectations(field, tokens.size(), false, !mapper.isIgnoringPositions());\n      for (int i = 0; i < tokens.size(); i++) {\n        mapper.map(tokens.get(i), frequencies.get(i).intValue(), (TermVectorOffsetInfo[]) null, positions.get(i));\n      }\n    }\n    termEnum.close();\n\n\n  }\n\n","sourceOld":"  /**\n   * Populates the mapper with terms available for the given field in a document\n   * by resolving the inverted index.\n   *\n   * @param indexReader\n   * @param field interned field name\n   * @param mapper\n   * @param documentNumber\n   * @throws IOException\n   */\n  private void build(IndexReader indexReader, String field, TermVectorMapper mapper, int documentNumber) throws IOException {\n\n    if (tokens == null) {\n      tokens = new ArrayList<String>(500);\n      positions = new ArrayList<int[]>(500);\n      frequencies = new ArrayList<Integer>(500);\n    } else {\n      tokens.clear();\n      frequencies.clear();\n      positions.clear();\n    }\n\n    TermEnum termEnum = indexReader.terms(new Term(field, \"\"));\n    if (termEnum.term() != null) {\n      while (termEnum.term().field() == field) {\n        TermPositions termPositions = indexReader.termPositions(termEnum.term());\n        if (termPositions.skipTo(documentNumber)) {\n  \n          frequencies.add(Integer.valueOf(termPositions.freq()));\n          tokens.add(termEnum.term().text());\n  \n  \n          if (!mapper.isIgnoringPositions()) {\n            int[] positions = new int[termPositions.freq()];\n            for (int i = 0; i < positions.length; i++) {\n              positions[i] = termPositions.nextPosition();\n            }\n            this.positions.add(positions);\n          } else {\n            positions.add(null);\n          }\n        }\n        termPositions.close();\n        if (!termEnum.next()) {\n          break;\n        }\n      }\n      mapper.setDocumentNumber(documentNumber);\n      mapper.setExpectations(field, tokens.size(), false, !mapper.isIgnoringPositions());\n      for (int i = 0; i < tokens.size(); i++) {\n        mapper.map(tokens.get(i), frequencies.get(i).intValue(), (TermVectorOffsetInfo[]) null, positions.get(i));\n      }\n    }\n    termEnum.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor#build(IndexReader,String,TermVectorMapper,int).mjava","pathOld":"lucene/contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor#build(IndexReader,String,TermVectorMapper,int).mjava","sourceNew":"  /**\n   * Populates the mapper with terms available for the given field in a document\n   * by resolving the inverted index.\n   *\n   * @param indexReader\n   * @param field interned field name\n   * @param mapper\n   * @param documentNumber\n   * @throws IOException\n   */\n  private void build(IndexReader indexReader, String field, TermVectorMapper mapper, int documentNumber) throws IOException {\n\n    if (tokens == null) {\n      tokens = new ArrayList<String>(500);\n      positions = new ArrayList<int[]>(500);\n      frequencies = new ArrayList<Integer>(500);\n    } else {\n      tokens.clear();\n      frequencies.clear();\n      positions.clear();\n    }\n\n    final Bits delDocs = MultiFields.getDeletedDocs(indexReader);\n\n    Terms terms = MultiFields.getTerms(indexReader, field);\n    boolean anyTerms = false;\n    if (terms != null) {\n      TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n      DocsAndPositionsEnum postings = null;\n      while(true) {\n        BytesRef text = termsEnum.next();\n        if (text != null) {\n          anyTerms = true;\n          if (!mapper.isIgnoringPositions()) {\n            docs = postings = termsEnum.docsAndPositions(delDocs, postings);\n          } else {\n            docs = termsEnum.docs(delDocs, docs);\n          }\n\n          int docID = docs.advance(documentNumber);\n          if (docID == documentNumber) {\n\n            frequencies.add(Integer.valueOf(docs.freq()));\n            tokens.add(text.utf8ToString());\n\n            if (!mapper.isIgnoringPositions()) {\n              int[] positions = new int[docs.freq()];\n              for (int i = 0; i < positions.length; i++) {\n                positions[i] = postings.nextPosition();\n              }\n              this.positions.add(positions);\n            } else {\n              positions.add(null);\n            }\n          }\n        } else {\n          break;\n        }\n      }\n\n      if (anyTerms) {\n        mapper.setDocumentNumber(documentNumber);\n        mapper.setExpectations(field, tokens.size(), false, !mapper.isIgnoringPositions());\n        for (int i = 0; i < tokens.size(); i++) {\n          mapper.map(tokens.get(i), frequencies.get(i).intValue(), (TermVectorOffsetInfo[]) null, positions.get(i));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Populates the mapper with terms available for the given field in a document\n   * by resolving the inverted index.\n   *\n   * @param indexReader\n   * @param field interned field name\n   * @param mapper\n   * @param documentNumber\n   * @throws IOException\n   */\n  private void build(IndexReader indexReader, String field, TermVectorMapper mapper, int documentNumber) throws IOException {\n\n    if (tokens == null) {\n      tokens = new ArrayList<String>(500);\n      positions = new ArrayList<int[]>(500);\n      frequencies = new ArrayList<Integer>(500);\n    } else {\n      tokens.clear();\n      frequencies.clear();\n      positions.clear();\n    }\n\n    TermEnum termEnum = indexReader.terms(new Term(field, \"\"));\n    if (termEnum.term() != null) {\n      while (termEnum.term().field() == field) {\n        TermPositions termPositions = indexReader.termPositions(termEnum.term());\n        if (termPositions.skipTo(documentNumber)) {\n  \n          frequencies.add(Integer.valueOf(termPositions.freq()));\n          tokens.add(termEnum.term().text());\n  \n  \n          if (!mapper.isIgnoringPositions()) {\n            int[] positions = new int[termPositions.freq()];\n            for (int i = 0; i < positions.length; i++) {\n              positions[i] = termPositions.nextPosition();\n            }\n            this.positions.add(positions);\n          } else {\n            positions.add(null);\n          }\n        }\n        termPositions.close();\n        if (!termEnum.next()) {\n          break;\n        }\n      }\n      mapper.setDocumentNumber(documentNumber);\n      mapper.setExpectations(field, tokens.size(), false, !mapper.isIgnoringPositions());\n      for (int i = 0; i < tokens.size(); i++) {\n        mapper.map(tokens.get(i), frequencies.get(i).intValue(), (TermVectorOffsetInfo[]) null, positions.get(i));\n      }\n    }\n    termEnum.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4f29ba80b723649f5feb7e37afe1a558dd2c1304","date":1278318805,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor#build(IndexReader,String,TermVectorMapper,int).mjava","pathOld":"lucene/contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor#build(IndexReader,String,TermVectorMapper,int).mjava","sourceNew":"  /**\n   * Populates the mapper with terms available for the given field in a document\n   * by resolving the inverted index.\n   *\n   * @param indexReader\n   * @param field interned field name\n   * @param mapper\n   * @param documentNumber\n   * @throws IOException\n   */\n  private void build(IndexReader indexReader, String field, TermVectorMapper mapper, int documentNumber) throws IOException {\n\n    if (tokens == null) {\n      tokens = new ArrayList<BytesRef>(500);\n      positions = new ArrayList<int[]>(500);\n      frequencies = new ArrayList<Integer>(500);\n    } else {\n      tokens.clear();\n      frequencies.clear();\n      positions.clear();\n    }\n\n    final Bits delDocs = MultiFields.getDeletedDocs(indexReader);\n\n    Terms terms = MultiFields.getTerms(indexReader, field);\n    boolean anyTerms = false;\n    if (terms != null) {\n      TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n      DocsAndPositionsEnum postings = null;\n      while(true) {\n        BytesRef text = termsEnum.next();\n        if (text != null) {\n          anyTerms = true;\n          if (!mapper.isIgnoringPositions()) {\n            docs = postings = termsEnum.docsAndPositions(delDocs, postings);\n          } else {\n            docs = termsEnum.docs(delDocs, docs);\n          }\n\n          int docID = docs.advance(documentNumber);\n          if (docID == documentNumber) {\n\n            frequencies.add(Integer.valueOf(docs.freq()));\n            tokens.add(new BytesRef(text));\n\n            if (!mapper.isIgnoringPositions()) {\n              int[] positions = new int[docs.freq()];\n              for (int i = 0; i < positions.length; i++) {\n                positions[i] = postings.nextPosition();\n              }\n              this.positions.add(positions);\n            } else {\n              positions.add(null);\n            }\n          }\n        } else {\n          break;\n        }\n      }\n\n      if (anyTerms) {\n        mapper.setDocumentNumber(documentNumber);\n        mapper.setExpectations(field, tokens.size(), false, !mapper.isIgnoringPositions());\n        for (int i = 0; i < tokens.size(); i++) {\n          mapper.map(tokens.get(i), frequencies.get(i).intValue(), (TermVectorOffsetInfo[]) null, positions.get(i));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Populates the mapper with terms available for the given field in a document\n   * by resolving the inverted index.\n   *\n   * @param indexReader\n   * @param field interned field name\n   * @param mapper\n   * @param documentNumber\n   * @throws IOException\n   */\n  private void build(IndexReader indexReader, String field, TermVectorMapper mapper, int documentNumber) throws IOException {\n\n    if (tokens == null) {\n      tokens = new ArrayList<String>(500);\n      positions = new ArrayList<int[]>(500);\n      frequencies = new ArrayList<Integer>(500);\n    } else {\n      tokens.clear();\n      frequencies.clear();\n      positions.clear();\n    }\n\n    final Bits delDocs = MultiFields.getDeletedDocs(indexReader);\n\n    Terms terms = MultiFields.getTerms(indexReader, field);\n    boolean anyTerms = false;\n    if (terms != null) {\n      TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n      DocsAndPositionsEnum postings = null;\n      while(true) {\n        BytesRef text = termsEnum.next();\n        if (text != null) {\n          anyTerms = true;\n          if (!mapper.isIgnoringPositions()) {\n            docs = postings = termsEnum.docsAndPositions(delDocs, postings);\n          } else {\n            docs = termsEnum.docs(delDocs, docs);\n          }\n\n          int docID = docs.advance(documentNumber);\n          if (docID == documentNumber) {\n\n            frequencies.add(Integer.valueOf(docs.freq()));\n            tokens.add(text.utf8ToString());\n\n            if (!mapper.isIgnoringPositions()) {\n              int[] positions = new int[docs.freq()];\n              for (int i = 0; i < positions.length; i++) {\n                positions[i] = postings.nextPosition();\n              }\n              this.positions.add(positions);\n            } else {\n              positions.add(null);\n            }\n          }\n        } else {\n          break;\n        }\n      }\n\n      if (anyTerms) {\n        mapper.setDocumentNumber(documentNumber);\n        mapper.setExpectations(field, tokens.size(), false, !mapper.isIgnoringPositions());\n        for (int i = 0; i < tokens.size(); i++) {\n          mapper.map(tokens.get(i), frequencies.get(i).intValue(), (TermVectorOffsetInfo[]) null, positions.get(i));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor#build(IndexReader,String,TermVectorMapper,int).mjava","pathOld":"lucene/contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor#build(IndexReader,String,TermVectorMapper,int).mjava","sourceNew":"  /**\n   * Populates the mapper with terms available for the given field in a document\n   * by resolving the inverted index.\n   *\n   * @param indexReader\n   * @param field interned field name\n   * @param mapper\n   * @param documentNumber\n   * @throws IOException\n   */\n  private void build(IndexReader indexReader, String field, TermVectorMapper mapper, int documentNumber) throws IOException {\n\n    if (tokens == null) {\n      tokens = new ArrayList<BytesRef>(500);\n      positions = new ArrayList<int[]>(500);\n      frequencies = new ArrayList<Integer>(500);\n    } else {\n      tokens.clear();\n      frequencies.clear();\n      positions.clear();\n    }\n\n    final Bits delDocs = MultiFields.getDeletedDocs(indexReader);\n\n    Terms terms = MultiFields.getTerms(indexReader, field);\n    boolean anyTerms = false;\n    if (terms != null) {\n      TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n      DocsAndPositionsEnum postings = null;\n      while(true) {\n        BytesRef text = termsEnum.next();\n        if (text != null) {\n          anyTerms = true;\n          if (!mapper.isIgnoringPositions()) {\n            docs = postings = termsEnum.docsAndPositions(delDocs, postings);\n          } else {\n            docs = termsEnum.docs(delDocs, docs);\n          }\n\n          int docID = docs.advance(documentNumber);\n          if (docID == documentNumber) {\n\n            frequencies.add(Integer.valueOf(docs.freq()));\n            tokens.add(new BytesRef(text));\n\n            if (!mapper.isIgnoringPositions()) {\n              int[] positions = new int[docs.freq()];\n              for (int i = 0; i < positions.length; i++) {\n                positions[i] = postings.nextPosition();\n              }\n              this.positions.add(positions);\n            } else {\n              positions.add(null);\n            }\n          }\n        } else {\n          break;\n        }\n      }\n\n      if (anyTerms) {\n        mapper.setDocumentNumber(documentNumber);\n        mapper.setExpectations(field, tokens.size(), false, !mapper.isIgnoringPositions());\n        for (int i = 0; i < tokens.size(); i++) {\n          mapper.map(tokens.get(i), frequencies.get(i).intValue(), (TermVectorOffsetInfo[]) null, positions.get(i));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Populates the mapper with terms available for the given field in a document\n   * by resolving the inverted index.\n   *\n   * @param indexReader\n   * @param field interned field name\n   * @param mapper\n   * @param documentNumber\n   * @throws IOException\n   */\n  private void build(IndexReader indexReader, String field, TermVectorMapper mapper, int documentNumber) throws IOException {\n\n    if (tokens == null) {\n      tokens = new ArrayList<String>(500);\n      positions = new ArrayList<int[]>(500);\n      frequencies = new ArrayList<Integer>(500);\n    } else {\n      tokens.clear();\n      frequencies.clear();\n      positions.clear();\n    }\n\n    final Bits delDocs = MultiFields.getDeletedDocs(indexReader);\n\n    Terms terms = MultiFields.getTerms(indexReader, field);\n    boolean anyTerms = false;\n    if (terms != null) {\n      TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n      DocsAndPositionsEnum postings = null;\n      while(true) {\n        BytesRef text = termsEnum.next();\n        if (text != null) {\n          anyTerms = true;\n          if (!mapper.isIgnoringPositions()) {\n            docs = postings = termsEnum.docsAndPositions(delDocs, postings);\n          } else {\n            docs = termsEnum.docs(delDocs, docs);\n          }\n\n          int docID = docs.advance(documentNumber);\n          if (docID == documentNumber) {\n\n            frequencies.add(Integer.valueOf(docs.freq()));\n            tokens.add(text.utf8ToString());\n\n            if (!mapper.isIgnoringPositions()) {\n              int[] positions = new int[docs.freq()];\n              for (int i = 0; i < positions.length; i++) {\n                positions[i] = postings.nextPosition();\n              }\n              this.positions.add(positions);\n            } else {\n              positions.add(null);\n            }\n          }\n        } else {\n          break;\n        }\n      }\n\n      if (anyTerms) {\n        mapper.setDocumentNumber(documentNumber);\n        mapper.setExpectations(field, tokens.size(), false, !mapper.isIgnoringPositions());\n        for (int i = 0; i < tokens.size(); i++) {\n          mapper.map(tokens.get(i), frequencies.get(i).intValue(), (TermVectorOffsetInfo[]) null, positions.get(i));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","date":1309960478,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor#build(IndexReader,String,TermVectorMapper,int).mjava","pathOld":"lucene/contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor#build(IndexReader,String,TermVectorMapper,int).mjava","sourceNew":"  /**\n   * Populates the mapper with terms available for the given field in a document\n   * by resolving the inverted index.\n   *\n   * @param indexReader\n   * @param field interned field name\n   * @param mapper\n   * @param documentNumber\n   * @throws IOException\n   */\n  private void build(IndexReader indexReader, String field, TermVectorMapper mapper, int documentNumber) throws IOException {\n\n    if (tokens == null) {\n      tokens = new ArrayList<BytesRef>(500);\n      positions = new ArrayList<int[]>(500);\n      frequencies = new ArrayList<Integer>(500);\n    } else {\n      tokens.clear();\n      frequencies.clear();\n      positions.clear();\n    }\n\n    final Bits liveDocs = MultiFields.getLiveDocs(indexReader);\n\n    Terms terms = MultiFields.getTerms(indexReader, field);\n    boolean anyTerms = false;\n    if (terms != null) {\n      TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n      DocsAndPositionsEnum postings = null;\n      while(true) {\n        BytesRef text = termsEnum.next();\n        if (text != null) {\n          anyTerms = true;\n          if (!mapper.isIgnoringPositions()) {\n            docs = postings = termsEnum.docsAndPositions(liveDocs, postings);\n          } else {\n            docs = termsEnum.docs(liveDocs, docs);\n          }\n\n          int docID = docs.advance(documentNumber);\n          if (docID == documentNumber) {\n\n            frequencies.add(Integer.valueOf(docs.freq()));\n            tokens.add(new BytesRef(text));\n\n            if (!mapper.isIgnoringPositions()) {\n              int[] positions = new int[docs.freq()];\n              for (int i = 0; i < positions.length; i++) {\n                positions[i] = postings.nextPosition();\n              }\n              this.positions.add(positions);\n            } else {\n              positions.add(null);\n            }\n          }\n        } else {\n          break;\n        }\n      }\n\n      if (anyTerms) {\n        mapper.setDocumentNumber(documentNumber);\n        mapper.setExpectations(field, tokens.size(), false, !mapper.isIgnoringPositions());\n        for (int i = 0; i < tokens.size(); i++) {\n          mapper.map(tokens.get(i), frequencies.get(i).intValue(), (TermVectorOffsetInfo[]) null, positions.get(i));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Populates the mapper with terms available for the given field in a document\n   * by resolving the inverted index.\n   *\n   * @param indexReader\n   * @param field interned field name\n   * @param mapper\n   * @param documentNumber\n   * @throws IOException\n   */\n  private void build(IndexReader indexReader, String field, TermVectorMapper mapper, int documentNumber) throws IOException {\n\n    if (tokens == null) {\n      tokens = new ArrayList<BytesRef>(500);\n      positions = new ArrayList<int[]>(500);\n      frequencies = new ArrayList<Integer>(500);\n    } else {\n      tokens.clear();\n      frequencies.clear();\n      positions.clear();\n    }\n\n    final Bits delDocs = MultiFields.getDeletedDocs(indexReader);\n\n    Terms terms = MultiFields.getTerms(indexReader, field);\n    boolean anyTerms = false;\n    if (terms != null) {\n      TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n      DocsAndPositionsEnum postings = null;\n      while(true) {\n        BytesRef text = termsEnum.next();\n        if (text != null) {\n          anyTerms = true;\n          if (!mapper.isIgnoringPositions()) {\n            docs = postings = termsEnum.docsAndPositions(delDocs, postings);\n          } else {\n            docs = termsEnum.docs(delDocs, docs);\n          }\n\n          int docID = docs.advance(documentNumber);\n          if (docID == documentNumber) {\n\n            frequencies.add(Integer.valueOf(docs.freq()));\n            tokens.add(new BytesRef(text));\n\n            if (!mapper.isIgnoringPositions()) {\n              int[] positions = new int[docs.freq()];\n              for (int i = 0; i < positions.length; i++) {\n                positions[i] = postings.nextPosition();\n              }\n              this.positions.add(positions);\n            } else {\n              positions.add(null);\n            }\n          }\n        } else {\n          break;\n        }\n      }\n\n      if (anyTerms) {\n        mapper.setDocumentNumber(documentNumber);\n        mapper.setExpectations(field, tokens.size(), false, !mapper.isIgnoringPositions());\n        for (int i = 0; i < tokens.size(); i++) {\n          mapper.map(tokens.get(i), frequencies.get(i).intValue(), (TermVectorOffsetInfo[]) null, positions.get(i));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor#build(IndexReader,String,TermVectorMapper,int).mjava","pathOld":"lucene/contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor#build(IndexReader,String,TermVectorMapper,int).mjava","sourceNew":"  /**\n   * Populates the mapper with terms available for the given field in a document\n   * by resolving the inverted index.\n   *\n   * @param indexReader\n   * @param field interned field name\n   * @param mapper\n   * @param documentNumber\n   * @throws IOException\n   */\n  private void build(IndexReader indexReader, String field, TermVectorMapper mapper, int documentNumber) throws IOException {\n\n    if (tokens == null) {\n      tokens = new ArrayList<BytesRef>(500);\n      positions = new ArrayList<int[]>(500);\n      frequencies = new ArrayList<Integer>(500);\n    } else {\n      tokens.clear();\n      frequencies.clear();\n      positions.clear();\n    }\n\n    final Bits liveDocs = MultiFields.getLiveDocs(indexReader);\n\n    Terms terms = MultiFields.getTerms(indexReader, field);\n    boolean anyTerms = false;\n    if (terms != null) {\n      TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n      DocsAndPositionsEnum postings = null;\n      while(true) {\n        BytesRef text = termsEnum.next();\n        if (text != null) {\n          anyTerms = true;\n          if (!mapper.isIgnoringPositions()) {\n            docs = postings = termsEnum.docsAndPositions(liveDocs, postings);\n          } else {\n            docs = termsEnum.docs(liveDocs, docs);\n          }\n\n          int docID = docs.advance(documentNumber);\n          if (docID == documentNumber) {\n\n            frequencies.add(Integer.valueOf(docs.freq()));\n            tokens.add(new BytesRef(text));\n\n            if (!mapper.isIgnoringPositions()) {\n              int[] positions = new int[docs.freq()];\n              for (int i = 0; i < positions.length; i++) {\n                positions[i] = postings.nextPosition();\n              }\n              this.positions.add(positions);\n            } else {\n              positions.add(null);\n            }\n          }\n        } else {\n          break;\n        }\n      }\n\n      if (anyTerms) {\n        mapper.setDocumentNumber(documentNumber);\n        mapper.setExpectations(field, tokens.size(), false, !mapper.isIgnoringPositions());\n        for (int i = 0; i < tokens.size(); i++) {\n          mapper.map(tokens.get(i), frequencies.get(i).intValue(), (TermVectorOffsetInfo[]) null, positions.get(i));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Populates the mapper with terms available for the given field in a document\n   * by resolving the inverted index.\n   *\n   * @param indexReader\n   * @param field interned field name\n   * @param mapper\n   * @param documentNumber\n   * @throws IOException\n   */\n  private void build(IndexReader indexReader, String field, TermVectorMapper mapper, int documentNumber) throws IOException {\n\n    if (tokens == null) {\n      tokens = new ArrayList<BytesRef>(500);\n      positions = new ArrayList<int[]>(500);\n      frequencies = new ArrayList<Integer>(500);\n    } else {\n      tokens.clear();\n      frequencies.clear();\n      positions.clear();\n    }\n\n    final Bits delDocs = MultiFields.getDeletedDocs(indexReader);\n\n    Terms terms = MultiFields.getTerms(indexReader, field);\n    boolean anyTerms = false;\n    if (terms != null) {\n      TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n      DocsAndPositionsEnum postings = null;\n      while(true) {\n        BytesRef text = termsEnum.next();\n        if (text != null) {\n          anyTerms = true;\n          if (!mapper.isIgnoringPositions()) {\n            docs = postings = termsEnum.docsAndPositions(delDocs, postings);\n          } else {\n            docs = termsEnum.docs(delDocs, docs);\n          }\n\n          int docID = docs.advance(documentNumber);\n          if (docID == documentNumber) {\n\n            frequencies.add(Integer.valueOf(docs.freq()));\n            tokens.add(new BytesRef(text));\n\n            if (!mapper.isIgnoringPositions()) {\n              int[] positions = new int[docs.freq()];\n              for (int i = 0; i < positions.length; i++) {\n                positions[i] = postings.nextPosition();\n              }\n              this.positions.add(positions);\n            } else {\n              positions.add(null);\n            }\n          }\n        } else {\n          break;\n        }\n      }\n\n      if (anyTerms) {\n        mapper.setDocumentNumber(documentNumber);\n        mapper.setExpectations(field, tokens.size(), false, !mapper.isIgnoringPositions());\n        for (int i = 0; i < tokens.size(); i++) {\n          mapper.map(tokens.get(i), frequencies.get(i).intValue(), (TermVectorOffsetInfo[]) null, positions.get(i));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor#build(IndexReader,String,TermVectorMapper,int).mjava","pathOld":"lucene/contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor#build(IndexReader,String,TermVectorMapper,int).mjava","sourceNew":"  /**\n   * Populates the mapper with terms available for the given field in a document\n   * by resolving the inverted index.\n   *\n   * @param indexReader\n   * @param field interned field name\n   * @param mapper\n   * @param documentNumber\n   * @throws IOException\n   */\n  private void build(IndexReader indexReader, String field, TermVectorMapper mapper, int documentNumber) throws IOException {\n\n    if (tokens == null) {\n      tokens = new ArrayList<BytesRef>(500);\n      positions = new ArrayList<int[]>(500);\n      frequencies = new ArrayList<Integer>(500);\n    } else {\n      tokens.clear();\n      frequencies.clear();\n      positions.clear();\n    }\n\n    final Bits liveDocs = MultiFields.getLiveDocs(indexReader);\n\n    Terms terms = MultiFields.getTerms(indexReader, field);\n    boolean anyTerms = false;\n    if (terms != null) {\n      TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n      DocsAndPositionsEnum postings = null;\n      while(true) {\n        BytesRef text = termsEnum.next();\n        if (text != null) {\n          anyTerms = true;\n          if (!mapper.isIgnoringPositions()) {\n            docs = postings = termsEnum.docsAndPositions(liveDocs, postings);\n          } else {\n            docs = termsEnum.docs(liveDocs, docs);\n          }\n\n          int docID = docs.advance(documentNumber);\n          if (docID == documentNumber) {\n\n            frequencies.add(Integer.valueOf(docs.freq()));\n            tokens.add(new BytesRef(text));\n\n            if (!mapper.isIgnoringPositions()) {\n              int[] positions = new int[docs.freq()];\n              for (int i = 0; i < positions.length; i++) {\n                positions[i] = postings.nextPosition();\n              }\n              this.positions.add(positions);\n            } else {\n              positions.add(null);\n            }\n          }\n        } else {\n          break;\n        }\n      }\n\n      if (anyTerms) {\n        mapper.setDocumentNumber(documentNumber);\n        mapper.setExpectations(field, tokens.size(), false, !mapper.isIgnoringPositions());\n        for (int i = 0; i < tokens.size(); i++) {\n          mapper.map(tokens.get(i), frequencies.get(i).intValue(), (TermVectorOffsetInfo[]) null, positions.get(i));\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Populates the mapper with terms available for the given field in a document\n   * by resolving the inverted index.\n   *\n   * @param indexReader\n   * @param field interned field name\n   * @param mapper\n   * @param documentNumber\n   * @throws IOException\n   */\n  private void build(IndexReader indexReader, String field, TermVectorMapper mapper, int documentNumber) throws IOException {\n\n    if (tokens == null) {\n      tokens = new ArrayList<BytesRef>(500);\n      positions = new ArrayList<int[]>(500);\n      frequencies = new ArrayList<Integer>(500);\n    } else {\n      tokens.clear();\n      frequencies.clear();\n      positions.clear();\n    }\n\n    final Bits delDocs = MultiFields.getDeletedDocs(indexReader);\n\n    Terms terms = MultiFields.getTerms(indexReader, field);\n    boolean anyTerms = false;\n    if (terms != null) {\n      TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n      DocsAndPositionsEnum postings = null;\n      while(true) {\n        BytesRef text = termsEnum.next();\n        if (text != null) {\n          anyTerms = true;\n          if (!mapper.isIgnoringPositions()) {\n            docs = postings = termsEnum.docsAndPositions(delDocs, postings);\n          } else {\n            docs = termsEnum.docs(delDocs, docs);\n          }\n\n          int docID = docs.advance(documentNumber);\n          if (docID == documentNumber) {\n\n            frequencies.add(Integer.valueOf(docs.freq()));\n            tokens.add(new BytesRef(text));\n\n            if (!mapper.isIgnoringPositions()) {\n              int[] positions = new int[docs.freq()];\n              for (int i = 0; i < positions.length; i++) {\n                positions[i] = postings.nextPosition();\n              }\n              this.positions.add(positions);\n            } else {\n              positions.add(null);\n            }\n          }\n        } else {\n          break;\n        }\n      }\n\n      if (anyTerms) {\n        mapper.setDocumentNumber(documentNumber);\n        mapper.setExpectations(field, tokens.size(), false, !mapper.isIgnoringPositions());\n        for (int i = 0; i < tokens.size(); i++) {\n          mapper.map(tokens.get(i), frequencies.get(i).intValue(), (TermVectorOffsetInfo[]) null, positions.get(i));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/contrib/misc/src/java/org/apache/lucene/index/TermVectorAccessor#build(IndexReader,String,TermVectorMapper,int).mjava","sourceNew":null,"sourceOld":"  /**\n   * Populates the mapper with terms available for the given field in a document\n   * by resolving the inverted index.\n   *\n   * @param indexReader\n   * @param field interned field name\n   * @param mapper\n   * @param documentNumber\n   * @throws IOException\n   */\n  private void build(IndexReader indexReader, String field, TermVectorMapper mapper, int documentNumber) throws IOException {\n\n    if (tokens == null) {\n      tokens = new ArrayList<BytesRef>(500);\n      positions = new ArrayList<int[]>(500);\n      frequencies = new ArrayList<Integer>(500);\n    } else {\n      tokens.clear();\n      frequencies.clear();\n      positions.clear();\n    }\n\n    final Bits liveDocs = MultiFields.getLiveDocs(indexReader);\n\n    Terms terms = MultiFields.getTerms(indexReader, field);\n    boolean anyTerms = false;\n    if (terms != null) {\n      TermsEnum termsEnum = terms.iterator();\n      DocsEnum docs = null;\n      DocsAndPositionsEnum postings = null;\n      while(true) {\n        BytesRef text = termsEnum.next();\n        if (text != null) {\n          anyTerms = true;\n          if (!mapper.isIgnoringPositions()) {\n            docs = postings = termsEnum.docsAndPositions(liveDocs, postings);\n          } else {\n            docs = termsEnum.docs(liveDocs, docs);\n          }\n\n          int docID = docs.advance(documentNumber);\n          if (docID == documentNumber) {\n\n            frequencies.add(Integer.valueOf(docs.freq()));\n            tokens.add(new BytesRef(text));\n\n            if (!mapper.isIgnoringPositions()) {\n              int[] positions = new int[docs.freq()];\n              for (int i = 0; i < positions.length; i++) {\n                positions[i] = postings.nextPosition();\n              }\n              this.positions.add(positions);\n            } else {\n              positions.add(null);\n            }\n          }\n        } else {\n          break;\n        }\n      }\n\n      if (anyTerms) {\n        mapper.setDocumentNumber(documentNumber);\n        mapper.setExpectations(field, tokens.size(), false, !mapper.isIgnoringPositions());\n        for (int i = 0; i < tokens.size(); i++) {\n          mapper.map(tokens.get(i), frequencies.get(i).intValue(), (TermVectorOffsetInfo[]) null, positions.get(i));\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cc749c053615f5871f3b95715fe292f34e70a53":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"5f4e87790277826a2aea119328600dfb07761f32":["955c32f886db6f6356c9fcdea6b1f1cb4effda24","4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["4f29ba80b723649f5feb7e37afe1a558dd2c1304","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["4f29ba80b723649f5feb7e37afe1a558dd2c1304","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3cc749c053615f5871f3b95715fe292f34e70a53"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","5f4e87790277826a2aea119328600dfb07761f32","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["3cc749c053615f5871f3b95715fe292f34e70a53","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["4f29ba80b723649f5feb7e37afe1a558dd2c1304","5f4e87790277826a2aea119328600dfb07761f32"],"3cc749c053615f5871f3b95715fe292f34e70a53":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5f4e87790277826a2aea119328600dfb07761f32":[],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"817d8435e9135b756f08ce6710ab0baac51bdf88":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5f4e87790277826a2aea119328600dfb07761f32","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}