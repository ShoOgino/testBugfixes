{"path":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","commits":[{"id":"db25c1f61b5ae826f10777da6551a832703967d5","date":1215306972,"type":1,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(DocListAndSet,Query,DocSet,Sort,int,int,int).mjava","sourceNew":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n    final HitCollector hitCollector = ( cmd.getTimeAllowed() > 0 ) ? new TimeLimitedCollector( setHC, cmd.getTimeAllowed() ) : setHC;\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      try {\n        searcher.search(query, new HitCollector() {\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            if (score > topscore[0]) topscore[0]=score;\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warning( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (cmd.getSort() != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, cmd.getSort().getSort(), len);\n\n      try {\n        searcher.search(query, new HitCollector() {\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            hq.insert(new FieldDoc(doc, score));\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warning( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      try {\n        searcher.search(query, new HitCollector() {\n          float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            if (numHits[0]++ < lastDocRequested || score >= minScore) {\n              // if docs are always delivered in order, we could use \"score>minScore\"\n              // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n              hq.insert(new ScoreDoc(doc, score));\n              minScore = ((ScoreDoc)hq.top()).score;\n            }\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warning( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n    \n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    DocSet qDocSet = setHC.getDocSet();\n    qr.setDocSet(filter==null ? qDocSet : qDocSet.intersection(filter));\n    return qDocSet;\n  }\n\n","sourceOld":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(DocListAndSet out, Query query, DocSet filter, Sort lsort, int offset, int len, int flags) throws IOException {\n    int last = offset+len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n\n    query = QueryUtils.makeQueryable(query);\n\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          if (score > topscore[0]) topscore[0]=score;\n        }\n      }\n      );\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (lsort != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, lsort.getSort(), offset+len);\n\n      searcher.search(query, new HitCollector() {\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          numHits[0]++;\n          hq.insert(new FieldDoc(doc, score));\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      searcher.search(query, new HitCollector() {\n        float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n        public void collect(int doc, float score) {\n          setHC.collect(doc,score);\n          if (filt!=null && !filt.exists(doc)) return;\n          if (numHits[0]++ < lastDocRequested || score >= minScore) {\n            // if docs are always delivered in order, we could use \"score>minScore\"\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            hq.insert(new ScoreDoc(doc, score));\n            minScore = ((ScoreDoc)hq.top()).score;\n          }\n        }\n      }\n      );\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (flags&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned) - offset;\n    if (sliceLen < 0) sliceLen=0;\n    out.docList = new DocSlice(offset,sliceLen,ids,scores,totalHits,maxScore);\n    DocSet qDocSet = setHC.getDocSet();\n    out.docSet = filter==null ? qDocSet : qDocSet.intersection(filter);\n    return qDocSet;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"00da8b51bdeae168a5b26ec506db372b219ca7a2","date":1221704186,"type":3,"author":"Ryan McKinley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","sourceNew":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n    final HitCollector hitCollector = ( cmd.getTimeAllowed() > 0 ) ? new TimeLimitedCollector( setHC, cmd.getTimeAllowed() ) : setHC;\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      try {\n        searcher.search(query, new HitCollector() {\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            if (score > topscore[0]) topscore[0]=score;\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (cmd.getSort() != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, cmd.getSort().getSort(), len);\n\n      try {\n        searcher.search(query, new HitCollector() {\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            hq.insert(new FieldDoc(doc, score));\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      try {\n        searcher.search(query, new HitCollector() {\n          float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            if (numHits[0]++ < lastDocRequested || score >= minScore) {\n              // if docs are always delivered in order, we could use \"score>minScore\"\n              // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n              hq.insert(new ScoreDoc(doc, score));\n              minScore = ((ScoreDoc)hq.top()).score;\n            }\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n    \n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    DocSet qDocSet = setHC.getDocSet();\n    qr.setDocSet(filter==null ? qDocSet : qDocSet.intersection(filter));\n    return qDocSet;\n  }\n\n","sourceOld":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n    final HitCollector hitCollector = ( cmd.getTimeAllowed() > 0 ) ? new TimeLimitedCollector( setHC, cmd.getTimeAllowed() ) : setHC;\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      try {\n        searcher.search(query, new HitCollector() {\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            if (score > topscore[0]) topscore[0]=score;\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warning( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (cmd.getSort() != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, cmd.getSort().getSort(), len);\n\n      try {\n        searcher.search(query, new HitCollector() {\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            hq.insert(new FieldDoc(doc, score));\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warning( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      try {\n        searcher.search(query, new HitCollector() {\n          float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            if (numHits[0]++ < lastDocRequested || score >= minScore) {\n              // if docs are always delivered in order, we could use \"score>minScore\"\n              // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n              hq.insert(new ScoreDoc(doc, score));\n              minScore = ((ScoreDoc)hq.top()).score;\n            }\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warning( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n    \n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    DocSet qDocSet = setHC.getDocSet();\n    qr.setDocSet(filter==null ? qDocSet : qDocSet.intersection(filter));\n    return qDocSet;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7856e050319831be8cd0cd1ae493fc7e91e9b05","date":1233674454,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","sourceNew":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n    final HitCollector hitCollector = ( cmd.getTimeAllowed() > 0 ) ? new TimeLimitedCollector( setHC, cmd.getTimeAllowed() ) : setHC;\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      try {\n        searcher.search(query, new HitCollector() {\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            if (score > topscore[0]) topscore[0]=score;\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (cmd.getSort() != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, cmd.getSort().getSort(), len);\n\n      try {\n        searcher.search(query, new HitCollector() {\n          private FieldDoc reusableFD;\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            if (reusableFD == null)\n              reusableFD = new FieldDoc(doc, score);\n            else {\n              reusableFD.score = score;\n              reusableFD.doc = doc;\n            }\n            reusableFD = (FieldDoc) hq.insertWithOverflow(reusableFD);\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      try {\n        searcher.search(query, new HitCollector() {\n          private ScoreDoc reusableSD;\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n              // if docs are always delivered in order, we could use \"score>minScore\"\n              // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n              int nhits = numHits[0]++;\n              if (reusableSD == null) {\n                reusableSD = new ScoreDoc(doc, score);\n              } else if (nhits < lastDocRequested || score >= reusableSD.score) {\n                // reusableSD holds the last \"rejected\" entry, so, if\n                // this new score is not better than that, there's no\n                // need to try inserting it\n                reusableSD.doc = doc;\n                reusableSD.score = score;\n              } else {\n                return;\n              }\n              reusableSD = (ScoreDoc) hq.insertWithOverflow(reusableSD);\n            }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n    \n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    DocSet qDocSet = setHC.getDocSet();\n    qr.setDocSet(filter==null ? qDocSet : qDocSet.intersection(filter));\n    return qDocSet;\n  }\n\n","sourceOld":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n    final HitCollector hitCollector = ( cmd.getTimeAllowed() > 0 ) ? new TimeLimitedCollector( setHC, cmd.getTimeAllowed() ) : setHC;\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      try {\n        searcher.search(query, new HitCollector() {\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            if (score > topscore[0]) topscore[0]=score;\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (cmd.getSort() != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, cmd.getSort().getSort(), len);\n\n      try {\n        searcher.search(query, new HitCollector() {\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            hq.insert(new FieldDoc(doc, score));\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      try {\n        searcher.search(query, new HitCollector() {\n          float minScore=Float.NEGATIVE_INFINITY;  // minimum score in the priority queue\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            if (numHits[0]++ < lastDocRequested || score >= minScore) {\n              // if docs are always delivered in order, we could use \"score>minScore\"\n              // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n              hq.insert(new ScoreDoc(doc, score));\n              minScore = ((ScoreDoc)hq.top()).score;\n            }\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n    \n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    DocSet qDocSet = setHC.getDocSet();\n    qr.setDocSet(filter==null ? qDocSet : qDocSet.intersection(filter));\n    return qDocSet;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7ceb8ef10f044820fbd058f02d5a8e26539d255c","date":1242149378,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","sourceNew":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n    final HitCollector hitCollector = ( cmd.getTimeAllowed() > 0 ) ? new TimeLimitedCollector( setHC, cmd.getTimeAllowed() ) : setHC;\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      try {\n        super.search(query, new HitCollector() {\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            if (score > topscore[0]) topscore[0]=score;\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (cmd.getSort() != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, cmd.getSort().getSort(), len);\n\n      try {\n        super.search(query, new HitCollector() {\n          private FieldDoc reusableFD;\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            if (reusableFD == null)\n              reusableFD = new FieldDoc(doc, score);\n            else {\n              reusableFD.score = score;\n              reusableFD.doc = doc;\n            }\n            reusableFD = (FieldDoc) hq.insertWithOverflow(reusableFD);\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      try {\n        super.search(query, new HitCollector() {\n          private ScoreDoc reusableSD;\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n              // if docs are always delivered in order, we could use \"score>minScore\"\n              // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n              int nhits = numHits[0]++;\n              if (reusableSD == null) {\n                reusableSD = new ScoreDoc(doc, score);\n              } else if (nhits < lastDocRequested || score >= reusableSD.score) {\n                // reusableSD holds the last \"rejected\" entry, so, if\n                // this new score is not better than that, there's no\n                // need to try inserting it\n                reusableSD.doc = doc;\n                reusableSD.score = score;\n              } else {\n                return;\n              }\n              reusableSD = (ScoreDoc) hq.insertWithOverflow(reusableSD);\n            }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n    \n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    DocSet qDocSet = setHC.getDocSet();\n    qr.setDocSet(filter==null ? qDocSet : qDocSet.intersection(filter));\n    return qDocSet;\n  }\n\n","sourceOld":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n    final HitCollector hitCollector = ( cmd.getTimeAllowed() > 0 ) ? new TimeLimitedCollector( setHC, cmd.getTimeAllowed() ) : setHC;\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      try {\n        searcher.search(query, new HitCollector() {\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            if (score > topscore[0]) topscore[0]=score;\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (cmd.getSort() != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, cmd.getSort().getSort(), len);\n\n      try {\n        searcher.search(query, new HitCollector() {\n          private FieldDoc reusableFD;\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            if (reusableFD == null)\n              reusableFD = new FieldDoc(doc, score);\n            else {\n              reusableFD.score = score;\n              reusableFD.doc = doc;\n            }\n            reusableFD = (FieldDoc) hq.insertWithOverflow(reusableFD);\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      try {\n        searcher.search(query, new HitCollector() {\n          private ScoreDoc reusableSD;\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n              // if docs are always delivered in order, we could use \"score>minScore\"\n              // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n              int nhits = numHits[0]++;\n              if (reusableSD == null) {\n                reusableSD = new ScoreDoc(doc, score);\n              } else if (nhits < lastDocRequested || score >= reusableSD.score) {\n                // reusableSD holds the last \"rejected\" entry, so, if\n                // this new score is not better than that, there's no\n                // need to try inserting it\n                reusableSD.doc = doc;\n                reusableSD.score = score;\n              } else {\n                return;\n              }\n              reusableSD = (ScoreDoc) hq.insertWithOverflow(reusableSD);\n            }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n    \n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    DocSet qDocSet = setHC.getDocSet();\n    qr.setDocSet(filter==null ? qDocSet : qDocSet.intersection(filter));\n    return qDocSet;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"af88ddf3d03b8f9d83ad08cafaa7438a1206e405","date":1242338740,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","sourceNew":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n    final HitCollector collector = ( cmd.getTimeAllowed() > 0 ) ? new TimeLimitedCollector( setHC, cmd.getTimeAllowed() ) : setHC;\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      try {\n        super.search(query, new HitCollector() {\n          public void collect(int doc, float score) {\n            collector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            if (score > topscore[0]) topscore[0]=score;\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (cmd.getSort() != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, cmd.getSort().getSort(), len);\n\n      try {\n        super.search(query, new HitCollector() {\n          private FieldDoc reusableFD;\n          public void collect(int doc, float score) {\n            collector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            if (reusableFD == null)\n              reusableFD = new FieldDoc(doc, score);\n            else {\n              reusableFD.score = score;\n              reusableFD.doc = doc;\n            }\n            reusableFD = (FieldDoc) hq.insertWithOverflow(reusableFD);\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      try {\n        super.search(query, new HitCollector() {\n          private ScoreDoc reusableSD;\n          public void collect(int doc, float score) {\n            collector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n              // if docs are always delivered in order, we could use \"score>minScore\"\n              // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n              int nhits = numHits[0]++;\n              if (reusableSD == null) {\n                reusableSD = new ScoreDoc(doc, score);\n              } else if (nhits < lastDocRequested || score >= reusableSD.score) {\n                // reusableSD holds the last \"rejected\" entry, so, if\n                // this new score is not better than that, there's no\n                // need to try inserting it\n                reusableSD.doc = doc;\n                reusableSD.score = score;\n              } else {\n                return;\n              }\n              reusableSD = (ScoreDoc) hq.insertWithOverflow(reusableSD);\n            }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n    \n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    DocSet qDocSet = setHC.getDocSet();\n    qr.setDocSet(filter==null ? qDocSet : qDocSet.intersection(filter));\n    return qDocSet;\n  }\n\n","sourceOld":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n    final HitCollector hitCollector = ( cmd.getTimeAllowed() > 0 ) ? new TimeLimitedCollector( setHC, cmd.getTimeAllowed() ) : setHC;\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      try {\n        super.search(query, new HitCollector() {\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            if (score > topscore[0]) topscore[0]=score;\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (cmd.getSort() != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, cmd.getSort().getSort(), len);\n\n      try {\n        super.search(query, new HitCollector() {\n          private FieldDoc reusableFD;\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            if (reusableFD == null)\n              reusableFD = new FieldDoc(doc, score);\n            else {\n              reusableFD.score = score;\n              reusableFD.doc = doc;\n            }\n            reusableFD = (FieldDoc) hq.insertWithOverflow(reusableFD);\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      try {\n        super.search(query, new HitCollector() {\n          private ScoreDoc reusableSD;\n          public void collect(int doc, float score) {\n            hitCollector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n              // if docs are always delivered in order, we could use \"score>minScore\"\n              // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n              int nhits = numHits[0]++;\n              if (reusableSD == null) {\n                reusableSD = new ScoreDoc(doc, score);\n              } else if (nhits < lastDocRequested || score >= reusableSD.score) {\n                // reusableSD holds the last \"rejected\" entry, so, if\n                // this new score is not better than that, there's no\n                // need to try inserting it\n                reusableSD.doc = doc;\n                reusableSD.score = score;\n              } else {\n                return;\n              }\n              reusableSD = (ScoreDoc) hq.insertWithOverflow(reusableSD);\n            }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n    \n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    DocSet qDocSet = setHC.getDocSet();\n    qr.setDocSet(filter==null ? qDocSet : qDocSet.intersection(filter));\n    return qDocSet;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30391436869a41b74d4ba7098c40d955b686a10c","date":1242835871,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","sourceNew":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(maxDoc()>>6, maxDoc());\n    final HitCollector collector = ( cmd.getTimeAllowed() > 0 ) ? new TimeLimitedCollector( setHC, cmd.getTimeAllowed() ) : setHC;\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      try {\n        super.search(query, new HitCollector() {\n          public void collect(int doc, float score) {\n            collector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            if (score > topscore[0]) topscore[0]=score;\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (cmd.getSort() != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, cmd.getSort().getSort(), len);\n\n      try {\n        super.search(query, new HitCollector() {\n          private FieldDoc reusableFD;\n          public void collect(int doc, float score) {\n            collector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            if (reusableFD == null)\n              reusableFD = new FieldDoc(doc, score);\n            else {\n              reusableFD.score = score;\n              reusableFD.doc = doc;\n            }\n            reusableFD = (FieldDoc) hq.insertWithOverflow(reusableFD);\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      try {\n        super.search(query, new HitCollector() {\n          private ScoreDoc reusableSD;\n          public void collect(int doc, float score) {\n            collector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n              // if docs are always delivered in order, we could use \"score>minScore\"\n              // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n              int nhits = numHits[0]++;\n              if (reusableSD == null) {\n                reusableSD = new ScoreDoc(doc, score);\n              } else if (nhits < lastDocRequested || score >= reusableSD.score) {\n                // reusableSD holds the last \"rejected\" entry, so, if\n                // this new score is not better than that, there's no\n                // need to try inserting it\n                reusableSD.doc = doc;\n                reusableSD.score = score;\n              } else {\n                return;\n              }\n              reusableSD = (ScoreDoc) hq.insertWithOverflow(reusableSD);\n            }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n    \n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    DocSet qDocSet = setHC.getDocSet();\n    qr.setDocSet(filter==null ? qDocSet : qDocSet.intersection(filter));\n    return qDocSet;\n  }\n\n","sourceOld":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(HASHSET_INVERSE_LOAD_FACTOR, HASHDOCSET_MAXSIZE, maxDoc());\n    final HitCollector collector = ( cmd.getTimeAllowed() > 0 ) ? new TimeLimitedCollector( setHC, cmd.getTimeAllowed() ) : setHC;\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      try {\n        super.search(query, new HitCollector() {\n          public void collect(int doc, float score) {\n            collector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            if (score > topscore[0]) topscore[0]=score;\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (cmd.getSort() != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, cmd.getSort().getSort(), len);\n\n      try {\n        super.search(query, new HitCollector() {\n          private FieldDoc reusableFD;\n          public void collect(int doc, float score) {\n            collector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            if (reusableFD == null)\n              reusableFD = new FieldDoc(doc, score);\n            else {\n              reusableFD.score = score;\n              reusableFD.doc = doc;\n            }\n            reusableFD = (FieldDoc) hq.insertWithOverflow(reusableFD);\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      try {\n        super.search(query, new HitCollector() {\n          private ScoreDoc reusableSD;\n          public void collect(int doc, float score) {\n            collector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n              // if docs are always delivered in order, we could use \"score>minScore\"\n              // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n              int nhits = numHits[0]++;\n              if (reusableSD == null) {\n                reusableSD = new ScoreDoc(doc, score);\n              } else if (nhits < lastDocRequested || score >= reusableSD.score) {\n                // reusableSD holds the last \"rejected\" entry, so, if\n                // this new score is not better than that, there's no\n                // need to try inserting it\n                reusableSD.doc = doc;\n                reusableSD.score = score;\n              } else {\n                return;\n              }\n              reusableSD = (ScoreDoc) hq.insertWithOverflow(reusableSD);\n            }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n    \n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    DocSet qDocSet = setHC.getDocSet();\n    qr.setDocSet(filter==null ? qDocSet : qDocSet.intersection(filter));\n    return qDocSet;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"52df4540d5cd0c887f5e56ef0f387d7489f5d44f","date":1243099614,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","sourceNew":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n///////////////////// NEW\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n\n    final DocSetHitCollector collector = new DocSetHitCollector(maxDoc()>>6, maxDoc());\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n    final long timeAllowed = cmd.getTimeAllowed();\n\n    final Filter luceneFilter = filter==null ? null : filter.getTopFilter();\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      HitCollector hc = new HitCollector() {\n          public void collect(int doc, float score) {\n            collector.collect(doc, score);\n            numHits[0]++;\n            if (score > topscore[0]) topscore[0]=score;\n          }\n      };\n\n      if( timeAllowed > 0 ) {\n        hc = new TimeLimitedCollector( hc, timeAllowed );\n      }\n      try {\n        super.search(query, luceneFilter, hc);\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (cmd.getSort() != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, cmd.getSort().getSort(), len);\n\n      HitCollector hc = new HitCollector() {\n          private FieldDoc reusableFD;\n          public void collect(int doc, float score) {\n            collector.collect(doc, score);            \n            numHits[0]++;\n            if (reusableFD == null)\n              reusableFD = new FieldDoc(doc, score);\n            else {\n              reusableFD.score = score;\n              reusableFD.doc = doc;\n            }\n            reusableFD = (FieldDoc) hq.insertWithOverflow(reusableFD);\n          }\n        };\n\n      if( timeAllowed > 0 ) {\n        hc = new TimeLimitedCollector( hc, timeAllowed );\n      }\n      try {\n        super.search(query, luceneFilter, hc);\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n      \n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      \n      HitCollector hc = new HitCollector() {\n          private ScoreDoc reusableSD;\n          public void collect(int doc, float score) {\n            collector.collect(doc, score);\n\n            // if docs are always delivered in order, we could use \"score>minScore\"\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            int nhits = numHits[0]++;\n            if (reusableSD == null) {\n              reusableSD = new ScoreDoc(doc, score);\n            } else if (nhits < lastDocRequested || score >= reusableSD.score) {\n              // reusableSD holds the last \"rejected\" entry, so, if\n              // this new score is not better than that, there's no\n              // need to try inserting it\n              reusableSD.doc = doc;\n              reusableSD.score = score;\n            } else {\n              return;\n            }\n            reusableSD = (ScoreDoc) hq.insertWithOverflow(reusableSD);\n          }\n      };\n\n      if( timeAllowed > 0 ) {\n        hc = new TimeLimitedCollector( hc, timeAllowed );\n      }\n      try {\n        super.search(query, luceneFilter, hc);\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n\n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    // TODO: if we collect results before the filter, we just need to intersect with\n    // that filter to generate the DocSet for qr.setDocSet()\n    qr.setDocSet(collector.getDocSet());\n\n    // TODO: currently we don't generate the DocSet for the base query.\n    // But the QueryDocSet == CompleteDocSet if filter==null.\n    return filter==null ? qr.getDocSet() : null;\n  }\n\n","sourceOld":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    final DocSetHitCollector setHC = new DocSetHitCollector(maxDoc()>>6, maxDoc());\n    final HitCollector collector = ( cmd.getTimeAllowed() > 0 ) ? new TimeLimitedCollector( setHC, cmd.getTimeAllowed() ) : setHC;\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    // TODO: perhaps unify getDocListAndSetNC and getDocListNC without imposing a significant performance hit\n\n    // Comment: gathering the set before the filter is applied allows one to cache\n    // the resulting DocSet under the query.  The drawback is that it requires an\n    // extra intersection with the filter at the end.  This will be a net win\n    // for expensive queries.\n\n    // Q: what if the final intersection results in a small set from two large\n    // sets... it won't be a HashDocSet or other small set.  One way around\n    // this would be to collect the resulting set as we go (the filter is\n    // checked anyway).\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      try {\n        super.search(query, new HitCollector() {\n          public void collect(int doc, float score) {\n            collector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            if (score > topscore[0]) topscore[0]=score;\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (cmd.getSort() != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final DocSet filt = filter;\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, cmd.getSort().getSort(), len);\n\n      try {\n        super.search(query, new HitCollector() {\n          private FieldDoc reusableFD;\n          public void collect(int doc, float score) {\n            collector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n            numHits[0]++;\n            if (reusableFD == null)\n              reusableFD = new FieldDoc(doc, score);\n            else {\n              reusableFD.score = score;\n              reusableFD.doc = doc;\n            }\n            reusableFD = (FieldDoc) hq.insertWithOverflow(reusableFD);\n          }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final DocSet filt = filter;\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      try {\n        super.search(query, new HitCollector() {\n          private ScoreDoc reusableSD;\n          public void collect(int doc, float score) {\n            collector.collect(doc,score);\n            if (filt!=null && !filt.exists(doc)) return;\n              // if docs are always delivered in order, we could use \"score>minScore\"\n              // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n              int nhits = numHits[0]++;\n              if (reusableSD == null) {\n                reusableSD = new ScoreDoc(doc, score);\n              } else if (nhits < lastDocRequested || score >= reusableSD.score) {\n                // reusableSD holds the last \"rejected\" entry, so, if\n                // this new score is not better than that, there's no\n                // need to try inserting it\n                reusableSD.doc = doc;\n                reusableSD.score = score;\n              } else {\n                return;\n              }\n              reusableSD = (ScoreDoc) hq.insertWithOverflow(reusableSD);\n            }\n        }\n        );\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n    \n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    DocSet qDocSet = setHC.getDocSet();\n    qr.setDocSet(filter==null ? qDocSet : qDocSet.intersection(filter));\n    return qDocSet;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"029ffe7502a7a8ff1f425020bc204311ade99687","date":1243301392,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","sourceNew":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n///////////////////// NEW\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n\n    final DocSetHitCollector collector = new DocSetHitCollector(maxDoc()>>6, maxDoc());\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n    final long timeAllowed = cmd.getTimeAllowed();\n\n    final Filter luceneFilter = filter==null ? null : filter.getTopFilter();\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      HitCollector hc = new HitCollector() {\n          public void collect(int doc, float score) {\n            collector.collect(doc, score);\n            numHits[0]++;\n            if (score > topscore[0]) topscore[0]=score;\n          }\n      };\n\n      if( timeAllowed > 0 ) {\n        hc = new TimeLimitedCollector( hc, timeAllowed );\n      }\n      try {\n        searcher.search(query, luceneFilter, hc);\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (cmd.getSort() != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, cmd.getSort().getSort(), len);\n\n      HitCollector hc = new HitCollector() {\n          private FieldDoc reusableFD;\n          public void collect(int doc, float score) {\n            collector.collect(doc, score);            \n            numHits[0]++;\n            if (reusableFD == null)\n              reusableFD = new FieldDoc(doc, score);\n            else {\n              reusableFD.score = score;\n              reusableFD.doc = doc;\n            }\n            reusableFD = (FieldDoc) hq.insertWithOverflow(reusableFD);\n          }\n        };\n\n      if( timeAllowed > 0 ) {\n        hc = new TimeLimitedCollector( hc, timeAllowed );\n      }\n      try {\n        searcher.search(query, luceneFilter, hc);\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n      \n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      \n      HitCollector hc = new HitCollector() {\n          private ScoreDoc reusableSD;\n          public void collect(int doc, float score) {\n            collector.collect(doc, score);\n\n            // if docs are always delivered in order, we could use \"score>minScore\"\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            int nhits = numHits[0]++;\n            if (reusableSD == null) {\n              reusableSD = new ScoreDoc(doc, score);\n            } else if (nhits < lastDocRequested || score >= reusableSD.score) {\n              // reusableSD holds the last \"rejected\" entry, so, if\n              // this new score is not better than that, there's no\n              // need to try inserting it\n              reusableSD.doc = doc;\n              reusableSD.score = score;\n            } else {\n              return;\n            }\n            reusableSD = (ScoreDoc) hq.insertWithOverflow(reusableSD);\n          }\n      };\n\n      if( timeAllowed > 0 ) {\n        hc = new TimeLimitedCollector( hc, timeAllowed );\n      }\n      try {\n        searcher.search(query, luceneFilter, hc);\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n\n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    // TODO: if we collect results before the filter, we just need to intersect with\n    // that filter to generate the DocSet for qr.setDocSet()\n    qr.setDocSet(collector.getDocSet());\n\n    // TODO: currently we don't generate the DocSet for the base query.\n    // But the QueryDocSet == CompleteDocSet if filter==null.\n    return filter==null ? qr.getDocSet() : null;\n  }\n\n","sourceOld":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n///////////////////// NEW\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n\n    final DocSetHitCollector collector = new DocSetHitCollector(maxDoc()>>6, maxDoc());\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n    final long timeAllowed = cmd.getTimeAllowed();\n\n    final Filter luceneFilter = filter==null ? null : filter.getTopFilter();\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      HitCollector hc = new HitCollector() {\n          public void collect(int doc, float score) {\n            collector.collect(doc, score);\n            numHits[0]++;\n            if (score > topscore[0]) topscore[0]=score;\n          }\n      };\n\n      if( timeAllowed > 0 ) {\n        hc = new TimeLimitedCollector( hc, timeAllowed );\n      }\n      try {\n        super.search(query, luceneFilter, hc);\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (cmd.getSort() != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, cmd.getSort().getSort(), len);\n\n      HitCollector hc = new HitCollector() {\n          private FieldDoc reusableFD;\n          public void collect(int doc, float score) {\n            collector.collect(doc, score);            \n            numHits[0]++;\n            if (reusableFD == null)\n              reusableFD = new FieldDoc(doc, score);\n            else {\n              reusableFD.score = score;\n              reusableFD.doc = doc;\n            }\n            reusableFD = (FieldDoc) hq.insertWithOverflow(reusableFD);\n          }\n        };\n\n      if( timeAllowed > 0 ) {\n        hc = new TimeLimitedCollector( hc, timeAllowed );\n      }\n      try {\n        super.search(query, luceneFilter, hc);\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n      \n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      \n      HitCollector hc = new HitCollector() {\n          private ScoreDoc reusableSD;\n          public void collect(int doc, float score) {\n            collector.collect(doc, score);\n\n            // if docs are always delivered in order, we could use \"score>minScore\"\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            int nhits = numHits[0]++;\n            if (reusableSD == null) {\n              reusableSD = new ScoreDoc(doc, score);\n            } else if (nhits < lastDocRequested || score >= reusableSD.score) {\n              // reusableSD holds the last \"rejected\" entry, so, if\n              // this new score is not better than that, there's no\n              // need to try inserting it\n              reusableSD.doc = doc;\n              reusableSD.score = score;\n            } else {\n              return;\n            }\n            reusableSD = (ScoreDoc) hq.insertWithOverflow(reusableSD);\n          }\n      };\n\n      if( timeAllowed > 0 ) {\n        hc = new TimeLimitedCollector( hc, timeAllowed );\n      }\n      try {\n        super.search(query, luceneFilter, hc);\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n\n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    // TODO: if we collect results before the filter, we just need to intersect with\n    // that filter to generate the DocSet for qr.setDocSet()\n    qr.setDocSet(collector.getDocSet());\n\n    // TODO: currently we don't generate the DocSet for the base query.\n    // But the QueryDocSet == CompleteDocSet if filter==null.\n    return filter==null ? qr.getDocSet() : null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ac3e6437547a34cce2b5405ce0cf9e3af578401e","date":1243373693,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","sourceNew":"  // any DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    DocSet set;\n\n    boolean needScores = (cmd.getFlags() & GET_SCORES) != 0;\n    int maxDoc = maxDoc();\n    int smallSetSize = maxDoc>>6;\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n    final long timeAllowed = cmd.getTimeAllowed();\n\n    final Filter luceneFilter = filter==null ? null : filter.getTopFilter();\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n\n      Collector collector;\n      DocSetCollector setCollector;\n\n       if (!needScores) {\n         collector = setCollector = new DocSetCollector(smallSetSize, maxDoc);\n       } else {\n         collector = setCollector = new DocSetDelegateCollector(smallSetSize, maxDoc, new Collector() {\n           Scorer scorer;\n           public void setScorer(Scorer scorer) throws IOException {\n             this.scorer = scorer;\n           }\n           public void collect(int doc) throws IOException {\n             float score = scorer.score();\n             if (score > topscore[0]) topscore[0]=score;\n           }\n           public void setNextReader(IndexReader reader, int docBase) throws IOException {\n           }\n         });\n       }\n\n       if( timeAllowed > 0 ) {\n         collector = new TimeLimitingCollector(collector, timeAllowed);\n       }\n       try {\n         super.search(query, luceneFilter, collector);\n       }\n       catch( TimeLimitingCollector.TimeExceededException x ) {\n         log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n         qr.setPartialResults(true);\n       }\n\n      set = setCollector.getDocSet();\n\n      nDocsReturned = 0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = set.size();\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else {\n\n      TopDocsCollector topCollector;\n\n      if (cmd.getSort() == null) {\n        topCollector = TopScoreDocCollector.create(len, true);\n      } else {\n        topCollector = TopFieldCollector.create(cmd.getSort(), len, false, needScores, needScores, true);\n      }\n\n      DocSetCollector setCollector = new DocSetDelegateCollector(maxDoc>>6, maxDoc, topCollector);\n      Collector collector = setCollector;\n\n      if( timeAllowed > 0 ) {\n        collector = new TimeLimitingCollector(collector, timeAllowed );\n      }\n      try {\n        super.search(query, luceneFilter, collector);\n      }\n      catch( TimeLimitingCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      set = setCollector.getDocSet();      \n\n      totalHits = topCollector.getTotalHits();\n      assert(totalHits == set.size());\n\n      TopDocs topDocs = topCollector.topDocs(0, len);\n      maxScore = totalHits>0 ? topDocs.getMaxScore() : 0.0f;\n      nDocsReturned = topDocs.scoreDocs.length;\n\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i=0; i<nDocsReturned; i++) {\n        ScoreDoc scoreDoc = topDocs.scoreDocs[i];\n        ids[i] = scoreDoc.doc;\n        if (scores != null) scores[i] = scoreDoc.score;\n      }\n    }\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n\n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    // TODO: if we collect results before the filter, we just need to intersect with\n    // that filter to generate the DocSet for qr.setDocSet()\n    qr.setDocSet(set);\n\n    // TODO: currently we don't generate the DocSet for the base query,\n    // but the QueryDocSet == CompleteDocSet if filter==null.\n    return filter==null ? qr.getDocSet() : null;\n  }\n\n","sourceOld":"  // the DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n///////////////////// NEW\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n\n    final DocSetHitCollector collector = new DocSetHitCollector(maxDoc()>>6, maxDoc());\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n    final long timeAllowed = cmd.getTimeAllowed();\n\n    final Filter luceneFilter = filter==null ? null : filter.getTopFilter();\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final DocSet filt = filter;\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n      final int[] numHits = new int[1];\n\n      HitCollector hc = new HitCollector() {\n          public void collect(int doc, float score) {\n            collector.collect(doc, score);\n            numHits[0]++;\n            if (score > topscore[0]) topscore[0]=score;\n          }\n      };\n\n      if( timeAllowed > 0 ) {\n        hc = new TimeLimitedCollector( hc, timeAllowed );\n      }\n      try {\n        searcher.search(query, luceneFilter, hc);\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n\n      nDocsReturned=0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else if (cmd.getSort() != null) {\n      // can't use TopDocs if there is a sort since it\n      // will do automatic score normalization.\n      // NOTE: this changed late in Lucene 1.9\n\n      final int[] numHits = new int[1];\n      final FieldSortedHitQueue hq = new FieldSortedHitQueue(reader, cmd.getSort().getSort(), len);\n\n      HitCollector hc = new HitCollector() {\n          private FieldDoc reusableFD;\n          public void collect(int doc, float score) {\n            collector.collect(doc, score);            \n            numHits[0]++;\n            if (reusableFD == null)\n              reusableFD = new FieldDoc(doc, score);\n            else {\n              reusableFD.score = score;\n              reusableFD.doc = doc;\n            }\n            reusableFD = (FieldDoc) hq.insertWithOverflow(reusableFD);\n          }\n        };\n\n      if( timeAllowed > 0 ) {\n        hc = new TimeLimitedCollector( hc, timeAllowed );\n      }\n      try {\n        searcher.search(query, luceneFilter, hc);\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n      \n\n      totalHits = numHits[0];\n      maxScore = totalHits>0 ? hq.getMaxScore() : 0.0f;\n\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        FieldDoc fieldDoc = (FieldDoc)hq.pop();\n        // fillFields is the point where score normalization happens\n        // hq.fillFields(fieldDoc)\n        ids[i] = fieldDoc.doc;\n        if (scores != null) scores[i] = fieldDoc.score;\n      }\n    } else {\n      // No Sort specified (sort by score descending)\n      // This case could be done with TopDocs, but would currently require\n      // getting a BitSet filter from a DocSet which may be inefficient.\n\n      final ScorePriorityQueue hq = new ScorePriorityQueue(lastDocRequested);\n      final int[] numHits = new int[1];\n      \n      HitCollector hc = new HitCollector() {\n          private ScoreDoc reusableSD;\n          public void collect(int doc, float score) {\n            collector.collect(doc, score);\n\n            // if docs are always delivered in order, we could use \"score>minScore\"\n            // but might BooleanScorer14 might still be used and deliver docs out-of-order?\n            int nhits = numHits[0]++;\n            if (reusableSD == null) {\n              reusableSD = new ScoreDoc(doc, score);\n            } else if (nhits < lastDocRequested || score >= reusableSD.score) {\n              // reusableSD holds the last \"rejected\" entry, so, if\n              // this new score is not better than that, there's no\n              // need to try inserting it\n              reusableSD.doc = doc;\n              reusableSD.score = score;\n            } else {\n              return;\n            }\n            reusableSD = (ScoreDoc) hq.insertWithOverflow(reusableSD);\n          }\n      };\n\n      if( timeAllowed > 0 ) {\n        hc = new TimeLimitedCollector( hc, timeAllowed );\n      }\n      try {\n        searcher.search(query, luceneFilter, hc);\n      }\n      catch( TimeLimitedCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      totalHits = numHits[0];\n      nDocsReturned = hq.size();\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      ScoreDoc sdoc =null;\n      for (int i = nDocsReturned -1; i >= 0; i--) {\n        sdoc = (ScoreDoc)hq.pop();\n        ids[i] = sdoc.doc;\n        if (scores != null) scores[i] = sdoc.score;\n      }\n      maxScore = sdoc ==null ? 0.0f : sdoc.score;\n    }\n\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n\n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    // TODO: if we collect results before the filter, we just need to intersect with\n    // that filter to generate the DocSet for qr.setDocSet()\n    qr.setDocSet(collector.getDocSet());\n\n    // TODO: currently we don't generate the DocSet for the base query.\n    // But the QueryDocSet == CompleteDocSet if filter==null.\n    return filter==null ? qr.getDocSet() : null;\n  }\n\n","bugFix":null,"bugIntro":["23550189554f52bad1625fceab84a71d20a4df3f","23550189554f52bad1625fceab84a71d20a4df3f","23550189554f52bad1625fceab84a71d20a4df3f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1e77721aaf23393f6ea7926045ae6f8efea0ce8e","date":1247678464,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","sourceNew":"  // any DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    DocSet set;\n\n    boolean needScores = (cmd.getFlags() & GET_SCORES) != 0;\n    int maxDoc = maxDoc();\n    int smallSetSize = maxDoc>>6;\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n    final long timeAllowed = cmd.getTimeAllowed();\n\n    final Filter luceneFilter = filter==null ? null : filter.getTopFilter();\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n\n      Collector collector;\n      DocSetCollector setCollector;\n\n       if (!needScores) {\n         collector = setCollector = new DocSetCollector(smallSetSize, maxDoc);\n       } else {\n         collector = setCollector = new DocSetDelegateCollector(smallSetSize, maxDoc, new Collector() {\n           Scorer scorer;\n           public void setScorer(Scorer scorer) throws IOException {\n             this.scorer = scorer;\n           }\n           public void collect(int doc) throws IOException {\n             float score = scorer.score();\n             if (score > topscore[0]) topscore[0]=score;\n           }\n           public void setNextReader(IndexReader reader, int docBase) throws IOException {\n           }\n           public boolean acceptsDocsOutOfOrder() {\n             return false;\n           }\n         });\n       }\n\n       if( timeAllowed > 0 ) {\n         collector = new TimeLimitingCollector(collector, timeAllowed);\n       }\n       try {\n         super.search(query, luceneFilter, collector);\n       }\n       catch( TimeLimitingCollector.TimeExceededException x ) {\n         log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n         qr.setPartialResults(true);\n       }\n\n      set = setCollector.getDocSet();\n\n      nDocsReturned = 0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = set.size();\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else {\n\n      TopDocsCollector topCollector;\n\n      if (cmd.getSort() == null) {\n        topCollector = TopScoreDocCollector.create(len, true);\n      } else {\n        topCollector = TopFieldCollector.create(cmd.getSort(), len, false, needScores, needScores, true);\n      }\n\n      DocSetCollector setCollector = new DocSetDelegateCollector(maxDoc>>6, maxDoc, topCollector);\n      Collector collector = setCollector;\n\n      if( timeAllowed > 0 ) {\n        collector = new TimeLimitingCollector(collector, timeAllowed );\n      }\n      try {\n        super.search(query, luceneFilter, collector);\n      }\n      catch( TimeLimitingCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      set = setCollector.getDocSet();      \n\n      totalHits = topCollector.getTotalHits();\n      assert(totalHits == set.size());\n\n      TopDocs topDocs = topCollector.topDocs(0, len);\n      maxScore = totalHits>0 ? topDocs.getMaxScore() : 0.0f;\n      nDocsReturned = topDocs.scoreDocs.length;\n\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i=0; i<nDocsReturned; i++) {\n        ScoreDoc scoreDoc = topDocs.scoreDocs[i];\n        ids[i] = scoreDoc.doc;\n        if (scores != null) scores[i] = scoreDoc.score;\n      }\n    }\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n\n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    // TODO: if we collect results before the filter, we just need to intersect with\n    // that filter to generate the DocSet for qr.setDocSet()\n    qr.setDocSet(set);\n\n    // TODO: currently we don't generate the DocSet for the base query,\n    // but the QueryDocSet == CompleteDocSet if filter==null.\n    return filter==null ? qr.getDocSet() : null;\n  }\n\n","sourceOld":"  // any DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    DocSet set;\n\n    boolean needScores = (cmd.getFlags() & GET_SCORES) != 0;\n    int maxDoc = maxDoc();\n    int smallSetSize = maxDoc>>6;\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n    final long timeAllowed = cmd.getTimeAllowed();\n\n    final Filter luceneFilter = filter==null ? null : filter.getTopFilter();\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n\n      Collector collector;\n      DocSetCollector setCollector;\n\n       if (!needScores) {\n         collector = setCollector = new DocSetCollector(smallSetSize, maxDoc);\n       } else {\n         collector = setCollector = new DocSetDelegateCollector(smallSetSize, maxDoc, new Collector() {\n           Scorer scorer;\n           public void setScorer(Scorer scorer) throws IOException {\n             this.scorer = scorer;\n           }\n           public void collect(int doc) throws IOException {\n             float score = scorer.score();\n             if (score > topscore[0]) topscore[0]=score;\n           }\n           public void setNextReader(IndexReader reader, int docBase) throws IOException {\n           }\n         });\n       }\n\n       if( timeAllowed > 0 ) {\n         collector = new TimeLimitingCollector(collector, timeAllowed);\n       }\n       try {\n         super.search(query, luceneFilter, collector);\n       }\n       catch( TimeLimitingCollector.TimeExceededException x ) {\n         log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n         qr.setPartialResults(true);\n       }\n\n      set = setCollector.getDocSet();\n\n      nDocsReturned = 0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = set.size();\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else {\n\n      TopDocsCollector topCollector;\n\n      if (cmd.getSort() == null) {\n        topCollector = TopScoreDocCollector.create(len, true);\n      } else {\n        topCollector = TopFieldCollector.create(cmd.getSort(), len, false, needScores, needScores, true);\n      }\n\n      DocSetCollector setCollector = new DocSetDelegateCollector(maxDoc>>6, maxDoc, topCollector);\n      Collector collector = setCollector;\n\n      if( timeAllowed > 0 ) {\n        collector = new TimeLimitingCollector(collector, timeAllowed );\n      }\n      try {\n        super.search(query, luceneFilter, collector);\n      }\n      catch( TimeLimitingCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      set = setCollector.getDocSet();      \n\n      totalHits = topCollector.getTotalHits();\n      assert(totalHits == set.size());\n\n      TopDocs topDocs = topCollector.topDocs(0, len);\n      maxScore = totalHits>0 ? topDocs.getMaxScore() : 0.0f;\n      nDocsReturned = topDocs.scoreDocs.length;\n\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i=0; i<nDocsReturned; i++) {\n        ScoreDoc scoreDoc = topDocs.scoreDocs[i];\n        ids[i] = scoreDoc.doc;\n        if (scores != null) scores[i] = scoreDoc.score;\n      }\n    }\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n\n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    // TODO: if we collect results before the filter, we just need to intersect with\n    // that filter to generate the DocSet for qr.setDocSet()\n    qr.setDocSet(set);\n\n    // TODO: currently we don't generate the DocSet for the base query,\n    // but the QueryDocSet == CompleteDocSet if filter==null.\n    return filter==null ? qr.getDocSet() : null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","pathOld":"src/java/org/apache/solr/search/SolrIndexSearcher#getDocListAndSetNC(QueryResult,QueryCommand).mjava","sourceNew":"  // any DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    DocSet set;\n\n    boolean needScores = (cmd.getFlags() & GET_SCORES) != 0;\n    int maxDoc = maxDoc();\n    int smallSetSize = maxDoc>>6;\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n    final long timeAllowed = cmd.getTimeAllowed();\n\n    final Filter luceneFilter = filter==null ? null : filter.getTopFilter();\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n\n      Collector collector;\n      DocSetCollector setCollector;\n\n       if (!needScores) {\n         collector = setCollector = new DocSetCollector(smallSetSize, maxDoc);\n       } else {\n         collector = setCollector = new DocSetDelegateCollector(smallSetSize, maxDoc, new Collector() {\n           Scorer scorer;\n           public void setScorer(Scorer scorer) throws IOException {\n             this.scorer = scorer;\n           }\n           public void collect(int doc) throws IOException {\n             float score = scorer.score();\n             if (score > topscore[0]) topscore[0]=score;\n           }\n           public void setNextReader(IndexReader reader, int docBase) throws IOException {\n           }\n           public boolean acceptsDocsOutOfOrder() {\n             return false;\n           }\n         });\n       }\n\n       if( timeAllowed > 0 ) {\n         collector = new TimeLimitingCollector(collector, timeAllowed);\n       }\n       try {\n         super.search(query, luceneFilter, collector);\n       }\n       catch( TimeLimitingCollector.TimeExceededException x ) {\n         log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n         qr.setPartialResults(true);\n       }\n\n      set = setCollector.getDocSet();\n\n      nDocsReturned = 0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = set.size();\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else {\n\n      TopDocsCollector topCollector;\n\n      if (cmd.getSort() == null) {\n        topCollector = TopScoreDocCollector.create(len, true);\n      } else {\n        topCollector = TopFieldCollector.create(cmd.getSort(), len, false, needScores, needScores, true);\n      }\n\n      DocSetCollector setCollector = new DocSetDelegateCollector(maxDoc>>6, maxDoc, topCollector);\n      Collector collector = setCollector;\n\n      if( timeAllowed > 0 ) {\n        collector = new TimeLimitingCollector(collector, timeAllowed );\n      }\n      try {\n        super.search(query, luceneFilter, collector);\n      }\n      catch( TimeLimitingCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      set = setCollector.getDocSet();      \n\n      totalHits = topCollector.getTotalHits();\n      assert(totalHits == set.size());\n\n      TopDocs topDocs = topCollector.topDocs(0, len);\n      maxScore = totalHits>0 ? topDocs.getMaxScore() : 0.0f;\n      nDocsReturned = topDocs.scoreDocs.length;\n\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i=0; i<nDocsReturned; i++) {\n        ScoreDoc scoreDoc = topDocs.scoreDocs[i];\n        ids[i] = scoreDoc.doc;\n        if (scores != null) scores[i] = scoreDoc.score;\n      }\n    }\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n\n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    // TODO: if we collect results before the filter, we just need to intersect with\n    // that filter to generate the DocSet for qr.setDocSet()\n    qr.setDocSet(set);\n\n    // TODO: currently we don't generate the DocSet for the base query,\n    // but the QueryDocSet == CompleteDocSet if filter==null.\n    return filter==null ? qr.getDocSet() : null;\n  }\n\n","sourceOld":"  // any DocSet returned is for the query only, without any filtering... that way it may\n  // be cached if desired.\n  private DocSet getDocListAndSetNC(QueryResult qr,QueryCommand cmd) throws IOException {\n    int len = cmd.getSupersetMaxDoc();\n    DocSet filter = cmd.getFilter()!=null ? cmd.getFilter() : getDocSet(cmd.getFilterList());\n    int last = len;\n    if (last < 0 || last > maxDoc()) last=maxDoc();\n    final int lastDocRequested = last;\n    int nDocsReturned;\n    int totalHits;\n    float maxScore;\n    int[] ids;\n    float[] scores;\n    DocSet set;\n\n    boolean needScores = (cmd.getFlags() & GET_SCORES) != 0;\n    int maxDoc = maxDoc();\n    int smallSetSize = maxDoc>>6;\n\n    Query query = QueryUtils.makeQueryable(cmd.getQuery());\n    final long timeAllowed = cmd.getTimeAllowed();\n\n    final Filter luceneFilter = filter==null ? null : filter.getTopFilter();\n\n    // handle zero case...\n    if (lastDocRequested<=0) {\n      final float[] topscore = new float[] { Float.NEGATIVE_INFINITY };\n\n      Collector collector;\n      DocSetCollector setCollector;\n\n       if (!needScores) {\n         collector = setCollector = new DocSetCollector(smallSetSize, maxDoc);\n       } else {\n         collector = setCollector = new DocSetDelegateCollector(smallSetSize, maxDoc, new Collector() {\n           Scorer scorer;\n           public void setScorer(Scorer scorer) throws IOException {\n             this.scorer = scorer;\n           }\n           public void collect(int doc) throws IOException {\n             float score = scorer.score();\n             if (score > topscore[0]) topscore[0]=score;\n           }\n           public void setNextReader(IndexReader reader, int docBase) throws IOException {\n           }\n           public boolean acceptsDocsOutOfOrder() {\n             return false;\n           }\n         });\n       }\n\n       if( timeAllowed > 0 ) {\n         collector = new TimeLimitingCollector(collector, timeAllowed);\n       }\n       try {\n         super.search(query, luceneFilter, collector);\n       }\n       catch( TimeLimitingCollector.TimeExceededException x ) {\n         log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n         qr.setPartialResults(true);\n       }\n\n      set = setCollector.getDocSet();\n\n      nDocsReturned = 0;\n      ids = new int[nDocsReturned];\n      scores = new float[nDocsReturned];\n      totalHits = set.size();\n      maxScore = totalHits>0 ? topscore[0] : 0.0f;\n    } else {\n\n      TopDocsCollector topCollector;\n\n      if (cmd.getSort() == null) {\n        topCollector = TopScoreDocCollector.create(len, true);\n      } else {\n        topCollector = TopFieldCollector.create(cmd.getSort(), len, false, needScores, needScores, true);\n      }\n\n      DocSetCollector setCollector = new DocSetDelegateCollector(maxDoc>>6, maxDoc, topCollector);\n      Collector collector = setCollector;\n\n      if( timeAllowed > 0 ) {\n        collector = new TimeLimitingCollector(collector, timeAllowed );\n      }\n      try {\n        super.search(query, luceneFilter, collector);\n      }\n      catch( TimeLimitingCollector.TimeExceededException x ) {\n        log.warn( \"Query: \" + query + \"; \" + x.getMessage() );\n        qr.setPartialResults(true);\n      }\n\n      set = setCollector.getDocSet();      \n\n      totalHits = topCollector.getTotalHits();\n      assert(totalHits == set.size());\n\n      TopDocs topDocs = topCollector.topDocs(0, len);\n      maxScore = totalHits>0 ? topDocs.getMaxScore() : 0.0f;\n      nDocsReturned = topDocs.scoreDocs.length;\n\n      ids = new int[nDocsReturned];\n      scores = (cmd.getFlags()&GET_SCORES)!=0 ? new float[nDocsReturned] : null;\n      for (int i=0; i<nDocsReturned; i++) {\n        ScoreDoc scoreDoc = topDocs.scoreDocs[i];\n        ids[i] = scoreDoc.doc;\n        if (scores != null) scores[i] = scoreDoc.score;\n      }\n    }\n\n    int sliceLen = Math.min(lastDocRequested,nDocsReturned);\n    if (sliceLen < 0) sliceLen=0;\n\n    qr.setDocList(new DocSlice(0,sliceLen,ids,scores,totalHits,maxScore));\n    // TODO: if we collect results before the filter, we just need to intersect with\n    // that filter to generate the DocSet for qr.setDocSet()\n    qr.setDocSet(set);\n\n    // TODO: currently we don't generate the DocSet for the base query,\n    // but the QueryDocSet == CompleteDocSet if filter==null.\n    return filter==null ? qr.getDocSet() : null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"52df4540d5cd0c887f5e56ef0f387d7489f5d44f":["30391436869a41b74d4ba7098c40d955b686a10c"],"ac3e6437547a34cce2b5405ce0cf9e3af578401e":["029ffe7502a7a8ff1f425020bc204311ade99687"],"029ffe7502a7a8ff1f425020bc204311ade99687":["52df4540d5cd0c887f5e56ef0f387d7489f5d44f"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"30391436869a41b74d4ba7098c40d955b686a10c":["af88ddf3d03b8f9d83ad08cafaa7438a1206e405"],"00da8b51bdeae168a5b26ec506db372b219ca7a2":["db25c1f61b5ae826f10777da6551a832703967d5"],"1e77721aaf23393f6ea7926045ae6f8efea0ce8e":["ac3e6437547a34cce2b5405ce0cf9e3af578401e"],"b7856e050319831be8cd0cd1ae493fc7e91e9b05":["00da8b51bdeae168a5b26ec506db372b219ca7a2"],"7ceb8ef10f044820fbd058f02d5a8e26539d255c":["b7856e050319831be8cd0cd1ae493fc7e91e9b05"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"db25c1f61b5ae826f10777da6551a832703967d5":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"ad94625fb8d088209f46650c8097196fec67f00c":["1e77721aaf23393f6ea7926045ae6f8efea0ce8e"],"af88ddf3d03b8f9d83ad08cafaa7438a1206e405":["7ceb8ef10f044820fbd058f02d5a8e26539d255c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"52df4540d5cd0c887f5e56ef0f387d7489f5d44f":["029ffe7502a7a8ff1f425020bc204311ade99687"],"ac3e6437547a34cce2b5405ce0cf9e3af578401e":["1e77721aaf23393f6ea7926045ae6f8efea0ce8e"],"029ffe7502a7a8ff1f425020bc204311ade99687":["ac3e6437547a34cce2b5405ce0cf9e3af578401e"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["db25c1f61b5ae826f10777da6551a832703967d5"],"30391436869a41b74d4ba7098c40d955b686a10c":["52df4540d5cd0c887f5e56ef0f387d7489f5d44f"],"00da8b51bdeae168a5b26ec506db372b219ca7a2":["b7856e050319831be8cd0cd1ae493fc7e91e9b05"],"1e77721aaf23393f6ea7926045ae6f8efea0ce8e":["ad94625fb8d088209f46650c8097196fec67f00c"],"b7856e050319831be8cd0cd1ae493fc7e91e9b05":["7ceb8ef10f044820fbd058f02d5a8e26539d255c"],"7ceb8ef10f044820fbd058f02d5a8e26539d255c":["af88ddf3d03b8f9d83ad08cafaa7438a1206e405"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"db25c1f61b5ae826f10777da6551a832703967d5":["00da8b51bdeae168a5b26ec506db372b219ca7a2"],"af88ddf3d03b8f9d83ad08cafaa7438a1206e405":["30391436869a41b74d4ba7098c40d955b686a10c"],"ad94625fb8d088209f46650c8097196fec67f00c":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ad94625fb8d088209f46650c8097196fec67f00c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}