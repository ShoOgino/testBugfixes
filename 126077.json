{"path":"lucene/spatial/src/test/org/apache/lucene/spatial/util/BaseGeoPointTestCase#searchSmallSet(Query,int).mjava","commits":[{"id":"d6c59bc551f4e523ce6a321280cc6733424fb824","date":1458837690,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/spatial/src/test/org/apache/lucene/spatial/util/BaseGeoPointTestCase#searchSmallSet(Query,int).mjava","pathOld":"/dev/null","sourceNew":"  /** return topdocs over a small set of points in field \"point\" */\n  private TopDocs searchSmallSet(Query query, int size) throws Exception {\n    // this is a simple systematic test, indexing these points\n    double[][] pts = new double[][] {\n        { 32.763420,          -96.774             },\n        { 32.7559529921407,   -96.7759895324707   },\n        { 32.77866942010977,  -96.77701950073242  },\n        { 32.7756745755423,   -96.7706036567688   },\n        { 27.703618681345585, -139.73458170890808 },\n        { 32.94823588839368,  -96.4538113027811   },\n        { 33.06047141970814,  -96.65084838867188  },\n        { 32.778650,          -96.7772            },\n        { -88.56029371730983, -177.23537676036358 },\n        { 33.541429799076354, -26.779373834241003 },\n        { 26.774024500421728, -77.35379276106497  },\n        { -90.0,              -14.796283808944777 },\n        { 32.94823588839368,  -178.8538113027811  },\n        { 32.94823588839368,  178.8538113027811   },\n        { 40.720611,          -73.998776          },\n        { -44.5,              -179.5              }\n    };\n    \n    Directory directory = newDirectory();\n\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n            newIndexWriterConfig(new MockAnalyzer(random()))\n                    .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n                    .setMergePolicy(newLogMergePolicy()));\n\n    for (double p[] : pts) {\n        Document doc = new Document();\n        addPointToDoc(\"point\", doc, p[0], p[1]);\n        writer.addDocument(doc);\n    }\n\n    // add explicit multi-valued docs\n    for (int i=0; i<pts.length; i+=2) {\n      Document doc = new Document();\n      addPointToDoc(\"point\", doc, pts[i][0], pts[i][1]);\n      addPointToDoc(\"point\", doc, pts[i+1][0], pts[i+1][1]);\n      writer.addDocument(doc);\n    }\n\n    // index random string documents\n    for (int i=0; i<random().nextInt(10); ++i) {\n      Document doc = new Document();\n      doc.add(new StringField(\"string\", Integer.toString(i), Field.Store.NO));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    IndexSearcher searcher = newSearcher(reader);\n    TopDocs topDocs = searcher.search(query, size);\n    reader.close();\n    directory.close();\n    return topDocs;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["b9e52892242a8c82e1b0c1bd4f1d404366b0501c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b9e52892242a8c82e1b0c1bd4f1d404366b0501c","date":1459531691,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/spatial/src/test/org/apache/lucene/spatial/util/BaseGeoPointTestCase#searchSmallSet(Query,int).mjava","pathOld":"lucene/spatial/src/test/org/apache/lucene/spatial/util/BaseGeoPointTestCase#searchSmallSet(Query,int).mjava","sourceNew":"  /** return topdocs over a small set of points in field \"point\" */\n  private TopDocs searchSmallSet(Query query, int size) throws Exception {\n    // this is a simple systematic test, indexing these points\n    double[][] pts = new double[][] {\n        { 32.763420,          -96.774             },\n        { 32.7559529921407,   -96.7759895324707   },\n        { 32.77866942010977,  -96.77701950073242  },\n        { 32.7756745755423,   -96.7706036567688   },\n        { 27.703618681345585, -139.73458170890808 },\n        { 32.94823588839368,  -96.4538113027811   },\n        { 33.06047141970814,  -96.65084838867188  },\n        { 32.778650,          -96.7772            },\n        { -88.56029371730983, -177.23537676036358 },\n        { 33.541429799076354, -26.779373834241003 },\n        { 26.774024500421728, -77.35379276106497  },\n        { -90.0,              -14.796283808944777 },\n        { 32.94823588839368,  -178.8538113027811  },\n        { 32.94823588839368,  178.8538113027811   },\n        { 40.720611,          -73.998776          },\n        { -44.5,              -179.5              }\n    };\n    \n    Directory directory = newDirectory();\n\n    // TODO: must these simple tests really rely on docid order?\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000));\n    iwc.setMergePolicy(newLogMergePolicy());\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, iwc);\n\n    for (double p[] : pts) {\n        Document doc = new Document();\n        addPointToDoc(\"point\", doc, p[0], p[1]);\n        writer.addDocument(doc);\n    }\n\n    // add explicit multi-valued docs\n    for (int i=0; i<pts.length; i+=2) {\n      Document doc = new Document();\n      addPointToDoc(\"point\", doc, pts[i][0], pts[i][1]);\n      addPointToDoc(\"point\", doc, pts[i+1][0], pts[i+1][1]);\n      writer.addDocument(doc);\n    }\n\n    // index random string documents\n    for (int i=0; i<random().nextInt(10); ++i) {\n      Document doc = new Document();\n      doc.add(new StringField(\"string\", Integer.toString(i), Field.Store.NO));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    IndexSearcher searcher = newSearcher(reader);\n    TopDocs topDocs = searcher.search(query, size);\n    reader.close();\n    directory.close();\n    return topDocs;\n  }\n\n","sourceOld":"  /** return topdocs over a small set of points in field \"point\" */\n  private TopDocs searchSmallSet(Query query, int size) throws Exception {\n    // this is a simple systematic test, indexing these points\n    double[][] pts = new double[][] {\n        { 32.763420,          -96.774             },\n        { 32.7559529921407,   -96.7759895324707   },\n        { 32.77866942010977,  -96.77701950073242  },\n        { 32.7756745755423,   -96.7706036567688   },\n        { 27.703618681345585, -139.73458170890808 },\n        { 32.94823588839368,  -96.4538113027811   },\n        { 33.06047141970814,  -96.65084838867188  },\n        { 32.778650,          -96.7772            },\n        { -88.56029371730983, -177.23537676036358 },\n        { 33.541429799076354, -26.779373834241003 },\n        { 26.774024500421728, -77.35379276106497  },\n        { -90.0,              -14.796283808944777 },\n        { 32.94823588839368,  -178.8538113027811  },\n        { 32.94823588839368,  178.8538113027811   },\n        { 40.720611,          -73.998776          },\n        { -44.5,              -179.5              }\n    };\n    \n    Directory directory = newDirectory();\n\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory,\n            newIndexWriterConfig(new MockAnalyzer(random()))\n                    .setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000))\n                    .setMergePolicy(newLogMergePolicy()));\n\n    for (double p[] : pts) {\n        Document doc = new Document();\n        addPointToDoc(\"point\", doc, p[0], p[1]);\n        writer.addDocument(doc);\n    }\n\n    // add explicit multi-valued docs\n    for (int i=0; i<pts.length; i+=2) {\n      Document doc = new Document();\n      addPointToDoc(\"point\", doc, pts[i][0], pts[i][1]);\n      addPointToDoc(\"point\", doc, pts[i+1][0], pts[i+1][1]);\n      writer.addDocument(doc);\n    }\n\n    // index random string documents\n    for (int i=0; i<random().nextInt(10); ++i) {\n      Document doc = new Document();\n      doc.add(new StringField(\"string\", Integer.toString(i), Field.Store.NO));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    IndexSearcher searcher = newSearcher(reader);\n    TopDocs topDocs = searcher.search(query, size);\n    reader.close();\n    directory.close();\n    return topDocs;\n  }\n\n","bugFix":["d6c59bc551f4e523ce6a321280cc6733424fb824"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"03bffb520caf6e9833c4b9a82ac67d19a1f3fc97","date":1459595815,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/spatial/src/test/org/apache/lucene/spatial/util/BaseGeoPointTestCase#searchSmallSet(Query,int).mjava","pathOld":"lucene/spatial/src/test/org/apache/lucene/spatial/util/BaseGeoPointTestCase#searchSmallSet(Query,int).mjava","sourceNew":"  /** return topdocs over a small set of points in field \"point\" */\n  private TopDocs searchSmallSet(Query query, int size) throws Exception {\n    // this is a simple systematic test, indexing these points\n    // TODO: fragile: does not understand quantization in any way yet uses extremely high precision!\n    double[][] pts = new double[][] {\n        { 32.763420,          -96.774             },\n        { 32.7559529921407,   -96.7759895324707   },\n        { 32.77866942010977,  -96.77701950073242  },\n        { 32.7756745755423,   -96.7706036567688   },\n        { 27.703618681345585, -139.73458170890808 },\n        { 32.94823588839368,  -96.4538113027811   },\n        { 33.06047141970814,  -96.65084838867188  },\n        { 32.778650,          -96.7772            },\n        { -88.56029371730983, -177.23537676036358 },\n        { 33.541429799076354, -26.779373834241003 },\n        { 26.774024500421728, -77.35379276106497  },\n        { -90.0,              -14.796283808944777 },\n        { 32.94823588839368,  -178.8538113027811  },\n        { 32.94823588839368,  178.8538113027811   },\n        { 40.720611,          -73.998776          },\n        { -44.5,              -179.5              }\n    };\n    \n    Directory directory = newDirectory();\n\n    // TODO: must these simple tests really rely on docid order?\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000));\n    iwc.setMergePolicy(newLogMergePolicy());\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, iwc);\n\n    for (double p[] : pts) {\n        Document doc = new Document();\n        addPointToDoc(\"point\", doc, p[0], p[1]);\n        writer.addDocument(doc);\n    }\n\n    // add explicit multi-valued docs\n    for (int i=0; i<pts.length; i+=2) {\n      Document doc = new Document();\n      addPointToDoc(\"point\", doc, pts[i][0], pts[i][1]);\n      addPointToDoc(\"point\", doc, pts[i+1][0], pts[i+1][1]);\n      writer.addDocument(doc);\n    }\n\n    // index random string documents\n    for (int i=0; i<random().nextInt(10); ++i) {\n      Document doc = new Document();\n      doc.add(new StringField(\"string\", Integer.toString(i), Field.Store.NO));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    IndexSearcher searcher = newSearcher(reader);\n    TopDocs topDocs = searcher.search(query, size);\n    reader.close();\n    directory.close();\n    return topDocs;\n  }\n\n","sourceOld":"  /** return topdocs over a small set of points in field \"point\" */\n  private TopDocs searchSmallSet(Query query, int size) throws Exception {\n    // this is a simple systematic test, indexing these points\n    double[][] pts = new double[][] {\n        { 32.763420,          -96.774             },\n        { 32.7559529921407,   -96.7759895324707   },\n        { 32.77866942010977,  -96.77701950073242  },\n        { 32.7756745755423,   -96.7706036567688   },\n        { 27.703618681345585, -139.73458170890808 },\n        { 32.94823588839368,  -96.4538113027811   },\n        { 33.06047141970814,  -96.65084838867188  },\n        { 32.778650,          -96.7772            },\n        { -88.56029371730983, -177.23537676036358 },\n        { 33.541429799076354, -26.779373834241003 },\n        { 26.774024500421728, -77.35379276106497  },\n        { -90.0,              -14.796283808944777 },\n        { 32.94823588839368,  -178.8538113027811  },\n        { 32.94823588839368,  178.8538113027811   },\n        { 40.720611,          -73.998776          },\n        { -44.5,              -179.5              }\n    };\n    \n    Directory directory = newDirectory();\n\n    // TODO: must these simple tests really rely on docid order?\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000));\n    iwc.setMergePolicy(newLogMergePolicy());\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, iwc);\n\n    for (double p[] : pts) {\n        Document doc = new Document();\n        addPointToDoc(\"point\", doc, p[0], p[1]);\n        writer.addDocument(doc);\n    }\n\n    // add explicit multi-valued docs\n    for (int i=0; i<pts.length; i+=2) {\n      Document doc = new Document();\n      addPointToDoc(\"point\", doc, pts[i][0], pts[i][1]);\n      addPointToDoc(\"point\", doc, pts[i+1][0], pts[i+1][1]);\n      writer.addDocument(doc);\n    }\n\n    // index random string documents\n    for (int i=0; i<random().nextInt(10); ++i) {\n      Document doc = new Document();\n      doc.add(new StringField(\"string\", Integer.toString(i), Field.Store.NO));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    IndexSearcher searcher = newSearcher(reader);\n    TopDocs topDocs = searcher.search(query, size);\n    reader.close();\n    directory.close();\n    return topDocs;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3e21d7642ee7e0c00429964e5b47504602fe218c","date":1460897579,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#searchSmallSet(Query,int).mjava","pathOld":"lucene/spatial/src/test/org/apache/lucene/spatial/util/BaseGeoPointTestCase#searchSmallSet(Query,int).mjava","sourceNew":"  /** return topdocs over a small set of points in field \"point\" */\n  private TopDocs searchSmallSet(Query query, int size) throws Exception {\n    // this is a simple systematic test, indexing these points\n    // TODO: fragile: does not understand quantization in any way yet uses extremely high precision!\n    double[][] pts = new double[][] {\n        { 32.763420,          -96.774             },\n        { 32.7559529921407,   -96.7759895324707   },\n        { 32.77866942010977,  -96.77701950073242  },\n        { 32.7756745755423,   -96.7706036567688   },\n        { 27.703618681345585, -139.73458170890808 },\n        { 32.94823588839368,  -96.4538113027811   },\n        { 33.06047141970814,  -96.65084838867188  },\n        { 32.778650,          -96.7772            },\n        { -88.56029371730983, -177.23537676036358 },\n        { 33.541429799076354, -26.779373834241003 },\n        { 26.774024500421728, -77.35379276106497  },\n        { -90.0,              -14.796283808944777 },\n        { 32.94823588839368,  -178.8538113027811  },\n        { 32.94823588839368,  178.8538113027811   },\n        { 40.720611,          -73.998776          },\n        { -44.5,              -179.5              }\n    };\n    \n    Directory directory = newDirectory();\n\n    // TODO: must these simple tests really rely on docid order?\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000));\n    iwc.setMergePolicy(newLogMergePolicy());\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, iwc);\n\n    for (double p[] : pts) {\n        Document doc = new Document();\n        addPointToDoc(\"point\", doc, p[0], p[1]);\n        writer.addDocument(doc);\n    }\n\n    // add explicit multi-valued docs\n    for (int i=0; i<pts.length; i+=2) {\n      Document doc = new Document();\n      addPointToDoc(\"point\", doc, pts[i][0], pts[i][1]);\n      addPointToDoc(\"point\", doc, pts[i+1][0], pts[i+1][1]);\n      writer.addDocument(doc);\n    }\n\n    // index random string documents\n    for (int i=0; i<random().nextInt(10); ++i) {\n      Document doc = new Document();\n      doc.add(new StringField(\"string\", Integer.toString(i), Field.Store.NO));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    IndexSearcher searcher = newSearcher(reader);\n    TopDocs topDocs = searcher.search(query, size);\n    reader.close();\n    directory.close();\n    return topDocs;\n  }\n\n","sourceOld":"  /** return topdocs over a small set of points in field \"point\" */\n  private TopDocs searchSmallSet(Query query, int size) throws Exception {\n    // this is a simple systematic test, indexing these points\n    // TODO: fragile: does not understand quantization in any way yet uses extremely high precision!\n    double[][] pts = new double[][] {\n        { 32.763420,          -96.774             },\n        { 32.7559529921407,   -96.7759895324707   },\n        { 32.77866942010977,  -96.77701950073242  },\n        { 32.7756745755423,   -96.7706036567688   },\n        { 27.703618681345585, -139.73458170890808 },\n        { 32.94823588839368,  -96.4538113027811   },\n        { 33.06047141970814,  -96.65084838867188  },\n        { 32.778650,          -96.7772            },\n        { -88.56029371730983, -177.23537676036358 },\n        { 33.541429799076354, -26.779373834241003 },\n        { 26.774024500421728, -77.35379276106497  },\n        { -90.0,              -14.796283808944777 },\n        { 32.94823588839368,  -178.8538113027811  },\n        { 32.94823588839368,  178.8538113027811   },\n        { 40.720611,          -73.998776          },\n        { -44.5,              -179.5              }\n    };\n    \n    Directory directory = newDirectory();\n\n    // TODO: must these simple tests really rely on docid order?\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000));\n    iwc.setMergePolicy(newLogMergePolicy());\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, iwc);\n\n    for (double p[] : pts) {\n        Document doc = new Document();\n        addPointToDoc(\"point\", doc, p[0], p[1]);\n        writer.addDocument(doc);\n    }\n\n    // add explicit multi-valued docs\n    for (int i=0; i<pts.length; i+=2) {\n      Document doc = new Document();\n      addPointToDoc(\"point\", doc, pts[i][0], pts[i][1]);\n      addPointToDoc(\"point\", doc, pts[i+1][0], pts[i+1][1]);\n      writer.addDocument(doc);\n    }\n\n    // index random string documents\n    for (int i=0; i<random().nextInt(10); ++i) {\n      Document doc = new Document();\n      doc.add(new StringField(\"string\", Integer.toString(i), Field.Store.NO));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    IndexSearcher searcher = newSearcher(reader);\n    TopDocs topDocs = searcher.search(query, size);\n    reader.close();\n    directory.close();\n    return topDocs;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f8cb57c50beb99a1245256e866350af8e5ea1f36","date":1460921840,"type":5,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#searchSmallSet(Query,int).mjava","pathOld":"lucene/spatial/src/test/org/apache/lucene/spatial/util/BaseGeoPointTestCase#searchSmallSet(Query,int).mjava","sourceNew":"  /** return topdocs over a small set of points in field \"point\" */\n  private TopDocs searchSmallSet(Query query, int size) throws Exception {\n    // this is a simple systematic test, indexing these points\n    // TODO: fragile: does not understand quantization in any way yet uses extremely high precision!\n    double[][] pts = new double[][] {\n        { 32.763420,          -96.774             },\n        { 32.7559529921407,   -96.7759895324707   },\n        { 32.77866942010977,  -96.77701950073242  },\n        { 32.7756745755423,   -96.7706036567688   },\n        { 27.703618681345585, -139.73458170890808 },\n        { 32.94823588839368,  -96.4538113027811   },\n        { 33.06047141970814,  -96.65084838867188  },\n        { 32.778650,          -96.7772            },\n        { -88.56029371730983, -177.23537676036358 },\n        { 33.541429799076354, -26.779373834241003 },\n        { 26.774024500421728, -77.35379276106497  },\n        { -90.0,              -14.796283808944777 },\n        { 32.94823588839368,  -178.8538113027811  },\n        { 32.94823588839368,  178.8538113027811   },\n        { 40.720611,          -73.998776          },\n        { -44.5,              -179.5              }\n    };\n    \n    Directory directory = newDirectory();\n\n    // TODO: must these simple tests really rely on docid order?\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000));\n    iwc.setMergePolicy(newLogMergePolicy());\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, iwc);\n\n    for (double p[] : pts) {\n        Document doc = new Document();\n        addPointToDoc(\"point\", doc, p[0], p[1]);\n        writer.addDocument(doc);\n    }\n\n    // add explicit multi-valued docs\n    for (int i=0; i<pts.length; i+=2) {\n      Document doc = new Document();\n      addPointToDoc(\"point\", doc, pts[i][0], pts[i][1]);\n      addPointToDoc(\"point\", doc, pts[i+1][0], pts[i+1][1]);\n      writer.addDocument(doc);\n    }\n\n    // index random string documents\n    for (int i=0; i<random().nextInt(10); ++i) {\n      Document doc = new Document();\n      doc.add(new StringField(\"string\", Integer.toString(i), Field.Store.NO));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    IndexSearcher searcher = newSearcher(reader);\n    TopDocs topDocs = searcher.search(query, size);\n    reader.close();\n    directory.close();\n    return topDocs;\n  }\n\n","sourceOld":"  /** return topdocs over a small set of points in field \"point\" */\n  private TopDocs searchSmallSet(Query query, int size) throws Exception {\n    // this is a simple systematic test, indexing these points\n    // TODO: fragile: does not understand quantization in any way yet uses extremely high precision!\n    double[][] pts = new double[][] {\n        { 32.763420,          -96.774             },\n        { 32.7559529921407,   -96.7759895324707   },\n        { 32.77866942010977,  -96.77701950073242  },\n        { 32.7756745755423,   -96.7706036567688   },\n        { 27.703618681345585, -139.73458170890808 },\n        { 32.94823588839368,  -96.4538113027811   },\n        { 33.06047141970814,  -96.65084838867188  },\n        { 32.778650,          -96.7772            },\n        { -88.56029371730983, -177.23537676036358 },\n        { 33.541429799076354, -26.779373834241003 },\n        { 26.774024500421728, -77.35379276106497  },\n        { -90.0,              -14.796283808944777 },\n        { 32.94823588839368,  -178.8538113027811  },\n        { 32.94823588839368,  178.8538113027811   },\n        { 40.720611,          -73.998776          },\n        { -44.5,              -179.5              }\n    };\n    \n    Directory directory = newDirectory();\n\n    // TODO: must these simple tests really rely on docid order?\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 100, 1000));\n    iwc.setMergePolicy(newLogMergePolicy());\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, iwc);\n\n    for (double p[] : pts) {\n        Document doc = new Document();\n        addPointToDoc(\"point\", doc, p[0], p[1]);\n        writer.addDocument(doc);\n    }\n\n    // add explicit multi-valued docs\n    for (int i=0; i<pts.length; i+=2) {\n      Document doc = new Document();\n      addPointToDoc(\"point\", doc, pts[i][0], pts[i][1]);\n      addPointToDoc(\"point\", doc, pts[i+1][0], pts[i+1][1]);\n      writer.addDocument(doc);\n    }\n\n    // index random string documents\n    for (int i=0; i<random().nextInt(10); ++i) {\n      Document doc = new Document();\n      doc.add(new StringField(\"string\", Integer.toString(i), Field.Store.NO));\n      writer.addDocument(doc);\n    }\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n\n    IndexSearcher searcher = newSearcher(reader);\n    TopDocs topDocs = searcher.search(query, size);\n    reader.close();\n    directory.close();\n    return topDocs;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b9e52892242a8c82e1b0c1bd4f1d404366b0501c":["d6c59bc551f4e523ce6a321280cc6733424fb824"],"03bffb520caf6e9833c4b9a82ac67d19a1f3fc97":["b9e52892242a8c82e1b0c1bd4f1d404366b0501c"],"3e21d7642ee7e0c00429964e5b47504602fe218c":["03bffb520caf6e9833c4b9a82ac67d19a1f3fc97"],"f8cb57c50beb99a1245256e866350af8e5ea1f36":["03bffb520caf6e9833c4b9a82ac67d19a1f3fc97","3e21d7642ee7e0c00429964e5b47504602fe218c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f8cb57c50beb99a1245256e866350af8e5ea1f36"],"d6c59bc551f4e523ce6a321280cc6733424fb824":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d6c59bc551f4e523ce6a321280cc6733424fb824"],"b9e52892242a8c82e1b0c1bd4f1d404366b0501c":["03bffb520caf6e9833c4b9a82ac67d19a1f3fc97"],"03bffb520caf6e9833c4b9a82ac67d19a1f3fc97":["3e21d7642ee7e0c00429964e5b47504602fe218c","f8cb57c50beb99a1245256e866350af8e5ea1f36"],"3e21d7642ee7e0c00429964e5b47504602fe218c":["f8cb57c50beb99a1245256e866350af8e5ea1f36"],"f8cb57c50beb99a1245256e866350af8e5ea1f36":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d6c59bc551f4e523ce6a321280cc6733424fb824":["b9e52892242a8c82e1b0c1bd4f1d404366b0501c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}