{"path":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","commits":[{"id":"4522ffca5a1f420c6a02198c9332d7c596a30ca5","date":1457270822,"type":1,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointReader reader : mergeState.pointReaders) {\n      if (reader instanceof Lucene60PointReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointReader reader : mergeState.pointReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointReaders.length;i++) {\n              PointReader reader = mergeState.pointReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointReader;\n                Lucene60PointReader reader60 = (Lucene60PointReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9a5a0f27d9486cd33de88627ed3d2ff8dc5074ca","date":1457777566,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"416f9e28900210be57b69bc12e2954fb98ed7ebe","date":1458479803,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ceaef6cfc68c8ab22a684192e469a8280f9e6e70","date":1462354657,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8","date":1462567286,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":["86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3d33e731a93d4b57e662ff094f64f94a745422d4","date":1463128289,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0ad30c6a479e764150a3316e57263319775f1df2","date":1463395403,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"10005c6013abbd1102f2463cf95604d4c8774c99","date":1469460814,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                // NOTE: not used, since BKDWriter.merge does a merge sort:\n                                                BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d08973aa47f2cf98a588293a53af4e948952ccfb","date":1469518724,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                // NOTE: not used, since BKDWriter.merge does a merge sort:\n                                                BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b0567940defa1ea6eb8a039d9d36e3682063f8a4","date":1469815320,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                // NOTE: not used, since BKDWriter.merge does a merge sort:\n                                                BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3b013574eedcdbac35dc7e35b0ee616ffc38895d","date":1470897818,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                // NOTE: not used, since BKDWriter.merge does a merge sort:\n                                                BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"523b9a0ab605d725bf2b9a7a2e1040f1c6599dd6","date":1476277226,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"367f57e2ee85b7f7e28cfe73370a22cf67624f65","date":1476778467,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                  singleValuePerDoc &= values.size() == values.getDocCount();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                  singleValuePerDoc &= values.size() == values.getDocCount();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null) {\n                totMaxSize += reader.size(fieldInfo.name);\n                singleValuePerDoc &= reader.size(fieldInfo.name) == reader.getDocCount(fieldInfo.name);\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            List<Integer> docIDBases = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    docIDBases.add(mergeState.docBase[i]);\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders, docIDBases);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"653128722fb3b4713ac331c621491a93f34a4a22","date":1479841816,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.needsIndexSort) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                  singleValuePerDoc &= values.size() == values.getDocCount();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                  singleValuePerDoc &= values.size() == values.getDocCount();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":["86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"727bb765ff2542275f6d31f67be18d7104bae148","date":1480353976,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.needsIndexSort) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                  singleValuePerDoc &= values.size() == values.getDocCount();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                  singleValuePerDoc &= values.size() == values.getDocCount();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"86a0a50d2d14aaee1e635bbec914468551f7f9a2","date":1482234306,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    /**\n     * If indexSort is activated and some of the leaves are not sorted the next test will catch that and the non-optimized merge will run.\n     * If the readers are all sorted then it's safe to perform a bulk merge of the points.\n     **/\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                  singleValuePerDoc &= values.size() == values.getDocCount();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.needsIndexSort) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                  singleValuePerDoc &= values.size() == values.getDocCount();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":["653128722fb3b4713ac331c621491a93f34a4a22","fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","date":1482251961,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    /**\n     * If indexSort is activated and some of the leaves are not sorted the next test will catch that and the non-optimized merge will run.\n     * If the readers are all sorted then it's safe to perform a bulk merge of the points.\n     **/\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                  singleValuePerDoc &= values.size() == values.getDocCount();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    if (mergeState.needsIndexSort) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      super.merge(mergeState);\n      return;\n    }\n\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                  singleValuePerDoc &= values.size() == values.getDocCount();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f6652c943595e92c187ee904c382863013eae28f","date":1539042663,"type":3,"author":"Nicholas Knize","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    /**\n     * If indexSort is activated and some of the leaves are not sorted the next test will catch that and the non-optimized merge will run.\n     * If the readers are all sorted then it's safe to perform a bulk merge of the points.\n     **/\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDataDimensionCount() != 0) {\n        if (fieldInfo.getPointDataDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDataDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                  singleValuePerDoc &= values.size() == values.getDocCount();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDataDimensionCount(),\n                                                fieldInfo.getPointIndexDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDataDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    /**\n     * If indexSort is activated and some of the leaves are not sorted the next test will catch that and the non-optimized merge will run.\n     * If the readers are all sorted then it's safe to perform a bulk merge of the points.\n     **/\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                  singleValuePerDoc &= values.size() == values.getDocCount();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"78bdc7d6906146edb12a1a6c1f765ba680ed5124","date":1549523533,"type":3,"author":"iverase","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    /**\n     * If indexSort is activated and some of the leaves are not sorted the next test will catch that and the non-optimized merge will run.\n     * If the readers are all sorted then it's safe to perform a bulk merge of the points.\n     **/\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDataDimensionCount() != 0) {\n        if (fieldInfo.getPointDataDimensionCount() == 1) {\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDataDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDataDimensionCount(),\n                                                fieldInfo.getPointIndexDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDataDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    /**\n     * If indexSort is activated and some of the leaves are not sorted the next test will catch that and the non-optimized merge will run.\n     * If the readers are all sorted then it's safe to perform a bulk merge of the points.\n     **/\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDataDimensionCount() != 0) {\n        if (fieldInfo.getPointDataDimensionCount() == 1) {\n\n          boolean singleValuePerDoc = true;\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDataDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                  singleValuePerDoc &= values.size() == values.getDocCount();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDataDimensionCount(),\n                                                fieldInfo.getPointIndexDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize,\n                                                singleValuePerDoc)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDataDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"59ed8c026ba85e3c42fb89605b2032dc6f9cc241","date":1581113294,"type":3,"author":"Nicholas Knize","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    /**\n     * If indexSort is activated and some of the leaves are not sorted the next test will catch that and the non-optimized merge will run.\n     * If the readers are all sorted then it's safe to perform a bulk merge of the points.\n     **/\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointIndexDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    /**\n     * If indexSort is activated and some of the leaves are not sorted the next test will catch that and the non-optimized merge will run.\n     * If the readers are all sorted then it's safe to perform a bulk merge of the points.\n     **/\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDataDimensionCount() != 0) {\n        if (fieldInfo.getPointDataDimensionCount() == 1) {\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDataDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDataDimensionCount(),\n                                                fieldInfo.getPointIndexDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDataDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"78e689a3b60e84c75dc6dd7b181a71fc19ef8482","date":1591689554,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    /**\n     * If indexSort is activated and some of the leaves are not sorted the next test will catch that and the non-optimized merge will run.\n     * If the readers are all sorted then it's safe to perform a bulk merge of the points.\n     **/\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointIndexDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            Runnable finalizer = writer.merge(dataOut, dataOut, dataOut, docMaps, bkdReaders);\n            if (finalizer != null) {\n              indexFPs.put(fieldInfo.name, dataOut.getFilePointer());\n              finalizer.run();\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    /**\n     * If indexSort is activated and some of the leaves are not sorted the next test will catch that and the non-optimized merge will run.\n     * If the readers are all sorted then it's safe to perform a bulk merge of the points.\n     **/\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointIndexDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"78e689a3b60e84c75dc6dd7b181a71fc19ef8482","date":1591689554,"type":6,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene86/Lucene86PointsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene60/Lucene60PointsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    /**\n     * If indexSort is activated and some of the leaves are not sorted the next test will catch that and the non-optimized merge will run.\n     * If the readers are all sorted then it's safe to perform a bulk merge of the points.\n     **/\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene86PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointIndexDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene86PointsReader;\n                Lucene86PointsReader reader60 = (Lucene86PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            Runnable finalizer = writer.merge(metaOut, indexOut, dataOut, docMaps, bkdReaders);\n            if (finalizer != null) {\n              metaOut.writeInt(fieldInfo.number);\n              finalizer.run();\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","sourceOld":"  @Override\n  public void merge(MergeState mergeState) throws IOException {\n    /**\n     * If indexSort is activated and some of the leaves are not sorted the next test will catch that and the non-optimized merge will run.\n     * If the readers are all sorted then it's safe to perform a bulk merge of the points.\n     **/\n    for(PointsReader reader : mergeState.pointsReaders) {\n      if (reader instanceof Lucene60PointsReader == false) {\n        // We can only bulk merge when all to-be-merged segments use our format:\n        super.merge(mergeState);\n        return;\n      }\n    }\n    for (PointsReader reader : mergeState.pointsReaders) {\n      if (reader != null) {\n        reader.checkIntegrity();\n      }\n    }\n\n    for (FieldInfo fieldInfo : mergeState.mergeFieldInfos) {\n      if (fieldInfo.getPointDimensionCount() != 0) {\n        if (fieldInfo.getPointDimensionCount() == 1) {\n\n          // Worst case total maximum size (if none of the points are deleted):\n          long totMaxSize = 0;\n          for(int i=0;i<mergeState.pointsReaders.length;i++) {\n            PointsReader reader = mergeState.pointsReaders[i];\n            if (reader != null) {\n              FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n              FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n              if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                PointValues values = reader.getValues(fieldInfo.name);\n                if (values != null) {\n                  totMaxSize += values.size();\n                }\n              }\n            }\n          }\n\n          //System.out.println(\"MERGE: field=\" + fieldInfo.name);\n          // Optimize the 1D case to use BKDWriter.merge, which does a single merge sort of the\n          // already sorted incoming segments, instead of trying to sort all points again as if\n          // we were simply reindexing them:\n          try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),\n                                                writeState.directory,\n                                                writeState.segmentInfo.name,\n                                                fieldInfo.getPointDimensionCount(),\n                                                fieldInfo.getPointIndexDimensionCount(),\n                                                fieldInfo.getPointNumBytes(),\n                                                maxPointsInLeafNode,\n                                                maxMBSortInHeap,\n                                                totMaxSize)) {\n            List<BKDReader> bkdReaders = new ArrayList<>();\n            List<MergeState.DocMap> docMaps = new ArrayList<>();\n            for(int i=0;i<mergeState.pointsReaders.length;i++) {\n              PointsReader reader = mergeState.pointsReaders[i];\n\n              if (reader != null) {\n\n                // we confirmed this up above\n                assert reader instanceof Lucene60PointsReader;\n                Lucene60PointsReader reader60 = (Lucene60PointsReader) reader;\n\n                // NOTE: we cannot just use the merged fieldInfo.number (instead of resolving to this\n                // reader's FieldInfo as we do below) because field numbers can easily be different\n                // when addIndexes(Directory...) copies over segments from another index:\n\n                FieldInfos readerFieldInfos = mergeState.fieldInfos[i];\n                FieldInfo readerFieldInfo = readerFieldInfos.fieldInfo(fieldInfo.name);\n                if (readerFieldInfo != null && readerFieldInfo.getPointDimensionCount() > 0) {\n                  BKDReader bkdReader = reader60.readers.get(readerFieldInfo.number);\n                  if (bkdReader != null) {\n                    bkdReaders.add(bkdReader);\n                    docMaps.add(mergeState.docMaps[i]);\n                  }\n                }\n              }\n            }\n\n            long fp = writer.merge(dataOut, docMaps, bkdReaders);\n            if (fp != -1) {\n              indexFPs.put(fieldInfo.name, fp);\n            }\n          }\n        } else {\n          mergeOneField(mergeState, fieldInfo);\n        }\n      }\n    }\n\n    finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b0567940defa1ea6eb8a039d9d36e3682063f8a4":["d08973aa47f2cf98a588293a53af4e948952ccfb"],"59ed8c026ba85e3c42fb89605b2032dc6f9cc241":["78bdc7d6906146edb12a1a6c1f765ba680ed5124"],"f6652c943595e92c187ee904c382863013eae28f":["86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"78bdc7d6906146edb12a1a6c1f765ba680ed5124":["f6652c943595e92c187ee904c382863013eae28f"],"0ad30c6a479e764150a3316e57263319775f1df2":["416f9e28900210be57b69bc12e2954fb98ed7ebe","3d33e731a93d4b57e662ff094f64f94a745422d4"],"10005c6013abbd1102f2463cf95604d4c8774c99":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["416f9e28900210be57b69bc12e2954fb98ed7ebe","0ad30c6a479e764150a3316e57263319775f1df2"],"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf":["727bb765ff2542275f6d31f67be18d7104bae148","86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"78e689a3b60e84c75dc6dd7b181a71fc19ef8482":["59ed8c026ba85e3c42fb89605b2032dc6f9cc241"],"fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8":["ceaef6cfc68c8ab22a684192e469a8280f9e6e70"],"523b9a0ab605d725bf2b9a7a2e1040f1c6599dd6":["b0567940defa1ea6eb8a039d9d36e3682063f8a4"],"367f57e2ee85b7f7e28cfe73370a22cf67624f65":["523b9a0ab605d725bf2b9a7a2e1040f1c6599dd6"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["416f9e28900210be57b69bc12e2954fb98ed7ebe","367f57e2ee85b7f7e28cfe73370a22cf67624f65"],"4522ffca5a1f420c6a02198c9332d7c596a30ca5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"727bb765ff2542275f6d31f67be18d7104bae148":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","653128722fb3b4713ac331c621491a93f34a4a22"],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["416f9e28900210be57b69bc12e2954fb98ed7ebe"],"9a5a0f27d9486cd33de88627ed3d2ff8dc5074ca":["4522ffca5a1f420c6a02198c9332d7c596a30ca5"],"416f9e28900210be57b69bc12e2954fb98ed7ebe":["9a5a0f27d9486cd33de88627ed3d2ff8dc5074ca"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["416f9e28900210be57b69bc12e2954fb98ed7ebe","fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8"],"86a0a50d2d14aaee1e635bbec914468551f7f9a2":["653128722fb3b4713ac331c621491a93f34a4a22"],"653128722fb3b4713ac331c621491a93f34a4a22":["367f57e2ee85b7f7e28cfe73370a22cf67624f65"],"d08973aa47f2cf98a588293a53af4e948952ccfb":["d470c8182e92b264680e34081b75e70a9f2b3c89","10005c6013abbd1102f2463cf95604d4c8774c99"],"3b013574eedcdbac35dc7e35b0ee616ffc38895d":["d08973aa47f2cf98a588293a53af4e948952ccfb","b0567940defa1ea6eb8a039d9d36e3682063f8a4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["78e689a3b60e84c75dc6dd7b181a71fc19ef8482"]},"commit2Childs":{"b0567940defa1ea6eb8a039d9d36e3682063f8a4":["523b9a0ab605d725bf2b9a7a2e1040f1c6599dd6","3b013574eedcdbac35dc7e35b0ee616ffc38895d"],"59ed8c026ba85e3c42fb89605b2032dc6f9cc241":["78e689a3b60e84c75dc6dd7b181a71fc19ef8482"],"f6652c943595e92c187ee904c382863013eae28f":["78bdc7d6906146edb12a1a6c1f765ba680ed5124"],"78bdc7d6906146edb12a1a6c1f765ba680ed5124":["59ed8c026ba85e3c42fb89605b2032dc6f9cc241"],"0ad30c6a479e764150a3316e57263319775f1df2":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"10005c6013abbd1102f2463cf95604d4c8774c99":["d08973aa47f2cf98a588293a53af4e948952ccfb"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["10005c6013abbd1102f2463cf95604d4c8774c99","d08973aa47f2cf98a588293a53af4e948952ccfb"],"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4522ffca5a1f420c6a02198c9332d7c596a30ca5"],"78e689a3b60e84c75dc6dd7b181a71fc19ef8482":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8":["3d33e731a93d4b57e662ff094f64f94a745422d4"],"523b9a0ab605d725bf2b9a7a2e1040f1c6599dd6":["367f57e2ee85b7f7e28cfe73370a22cf67624f65"],"367f57e2ee85b7f7e28cfe73370a22cf67624f65":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","653128722fb3b4713ac331c621491a93f34a4a22"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["727bb765ff2542275f6d31f67be18d7104bae148"],"727bb765ff2542275f6d31f67be18d7104bae148":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf"],"4522ffca5a1f420c6a02198c9332d7c596a30ca5":["9a5a0f27d9486cd33de88627ed3d2ff8dc5074ca"],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8"],"9a5a0f27d9486cd33de88627ed3d2ff8dc5074ca":["416f9e28900210be57b69bc12e2954fb98ed7ebe"],"416f9e28900210be57b69bc12e2954fb98ed7ebe":["0ad30c6a479e764150a3316e57263319775f1df2","d470c8182e92b264680e34081b75e70a9f2b3c89","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","ceaef6cfc68c8ab22a684192e469a8280f9e6e70","3d33e731a93d4b57e662ff094f64f94a745422d4"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["0ad30c6a479e764150a3316e57263319775f1df2"],"d08973aa47f2cf98a588293a53af4e948952ccfb":["b0567940defa1ea6eb8a039d9d36e3682063f8a4","3b013574eedcdbac35dc7e35b0ee616ffc38895d"],"86a0a50d2d14aaee1e635bbec914468551f7f9a2":["f6652c943595e92c187ee904c382863013eae28f","5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf"],"653128722fb3b4713ac331c621491a93f34a4a22":["727bb765ff2542275f6d31f67be18d7104bae148","86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"3b013574eedcdbac35dc7e35b0ee616ffc38895d":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","3b013574eedcdbac35dc7e35b0ee616ffc38895d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}