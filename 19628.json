{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","sourceNew":"    public void testFlushWithNoMerging() throws IOException {\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setMaxBufferedDocs(2).\n              setMergePolicy(newLogMergePolicy(10))\n      );\n      Document doc = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      doc.add(newField(\"field\", \"aaa\", customType));\n      for(int i=0;i<19;i++)\n        writer.addDocument(doc);\n      writer.flush(false, true);\n      writer.close();\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      // Since we flushed w/o allowing merging we should now\n      // have 10 segments\n      assertEquals(10, sis.size());\n      dir.close();\n    }\n\n","sourceOld":"    public void testFlushWithNoMerging() throws IOException {\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setMaxBufferedDocs(2).\n              setMergePolicy(newLogMergePolicy(10))\n      );\n      Document doc = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      doc.add(newField(\"field\", \"aaa\", customType));\n      for(int i=0;i<19;i++)\n        writer.addDocument(doc);\n      writer.flush(false, true);\n      writer.close();\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      // Since we flushed w/o allowing merging we should now\n      // have 10 segments\n      assertEquals(10, sis.size());\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","sourceNew":"    public void testFlushWithNoMerging() throws IOException {\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n              setMaxBufferedDocs(2).\n              setMergePolicy(newLogMergePolicy(10))\n      );\n      Document doc = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      doc.add(newField(\"field\", \"aaa\", customType));\n      for(int i=0;i<19;i++)\n        writer.addDocument(doc);\n      writer.flush(false, true);\n      writer.close();\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      // Since we flushed w/o allowing merging we should now\n      // have 10 segments\n      assertEquals(10, sis.size());\n      dir.close();\n    }\n\n","sourceOld":"    public void testFlushWithNoMerging() throws IOException {\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setMaxBufferedDocs(2).\n              setMergePolicy(newLogMergePolicy(10))\n      );\n      Document doc = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      doc.add(newField(\"field\", \"aaa\", customType));\n      for(int i=0;i<19;i++)\n        writer.addDocument(doc);\n      writer.flush(false, true);\n      writer.close();\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      // Since we flushed w/o allowing merging we should now\n      // have 10 segments\n      assertEquals(10, sis.size());\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","sourceNew":"    public void testFlushWithNoMerging() throws IOException {\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n              setMaxBufferedDocs(2).\n              setMergePolicy(newLogMergePolicy(10))\n      );\n      Document doc = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      doc.add(newField(\"field\", \"aaa\", customType));\n      for(int i=0;i<19;i++)\n        writer.addDocument(doc);\n      writer.flush(false, true);\n      writer.shutdown();\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      // Since we flushed w/o allowing merging we should now\n      // have 10 segments\n      assertEquals(10, sis.size());\n      dir.close();\n    }\n\n","sourceOld":"    public void testFlushWithNoMerging() throws IOException {\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n              setMaxBufferedDocs(2).\n              setMergePolicy(newLogMergePolicy(10))\n      );\n      Document doc = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      doc.add(newField(\"field\", \"aaa\", customType));\n      for(int i=0;i<19;i++)\n        writer.addDocument(doc);\n      writer.flush(false, true);\n      writer.close();\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      // Since we flushed w/o allowing merging we should now\n      // have 10 segments\n      assertEquals(10, sis.size());\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","sourceNew":"    public void testFlushWithNoMerging() throws IOException {\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(new MockAnalyzer(random()))\n              .setMaxBufferedDocs(2)\n              .setMergePolicy(newLogMergePolicy(10))\n      );\n      Document doc = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      doc.add(newField(\"field\", \"aaa\", customType));\n      for(int i=0;i<19;i++)\n        writer.addDocument(doc);\n      writer.flush(false, true);\n      writer.shutdown();\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      // Since we flushed w/o allowing merging we should now\n      // have 10 segments\n      assertEquals(10, sis.size());\n      dir.close();\n    }\n\n","sourceOld":"    public void testFlushWithNoMerging() throws IOException {\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n              setMaxBufferedDocs(2).\n              setMergePolicy(newLogMergePolicy(10))\n      );\n      Document doc = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      doc.add(newField(\"field\", \"aaa\", customType));\n      for(int i=0;i<19;i++)\n        writer.addDocument(doc);\n      writer.flush(false, true);\n      writer.shutdown();\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      // Since we flushed w/o allowing merging we should now\n      // have 10 segments\n      assertEquals(10, sis.size());\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","sourceNew":"    public void testFlushWithNoMerging() throws IOException {\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(new MockAnalyzer(random()))\n              .setMaxBufferedDocs(2)\n              .setMergePolicy(newLogMergePolicy(10))\n      );\n      Document doc = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      doc.add(newField(\"field\", \"aaa\", customType));\n      for(int i=0;i<19;i++)\n        writer.addDocument(doc);\n      writer.flush(false, true);\n      writer.close();\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      // Since we flushed w/o allowing merging we should now\n      // have 10 segments\n      assertEquals(10, sis.size());\n      dir.close();\n    }\n\n","sourceOld":"    public void testFlushWithNoMerging() throws IOException {\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(new MockAnalyzer(random()))\n              .setMaxBufferedDocs(2)\n              .setMergePolicy(newLogMergePolicy(10))\n      );\n      Document doc = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      doc.add(newField(\"field\", \"aaa\", customType));\n      for(int i=0;i<19;i++)\n        writer.addDocument(doc);\n      writer.flush(false, true);\n      writer.shutdown();\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      // Since we flushed w/o allowing merging we should now\n      // have 10 segments\n      assertEquals(10, sis.size());\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3384e6013a93e4d11b7d75388693f8d0388602bf","date":1413951663,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","sourceNew":"    public void testFlushWithNoMerging() throws IOException {\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(new MockAnalyzer(random()))\n              .setMaxBufferedDocs(2)\n              .setMergePolicy(newLogMergePolicy(10))\n      );\n      Document doc = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      doc.add(newField(\"field\", \"aaa\", customType));\n      for(int i=0;i<19;i++)\n        writer.addDocument(doc);\n      writer.flush(false, true);\n      writer.close();\n      SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n      // Since we flushed w/o allowing merging we should now\n      // have 10 segments\n      assertEquals(10, sis.size());\n      dir.close();\n    }\n\n","sourceOld":"    public void testFlushWithNoMerging() throws IOException {\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(new MockAnalyzer(random()))\n              .setMaxBufferedDocs(2)\n              .setMergePolicy(newLogMergePolicy(10))\n      );\n      Document doc = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      doc.add(newField(\"field\", \"aaa\", customType));\n      for(int i=0;i<19;i++)\n        writer.addDocument(doc);\n      writer.flush(false, true);\n      writer.close();\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      // Since we flushed w/o allowing merging we should now\n      // have 10 segments\n      assertEquals(10, sis.size());\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"db68c63cbfaa8698b9c4475f75ed2b9c9696d238","date":1414118621,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","sourceNew":"    public void testFlushWithNoMerging() throws IOException {\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(new MockAnalyzer(random()))\n              .setMaxBufferedDocs(2)\n              .setMergePolicy(newLogMergePolicy(10))\n      );\n      Document doc = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      doc.add(newField(\"field\", \"aaa\", customType));\n      for(int i=0;i<19;i++)\n        writer.addDocument(doc);\n      writer.flush(false, true);\n      writer.close();\n      SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n      // Since we flushed w/o allowing merging we should now\n      // have 10 segments\n      assertEquals(10, sis.size());\n      dir.close();\n    }\n\n","sourceOld":"    public void testFlushWithNoMerging() throws IOException {\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(new MockAnalyzer(random()))\n              .setMaxBufferedDocs(2)\n              .setMergePolicy(newLogMergePolicy(10))\n      );\n      Document doc = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      doc.add(newField(\"field\", \"aaa\", customType));\n      for(int i=0;i<19;i++)\n        writer.addDocument(doc);\n      writer.flush(false, true);\n      writer.close();\n      SegmentInfos sis = new SegmentInfos();\n      sis.read(dir);\n      // Since we flushed w/o allowing merging we should now\n      // have 10 segments\n      assertEquals(10, sis.size());\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1c331bf2c0db325b2153017708714a1573f2ce35","date":1447166158,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","sourceNew":"    public void testFlushWithNoMerging() throws IOException {\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(new MockAnalyzer(random()))\n              .setMaxBufferedDocs(2)\n              .setMergePolicy(newLogMergePolicy(10))\n      );\n      Document doc = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      doc.add(newField(\"field\", \"aaa\", customType));\n      for(int i=0;i<19;i++) {\n        writer.addDocument(doc);\n      }\n      writer.flush(false, true);\n      writer.close();\n      SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n      // Since we flushed w/o allowing merging we should now\n      // have 10 segments\n      assertEquals(10, sis.size());\n      dir.close();\n    }\n\n","sourceOld":"    public void testFlushWithNoMerging() throws IOException {\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(new MockAnalyzer(random()))\n              .setMaxBufferedDocs(2)\n              .setMergePolicy(newLogMergePolicy(10))\n      );\n      Document doc = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      doc.add(newField(\"field\", \"aaa\", customType));\n      for(int i=0;i<19;i++)\n        writer.addDocument(doc);\n      writer.flush(false, true);\n      writer.close();\n      SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n      // Since we flushed w/o allowing merging we should now\n      // have 10 segments\n      assertEquals(10, sis.size());\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6bfe104fc023fadc9e709f8d17403d2cc61133fe","date":1454446396,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","sourceNew":"  public void testFlushWithNoMerging() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n                                         dir,\n                                         newIndexWriterConfig(new MockAnalyzer(random()))\n                                         .setMaxBufferedDocs(2)\n                                         .setMergePolicy(newLogMergePolicy(10))\n                                         );\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"field\", \"aaa\", customType));\n    for(int i=0;i<19;i++)\n      writer.addDocument(doc);\n    writer.flush(false, true);\n    writer.close();\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    // Since we flushed w/o allowing merging we should now\n    // have 10 segments\n    assertEquals(10, sis.size());\n    dir.close();\n  }\n\n","sourceOld":"    public void testFlushWithNoMerging() throws IOException {\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(new MockAnalyzer(random()))\n              .setMaxBufferedDocs(2)\n              .setMergePolicy(newLogMergePolicy(10))\n      );\n      Document doc = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      doc.add(newField(\"field\", \"aaa\", customType));\n      for(int i=0;i<19;i++) {\n        writer.addDocument(doc);\n      }\n      writer.flush(false, true);\n      writer.close();\n      SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n      // Since we flushed w/o allowing merging we should now\n      // have 10 segments\n      assertEquals(10, sis.size());\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b470f36a9372c97283360b1304eacbde22df6c0d","date":1454765175,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","sourceNew":"  public void testFlushWithNoMerging() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n                                         dir,\n                                         newIndexWriterConfig(new MockAnalyzer(random()))\n                                         .setMaxBufferedDocs(2)\n                                         .setMergePolicy(newLogMergePolicy(10))\n                                         );\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"field\", \"aaa\", customType));\n    for(int i=0;i<19;i++)\n      writer.addDocument(doc);\n    writer.flush(false, true);\n    writer.close();\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    // Since we flushed w/o allowing merging we should now\n    // have 10 segments\n    assertEquals(10, sis.size());\n    dir.close();\n  }\n\n","sourceOld":"    public void testFlushWithNoMerging() throws IOException {\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(new MockAnalyzer(random()))\n              .setMaxBufferedDocs(2)\n              .setMergePolicy(newLogMergePolicy(10))\n      );\n      Document doc = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      doc.add(newField(\"field\", \"aaa\", customType));\n      for(int i=0;i<19;i++) {\n        writer.addDocument(doc);\n      }\n      writer.flush(false, true);\n      writer.close();\n      SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n      // Since we flushed w/o allowing merging we should now\n      // have 10 segments\n      assertEquals(10, sis.size());\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1e6acbaae7af722f17204ceccf0f7db5753eccf3","date":1454775255,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","sourceNew":"  public void testFlushWithNoMerging() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n                                         dir,\n                                         newIndexWriterConfig(new MockAnalyzer(random()))\n                                         .setMaxBufferedDocs(2)\n                                         .setMergePolicy(newLogMergePolicy(10))\n                                         );\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"field\", \"aaa\", customType));\n    for(int i=0;i<19;i++)\n      writer.addDocument(doc);\n    writer.flush(false, true);\n    writer.close();\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    // Since we flushed w/o allowing merging we should now\n    // have 10 segments\n    assertEquals(10, sis.size());\n    dir.close();\n  }\n\n","sourceOld":"    public void testFlushWithNoMerging() throws IOException {\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(new MockAnalyzer(random()))\n              .setMaxBufferedDocs(2)\n              .setMergePolicy(newLogMergePolicy(10))\n      );\n      Document doc = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      doc.add(newField(\"field\", \"aaa\", customType));\n      for(int i=0;i<19;i++) {\n        writer.addDocument(doc);\n      }\n      writer.flush(false, true);\n      writer.close();\n      SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n      // Since we flushed w/o allowing merging we should now\n      // have 10 segments\n      assertEquals(10, sis.size());\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a207d19eac354d649c3f0e2cce070017c78125e","date":1454776470,"type":3,"author":"Erick Erickson","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriter#testFlushWithNoMerging().mjava","sourceNew":"  public void testFlushWithNoMerging() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter(\n                                         dir,\n                                         newIndexWriterConfig(new MockAnalyzer(random()))\n                                         .setMaxBufferedDocs(2)\n                                         .setMergePolicy(newLogMergePolicy(10))\n                                         );\n    Document doc = new Document();\n    FieldType customType = new FieldType(TextField.TYPE_STORED);\n    customType.setStoreTermVectors(true);\n    customType.setStoreTermVectorPositions(true);\n    customType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"field\", \"aaa\", customType));\n    for(int i=0;i<19;i++)\n      writer.addDocument(doc);\n    writer.flush(false, true);\n    writer.close();\n    SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n    // Since we flushed w/o allowing merging we should now\n    // have 10 segments\n    assertEquals(10, sis.size());\n    dir.close();\n  }\n\n","sourceOld":"    public void testFlushWithNoMerging() throws IOException {\n      Directory dir = newDirectory();\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(new MockAnalyzer(random()))\n              .setMaxBufferedDocs(2)\n              .setMergePolicy(newLogMergePolicy(10))\n      );\n      Document doc = new Document();\n      FieldType customType = new FieldType(TextField.TYPE_STORED);\n      customType.setStoreTermVectors(true);\n      customType.setStoreTermVectorPositions(true);\n      customType.setStoreTermVectorOffsets(true);\n      doc.add(newField(\"field\", \"aaa\", customType));\n      for(int i=0;i<19;i++) {\n        writer.addDocument(doc);\n      }\n      writer.flush(false, true);\n      writer.close();\n      SegmentInfos sis = SegmentInfos.readLatestCommit(dir);\n      // Since we flushed w/o allowing merging we should now\n      // have 10 segments\n      assertEquals(10, sis.size());\n      dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5a207d19eac354d649c3f0e2cce070017c78125e":["1c331bf2c0db325b2153017708714a1573f2ce35","b470f36a9372c97283360b1304eacbde22df6c0d"],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":["1c331bf2c0db325b2153017708714a1573f2ce35","b470f36a9372c97283360b1304eacbde22df6c0d"],"3384e6013a93e4d11b7d75388693f8d0388602bf":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"1c331bf2c0db325b2153017708714a1573f2ce35":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"b470f36a9372c97283360b1304eacbde22df6c0d":["1c331bf2c0db325b2153017708714a1573f2ce35","6bfe104fc023fadc9e709f8d17403d2cc61133fe"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"6bfe104fc023fadc9e709f8d17403d2cc61133fe":["1c331bf2c0db325b2153017708714a1573f2ce35"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["d0ef034a4f10871667ae75181537775ddcf8ade4","3384e6013a93e4d11b7d75388693f8d0388602bf"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5a207d19eac354d649c3f0e2cce070017c78125e"]},"commit2Childs":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"5a207d19eac354d649c3f0e2cce070017c78125e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":[],"3384e6013a93e4d11b7d75388693f8d0388602bf":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"1c331bf2c0db325b2153017708714a1573f2ce35":["5a207d19eac354d649c3f0e2cce070017c78125e","1e6acbaae7af722f17204ceccf0f7db5753eccf3","b470f36a9372c97283360b1304eacbde22df6c0d","6bfe104fc023fadc9e709f8d17403d2cc61133fe"],"b470f36a9372c97283360b1304eacbde22df6c0d":["5a207d19eac354d649c3f0e2cce070017c78125e","1e6acbaae7af722f17204ceccf0f7db5753eccf3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["3384e6013a93e4d11b7d75388693f8d0388602bf","db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"6bfe104fc023fadc9e709f8d17403d2cc61133fe":["b470f36a9372c97283360b1304eacbde22df6c0d"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["1c331bf2c0db325b2153017708714a1573f2ce35"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["1e6acbaae7af722f17204ceccf0f7db5753eccf3","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}