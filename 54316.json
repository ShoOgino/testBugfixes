{"path":"lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","commits":[{"id":"f0b9dc55f42953d6740cddbc92cb0d19fe1ba0ce","date":1297021734,"type":1,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","sourceNew":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    SavedStreams saved = (SavedStreams) getPreviousTokenStream();\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      if (payload){\n        saved.filter = new SimplePayloadFilter(saved.filter, fieldName);\n      }\n      setPreviousTokenStream(saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      saved.filter.reset();\n      return saved.filter;\n    }\n  }\n\n","sourceOld":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    SavedStreams saved = (SavedStreams) getPreviousTokenStream();\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      if (payload){\n        saved.filter = new SimplePayloadFilter(saved.filter, fieldName);\n      }\n      setPreviousTokenStream(saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      saved.filter.reset();\n      return saved.filter;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":1,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","sourceNew":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    SavedStreams saved = (SavedStreams) getPreviousTokenStream();\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      if (payload){\n        saved.filter = new SimplePayloadFilter(saved.filter, fieldName);\n      }\n      setPreviousTokenStream(saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      saved.filter.reset();\n      return saved.filter;\n    }\n  }\n\n","sourceOld":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    SavedStreams saved = (SavedStreams) getPreviousTokenStream();\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      if (payload){\n        saved.filter = new SimplePayloadFilter(saved.filter, fieldName);\n      }\n      setPreviousTokenStream(saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      saved.filter.reset();\n      return saved.filter;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":1,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","pathOld":"lucene/src/test/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","sourceNew":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    SavedStreams saved = (SavedStreams) getPreviousTokenStream();\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      if (payload){\n        saved.filter = new SimplePayloadFilter(saved.filter, fieldName);\n      }\n      setPreviousTokenStream(saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      saved.filter.reset();\n      return saved.filter;\n    }\n  }\n\n","sourceOld":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    SavedStreams saved = (SavedStreams) getPreviousTokenStream();\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      if (payload){\n        saved.filter = new SimplePayloadFilter(saved.filter, fieldName);\n      }\n      setPreviousTokenStream(saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      saved.filter.reset();\n      return saved.filter;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","sourceNew":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    Map<String,SavedStreams> map = (Map) getPreviousTokenStream();\n    if (map == null) {\n      map = new HashMap<String,SavedStreams>();\n      setPreviousTokenStream(map);\n    }\n    \n    SavedStreams saved = map.get(fieldName);\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      saved.filter = maybePayload(saved.filter, fieldName);\n      map.put(fieldName, saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      saved.filter.reset();\n      return saved.filter;\n    }\n  }\n\n","sourceOld":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    SavedStreams saved = (SavedStreams) getPreviousTokenStream();\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      if (payload){\n        saved.filter = new SimplePayloadFilter(saved.filter, fieldName);\n      }\n      setPreviousTokenStream(saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      saved.filter.reset();\n      return saved.filter;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","sourceNew":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    Map<String,SavedStreams> map = (Map) getPreviousTokenStream();\n    if (map == null) {\n      map = new HashMap<String,SavedStreams>();\n      setPreviousTokenStream(map);\n    }\n    \n    SavedStreams saved = map.get(fieldName);\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      saved.filter = maybePayload(saved.filter, fieldName);\n      map.put(fieldName, saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      saved.filter.reset();\n      return saved.filter;\n    }\n  }\n\n","sourceOld":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    SavedStreams saved = (SavedStreams) getPreviousTokenStream();\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      if (payload){\n        saved.filter = new SimplePayloadFilter(saved.filter, fieldName);\n      }\n      setPreviousTokenStream(saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      saved.filter.reset();\n      return saved.filter;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a671ba75586d364af8239838dbec7964a3624c5c","date":1303640849,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","sourceNew":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    @SuppressWarnings(\"unchecked\") Map<String,SavedStreams> map = (Map) getPreviousTokenStream();\n    if (map == null) {\n      map = new HashMap<String,SavedStreams>();\n      setPreviousTokenStream(map);\n    }\n    \n    SavedStreams saved = map.get(fieldName);\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      saved.filter = maybePayload(saved.filter, fieldName);\n      map.put(fieldName, saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      saved.filter.reset();\n      return saved.filter;\n    }\n  }\n\n","sourceOld":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    Map<String,SavedStreams> map = (Map) getPreviousTokenStream();\n    if (map == null) {\n      map = new HashMap<String,SavedStreams>();\n      setPreviousTokenStream(map);\n    }\n    \n    SavedStreams saved = map.get(fieldName);\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      saved.filter = maybePayload(saved.filter, fieldName);\n      map.put(fieldName, saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      saved.filter.reset();\n      return saved.filter;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24f1664166601a0f7376d051dda5dd63c068c313","date":1303641250,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","sourceNew":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    @SuppressWarnings(\"unchecked\") Map<String,SavedStreams> map = (Map) getPreviousTokenStream();\n    if (map == null) {\n      map = new HashMap<String,SavedStreams>();\n      setPreviousTokenStream(map);\n    }\n    \n    SavedStreams saved = map.get(fieldName);\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      saved.filter = maybePayload(saved.filter, fieldName);\n      map.put(fieldName, saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      saved.filter.reset();\n      return saved.filter;\n    }\n  }\n\n","sourceOld":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    Map<String,SavedStreams> map = (Map) getPreviousTokenStream();\n    if (map == null) {\n      map = new HashMap<String,SavedStreams>();\n      setPreviousTokenStream(map);\n    }\n    \n    SavedStreams saved = map.get(fieldName);\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      saved.filter = maybePayload(saved.filter, fieldName);\n      map.put(fieldName, saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      saved.filter.reset();\n      return saved.filter;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","sourceNew":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    @SuppressWarnings(\"unchecked\") Map<String,SavedStreams> map = (Map) getPreviousTokenStream();\n    if (map == null) {\n      map = new HashMap<String,SavedStreams>();\n      setPreviousTokenStream(map);\n    }\n    \n    SavedStreams saved = map.get(fieldName);\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      saved.filter = maybePayload(saved.filter, fieldName);\n      map.put(fieldName, saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      saved.filter.reset();\n      return saved.filter;\n    }\n  }\n\n","sourceOld":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    SavedStreams saved = (SavedStreams) getPreviousTokenStream();\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      if (payload){\n        saved.filter = new SimplePayloadFilter(saved.filter, fieldName);\n      }\n      setPreviousTokenStream(saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      saved.filter.reset();\n      return saved.filter;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4","date":1305207152,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","sourceNew":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    @SuppressWarnings(\"unchecked\") Map<String,SavedStreams> map = (Map) getPreviousTokenStream();\n    if (map == null) {\n      map = new HashMap<String,SavedStreams>();\n      setPreviousTokenStream(map);\n    }\n    \n    SavedStreams saved = map.get(fieldName);\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.tokenizer.setEnableChecks(enableChecks);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      saved.filter = maybePayload(saved.filter, fieldName);\n      map.put(fieldName, saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      return saved.filter;\n    }\n  }\n\n","sourceOld":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    @SuppressWarnings(\"unchecked\") Map<String,SavedStreams> map = (Map) getPreviousTokenStream();\n    if (map == null) {\n      map = new HashMap<String,SavedStreams>();\n      setPreviousTokenStream(map);\n    }\n    \n    SavedStreams saved = map.get(fieldName);\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      saved.filter = maybePayload(saved.filter, fieldName);\n      map.put(fieldName, saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      saved.filter.reset();\n      return saved.filter;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","sourceNew":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    @SuppressWarnings(\"unchecked\") Map<String,SavedStreams> map = (Map) getPreviousTokenStream();\n    if (map == null) {\n      map = new HashMap<String,SavedStreams>();\n      setPreviousTokenStream(map);\n    }\n    \n    SavedStreams saved = map.get(fieldName);\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.tokenizer.setEnableChecks(enableChecks);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      saved.filter = maybePayload(saved.filter, fieldName);\n      map.put(fieldName, saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      return saved.filter;\n    }\n  }\n\n","sourceOld":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    @SuppressWarnings(\"unchecked\") Map<String,SavedStreams> map = (Map) getPreviousTokenStream();\n    if (map == null) {\n      map = new HashMap<String,SavedStreams>();\n      setPreviousTokenStream(map);\n    }\n    \n    SavedStreams saved = map.get(fieldName);\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      saved.filter = maybePayload(saved.filter, fieldName);\n      map.put(fieldName, saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      saved.filter.reset();\n      return saved.filter;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","sourceNew":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    @SuppressWarnings(\"unchecked\") Map<String,SavedStreams> map = (Map) getPreviousTokenStream();\n    if (map == null) {\n      map = new HashMap<String,SavedStreams>();\n      setPreviousTokenStream(map);\n    }\n    \n    SavedStreams saved = map.get(fieldName);\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.tokenizer.setEnableChecks(enableChecks);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      saved.filter = maybePayload(saved.filter, fieldName);\n      map.put(fieldName, saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      return saved.filter;\n    }\n  }\n\n","sourceOld":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    SavedStreams saved = (SavedStreams) getPreviousTokenStream();\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      if (payload){\n        saved.filter = new SimplePayloadFilter(saved.filter, fieldName);\n      }\n      setPreviousTokenStream(saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      saved.filter.reset();\n      return saved.filter;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"53ae89cd75b0acbdfb8890710c6742f3fb80e65d","date":1315806626,"type":4,"author":"Christopher John Male","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/MockAnalyzer#reusableTokenStream(String,Reader).mjava","sourceNew":null,"sourceOld":"  @Override\n  public TokenStream reusableTokenStream(String fieldName, Reader reader)\n      throws IOException {\n    @SuppressWarnings(\"unchecked\") Map<String,SavedStreams> map = (Map) getPreviousTokenStream();\n    if (map == null) {\n      map = new HashMap<String,SavedStreams>();\n      setPreviousTokenStream(map);\n    }\n    \n    SavedStreams saved = map.get(fieldName);\n    if (saved == null) {\n      saved = new SavedStreams();\n      saved.tokenizer = new MockTokenizer(reader, runAutomaton, lowerCase);\n      saved.tokenizer.setEnableChecks(enableChecks);\n      saved.filter = new MockTokenFilter(saved.tokenizer, filter, enablePositionIncrements);\n      saved.filter = maybePayload(saved.filter, fieldName);\n      map.put(fieldName, saved);\n      return saved.filter;\n    } else {\n      saved.tokenizer.reset(reader);\n      return saved.filter;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"24f1664166601a0f7376d051dda5dd63c068c313":["962d04139994fce5193143ef35615499a9a96d78","a671ba75586d364af8239838dbec7964a3624c5c"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","a671ba75586d364af8239838dbec7964a3624c5c"],"962d04139994fce5193143ef35615499a9a96d78":["bde51b089eb7f86171eb3406e38a274743f9b7ac","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"53ae89cd75b0acbdfb8890710c6742f3fb80e65d":["e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"f0b9dc55f42953d6740cddbc92cb0d19fe1ba0ce":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["f0b9dc55f42953d6740cddbc92cb0d19fe1ba0ce"],"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4":["a671ba75586d364af8239838dbec7964a3624c5c"],"a671ba75586d364af8239838dbec7964a3624c5c":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"a3776dccca01c11e7046323cfad46a3b4a471233":["f0b9dc55f42953d6740cddbc92cb0d19fe1ba0ce","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f0b9dc55f42953d6740cddbc92cb0d19fe1ba0ce"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["135621f3a0670a9394eb563224a3b76cc4dddc0f","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f0b9dc55f42953d6740cddbc92cb0d19fe1ba0ce"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["53ae89cd75b0acbdfb8890710c6742f3fb80e65d"]},"commit2Childs":{"24f1664166601a0f7376d051dda5dd63c068c313":[],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"962d04139994fce5193143ef35615499a9a96d78":["24f1664166601a0f7376d051dda5dd63c068c313"],"53ae89cd75b0acbdfb8890710c6742f3fb80e65d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"f0b9dc55f42953d6740cddbc92cb0d19fe1ba0ce":["f2c5f0cb44df114db4228c8f77861714b5cabaea","a3776dccca01c11e7046323cfad46a3b4a471233","29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["962d04139994fce5193143ef35615499a9a96d78","a671ba75586d364af8239838dbec7964a3624c5c"],"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4":["53ae89cd75b0acbdfb8890710c6742f3fb80e65d","a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"a671ba75586d364af8239838dbec7964a3624c5c":["24f1664166601a0f7376d051dda5dd63c068c313","135621f3a0670a9394eb563224a3b76cc4dddc0f","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f0b9dc55f42953d6740cddbc92cb0d19fe1ba0ce","29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":[],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["962d04139994fce5193143ef35615499a9a96d78"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["24f1664166601a0f7376d051dda5dd63c068c313","a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}