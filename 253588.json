{"path":"backwards/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","commits":[{"id":"480d01e5b0ef8efb136d51670fec297ae5ae2c9c","date":1268821447,"type":0,"author":"Uwe Schindler","isMerge":false,"pathNew":"backwards/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"/dev/null","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", merge.optimize+\"\");\n    details.put(\"mergeFactor\", end+\"\");\n    details.put(\"mergeDocStores\", mergeDocStores+\"\");\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/backwards/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","pathOld":"backwards/src/java/org/apache/lucene/index/IndexWriter#_mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", merge.optimize+\"\");\n    details.put(\"mergeFactor\", end+\"\");\n    details.put(\"mergeDocStores\", mergeDocStores+\"\");\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  final synchronized private void _mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (hitOOM) {\n      throw new IllegalStateException(\"this writer hit an OutOfMemoryError; cannot merge\");\n    }\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    applyDeletes();\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO).\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment)) {\n        doFlushDocStore = true;\n      }\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"now flush at merge\");\n      doFlush(true, false);\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile,\n                                 false);\n\n\n    Map<String,String> details = new HashMap<String,String>();\n    details.put(\"optimize\", merge.optimize+\"\");\n    details.put(\"mergeFactor\", end+\"\");\n    details.put(\"mergeDocStores\", mergeDocStores+\"\");\n    setDiagnostics(merge.info, \"merge\", details);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"480d01e5b0ef8efb136d51670fec297ae5ae2c9c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["480d01e5b0ef8efb136d51670fec297ae5ae2c9c"]},"commit2Childs":{"480d01e5b0ef8efb136d51670fec297ae5ae2c9c":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["480d01e5b0ef8efb136d51670fec297ae5ae2c9c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}