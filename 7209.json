{"path":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(IndexWriter).mjava","commits":[{"id":"7af110b00ea8df9429309d83e38e0533d82e144f","date":1376924768,"type":1,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(IndexWriter).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(DocumentsWriter).mjava","sourceNew":"  private int innerPurge(IndexWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    int numPurged = 0;\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        numPurged++;\n        try {\n          /*\n           * if we block on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n          \n        } finally {\n          synchronized (this) {\n            // finally remove the published ticket from the queue\n            final FlushTicket poll = queue.poll();\n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n    return numPurged;\n  }\n\n","sourceOld":"  private void innerPurge(DocumentsWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        try {\n          /*\n           * if we block on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n        } finally {\n          synchronized (this) {\n            // finally remove the published ticket from the queue\n            final FlushTicket poll = queue.poll();\n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31d4861802ca404d78ca1d15f4550eec415b9199","date":1376947894,"type":1,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(IndexWriter).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(DocumentsWriter).mjava","sourceNew":"  private int innerPurge(IndexWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    int numPurged = 0;\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        numPurged++;\n        try {\n          /*\n           * if we block on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n          \n        } finally {\n          synchronized (this) {\n            // finally remove the published ticket from the queue\n            final FlushTicket poll = queue.poll();\n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n    return numPurged;\n  }\n\n","sourceOld":"  private void innerPurge(DocumentsWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        try {\n          /*\n           * if we block on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n        } finally {\n          synchronized (this) {\n            // finally remove the published ticket from the queue\n            final FlushTicket poll = queue.poll();\n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":0,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(IndexWriter).mjava","pathOld":"/dev/null","sourceNew":"  private int innerPurge(IndexWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    int numPurged = 0;\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        numPurged++;\n        try {\n          /*\n           * if we block on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n          \n        } finally {\n          synchronized (this) {\n            // finally remove the published ticket from the queue\n            final FlushTicket poll = queue.poll();\n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n    return numPurged;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"022a16646a72265e17bbad4aef83cd54efd15804","date":1499023964,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(IndexWriter).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(IndexWriter).mjava","sourceNew":"  private int innerPurge(IndexWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    int numPurged = 0;\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        numPurged++;\n        try {\n          /*\n           * if we block on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n          \n        } finally {\n          synchronized (this) {\n            // finally remove the published ticket from the queue\n            final FlushTicket poll = queue.poll();\n\n            // we hold the purgeLock so no other thread should have polled:\n            assert poll == head;\n            \n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n    return numPurged;\n  }\n\n","sourceOld":"  private int innerPurge(IndexWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    int numPurged = 0;\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        numPurged++;\n        try {\n          /*\n           * if we block on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n          \n        } finally {\n          synchronized (this) {\n            // finally remove the published ticket from the queue\n            final FlushTicket poll = queue.poll();\n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n    return numPurged;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6324f236dd5a5430f2ff91ebd6cb3f0ae8d34a35","date":1499066739,"type":3,"author":"Adrien Grand","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(IndexWriter).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(IndexWriter).mjava","sourceNew":"  private int innerPurge(IndexWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    int numPurged = 0;\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        numPurged++;\n        try {\n          /*\n           * if we block on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n          \n        } finally {\n          synchronized (this) {\n            // finally remove the published ticket from the queue\n            final FlushTicket poll = queue.poll();\n\n            // we hold the purgeLock so no other thread should have polled:\n            assert poll == head;\n            \n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n    return numPurged;\n  }\n\n","sourceOld":"  private int innerPurge(IndexWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    int numPurged = 0;\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        numPurged++;\n        try {\n          /*\n           * if we block on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n          \n        } finally {\n          synchronized (this) {\n            // finally remove the published ticket from the queue\n            final FlushTicket poll = queue.poll();\n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n    return numPurged;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30c8e5574b55d57947e989443dfde611646530ee","date":1499131153,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(IndexWriter).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(IndexWriter).mjava","sourceNew":"  private int innerPurge(IndexWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    int numPurged = 0;\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        numPurged++;\n        try {\n          /*\n           * if we block on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n          \n        } finally {\n          synchronized (this) {\n            // finally remove the published ticket from the queue\n            final FlushTicket poll = queue.poll();\n\n            // we hold the purgeLock so no other thread should have polled:\n            assert poll == head;\n            \n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n    return numPurged;\n  }\n\n","sourceOld":"  private int innerPurge(IndexWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    int numPurged = 0;\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        numPurged++;\n        try {\n          /*\n           * if we block on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n          \n        } finally {\n          synchronized (this) {\n            // finally remove the published ticket from the queue\n            final FlushTicket poll = queue.poll();\n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n    return numPurged;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6b8498afacfc8322268ca0d659d274fcce08d557","date":1524577248,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(IOUtils.IOConsumer[FlushTicket]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue#innerPurge(IndexWriter).mjava","sourceNew":"  private void innerPurge(IOUtils.IOConsumer<FlushTicket> consumer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        try {\n          /*\n           * if we block on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since there could\n           * be a ticket still in the queue. \n           */\n          consumer.accept(head);\n\n        } finally {\n          synchronized (this) {\n            // finally remove the published ticket from the queue\n            final FlushTicket poll = queue.poll();\n            decTickets();\n            // we hold the purgeLock so no other thread should have polled:\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n  }\n\n","sourceOld":"  private int innerPurge(IndexWriter writer) throws IOException {\n    assert purgeLock.isHeldByCurrentThread();\n    int numPurged = 0;\n    while (true) {\n      final FlushTicket head;\n      final boolean canPublish;\n      synchronized (this) {\n        head = queue.peek();\n        canPublish = head != null && head.canPublish(); // do this synced \n      }\n      if (canPublish) {\n        numPurged++;\n        try {\n          /*\n           * if we block on publish -> lock IW -> lock BufferedDeletes we don't block\n           * concurrent segment flushes just because they want to append to the queue.\n           * the downside is that we need to force a purge on fullFlush since ther could\n           * be a ticket still in the queue. \n           */\n          head.publish(writer);\n          \n        } finally {\n          synchronized (this) {\n            // finally remove the published ticket from the queue\n            final FlushTicket poll = queue.poll();\n\n            // we hold the purgeLock so no other thread should have polled:\n            assert poll == head;\n            \n            ticketCount.decrementAndGet();\n            assert poll == head;\n          }\n        }\n      } else {\n        break;\n      }\n    }\n    return numPurged;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7af110b00ea8df9429309d83e38e0533d82e144f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"6b8498afacfc8322268ca0d659d274fcce08d557":["022a16646a72265e17bbad4aef83cd54efd15804"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","7af110b00ea8df9429309d83e38e0533d82e144f"],"31d4861802ca404d78ca1d15f4550eec415b9199":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","7af110b00ea8df9429309d83e38e0533d82e144f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"022a16646a72265e17bbad4aef83cd54efd15804":["7af110b00ea8df9429309d83e38e0533d82e144f"],"30c8e5574b55d57947e989443dfde611646530ee":["7af110b00ea8df9429309d83e38e0533d82e144f","022a16646a72265e17bbad4aef83cd54efd15804"],"6324f236dd5a5430f2ff91ebd6cb3f0ae8d34a35":["7af110b00ea8df9429309d83e38e0533d82e144f","022a16646a72265e17bbad4aef83cd54efd15804"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["6b8498afacfc8322268ca0d659d274fcce08d557"]},"commit2Childs":{"7af110b00ea8df9429309d83e38e0533d82e144f":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","31d4861802ca404d78ca1d15f4550eec415b9199","022a16646a72265e17bbad4aef83cd54efd15804","30c8e5574b55d57947e989443dfde611646530ee","6324f236dd5a5430f2ff91ebd6cb3f0ae8d34a35"],"6b8498afacfc8322268ca0d659d274fcce08d557":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"31d4861802ca404d78ca1d15f4550eec415b9199":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7af110b00ea8df9429309d83e38e0533d82e144f","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","31d4861802ca404d78ca1d15f4550eec415b9199"],"022a16646a72265e17bbad4aef83cd54efd15804":["6b8498afacfc8322268ca0d659d274fcce08d557","30c8e5574b55d57947e989443dfde611646530ee","6324f236dd5a5430f2ff91ebd6cb3f0ae8d34a35"],"30c8e5574b55d57947e989443dfde611646530ee":[],"6324f236dd5a5430f2ff91ebd6cb3f0ae8d34a35":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","31d4861802ca404d78ca1d15f4550eec415b9199","30c8e5574b55d57947e989443dfde611646530ee","6324f236dd5a5430f2ff91ebd6cb3f0ae8d34a35","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}