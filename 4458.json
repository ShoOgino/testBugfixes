{"path":"lucene/test-framework/src/java/org/apache/lucene/index/BaseMergePolicyTestCase#doTestSimulateUpdates(MergePolicy,int,int).mjava","commits":[{"id":"da0d58b6bf72ebfd4d6722289ea725809c20c987","date":1531207054,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseMergePolicyTestCase#doTestSimulateUpdates(MergePolicy,int,int).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Simulate an update use-case where documents are uniformly updated across segments.\n   * {@code totalDocs} exist in the index in the end, and flushes contribute at most\n   * {@code maxDocsPerFlush} documents.\n   */\n  protected void doTestSimulateUpdates(MergePolicy mergePolicy, int totalDocs, int maxDocsPerFlush) throws IOException {\n    IOStats stats = new IOStats();\n    AtomicLong segNameGenerator = new AtomicLong();\n    MergeContext mergeContext = new MockMergeContext(SegmentCommitInfo::getDelCount);\n    SegmentInfos segmentInfos = new SegmentInfos(Version.LATEST.major);\n    final double avgDocSizeMB = 5. / 1024; // 5kB\n    for (int numDocs = 0; numDocs < totalDocs; ) {\n      int flushDocCount = TestUtil.nextInt(random(), 1, maxDocsPerFlush);\n      // how many of these documents are actually updates\n      int delCount = (int) (flushDocCount * 0.9 * numDocs / totalDocs);\n      numDocs += flushDocCount - delCount;\n      segmentInfos = applyDeletes(segmentInfos, delCount);\n      double flushSize = flushDocCount * avgDocSizeMB;\n      stats.flushBytesWritten += flushSize * 1024 * 1024;\n      segmentInfos.add(makeSegmentCommitInfo(\"_\" + segNameGenerator.getAndIncrement(), flushDocCount, 0, flushSize, IndexWriter.SOURCE_FLUSH));\n      MergeSpecification merges = mergePolicy.findMerges(MergeTrigger.SEGMENT_FLUSH, segmentInfos, mergeContext);\n      while (merges != null) {\n        assertMerge(mergePolicy, merges);\n        for (OneMerge oneMerge : merges.merges) {\n          segmentInfos = applyMerge(segmentInfos, oneMerge, \"_\" + segNameGenerator.getAndIncrement(), stats);\n        }\n        merges = mergePolicy.findMerges(MergeTrigger.MERGE_FINISHED, segmentInfos, mergeContext);\n      }\n      assertSegmentInfos(mergePolicy, segmentInfos);\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"Write amplification for update: \" + (double) (stats.flushBytesWritten + stats.mergeBytesWritten) / stats.flushBytesWritten);\n      int totalDelCount = segmentInfos.asList().stream()\n          .mapToInt(SegmentCommitInfo::getDelCount)\n          .sum();\n      int totalMaxDoc = segmentInfos.asList().stream()\n          .map(s -> s.info)\n          .mapToInt(SegmentInfo::maxDoc)\n          .sum();\n      System.out.println(\"Final live ratio: \" + (1 - (double) totalDelCount / totalMaxDoc));\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","date":1531589977,"type":0,"author":"Michael Braun","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseMergePolicyTestCase#doTestSimulateUpdates(MergePolicy,int,int).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Simulate an update use-case where documents are uniformly updated across segments.\n   * {@code totalDocs} exist in the index in the end, and flushes contribute at most\n   * {@code maxDocsPerFlush} documents.\n   */\n  protected void doTestSimulateUpdates(MergePolicy mergePolicy, int totalDocs, int maxDocsPerFlush) throws IOException {\n    IOStats stats = new IOStats();\n    AtomicLong segNameGenerator = new AtomicLong();\n    MergeContext mergeContext = new MockMergeContext(SegmentCommitInfo::getDelCount);\n    SegmentInfos segmentInfos = new SegmentInfos(Version.LATEST.major);\n    final double avgDocSizeMB = 5. / 1024; // 5kB\n    for (int numDocs = 0; numDocs < totalDocs; ) {\n      int flushDocCount = TestUtil.nextInt(random(), 1, maxDocsPerFlush);\n      // how many of these documents are actually updates\n      int delCount = (int) (flushDocCount * 0.9 * numDocs / totalDocs);\n      numDocs += flushDocCount - delCount;\n      segmentInfos = applyDeletes(segmentInfos, delCount);\n      double flushSize = flushDocCount * avgDocSizeMB;\n      stats.flushBytesWritten += flushSize * 1024 * 1024;\n      segmentInfos.add(makeSegmentCommitInfo(\"_\" + segNameGenerator.getAndIncrement(), flushDocCount, 0, flushSize, IndexWriter.SOURCE_FLUSH));\n      MergeSpecification merges = mergePolicy.findMerges(MergeTrigger.SEGMENT_FLUSH, segmentInfos, mergeContext);\n      while (merges != null) {\n        assertMerge(mergePolicy, merges);\n        for (OneMerge oneMerge : merges.merges) {\n          segmentInfos = applyMerge(segmentInfos, oneMerge, \"_\" + segNameGenerator.getAndIncrement(), stats);\n        }\n        merges = mergePolicy.findMerges(MergeTrigger.MERGE_FINISHED, segmentInfos, mergeContext);\n      }\n      assertSegmentInfos(mergePolicy, segmentInfos);\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"Write amplification for update: \" + (double) (stats.flushBytesWritten + stats.mergeBytesWritten) / stats.flushBytesWritten);\n      int totalDelCount = segmentInfos.asList().stream()\n          .mapToInt(SegmentCommitInfo::getDelCount)\n          .sum();\n      int totalMaxDoc = segmentInfos.asList().stream()\n          .map(s -> s.info)\n          .mapToInt(SegmentInfo::maxDoc)\n          .sum();\n      System.out.println(\"Final live ratio: \" + (1 - (double) totalDelCount / totalMaxDoc));\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":0,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseMergePolicyTestCase#doTestSimulateUpdates(MergePolicy,int,int).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Simulate an update use-case where documents are uniformly updated across segments.\n   * {@code totalDocs} exist in the index in the end, and flushes contribute at most\n   * {@code maxDocsPerFlush} documents.\n   */\n  protected void doTestSimulateUpdates(MergePolicy mergePolicy, int totalDocs, int maxDocsPerFlush) throws IOException {\n    IOStats stats = new IOStats();\n    AtomicLong segNameGenerator = new AtomicLong();\n    MergeContext mergeContext = new MockMergeContext(SegmentCommitInfo::getDelCount);\n    SegmentInfos segmentInfos = new SegmentInfos(Version.LATEST.major);\n    final double avgDocSizeMB = 5. / 1024; // 5kB\n    for (int numDocs = 0; numDocs < totalDocs; ) {\n      int flushDocCount = TestUtil.nextInt(random(), 1, maxDocsPerFlush);\n      // how many of these documents are actually updates\n      int delCount = (int) (flushDocCount * 0.9 * numDocs / totalDocs);\n      numDocs += flushDocCount - delCount;\n      segmentInfos = applyDeletes(segmentInfos, delCount);\n      double flushSize = flushDocCount * avgDocSizeMB;\n      stats.flushBytesWritten += flushSize * 1024 * 1024;\n      segmentInfos.add(makeSegmentCommitInfo(\"_\" + segNameGenerator.getAndIncrement(), flushDocCount, 0, flushSize, IndexWriter.SOURCE_FLUSH));\n      MergeSpecification merges = mergePolicy.findMerges(MergeTrigger.SEGMENT_FLUSH, segmentInfos, mergeContext);\n      while (merges != null) {\n        assertMerge(mergePolicy, merges);\n        for (OneMerge oneMerge : merges.merges) {\n          segmentInfos = applyMerge(segmentInfos, oneMerge, \"_\" + segNameGenerator.getAndIncrement(), stats);\n        }\n        merges = mergePolicy.findMerges(MergeTrigger.MERGE_FINISHED, segmentInfos, mergeContext);\n      }\n      assertSegmentInfos(mergePolicy, segmentInfos);\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"Write amplification for update: \" + (double) (stats.flushBytesWritten + stats.mergeBytesWritten) / stats.flushBytesWritten);\n      int totalDelCount = segmentInfos.asList().stream()\n          .mapToInt(SegmentCommitInfo::getDelCount)\n          .sum();\n      int totalMaxDoc = segmentInfos.asList().stream()\n          .map(s -> s.info)\n          .mapToInt(SegmentInfo::maxDoc)\n          .sum();\n      System.out.println(\"Final live ratio: \" + (1 - (double) totalDelCount / totalMaxDoc));\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1cddcfccab27b3211b29120b51033ab04e569ff9","date":1580914291,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseMergePolicyTestCase#doTestSimulateUpdates(MergePolicy,int,int).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseMergePolicyTestCase#doTestSimulateUpdates(MergePolicy,int,int).mjava","sourceNew":"  /**\n   * Simulate an update use-case where documents are uniformly updated across segments.\n   * {@code totalDocs} exist in the index in the end, and flushes contribute at most\n   * {@code maxDocsPerFlush} documents.\n   */\n  protected void doTestSimulateUpdates(MergePolicy mergePolicy, int totalDocs, int maxDocsPerFlush) throws IOException {\n    IOStats stats = new IOStats();\n    AtomicLong segNameGenerator = new AtomicLong();\n    MergeContext mergeContext = new MockMergeContext(SegmentCommitInfo::getDelCount);\n    SegmentInfos segmentInfos = new SegmentInfos(Version.LATEST.major);\n    final double avgDocSizeMB = 5. / 1024; // 5kB\n    for (int numDocs = 0; numDocs < totalDocs; ) {\n      final int flushDocCount;\n      if (usually()) {\n        // reasonable value\n        flushDocCount = TestUtil.nextInt(random(), maxDocsPerFlush/2, maxDocsPerFlush);\n      } else {\n        // crazy value\n        flushDocCount = TestUtil.nextInt(random(), 1, maxDocsPerFlush);\n      }\n      // how many of these documents are actually updates\n      int delCount = (int) (flushDocCount * 0.9 * numDocs / totalDocs);\n      numDocs += flushDocCount - delCount;\n      segmentInfos = applyDeletes(segmentInfos, delCount);\n      double flushSize = flushDocCount * avgDocSizeMB;\n      stats.flushBytesWritten += flushSize * 1024 * 1024;\n      segmentInfos.add(makeSegmentCommitInfo(\"_\" + segNameGenerator.getAndIncrement(), flushDocCount, 0, flushSize, IndexWriter.SOURCE_FLUSH));\n      MergeSpecification merges = mergePolicy.findMerges(MergeTrigger.SEGMENT_FLUSH, segmentInfos, mergeContext);\n      while (merges != null) {\n        assertMerge(mergePolicy, merges);\n        for (OneMerge oneMerge : merges.merges) {\n          segmentInfos = applyMerge(segmentInfos, oneMerge, \"_\" + segNameGenerator.getAndIncrement(), stats);\n        }\n        merges = mergePolicy.findMerges(MergeTrigger.MERGE_FINISHED, segmentInfos, mergeContext);\n      }\n      assertSegmentInfos(mergePolicy, segmentInfos);\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"Write amplification for update: \" + (double) (stats.flushBytesWritten + stats.mergeBytesWritten) / stats.flushBytesWritten);\n      int totalDelCount = segmentInfos.asList().stream()\n          .mapToInt(SegmentCommitInfo::getDelCount)\n          .sum();\n      int totalMaxDoc = segmentInfos.asList().stream()\n          .map(s -> s.info)\n          .mapToInt(SegmentInfo::maxDoc)\n          .sum();\n      System.out.println(\"Final live ratio: \" + (1 - (double) totalDelCount / totalMaxDoc));\n    }\n  }\n\n","sourceOld":"  /**\n   * Simulate an update use-case where documents are uniformly updated across segments.\n   * {@code totalDocs} exist in the index in the end, and flushes contribute at most\n   * {@code maxDocsPerFlush} documents.\n   */\n  protected void doTestSimulateUpdates(MergePolicy mergePolicy, int totalDocs, int maxDocsPerFlush) throws IOException {\n    IOStats stats = new IOStats();\n    AtomicLong segNameGenerator = new AtomicLong();\n    MergeContext mergeContext = new MockMergeContext(SegmentCommitInfo::getDelCount);\n    SegmentInfos segmentInfos = new SegmentInfos(Version.LATEST.major);\n    final double avgDocSizeMB = 5. / 1024; // 5kB\n    for (int numDocs = 0; numDocs < totalDocs; ) {\n      int flushDocCount = TestUtil.nextInt(random(), 1, maxDocsPerFlush);\n      // how many of these documents are actually updates\n      int delCount = (int) (flushDocCount * 0.9 * numDocs / totalDocs);\n      numDocs += flushDocCount - delCount;\n      segmentInfos = applyDeletes(segmentInfos, delCount);\n      double flushSize = flushDocCount * avgDocSizeMB;\n      stats.flushBytesWritten += flushSize * 1024 * 1024;\n      segmentInfos.add(makeSegmentCommitInfo(\"_\" + segNameGenerator.getAndIncrement(), flushDocCount, 0, flushSize, IndexWriter.SOURCE_FLUSH));\n      MergeSpecification merges = mergePolicy.findMerges(MergeTrigger.SEGMENT_FLUSH, segmentInfos, mergeContext);\n      while (merges != null) {\n        assertMerge(mergePolicy, merges);\n        for (OneMerge oneMerge : merges.merges) {\n          segmentInfos = applyMerge(segmentInfos, oneMerge, \"_\" + segNameGenerator.getAndIncrement(), stats);\n        }\n        merges = mergePolicy.findMerges(MergeTrigger.MERGE_FINISHED, segmentInfos, mergeContext);\n      }\n      assertSegmentInfos(mergePolicy, segmentInfos);\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"Write amplification for update: \" + (double) (stats.flushBytesWritten + stats.mergeBytesWritten) / stats.flushBytesWritten);\n      int totalDelCount = segmentInfos.asList().stream()\n          .mapToInt(SegmentCommitInfo::getDelCount)\n          .sum();\n      int totalMaxDoc = segmentInfos.asList().stream()\n          .map(s -> s.info)\n          .mapToInt(SegmentInfo::maxDoc)\n          .sum();\n      System.out.println(\"Final live ratio: \" + (1 - (double) totalDelCount / totalMaxDoc));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"da0d58b6bf72ebfd4d6722289ea725809c20c987":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","da0d58b6bf72ebfd4d6722289ea725809c20c987"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1cddcfccab27b3211b29120b51033ab04e569ff9":["da0d58b6bf72ebfd4d6722289ea725809c20c987"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["1cddcfccab27b3211b29120b51033ab04e569ff9"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","da0d58b6bf72ebfd4d6722289ea725809c20c987"]},"commit2Childs":{"da0d58b6bf72ebfd4d6722289ea725809c20c987":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","1cddcfccab27b3211b29120b51033ab04e569ff9","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["da0d58b6bf72ebfd4d6722289ea725809c20c987","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"1cddcfccab27b3211b29120b51033ab04e569ff9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":[]},"heads":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","cd5edd1f2b162a5cfa08efd17851a07373a96817","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}