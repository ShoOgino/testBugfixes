{"path":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(Directory,String).mjava","commits":[{"id":"8be580b58bcc650d428f3f22de81cadcf51d650a","date":1325279655,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(Directory,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(File,String).mjava","sourceNew":"  public void searchIndex(Directory dir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    IndexReader reader = IndexReader.open(dir);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (liveDocs.get(i)) {\n        Document d = reader.document(i);\n        List<IndexableField> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          IndexableField f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        Terms tfv = reader.getTermVectors(i).terms(\"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + oldName, tfv);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    reader.close();\n  }\n\n","sourceOld":"  public void searchIndex(File indexDir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    Directory dir = newFSDirectory(indexDir);\n    IndexReader reader = IndexReader.open(dir);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (liveDocs.get(i)) {\n        Document d = reader.document(i);\n        List<IndexableField> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          IndexableField f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        Terms tfv = reader.getTermVectors(i).terms(\"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + indexDir.getName(), tfv);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(Directory,String).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility#searchIndex(Directory,String).mjava","sourceNew":"  public void searchIndex(Directory dir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    IndexReader reader = IndexReader.open(dir);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (liveDocs.get(i)) {\n        Document d = reader.document(i);\n        List<IndexableField> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          IndexableField f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        Terms tfv = reader.getTermVectors(i).terms(\"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + oldName, tfv);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    reader.close();\n  }\n\n","sourceOld":"  public void searchIndex(Directory dir, String oldName) throws IOException {\n    //QueryParser parser = new QueryParser(\"contents\", new MockAnalyzer(random));\n    //Query query = parser.parse(\"handle:1\");\n\n    IndexReader reader = IndexReader.open(dir);\n    IndexSearcher searcher = new IndexSearcher(reader);\n\n    _TestUtil.checkIndex(dir);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n\n    for(int i=0;i<35;i++) {\n      if (liveDocs.get(i)) {\n        Document d = reader.document(i);\n        List<IndexableField> fields = d.getFields();\n        if (d.getField(\"content3\") == null) {\n          final int numFields = 5;\n          assertEquals(numFields, fields.size());\n          IndexableField f =  d.getField(\"id\");\n          assertEquals(\"\"+i, f.stringValue());\n\n          f = d.getField(\"utf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n\n          f =  d.getField(\"autf8\");\n          assertEquals(\"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne \\u0000 \\u2620 ab\\ud917\\udc17cd\", f.stringValue());\n      \n          f = d.getField(\"content2\");\n          assertEquals(\"here is more content with aaa aaa aaa\", f.stringValue());\n\n          f = d.getField(\"fie\\u2C77ld\");\n          assertEquals(\"field with non-ascii name\", f.stringValue());\n        }\n\n        Terms tfv = reader.getTermVectors(i).terms(\"utf8\");\n        assertNotNull(\"docID=\" + i + \" index=\" + oldName, tfv);\n      } else\n        // Only ID 7 is deleted\n        assertEquals(7, i);\n    }\n    \n    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(\"content\", \"aaa\")), null, 1000).scoreDocs;\n\n    // First document should be #21 since it's norm was\n    // increased:\n    Document d = searcher.getIndexReader().document(hits[0].doc);\n    assertEquals(\"didn't get the right document first\", \"21\", d.get(\"id\"));\n\n    doTestHits(hits, 34, searcher.getIndexReader());\n\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"\\u0000\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"Lu\\uD834\\uDD1Ece\\uD834\\uDD60ne\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n    hits = searcher.search(new TermQuery(new Term(\"utf8\", \"ab\\ud917\\udc17cd\")), null, 1000).scoreDocs;\n    assertEquals(34, hits.length);\n\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["8be580b58bcc650d428f3f22de81cadcf51d650a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8be580b58bcc650d428f3f22de81cadcf51d650a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["8be580b58bcc650d428f3f22de81cadcf51d650a"],"8be580b58bcc650d428f3f22de81cadcf51d650a":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}