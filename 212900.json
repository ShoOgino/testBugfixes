{"path":"lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PostingsHighlighter#highlightDoc(Term[],TermContext[],int,float[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","commits":[{"id":"ffabe030a2b84ad50adb7265da07ee78f1c58f6a","date":1355239263,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PostingsHighlighter#highlightDoc(Term[],TermContext[],int,float[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","pathOld":"/dev/null","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(Term termTexts[], TermContext[] terms, int ord, float[] weights, \n      int contentLength, BreakIterator bi, int doc, TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        TermState ts = terms[i].get(ord);\n        if (ts == null) {\n          continue;\n        }\n        termsEnum.seekExact(termTexts[i].bytes(), ts);\n        DocsAndPositionsEnum de2 = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de2 == null) {\n          continue;\n        } else {\n          de = postings[i] = de2;\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (right.score == left.score) {\n          return right.startOffset - left.endOffset;\n        } else {\n          return right.score > left.score ? 1 : -1;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start > current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, termTexts[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n    return new Passage[0];\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["86b90b0ece36c02b9f8b3a26374109d04b76274d"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b2b02091dbb6637db0db1d7b5151f8ced718f552","date":1356549268,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PostingsHighlighter#highlightDoc(Term[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PostingsHighlighter#highlightDoc(Term[],TermContext[],int,float[],int,BreakIterator,int,TermsEnum,DocsAndPositionsEnum[],int).mjava","sourceNew":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(Term terms[], int contentLength, BreakIterator bi, int doc, \n      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    float weights[] = new float[terms.length];\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        if (!termsEnum.seekExact(terms[i].bytes(), true)) {\n          continue; // term not found\n        }\n        DocsAndPositionsEnum de2 = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de2 == null) {\n          continue;\n        } else {\n          de = postings[i] = de2;\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        weights[i] = scorer.weight(contentLength, de.freq());\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (right.score == left.score) {\n          return right.startOffset - left.endOffset;\n        } else {\n          return right.score > left.score ? 1 : -1;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start > current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, terms[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n    return new Passage[0];\n  }\n\n","sourceOld":"  // algorithm: treat sentence snippets as miniature documents\n  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s\n  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))\n  private Passage[] highlightDoc(Term termTexts[], TermContext[] terms, int ord, float[] weights, \n      int contentLength, BreakIterator bi, int doc, TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {\n    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();\n    // initialize postings\n    for (int i = 0; i < terms.length; i++) {\n      DocsAndPositionsEnum de = postings[i];\n      int pDoc;\n      if (de == EMPTY) {\n        continue;\n      } else if (de == null) {\n        postings[i] = EMPTY; // initially\n        TermState ts = terms[i].get(ord);\n        if (ts == null) {\n          continue;\n        }\n        termsEnum.seekExact(termTexts[i].bytes(), ts);\n        DocsAndPositionsEnum de2 = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);\n        if (de2 == null) {\n          continue;\n        } else {\n          de = postings[i] = de2;\n        }\n        pDoc = de.advance(doc);\n      } else {\n        pDoc = de.docID();\n        if (pDoc < doc) {\n          pDoc = de.advance(doc);\n        }\n      }\n\n      if (doc == pDoc) {\n        de.nextPosition();\n        pq.add(new OffsetsEnum(de, i));\n      }\n    }\n    \n    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination\n    \n    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {\n      @Override\n      public int compare(Passage left, Passage right) {\n        if (right.score == left.score) {\n          return right.startOffset - left.endOffset;\n        } else {\n          return right.score > left.score ? 1 : -1;\n        }\n      }\n    });\n    Passage current = new Passage();\n    \n    OffsetsEnum off;\n    while ((off = pq.poll()) != null) {\n      final DocsAndPositionsEnum dp = off.dp;\n      int start = dp.startOffset();\n      if (start == -1) {\n        throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n      }\n      int end = dp.endOffset();\n      if (start > current.endOffset) {\n        if (current.startOffset >= 0) {\n          // finalize current\n          current.score *= scorer.norm(current.startOffset);\n          // new sentence: first add 'current' to queue \n          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {\n            current.reset(); // can't compete, just reset it\n          } else {\n            passageQueue.offer(current);\n            if (passageQueue.size() > n) {\n              current = passageQueue.poll();\n              current.reset();\n            } else {\n              current = new Passage();\n            }\n          }\n        }\n        // if we exceed limit, we are done\n        if (start >= contentLength) {\n          Passage passages[] = new Passage[passageQueue.size()];\n          passageQueue.toArray(passages);\n          // sort in ascending order\n          Arrays.sort(passages, new Comparator<Passage>() {\n            @Override\n            public int compare(Passage left, Passage right) {\n              return left.startOffset - right.startOffset;\n            }\n          });\n          return passages;\n        }\n        // advance breakiterator\n        assert BreakIterator.DONE < 0;\n        current.startOffset = Math.max(bi.preceding(start+1), 0);\n        current.endOffset = Math.min(bi.next(), contentLength);\n      }\n      int tf = 0;\n      while (true) {\n        tf++;\n        current.addMatch(start, end, termTexts[off.id]);\n        if (off.pos == dp.freq()) {\n          break; // removed from pq\n        } else {\n          off.pos++;\n          dp.nextPosition();\n          start = dp.startOffset();\n          end = dp.endOffset();\n        }\n        if (start >= current.endOffset) {\n          pq.offer(off);\n          break;\n        }\n      }\n      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);\n    }\n    return new Passage[0];\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"b2b02091dbb6637db0db1d7b5151f8ced718f552":["ffabe030a2b84ad50adb7265da07ee78f1c58f6a"],"ffabe030a2b84ad50adb7265da07ee78f1c58f6a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b2b02091dbb6637db0db1d7b5151f8ced718f552"]},"commit2Childs":{"b2b02091dbb6637db0db1d7b5151f8ced718f552":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ffabe030a2b84ad50adb7265da07ee78f1c58f6a":["b2b02091dbb6637db0db1d7b5151f8ced718f552"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ffabe030a2b84ad50adb7265da07ee78f1c58f6a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}