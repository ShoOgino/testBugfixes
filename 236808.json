{"path":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","commits":[{"id":"150488c1317972164a9a824be05b1ba2ba0fc68c","date":1284316090,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#setUp().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    Random random = newStaticRandom(TestBoolean2.class);\n    directory = newDirectory(random);\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(random, field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(random, \"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(random, \"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f9dba8ffba48fba97d7a90fd45f6da87ba55736","date":1285244347,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    Random random = newStaticRandom(TestBoolean2.class);\n    directory = newDirectory(random);\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(random, field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(random, \"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(random, \"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"53a31399f2471493d67b19a95c028a74e0113b6a","date":1289817072,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9ab1f5591dc05f1f2b5407d809c9699f75554a32","date":1290008586,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4e8cc373c801e54cec75daf9f52792cb4b17f536","date":1291116159,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(new Directory[] {copy});\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"/dev/null","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c19f985e36a65cc969e8e564fe337a0d41512075","date":1296330536,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"790e1fde4caa765b3faaad3fbcd25c6973450336","date":1296689245,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory);\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = new IndexSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"01e5948db9a07144112d2f08f28ca2e3cd880348","date":1301759232,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"45669a651c970812a680841b97a77cce06af559f","date":1301922222,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"639c36565ce03aed5b0fce7c9e4448e53a1f7efd","date":1308580104,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ddc4c914be86e34b54f70023f45a60fa7f04e929","date":1310115160,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5d004d0e0b3f65bb40da76d476d659d7888270e8","date":1310158940,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], TextField.TYPE_UNSTORED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], Field.Store.NO, Field.Index.ANALYZED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", Field.Store.NO, Field.Index.ANALYZED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a3a0403b45dfe384fae4a1b6e96c3265d000c498","date":1321445981,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], TextField.TYPE_UNSTORED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = IndexReader.open(directory);\n    searcher = new IndexSearcher(littleReader);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], TextField.TYPE_UNSTORED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    searcher = new IndexSearcher(directory, true);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":["b0c73b4e6b72cca35c7f115ab543ce9dcf50d8b5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], TextField.TYPE_UNSTORED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = IndexReader.open(directory);\n    searcher = new IndexSearcher(littleReader);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], TextField.TYPE_UNSTORED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = IndexReader.open(directory);\n    searcher = new IndexSearcher(littleReader);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a3a0403b45dfe384fae4a1b6e96c3265d000c498"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"c19f985e36a65cc969e8e564fe337a0d41512075":["4e8cc373c801e54cec75daf9f52792cb4b17f536"],"5f9dba8ffba48fba97d7a90fd45f6da87ba55736":["150488c1317972164a9a824be05b1ba2ba0fc68c"],"01e5948db9a07144112d2f08f28ca2e3cd880348":["790e1fde4caa765b3faaad3fbcd25c6973450336"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["01e5948db9a07144112d2f08f28ca2e3cd880348"],"53a31399f2471493d67b19a95c028a74e0113b6a":["5f9dba8ffba48fba97d7a90fd45f6da87ba55736"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["3bb13258feba31ab676502787ab2e1779f129b7a","790e1fde4caa765b3faaad3fbcd25c6973450336"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a3a0403b45dfe384fae4a1b6e96c3265d000c498":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":["a3776dccca01c11e7046323cfad46a3b4a471233","ddc4c914be86e34b54f70023f45a60fa7f04e929"],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["f2c5f0cb44df114db4228c8f77861714b5cabaea","639c36565ce03aed5b0fce7c9e4448e53a1f7efd"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","790e1fde4caa765b3faaad3fbcd25c6973450336"],"9ab1f5591dc05f1f2b5407d809c9699f75554a32":["5f9dba8ffba48fba97d7a90fd45f6da87ba55736","53a31399f2471493d67b19a95c028a74e0113b6a"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"962d04139994fce5193143ef35615499a9a96d78":["45669a651c970812a680841b97a77cce06af559f","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"790e1fde4caa765b3faaad3fbcd25c6973450336":["c19f985e36a65cc969e8e564fe337a0d41512075"],"a3776dccca01c11e7046323cfad46a3b4a471233":["790e1fde4caa765b3faaad3fbcd25c6973450336","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"150488c1317972164a9a824be05b1ba2ba0fc68c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"45669a651c970812a680841b97a77cce06af559f":["bde51b089eb7f86171eb3406e38a274743f9b7ac","01e5948db9a07144112d2f08f28ca2e3cd880348"],"3bb13258feba31ab676502787ab2e1779f129b7a":["9ab1f5591dc05f1f2b5407d809c9699f75554a32","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["53a31399f2471493d67b19a95c028a74e0113b6a"]},"commit2Childs":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"c19f985e36a65cc969e8e564fe337a0d41512075":["790e1fde4caa765b3faaad3fbcd25c6973450336"],"5f9dba8ffba48fba97d7a90fd45f6da87ba55736":["53a31399f2471493d67b19a95c028a74e0113b6a","9ab1f5591dc05f1f2b5407d809c9699f75554a32"],"01e5948db9a07144112d2f08f28ca2e3cd880348":["f2c5f0cb44df114db4228c8f77861714b5cabaea","45669a651c970812a680841b97a77cce06af559f"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["639c36565ce03aed5b0fce7c9e4448e53a1f7efd","ddc4c914be86e34b54f70023f45a60fa7f04e929","135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","a3776dccca01c11e7046323cfad46a3b4a471233"],"53a31399f2471493d67b19a95c028a74e0113b6a":["9ab1f5591dc05f1f2b5407d809c9699f75554a32","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","150488c1317972164a9a824be05b1ba2ba0fc68c"],"a3a0403b45dfe384fae4a1b6e96c3265d000c498":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":[],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["5d004d0e0b3f65bb40da76d476d659d7888270e8","1509f151d7692d84fae414b2b799ac06ba60fcb4"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["a3a0403b45dfe384fae4a1b6e96c3265d000c498"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["45669a651c970812a680841b97a77cce06af559f"],"9ab1f5591dc05f1f2b5407d809c9699f75554a32":["3bb13258feba31ab676502787ab2e1779f129b7a"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"962d04139994fce5193143ef35615499a9a96d78":[],"790e1fde4caa765b3faaad3fbcd25c6973450336":["01e5948db9a07144112d2f08f28ca2e3cd880348","29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac","a3776dccca01c11e7046323cfad46a3b4a471233"],"a3776dccca01c11e7046323cfad46a3b4a471233":["5d004d0e0b3f65bb40da76d476d659d7888270e8"],"150488c1317972164a9a824be05b1ba2ba0fc68c":["5f9dba8ffba48fba97d7a90fd45f6da87ba55736"],"45669a651c970812a680841b97a77cce06af559f":["962d04139994fce5193143ef35615499a9a96d78"],"3bb13258feba31ab676502787ab2e1779f129b7a":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","c19f985e36a65cc969e8e564fe337a0d41512075","3bb13258feba31ab676502787ab2e1779f129b7a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5d004d0e0b3f65bb40da76d476d659d7888270e8","135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}