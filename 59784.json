{"path":"src/java/org/apache/lucene/index/IndexWriter#mergeInit(MergePolicy.OneMerge).mjava","commits":[{"id":"b1405362241b561f5590ff4a87d5d6e173bcd9cf","date":1190107634,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeInit(MergePolicy.OneMerge).mjava","pathOld":"/dev/null","sourceNew":"  /** Does initial setup for a merge, which is fast but holds\n   *  the synchronized lock on IndexWriter instance. */\n  final synchronized void mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n\n    assert merge.registerDone;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n    final int numSegments = segmentInfos.size();\n\n    final int start = ensureContiguousMerge(merge);\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment))\n        doFlushDocStore = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      flush(false, true);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = merge.segmentsClone.info(i);\n\n      // IncRef all files for this segment info to make sure\n      // they are not removed while we are trying to merge.\n      if (si.dir == directory)\n        deleter.incRef(si.files());\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["878eedeaae8b281cc57edbb48be7876469cec585","346d5897e4c4e77ed5dbd31f7730ff30973d5971","d9d40c43a41eb2ee87c78ef5d4db212c8ec7c29c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"67006a60923e2124212d3baa0d29b444bcbd8373","date":1191425052,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeInit(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does initial setup for a merge, which is fast but holds\n   *  the synchronized lock on IndexWriter instance. */\n  final synchronized void mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n\n    assert merge.registerDone;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n    final int numSegments = segmentInfos.size();\n\n    final int start = ensureContiguousMerge(merge);\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment))\n        doFlushDocStore = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"flush at merge\");\n      flush(false, true);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = merge.segmentsClone.info(i);\n\n      // IncRef all files for this segment info to make sure\n      // they are not removed while we are trying to merge.\n      if (si.dir == directory)\n        deleter.incRef(si.files());\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile);\n  }\n\n","sourceOld":"  /** Does initial setup for a merge, which is fast but holds\n   *  the synchronized lock on IndexWriter instance. */\n  final synchronized void mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n\n    assert merge.registerDone;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n    final int numSegments = segmentInfos.size();\n\n    final int start = ensureContiguousMerge(merge);\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment))\n        doFlushDocStore = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      flush(false, true);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = merge.segmentsClone.info(i);\n\n      // IncRef all files for this segment info to make sure\n      // they are not removed while we are trying to merge.\n      if (si.dir == directory)\n        deleter.incRef(si.files());\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile);\n  }\n\n","bugFix":null,"bugIntro":["878eedeaae8b281cc57edbb48be7876469cec585"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b6a1f29c9b1051488fd5fa7d56c98db5f4388408","date":1196281221,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeInit(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does initial setup for a merge, which is fast but holds\n   *  the synchronized lock on IndexWriter instance. */\n  final synchronized void mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n\n    assert merge.registerDone;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    ensureContiguousMerge(merge);\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment))\n        doFlushDocStore = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"flush at merge\");\n      flush(false, true);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = merge.segmentsClone.info(i);\n\n      // IncRef all files for this segment info to make sure\n      // they are not removed while we are trying to merge.\n      if (si.dir == directory)\n        deleter.incRef(si.files());\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile);\n  }\n\n","sourceOld":"  /** Does initial setup for a merge, which is fast but holds\n   *  the synchronized lock on IndexWriter instance. */\n  final synchronized void mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n\n    assert merge.registerDone;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n    final int numSegments = segmentInfos.size();\n\n    final int start = ensureContiguousMerge(merge);\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment))\n        doFlushDocStore = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"flush at merge\");\n      flush(false, true);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = merge.segmentsClone.info(i);\n\n      // IncRef all files for this segment info to make sure\n      // they are not removed while we are trying to merge.\n      if (si.dir == directory)\n        deleter.incRef(si.files());\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d9d40c43a41eb2ee87c78ef5d4db212c8ec7c29c","date":1196806748,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeInit(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does initial setup for a merge, which is fast but holds\n   *  the synchronized lock on IndexWriter instance. */\n  final synchronized void mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    if (merge.isAborted())\n      throw new IOException(\"merge is aborted\");\n\n    assert merge.registerDone;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    ensureContiguousMerge(merge);\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment))\n        doFlushDocStore = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"flush at merge\");\n      flush(false, true);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = merge.segmentsClone.info(i);\n\n      // IncRef all files for this segment info to make sure\n      // they are not removed while we are trying to merge.\n      if (si.dir == directory)\n        deleter.incRef(si.files());\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile);\n  }\n\n","sourceOld":"  /** Does initial setup for a merge, which is fast but holds\n   *  the synchronized lock on IndexWriter instance. */\n  final synchronized void mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n\n    assert merge.registerDone;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    ensureContiguousMerge(merge);\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment))\n        doFlushDocStore = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"flush at merge\");\n      flush(false, true);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = merge.segmentsClone.info(i);\n\n      // IncRef all files for this segment info to make sure\n      // they are not removed while we are trying to merge.\n      if (si.dir == directory)\n        deleter.incRef(si.files());\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile);\n  }\n\n","bugFix":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"bugIntro":["878eedeaae8b281cc57edbb48be7876469cec585","346d5897e4c4e77ed5dbd31f7730ff30973d5971"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"346d5897e4c4e77ed5dbd31f7730ff30973d5971","date":1198317988,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeInit(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does initial setup for a merge, which is fast but holds\n   *  the synchronized lock on IndexWriter instance. */\n  final synchronized void mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert merge.registerDone;\n\n    if (merge.isAborted())\n      return;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    ensureContiguousMerge(merge);\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment))\n        doFlushDocStore = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"flush at merge\");\n      flush(false, true);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = merge.segmentsClone.info(i);\n\n      // IncRef all files for this segment info to make sure\n      // they are not removed while we are trying to merge.\n      if (si.dir == directory)\n        deleter.incRef(si.files());\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile);\n  }\n\n","sourceOld":"  /** Does initial setup for a merge, which is fast but holds\n   *  the synchronized lock on IndexWriter instance. */\n  final synchronized void mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    if (merge.isAborted())\n      throw new IOException(\"merge is aborted\");\n\n    assert merge.registerDone;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    ensureContiguousMerge(merge);\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment))\n        doFlushDocStore = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"flush at merge\");\n      flush(false, true);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = merge.segmentsClone.info(i);\n\n      // IncRef all files for this segment info to make sure\n      // they are not removed while we are trying to merge.\n      if (si.dir == directory)\n        deleter.incRef(si.files());\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile);\n  }\n\n","bugFix":["d9d40c43a41eb2ee87c78ef5d4db212c8ec7c29c","b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"bugIntro":["878eedeaae8b281cc57edbb48be7876469cec585"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e82780afe6097066eb5befb86e9432f077667e3d","date":1202756169,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeInit(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does initial setup for a merge, which is fast but holds\n   *  the synchronized lock on IndexWriter instance. */\n  final synchronized void mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    ensureContiguousMerge(merge);\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment))\n        doFlushDocStore = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"flush at merge\");\n      flush(false, true);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = merge.segmentsClone.info(i);\n\n      // IncRef all files for this segment info to make sure\n      // they are not removed while we are trying to merge.\n      if (si.dir == directory)\n        deleter.incRef(si.files());\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  /** Does initial setup for a merge, which is fast but holds\n   *  the synchronized lock on IndexWriter instance. */\n  final synchronized void mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert merge.registerDone;\n\n    if (merge.isAborted())\n      return;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    ensureContiguousMerge(merge);\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment))\n        doFlushDocStore = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"flush at merge\");\n      flush(false, true);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = merge.segmentsClone.info(i);\n\n      // IncRef all files for this segment info to make sure\n      // they are not removed while we are trying to merge.\n      if (si.dir == directory)\n        deleter.incRef(si.files());\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile);\n  }\n\n","bugFix":null,"bugIntro":["878eedeaae8b281cc57edbb48be7876469cec585"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be","date":1204801324,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeInit(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does initial setup for a merge, which is fast but holds\n   *  the synchronized lock on IndexWriter instance. */\n  final synchronized void mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    boolean changed = applyDeletes();\n\n    // If autoCommit == true then all deletes should have\n    // been flushed when we flushed the last segment\n    assert !changed || !autoCommit;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment))\n        doFlushDocStore = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"flush at merge\");\n      flush(false, true, false);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = merge.segmentsClone.info(i);\n\n      // IncRef all files for this segment info to make sure\n      // they are not removed while we are trying to merge.\n      if (si.dir == directory)\n        deleter.incRef(si.files());\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","sourceOld":"  /** Does initial setup for a merge, which is fast but holds\n   *  the synchronized lock on IndexWriter instance. */\n  final synchronized void mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    ensureContiguousMerge(merge);\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment))\n        doFlushDocStore = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"flush at merge\");\n      flush(false, true);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = merge.segmentsClone.info(i);\n\n      // IncRef all files for this segment info to make sure\n      // they are not removed while we are trying to merge.\n      if (si.dir == directory)\n        deleter.incRef(si.files());\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":null,"bugIntro":["878eedeaae8b281cc57edbb48be7876469cec585"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"878eedeaae8b281cc57edbb48be7876469cec585","date":1205050740,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeInit(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does initial setup for a merge, which is fast but holds\n   *  the synchronized lock on IndexWriter instance.  */\n  final synchronized void mergeInit(MergePolicy.OneMerge merge) throws IOException {\n    boolean success = false;\n    try {\n      _mergeInit(merge);\n      success = true;\n    } finally {\n      if (!success) {\n        mergeFinish(merge);\n        runningMerges.remove(merge);\n      }\n    }\n  }\n\n","sourceOld":"  /** Does initial setup for a merge, which is fast but holds\n   *  the synchronized lock on IndexWriter instance. */\n  final synchronized void mergeInit(MergePolicy.OneMerge merge) throws IOException {\n\n    assert testPoint(\"startMergeInit\");\n\n    assert merge.registerDone;\n    assert !merge.optimize || merge.maxNumSegmentsOptimize > 0;\n\n    if (merge.info != null)\n      // mergeInit already done\n      return;\n\n    if (merge.isAborted())\n      return;\n\n    boolean changed = applyDeletes();\n\n    // If autoCommit == true then all deletes should have\n    // been flushed when we flushed the last segment\n    assert !changed || !autoCommit;\n\n    final SegmentInfos sourceSegments = merge.segments;\n    final int end = sourceSegments.size();\n\n    // Check whether this merge will allow us to skip\n    // merging the doc stores (stored field & vectors).\n    // This is a very substantial optimization (saves tons\n    // of IO) that can only be applied with\n    // autoCommit=false.\n\n    Directory lastDir = directory;\n    String lastDocStoreSegment = null;\n    int next = -1;\n\n    boolean mergeDocStores = false;\n    boolean doFlushDocStore = false;\n    final String currentDocStoreSegment = docWriter.getDocStoreSegment();\n\n    // Test each segment to be merged: check if we need to\n    // flush/merge doc stores\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = sourceSegments.info(i);\n\n      // If it has deletions we must merge the doc stores\n      if (si.hasDeletions())\n        mergeDocStores = true;\n\n      // If it has its own (private) doc stores we must\n      // merge the doc stores\n      if (-1 == si.getDocStoreOffset())\n        mergeDocStores = true;\n\n      // If it has a different doc store segment than\n      // previous segments, we must merge the doc stores\n      String docStoreSegment = si.getDocStoreSegment();\n      if (docStoreSegment == null)\n        mergeDocStores = true;\n      else if (lastDocStoreSegment == null)\n        lastDocStoreSegment = docStoreSegment;\n      else if (!lastDocStoreSegment.equals(docStoreSegment))\n        mergeDocStores = true;\n\n      // Segments' docScoreOffsets must be in-order,\n      // contiguous.  For the default merge policy now\n      // this will always be the case but for an arbitrary\n      // merge policy this may not be the case\n      if (-1 == next)\n        next = si.getDocStoreOffset() + si.docCount;\n      else if (next != si.getDocStoreOffset())\n        mergeDocStores = true;\n      else\n        next = si.getDocStoreOffset() + si.docCount;\n      \n      // If the segment comes from a different directory\n      // we must merge\n      if (lastDir != si.dir)\n        mergeDocStores = true;\n\n      // If the segment is referencing the current \"live\"\n      // doc store outputs then we must merge\n      if (si.getDocStoreOffset() != -1 && currentDocStoreSegment != null && si.getDocStoreSegment().equals(currentDocStoreSegment))\n        doFlushDocStore = true;\n    }\n\n    final int docStoreOffset;\n    final String docStoreSegment;\n    final boolean docStoreIsCompoundFile;\n\n    if (mergeDocStores) {\n      docStoreOffset = -1;\n      docStoreSegment = null;\n      docStoreIsCompoundFile = false;\n    } else {\n      SegmentInfo si = sourceSegments.info(0);        \n      docStoreOffset = si.getDocStoreOffset();\n      docStoreSegment = si.getDocStoreSegment();\n      docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();\n    }\n\n    if (mergeDocStores && doFlushDocStore) {\n      // SegmentMerger intends to merge the doc stores\n      // (stored fields, vectors), and at least one of the\n      // segments to be merged refers to the currently\n      // live doc stores.\n\n      // TODO: if we know we are about to merge away these\n      // newly flushed doc store files then we should not\n      // make compound file out of them...\n      if (infoStream != null)\n        message(\"flush at merge\");\n      flush(false, true, false);\n    }\n\n    // We must take a full copy at this point so that we can\n    // properly merge deletes in commitMerge()\n    merge.segmentsClone = (SegmentInfos) merge.segments.clone();\n\n    for (int i = 0; i < end; i++) {\n      SegmentInfo si = merge.segmentsClone.info(i);\n\n      // IncRef all files for this segment info to make sure\n      // they are not removed while we are trying to merge.\n      if (si.dir == directory)\n        deleter.incRef(si.files());\n    }\n\n    merge.increfDone = true;\n\n    merge.mergeDocStores = mergeDocStores;\n\n    // Bind a new segment name here so even with\n    // ConcurrentMergePolicy we keep deterministic segment\n    // names.\n    merge.info = new SegmentInfo(newSegmentName(), 0,\n                                 directory, false, true,\n                                 docStoreOffset,\n                                 docStoreSegment,\n                                 docStoreIsCompoundFile);\n\n    // Also enroll the merged segment into mergingSegments;\n    // this prevents it from getting selected for a merge\n    // after our merge is done but while we are building the\n    // CFS:\n    mergingSegments.add(merge.info);\n  }\n\n","bugFix":["d9d40c43a41eb2ee87c78ef5d4db212c8ec7c29c","b1405362241b561f5590ff4a87d5d6e173bcd9cf","346d5897e4c4e77ed5dbd31f7730ff30973d5971","67006a60923e2124212d3baa0d29b444bcbd8373","a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be","e82780afe6097066eb5befb86e9432f077667e3d"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ba7fd1181f778e9954547e8e6a47587ebf08e3fb","date":1238267455,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#mergeInit(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does initial setup for a merge, which is fast but holds\n   *  the synchronized lock on IndexWriter instance.  */\n  final synchronized void mergeInit(MergePolicy.OneMerge merge) throws IOException {\n    boolean success = false;\n    try {\n      _mergeInit(merge);\n      success = true;\n    } finally {\n      if (!success) {\n        mergeFinish(merge);\n      }\n    }\n  }\n\n","sourceOld":"  /** Does initial setup for a merge, which is fast but holds\n   *  the synchronized lock on IndexWriter instance.  */\n  final synchronized void mergeInit(MergePolicy.OneMerge merge) throws IOException {\n    boolean success = false;\n    try {\n      _mergeInit(merge);\n      success = true;\n    } finally {\n      if (!success) {\n        mergeFinish(merge);\n        runningMerges.remove(merge);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/IndexWriter#mergeInit(MergePolicy.OneMerge).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#mergeInit(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does initial setup for a merge, which is fast but holds\n   *  the synchronized lock on IndexWriter instance.  */\n  final synchronized void mergeInit(MergePolicy.OneMerge merge) throws IOException {\n    boolean success = false;\n    try {\n      _mergeInit(merge);\n      success = true;\n    } finally {\n      if (!success) {\n        mergeFinish(merge);\n      }\n    }\n  }\n\n","sourceOld":"  /** Does initial setup for a merge, which is fast but holds\n   *  the synchronized lock on IndexWriter instance.  */\n  final synchronized void mergeInit(MergePolicy.OneMerge merge) throws IOException {\n    boolean success = false;\n    try {\n      _mergeInit(merge);\n      success = true;\n    } finally {\n      if (!success) {\n        mergeFinish(merge);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"878eedeaae8b281cc57edbb48be7876469cec585":["a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be"],"346d5897e4c4e77ed5dbd31f7730ff30973d5971":["d9d40c43a41eb2ee87c78ef5d4db212c8ec7c29c"],"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d9d40c43a41eb2ee87c78ef5d4db212c8ec7c29c":["b6a1f29c9b1051488fd5fa7d56c98db5f4388408"],"67006a60923e2124212d3baa0d29b444bcbd8373":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be":["e82780afe6097066eb5befb86e9432f077667e3d"],"e82780afe6097066eb5befb86e9432f077667e3d":["346d5897e4c4e77ed5dbd31f7730ff30973d5971"],"ba7fd1181f778e9954547e8e6a47587ebf08e3fb":["878eedeaae8b281cc57edbb48be7876469cec585"],"b6a1f29c9b1051488fd5fa7d56c98db5f4388408":["67006a60923e2124212d3baa0d29b444bcbd8373"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["ba7fd1181f778e9954547e8e6a47587ebf08e3fb"]},"commit2Childs":{"878eedeaae8b281cc57edbb48be7876469cec585":["ba7fd1181f778e9954547e8e6a47587ebf08e3fb"],"346d5897e4c4e77ed5dbd31f7730ff30973d5971":["e82780afe6097066eb5befb86e9432f077667e3d"],"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["67006a60923e2124212d3baa0d29b444bcbd8373"],"d9d40c43a41eb2ee87c78ef5d4db212c8ec7c29c":["346d5897e4c4e77ed5dbd31f7730ff30973d5971"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"67006a60923e2124212d3baa0d29b444bcbd8373":["b6a1f29c9b1051488fd5fa7d56c98db5f4388408"],"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be":["878eedeaae8b281cc57edbb48be7876469cec585"],"e82780afe6097066eb5befb86e9432f077667e3d":["a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be"],"ba7fd1181f778e9954547e8e6a47587ebf08e3fb":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"b6a1f29c9b1051488fd5fa7d56c98db5f4388408":["d9d40c43a41eb2ee87c78ef5d4db212c8ec7c29c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}