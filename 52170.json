{"path":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity.ExactDocScorer).mjava","commits":[{"id":"0f3cee3d20b0c786e6fca20539454262e29edcab","date":1310101685,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity.ExactDocScorer).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity,byte[]).mjava","sourceNew":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity.ExactDocScorer docScorer) throws IOException {\n    super(weight);\n    this.docScorer = docScorer;\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n  }\n\n","sourceOld":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity similarity, byte[] norms) throws IOException {\n    super(weight);\n    this.similarity = similarity;\n    this.norms = norms;\n    this.value = weight.getValue();\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n\n    for (int i = 0; i < SCORE_CACHE_SIZE; i++) {\n      scoreCache[i] = similarity.tf((float) i) * value;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f0b9507caf22f292ac0e5e59f62db4275adf4511","date":1310107283,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity.ExactDocScorer).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity,byte[]).mjava","sourceNew":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity.ExactDocScorer docScorer) throws IOException {\n    super(weight);\n    this.docScorer = docScorer;\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n  }\n\n","sourceOld":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity similarity, byte[] norms) throws IOException {\n    super(weight);\n    this.similarity = similarity;\n    this.norms = norms;\n    this.value = weight.getValue();\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n\n    for (int i = 0; i < SCORE_CACHE_SIZE; i++) {\n      scoreCache[i] = similarity.tf((float) i) * value;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1291e4568eb7d9463d751627596ef14baf4c1603","date":1310112572,"type":1,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity.ExactDocScorer).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity,byte[]).mjava","sourceNew":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity.ExactDocScorer docScorer) throws IOException {\n    super(weight);\n    this.docScorer = docScorer;\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n  }\n\n","sourceOld":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity similarity, byte[] norms) throws IOException {\n    super(weight);\n    this.similarity = similarity;\n    this.norms = norms;\n    this.value = weight.getValue();\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n\n    for (int i = 0; i < SCORE_CACHE_SIZE; i++) {\n      scoreCache[i] = similarity.tf((float) i) * value;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"660345363f16c7d4c38aa11a35bf59aa99466cf1","date":1328227279,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity.ExactSimScorer).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/ExactPhraseScorer#ExactPhraseScorer(Weight,PhraseQuery.PostingsAndFreq[],Similarity.ExactDocScorer).mjava","sourceNew":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity.ExactSimScorer docScorer) throws IOException {\n    super(weight);\n    this.docScorer = docScorer;\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n  }\n\n","sourceOld":"  ExactPhraseScorer(Weight weight, PhraseQuery.PostingsAndFreq[] postings,\n                    Similarity.ExactDocScorer docScorer) throws IOException {\n    super(weight);\n    this.docScorer = docScorer;\n\n    chunkStates = new ChunkState[postings.length];\n\n    endMinus1 = postings.length-1;\n\n    for(int i=0;i<postings.length;i++) {\n\n      // Coarse optimization: advance(target) is fairly\n      // costly, so, if the relative freq of the 2nd\n      // rarest term is not that much (> 1/5th) rarer than\n      // the first term, then we just use .nextDoc() when\n      // ANDing.  This buys ~15% gain for phrases where\n      // freq of rarest 2 terms is close:\n      final boolean useAdvance = postings[i].docFreq > 5*postings[0].docFreq;\n      chunkStates[i] = new ChunkState(postings[i].postings, -postings[i].position, useAdvance);\n      if (i > 0 && postings[i].postings.nextDoc() == DocsEnum.NO_MORE_DOCS) {\n        noDocs = true;\n        return;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1291e4568eb7d9463d751627596ef14baf4c1603":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","0f3cee3d20b0c786e6fca20539454262e29edcab"],"660345363f16c7d4c38aa11a35bf59aa99466cf1":["0f3cee3d20b0c786e6fca20539454262e29edcab"],"0f3cee3d20b0c786e6fca20539454262e29edcab":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"f0b9507caf22f292ac0e5e59f62db4275adf4511":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","0f3cee3d20b0c786e6fca20539454262e29edcab"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["660345363f16c7d4c38aa11a35bf59aa99466cf1"]},"commit2Childs":{"1291e4568eb7d9463d751627596ef14baf4c1603":[],"0f3cee3d20b0c786e6fca20539454262e29edcab":["1291e4568eb7d9463d751627596ef14baf4c1603","660345363f16c7d4c38aa11a35bf59aa99466cf1","f0b9507caf22f292ac0e5e59f62db4275adf4511"],"660345363f16c7d4c38aa11a35bf59aa99466cf1":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"f0b9507caf22f292ac0e5e59f62db4275adf4511":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1291e4568eb7d9463d751627596ef14baf4c1603","0f3cee3d20b0c786e6fca20539454262e29edcab","f0b9507caf22f292ac0e5e59f62db4275adf4511"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["1291e4568eb7d9463d751627596ef14baf4c1603","f0b9507caf22f292ac0e5e59f62db4275adf4511","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}