{"path":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"94f22e8ded82086fdbf9c490b0bb0f2d8165fc40","date":1270818726,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6c18273ea5b3974d2f30117f46f1ae416c28f727","date":1279708040,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          parent.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = parent.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success) {\n              docState.docWriter.setAborting();\n            }\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              parent.stringReader.init(stringValue);\n              reader = parent.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"765471a9091dc4cb586cf8db5ed0d9c40f534e8e","date":1285603829,"type":3,"author":"Koji Sekiguchi","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          parent.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = parent.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success) {\n              docState.docWriter.setAborting();\n            }\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              parent.stringReader.init(stringValue);\n              reader = parent.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          parent.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = parent.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success) {\n              docState.docWriter.setAborting();\n            }\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              parent.stringReader.init(stringValue);\n              reader = parent.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2ead1a392dc4e3ac356fca7a1935bccb09390b18","date":1293383039,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ff9cf7165d6cbafe4ef4431ecc2dc1af9cb2316c","date":1294014627,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          parent.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = parent.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success) {\n              docState.docWriter.setAborting();\n            }\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              parent.stringReader.init(stringValue);\n              reader = parent.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          parent.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = parent.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success) {\n              docState.docWriter.setAborting();\n            }\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              parent.stringReader.init(stringValue);\n              reader = parent.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70ad682703b8585f5d0a637efec044d57ec05efb","date":1294259117,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c2047784e704fe141e0ff36affac8a7cb6c7bbec","date":1295352100,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.length++;\n              fieldState.position++;\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e79a6d080bdd5b2a8f56342cf571b5476de04180","date":1295638686,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          parent.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = parent.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success) {\n              docState.docWriter.setAborting();\n            }\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              parent.stringReader.init(stringValue);\n              reader = parent.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          parent.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = parent.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success) {\n              docState.docWriter.setAborting();\n            }\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              parent.stringReader.init(stringValue);\n              reader = parent.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.length++;\n              fieldState.position++;\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          parent.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = parent.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success) {\n              docState.docWriter.setAborting();\n            }\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              parent.stringReader.init(stringValue);\n              reader = parent.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.length++;\n              fieldState.position++;\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          parent.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = parent.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success) {\n              docState.docWriter.setAborting();\n            }\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              parent.stringReader.init(stringValue);\n              reader = parent.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.length++;\n              fieldState.position++;\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          parent.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = parent.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success) {\n              docState.docWriter.setAborting();\n            }\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              parent.stringReader.init(stringValue);\n              reader = parent.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.length++;\n              fieldState.position++;\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a045054ae144972f58092f8a3d398c30f12fa21f","date":1310880650,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          parent.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = parent.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success) {\n              docState.docWriter.setAborting();\n            }\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              parent.stringReader.init(stringValue);\n              reader = parent.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          parent.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = parent.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success) {\n              docState.docWriter.setAborting();\n            }\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              parent.stringReader.init(stringValue);\n              reader = parent.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(IndexableField[],int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final IndexableField[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset();\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final IndexableField field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.indexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        // TODO (LUCENE-2309): this analysis logic should be\n        // outside of indexer -- field should simply give us\n        // a TokenStream, even for multi-valued fields\n\n        if (!field.tokenized()) {\t\t  // un-tokenized field\n          final String stringValue = field.stringValue();\n          assert stringValue != null;\n          final int valueLength = stringValue.length();\n          parent.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = parent.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success) {\n              docState.docWriter.setAborting();\n            }\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) {\n            stream = streamValue;\n          } else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null) {\n              reader = readerValue;\n            } else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              parent.stringReader.init(stringValue);\n              reader = parent.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.boost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n        \n        if (i > 0)\n          fieldState.position += docState.analyzer == null ? 0 : docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          parent.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = parent.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success) {\n              docState.docWriter.setAborting();\n            }\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null) {\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              }\n              parent.stringReader.init(stringValue);\n              reader = parent.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success) {\n                  docState.docWriter.setAborting();\n                }\n              }\n              fieldState.length++;\n              fieldState.position++;\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.offset += docState.analyzer == null ? 0 : docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n\n      // LUCENE-2387: don't hang onto the field, so GC can\n      // reclaim\n      fields[i] = null;\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":["a7c7a5405c388fd86e5962126be8ad09283eb5cc","782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"70ad682703b8585f5d0a637efec044d57ec05efb":["765471a9091dc4cb586cf8db5ed0d9c40f534e8e","2ead1a392dc4e3ac356fca7a1935bccb09390b18"],"94f22e8ded82086fdbf9c490b0bb0f2d8165fc40":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"6c18273ea5b3974d2f30117f46f1ae416c28f727":["94f22e8ded82086fdbf9c490b0bb0f2d8165fc40"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["c2047784e704fe141e0ff36affac8a7cb6c7bbec","e79a6d080bdd5b2a8f56342cf571b5476de04180"],"2ead1a392dc4e3ac356fca7a1935bccb09390b18":["765471a9091dc4cb586cf8db5ed0d9c40f534e8e"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["6c18273ea5b3974d2f30117f46f1ae416c28f727","765471a9091dc4cb586cf8db5ed0d9c40f534e8e"],"a045054ae144972f58092f8a3d398c30f12fa21f":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"e79a6d080bdd5b2a8f56342cf571b5476de04180":["ff9cf7165d6cbafe4ef4431ecc2dc1af9cb2316c","c2047784e704fe141e0ff36affac8a7cb6c7bbec"],"765471a9091dc4cb586cf8db5ed0d9c40f534e8e":["94f22e8ded82086fdbf9c490b0bb0f2d8165fc40"],"c2047784e704fe141e0ff36affac8a7cb6c7bbec":["2ead1a392dc4e3ac356fca7a1935bccb09390b18"],"ff9cf7165d6cbafe4ef4431ecc2dc1af9cb2316c":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","2ead1a392dc4e3ac356fca7a1935bccb09390b18"],"a3776dccca01c11e7046323cfad46a3b4a471233":["c2047784e704fe141e0ff36affac8a7cb6c7bbec","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["70ad682703b8585f5d0a637efec044d57ec05efb","c2047784e704fe141e0ff36affac8a7cb6c7bbec"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["a045054ae144972f58092f8a3d398c30f12fa21f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"70ad682703b8585f5d0a637efec044d57ec05efb":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"94f22e8ded82086fdbf9c490b0bb0f2d8165fc40":["6c18273ea5b3974d2f30117f46f1ae416c28f727","765471a9091dc4cb586cf8db5ed0d9c40f534e8e"],"6c18273ea5b3974d2f30117f46f1ae416c28f727":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["94f22e8ded82086fdbf9c490b0bb0f2d8165fc40"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["135621f3a0670a9394eb563224a3b76cc4dddc0f","a045054ae144972f58092f8a3d398c30f12fa21f","a3776dccca01c11e7046323cfad46a3b4a471233"],"2ead1a392dc4e3ac356fca7a1935bccb09390b18":["70ad682703b8585f5d0a637efec044d57ec05efb","c2047784e704fe141e0ff36affac8a7cb6c7bbec","ff9cf7165d6cbafe4ef4431ecc2dc1af9cb2316c"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["ff9cf7165d6cbafe4ef4431ecc2dc1af9cb2316c"],"a045054ae144972f58092f8a3d398c30f12fa21f":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"e79a6d080bdd5b2a8f56342cf571b5476de04180":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"765471a9091dc4cb586cf8db5ed0d9c40f534e8e":["70ad682703b8585f5d0a637efec044d57ec05efb","2ead1a392dc4e3ac356fca7a1935bccb09390b18","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"c2047784e704fe141e0ff36affac8a7cb6c7bbec":["b3e06be49006ecac364d39d12b9c9f74882f9b9f","e79a6d080bdd5b2a8f56342cf571b5476de04180","a3776dccca01c11e7046323cfad46a3b4a471233","29ef99d61cda9641b6250bf9567329a6e65f901d"],"ff9cf7165d6cbafe4ef4431ecc2dc1af9cb2316c":["e79a6d080bdd5b2a8f56342cf571b5476de04180"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}