{"path":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n\n    Directory dir = new RAMDirectory();\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setMaxBufferedDocs(10));\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create a deletable file:\n    copyFile(dir, \"_0.cfs\", \"deletable\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_3\", \"segments\");\n    copyFile(dir, \"segments_3\", \"segments_2\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_2.cfs\", \"_3.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n\n    Directory dir = new RAMDirectory();\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setMaxBufferedDocs(10));\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create a deletable file:\n    copyFile(dir, \"_0.cfs\", \"deletable\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_3\", \"segments\");\n    copyFile(dir, \"segments_3\", \"segments_2\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_2.cfs\", \"_3.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7ab99e8c71442b92c320e218141dee04a9b91ce8","date":1269203801,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n\n    Directory dir = new RAMDirectory();\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT))\n        .setMaxBufferedDocs(10));\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create a deletable file:\n    copyFile(dir, \"_0.cfs\", \"deletable\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_3\", \"segments\");\n    copyFile(dir, \"segments_3\", \"segments_2\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_2.cfs\", \"_3.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n\n    Directory dir = new RAMDirectory();\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setMaxBufferedDocs(10));\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create a deletable file:\n    copyFile(dir, \"_0.cfs\", \"deletable\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_3\", \"segments\");\n    copyFile(dir, \"segments_3\", \"segments_2\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_2.cfs\", \"_3.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b48e4082e2f39f1eb6f935ea9a1203c5e8d830a9","date":1270985469,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n\n    Directory dir = new RAMDirectory();\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT))\n        .setMaxBufferedDocs(10));\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create a deletable file:\n    copyFile(dir, \"_0.cfs\", \"deletable\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n\n    Directory dir = new RAMDirectory();\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT))\n        .setMaxBufferedDocs(10));\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create a deletable file:\n    copyFile(dir, \"_0.cfs\", \"deletable\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_3\", \"segments\");\n    copyFile(dir, \"segments_3\", \"segments_2\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_2.cfs\", \"_3.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d8d3f45cdd3ff689aaf7a3aab99e2df31305ac10","date":1270996866,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n\n    Directory dir = new RAMDirectory();\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT))\n        .setMaxBufferedDocs(10));\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create a deletable file:\n    copyFile(dir, \"_0.cfs\", \"deletable\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_3\", \"segments\");\n    copyFile(dir, \"segments_3\", \"segments_2\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_2.cfs\", \"_3.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n\n    Directory dir = new RAMDirectory();\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT))\n        .setMaxBufferedDocs(10));\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create a deletable file:\n    copyFile(dir, \"_0.cfs\", \"deletable\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":["69a923a22517eb7ff0bad9c6d1a7d45cc0696bd4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"69a923a22517eb7ff0bad9c6d1a7d45cc0696bd4","date":1271167458,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n\n    Directory dir = new RAMDirectory();\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT))\n        .setMaxBufferedDocs(10));\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create a deletable file:\n    copyFile(dir, \"_0.cfs\", \"deletable\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n\n    Directory dir = new RAMDirectory();\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT))\n        .setMaxBufferedDocs(10));\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create a deletable file:\n    copyFile(dir, \"_0.cfs\", \"deletable\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_3\", \"segments\");\n    copyFile(dir, \"segments_3\", \"segments_2\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_2.cfs\", \"_3.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":["d8d3f45cdd3ff689aaf7a3aab99e2df31305ac10"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d572389229127c297dd1fa5ce4758e1cec41e799","date":1273610938,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n\n    Directory dir = new RAMDirectory();\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(10));\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create a deletable file:\n    copyFile(dir, \"_0.cfs\", \"deletable\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n\n    Directory dir = new RAMDirectory();\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT))\n        .setMaxBufferedDocs(10));\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create a deletable file:\n    copyFile(dir, \"_0.cfs\", \"deletable\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6267e1ce56c2eec111425690cd04e251b6f14952","date":1275222352,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n\n    Directory dir = new RAMDirectory();\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(10));\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n\n    Directory dir = new RAMDirectory();\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(10));\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create a deletable file:\n    copyFile(dir, \"_0.cfs\", \"deletable\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8dc26bfa5ebbc55b5a04fbec545dfcec647b046b","date":1280297653,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n\n    Directory dir = new RAMDirectory();\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(10));\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n\n    Directory dir = new RAMDirectory();\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(10));\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b21422ff1d1d56499dec481f193b402e5e8def5b","date":1281472367,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Random random = newRandom();\n    Directory dir = new RAMDirectory();\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(random,\n        TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(10));\n    ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(10);\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(true);\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n\n    Directory dir = new RAMDirectory();\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(10));\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c","date":1281477834,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Random random = newRandom();\n    MockRAMDirectory dir = new MockRAMDirectory();\n    dir.setPreventDoubleWrite(false);\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(random,\n        TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(10));\n    ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(10);\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(true);\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Random random = newRandom();\n    Directory dir = new RAMDirectory();\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(random,\n        TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(10));\n    ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(10);\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(true);\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":["d19974432be9aed28ee7dca73bdf01d139e763a9"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c","date":1281646583,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Random random = newRandom();\n    MockRAMDirectory dir = newDirectory(random);\n    dir.setPreventDoubleWrite(false);\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(random,\n        TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(10));\n    ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(10);\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(true);\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Random random = newRandom();\n    MockRAMDirectory dir = new MockRAMDirectory();\n    dir.setPreventDoubleWrite(false);\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(random,\n        TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(10));\n    ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(10);\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(true);\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a05409176bd65129d67a785ee70e881e238a9aef","date":1282582843,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    Random random = newRandom();\n    MockDirectoryWrapper dir = newDirectory(random);\n    dir.setPreventDoubleWrite(false);\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(random,\n        TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(10));\n    ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(10);\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(true);\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Random random = newRandom();\n    MockRAMDirectory dir = newDirectory(random);\n    dir.setPreventDoubleWrite(false);\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(random,\n        TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(10));\n    ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(10);\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(true);\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1f653cfcf159baeaafe5d01682a911e95bba4012","date":1284122058,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(10));\n    ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(10);\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(true);\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    Random random = newRandom();\n    MockDirectoryWrapper dir = newDirectory(random);\n    dir.setPreventDoubleWrite(false);\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(random,\n        TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(10));\n    ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(10);\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(true);\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":["d19974432be9aed28ee7dca73bdf01d139e763a9"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4e8cc373c801e54cec75daf9f52792cb4b17f536","date":1291116159,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(newLogMergePolicy(true, 10))\n    );\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(10));\n    ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(10);\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(true);\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"38a62612cfa4e104080d89d7751a8f1a258ac335","date":1291442315,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    mergePolicy.setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(newLogMergePolicy(true, 10))\n    );\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":["0791b41f65aecff2e75db0c1ebf95d745a5ab1b1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(newLogMergePolicy(true, 10))\n    );\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(10));\n    ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(10);\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(true);\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4a69e5860d014751cc9329dfeb441a6d8fd1ed8e","date":1291833341,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    mergePolicy.setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(newLogMergePolicy(true, 10))\n    );\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5","date":1292695408,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    mergePolicy.setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    mergePolicy.setUseCompoundDocStore(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n\n    Set<String> dif = difFiles(files, files2);\n\n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n\n    Directory dir = new RAMDirectory();\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer())\n        .setMaxBufferedDocs(10));\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7a54e23e03b47f3d568ab3020bdd386e4b2f0a05","date":1294877328,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", Similarity.getDefault().encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", Similarity.getDefault().encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n\n    Set<String> dif = difFiles(files, files2);\n\n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n\n    Set<String> dif = difFiles(files, files2);\n\n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fd9325c7ff9928fabe81c28553b41fc7aa57dfab","date":1295896411,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity().get(\"content\");\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", Similarity.getDefault().encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bb9b72f7c3d7827c64dd4ec580ded81778da361d","date":1295897920,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity().get(\"content\");\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n\n    Set<String> dif = difFiles(files, files2);\n\n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", Similarity.getDefault().encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n\n    Set<String> dif = difFiles(files, files2);\n\n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity().get(\"content\");\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", (float) 1.5);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0c7a8f7304b75b1528814c5820fa23a96816c27","date":1298314239,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity().get(\"content\");\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity().get(\"content\");\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity().get(\"content\");\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n\n    Set<String> dif = difFiles(files, files2);\n\n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity().get(\"content\");\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n\n    Set<String> dif = difFiles(files, files2);\n\n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"14ec33385f6fbb6ce172882d14605790418a5d31","date":1298910796,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity().get(\"content\");\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity().get(\"content\");\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a31c91eda919456f5f9237b086174385292f9935","date":1299074041,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity().get(\"content\");\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1224a4027481acce15495b03bce9b48b93b42722","date":1300792329,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity().get(\"content\");\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for(i=0;i<fieldInfos.size();i++) {\n      FieldInfo fi = fieldInfos.fieldInfo(i);\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = i;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0ef0193974807e4bddf5432a6b0287fe4d6c9df","date":1301476645,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n\n    Set<String> dif = difFiles(files, files2);\n\n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity().get(\"content\");\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n\n    Set<String> dif = difFiles(files, files2);\n\n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n\n    Set<String> dif = difFiles(files, files2);\n\n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n\n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n\n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n\n    Set<String> dif = difFiles(files, files2);\n\n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7f6de6c702421cde625195435f521f7fdac46277","date":1307356635,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"77cf4379b2824f6ea34b091c495d6e95c38ff9e2","date":1307610475,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","date":1307729864,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    copyFile(dir, \"_1.cfs\", \"_2.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"639c36565ce03aed5b0fce7c9e4448e53a1f7efd","date":1308580104,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\", IOContext.DEFAULT);\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0aab6e810b4b0d3743d6a048be0602801f4b3920","date":1308671625,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = dir.openCompoundInput(\"_2.cfs\", 1024);\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = dir.openCompoundInput(\"_2.cfs\", 1024);\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\");\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b6f9be74ca7baaef11857ad002cad40419979516","date":1309449808,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\", newIOContext(random));\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\", IOContext.DEFAULT);\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = dir.openCompoundInput(\"_2.cfs\", newIOContext(random));\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileReader cfsReader = new CompoundFileReader(dir, \"_2.cfs\", newIOContext(random));\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0f3cee3d20b0c786e6fca20539454262e29edcab","date":1310101685,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    DefaultSimilarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = dir.openCompoundInput(\"_2.cfs\", 1024);\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = dir.openCompoundInput(\"_2.cfs\", 1024);\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f0b9507caf22f292ac0e5e59f62db4275adf4511","date":1310107283,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    DefaultSimilarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = dir.openCompoundInput(\"_2.cfs\", 1024);\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = dir.openCompoundInput(\"_2.cfs\", 1024);\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1291e4568eb7d9463d751627596ef14baf4c1603","date":1310112572,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    DefaultSimilarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = dir.openCompoundInput(\"_2.cfs\", newIOContext(random));\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    Similarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = dir.openCompoundInput(\"_2.cfs\", newIOContext(random));\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ddc4c914be86e34b54f70023f45a60fa7f04e929","date":1310115160,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    DefaultSimilarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = dir.openCompoundInput(\"_2.cfs\", newIOContext(random));\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    DefaultSimilarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = dir.openCompoundInput(\"_2.cfs\", 1024);\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5d004d0e0b3f65bb40da76d476d659d7888270e8","date":1310158940,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    DefaultSimilarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = dir.openCompoundInput(\"_2.cfs\", newIOContext(random));\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    DefaultSimilarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = dir.openCompoundInput(\"_2.cfs\", 1024);\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"60ba444201d2570214b6fcf1d15600dc1a01f548","date":1313868045,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    DefaultSimilarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = dir.openCompoundInput(\"_2.cfs\", newIOContext(random));\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    DefaultSimilarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = dir.openCompoundInput(\"_2.cfs\", newIOContext(random));\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndif: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"67aadace85f701c87a4e0721eedcda25d8415a70","date":1314201925,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    DefaultSimilarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = new CompoundFileDirectory(dir, \"_2.cfs\", newIOContext(random), false);\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    DefaultSimilarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = dir.openCompoundInput(\"_2.cfs\", newIOContext(random));\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    DefaultSimilarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = new CompoundFileDirectory(dir, \"_2.cfs\", newIOContext(random), false);\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    writer.setInfoStream(VERBOSE ? System.out : null);\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    DefaultSimilarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = new CompoundFileDirectory(dir, \"_2.cfs\", newIOContext(random), false);\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    assertTrue(dir.fileExists(\"_3.fdt\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    DefaultSimilarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = new CompoundFileDirectory(dir, \"_2.cfs\", newIOContext(random), false);\n    FieldInfosReader infosReader = Codec.getDefault().fieldInfosFormat().getFieldInfosReader();\n    FieldInfos fieldInfos = infosReader.read(cfsReader, \"2\", IOContext.READONCE);\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    DefaultSimilarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = new CompoundFileDirectory(dir, \"_2.cfs\", newIOContext(random), false);\n    FieldInfos fieldInfos = new FieldInfos(cfsReader, \"_2.fnm\");\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4122a26e1fd0457a340616673a3d3aada370f713","date":1322955654,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    DefaultSimilarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = new CompoundFileDirectory(dir, \"_2.cfs\", newIOContext(random), false);\n    FieldInfosReader infosReader = Codec.getDefault().fieldInfosFormat().getFieldInfosReader();\n    FieldInfos fieldInfos = infosReader.read(cfsReader, \"2\", IOContext.READONCE);\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"75ec8c9aaa10ac00b30fd4c2465409770c838f7b","date":1323020115,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3615ce4a1f785ae1b779244de52c6a7d99227e60","date":1323422019,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    DefaultSimilarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = new CompoundFileDirectory(dir, \"_2.cfs\", newIOContext(random), false);\n    FieldInfosReader infosReader = Codec.getDefault().fieldInfosFormat().getFieldInfosReader();\n    FieldInfos fieldInfos = infosReader.read(cfsReader, \"2\", IOContext.READONCE);\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","date":1323437438,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    IndexReader reader = IndexReader.open(dir, false);\n    Term searchTerm = new Term(\"id\", \"7\");\n    int delCount = reader.deleteDocuments(searchTerm);\n    assertEquals(\"didn't delete the right number of documents\", 1, delCount);\n    DefaultSimilarity sim = new DefaultSimilarity();\n    // Set one norm so we get a .s0 file:\n    reader.setNorm(21, \"content\", sim.encodeNormValue(1.5f));\n    reader.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // The numbering of fields can vary depending on which\n    // JRE is in use.  On some JREs we see content bound to\n    // field 0; on others, field 1.  So, here we have to\n    // figure out which field number corresponds to\n    // \"content\", and then set our expected file names below\n    // accordingly:\n    CompoundFileDirectory cfsReader = new CompoundFileDirectory(dir, \"_2.cfs\", newIOContext(random), false);\n    FieldInfosReader infosReader = Codec.getDefault().fieldInfosFormat().getFieldInfosReader();\n    FieldInfos fieldInfos = infosReader.read(cfsReader, \"2\", IOContext.READONCE);\n    int contentFieldIndex = -1;\n    for (FieldInfo fi : fieldInfos) {\n      if (fi.name.equals(\"content\")) {\n        contentFieldIndex = fi.number;\n        break;\n      }\n    }\n    cfsReader.close();\n    assertTrue(\"could not locate the 'content' field number in the _2.cfs segment\", contentFieldIndex != -1);\n\n    String normSuffix = \"s\" + contentFieldIndex;\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that actually has a separate norms file\n    // already, using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_2_2.f\" + contentFieldIndex);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.\" + normSuffix);\n\n    // Create a bogus separate norms file for a\n    // segment/field that does not have a separate norms\n    // file already using the \"not compound file\" extension:\n    copyFile(dir, \"_2_1.\" + normSuffix, \"_1_1.f\" + contentFieldIndex);\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b6242d3e8721ca39385f5aaf3c6b660f1d3c3992","date":1327077602,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":["5f6bd27530a2846413fe2d00030493c0e2d3a072"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31","date":1327836826,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fd92b8bcc88e969302510acf77bd6970da3994c4","date":1327839530,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1.del\", \"_0_2.del\");\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1.del\", \"_1_1.del\");\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1.del\", \"_188_1.del\");\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter#testDeleteLeftoverFiles().mjava","sourceNew":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","sourceOld":"  public void testDeleteLeftoverFiles() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setPreventDoubleWrite(false);\n\n    LogMergePolicy mergePolicy = newLogMergePolicy(true, 10);\n    mergePolicy.setNoCFSRatio(1); // This test expects all of its segments to be in CFS\n\n    IndexWriter writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(10).\n            setMergePolicy(mergePolicy)\n    );\n\n    int i;\n    for(i=0;i<35;i++) {\n      addDoc(writer, i);\n    }\n    mergePolicy.setUseCompoundFile(false);\n    for(;i<45;i++) {\n      addDoc(writer, i);\n    }\n    writer.close();\n\n    // Delete one doc so we get a .del file:\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES)\n    );\n    Term searchTerm = new Term(\"id\", \"7\");\n    writer.deleteDocuments(searchTerm);\n    writer.close();\n\n    // Now, artificially create an extra .del file & extra\n    // .s0 file:\n    String[] files = dir.listAll();\n\n    /*\n    for(int j=0;j<files.length;j++) {\n      System.out.println(j + \": \" + files[j]);\n    }\n    */\n\n    // TODO: fix this test better\n    String ext = Codec.getDefault().getName().equals(\"SimpleText\") ? \".liv\" : \".del\";\n    \n    // Create a bogus separate del file for a\n    // segment that already has a separate del file: \n    copyFile(dir, \"_0_1\" + ext, \"_0_2\" + ext);\n\n    // Create a bogus separate del file for a\n    // segment that does not yet have a separate del file:\n    copyFile(dir, \"_0_1\" + ext, \"_1_1\" + ext);\n\n    // Create a bogus separate del file for a\n    // non-existent segment:\n    copyFile(dir, \"_0_1\" + ext, \"_188_1\" + ext);\n\n    // Create a bogus segment file:\n    copyFile(dir, \"_0.cfs\", \"_188.cfs\");\n\n    // Create a bogus fnm file when the CFS already exists:\n    copyFile(dir, \"_0.cfs\", \"_0.fnm\");\n    \n    // Create some old segments file:\n    copyFile(dir, \"segments_2\", \"segments\");\n    copyFile(dir, \"segments_2\", \"segments_1\");\n\n    // Create a bogus cfs file shadowing a non-cfs segment:\n    \n    // TODO: assert is bogus (relies upon codec-specific filenames)\n    assertTrue(dir.fileExists(\"_3.fdt\") || dir.fileExists(\"_3.fld\"));\n    assertTrue(!dir.fileExists(\"_3.cfs\"));\n    copyFile(dir, \"_1.cfs\", \"_3.cfs\");\n    \n    String[] filesPre = dir.listAll();\n\n    // Open & close a writer: it should delete the above 4\n    // files and nothing more:\n    writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setOpenMode(OpenMode.APPEND));\n    writer.close();\n\n    String[] files2 = dir.listAll();\n    dir.close();\n\n    Arrays.sort(files);\n    Arrays.sort(files2);\n    \n    Set<String> dif = difFiles(files, files2);\n    \n    if (!Arrays.equals(files, files2)) {\n      fail(\"IndexFileDeleter failed to delete unreferenced extra files: should have deleted \" + (filesPre.length-files.length) + \" files but only deleted \" + (filesPre.length - files2.length) + \"; expected files:\\n    \" + asString(files) + \"\\n  actual files:\\n    \" + asString(files2)+\"\\ndiff: \"+dif);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":["3cc749c053615f5871f3b95715fe292f34e70a53","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["8dc26bfa5ebbc55b5a04fbec545dfcec647b046b","4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"fd9325c7ff9928fabe81c28553b41fc7aa57dfab":["7a54e23e03b47f3d568ab3020bdd386e4b2f0a05"],"14ec33385f6fbb6ce172882d14605790418a5d31":["b0c7a8f7304b75b1528814c5820fa23a96816c27"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["a05409176bd65129d67a785ee70e881e238a9aef"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","fd9325c7ff9928fabe81c28553b41fc7aa57dfab"],"1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c":["b21422ff1d1d56499dec481f193b402e5e8def5b"],"a05409176bd65129d67a785ee70e881e238a9aef":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"b6242d3e8721ca39385f5aaf3c6b660f1d3c3992":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["38a62612cfa4e104080d89d7751a8f1a258ac335"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["bb9b72f7c3d7827c64dd4ec580ded81778da361d","b0c7a8f7304b75b1528814c5820fa23a96816c27"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["a3776dccca01c11e7046323cfad46a3b4a471233","7f6de6c702421cde625195435f521f7fdac46277"],"0f3cee3d20b0c786e6fca20539454262e29edcab":["0aab6e810b4b0d3743d6a048be0602801f4b3920"],"38a62612cfa4e104080d89d7751a8f1a258ac335":["4e8cc373c801e54cec75daf9f52792cb4b17f536"],"7ab99e8c71442b92c320e218141dee04a9b91ce8":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"06584e6e98d592b34e1329b384182f368d2025e8":["67aadace85f701c87a4e0721eedcda25d8415a70"],"0aab6e810b4b0d3743d6a048be0602801f4b3920":["7f6de6c702421cde625195435f521f7fdac46277"],"d572389229127c297dd1fa5ce4758e1cec41e799":["69a923a22517eb7ff0bad9c6d1a7d45cc0696bd4"],"962d04139994fce5193143ef35615499a9a96d78":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"2553b00f699380c64959ccb27991289aae87be2e":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","0aab6e810b4b0d3743d6a048be0602801f4b3920"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["b6f9be74ca7baaef11857ad002cad40419979516","0aab6e810b4b0d3743d6a048be0602801f4b3920"],"6267e1ce56c2eec111425690cd04e251b6f14952":["d572389229127c297dd1fa5ce4758e1cec41e799"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["29ef99d61cda9641b6250bf9567329a6e65f901d","1224a4027481acce15495b03bce9b48b93b42722"],"a3776dccca01c11e7046323cfad46a3b4a471233":["1224a4027481acce15495b03bce9b48b93b42722","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":["135621f3a0670a9394eb563224a3b76cc4dddc0f","7f6de6c702421cde625195435f521f7fdac46277"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["3cc749c053615f5871f3b95715fe292f34e70a53","75ec8c9aaa10ac00b30fd4c2465409770c838f7b"],"c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31":["3615ce4a1f785ae1b779244de52c6a7d99227e60","b6242d3e8721ca39385f5aaf3c6b660f1d3c3992"],"67aadace85f701c87a4e0721eedcda25d8415a70":["60ba444201d2570214b6fcf1d15600dc1a01f548"],"b48e4082e2f39f1eb6f935ea9a1203c5e8d830a9":["7ab99e8c71442b92c320e218141dee04a9b91ce8"],"75ec8c9aaa10ac00b30fd4c2465409770c838f7b":["4122a26e1fd0457a340616673a3d3aada370f713"],"fd92b8bcc88e969302510acf77bd6970da3994c4":["3615ce4a1f785ae1b779244de52c6a7d99227e60","c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["7f6de6c702421cde625195435f521f7fdac46277"],"4a69e5860d014751cc9329dfeb441a6d8fd1ed8e":["3bb13258feba31ab676502787ab2e1779f129b7a"],"bb9b72f7c3d7827c64dd4ec580ded81778da361d":["868da859b43505d9d2a023bfeae6dd0c795f5295","fd9325c7ff9928fabe81c28553b41fc7aa57dfab"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["bde51b089eb7f86171eb3406e38a274743f9b7ac","1224a4027481acce15495b03bce9b48b93b42722"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["1224a4027481acce15495b03bce9b48b93b42722"],"69a923a22517eb7ff0bad9c6d1a7d45cc0696bd4":["d8d3f45cdd3ff689aaf7a3aab99e2df31305ac10"],"f0b9507caf22f292ac0e5e59f62db4275adf4511":["2553b00f699380c64959ccb27991289aae87be2e","0f3cee3d20b0c786e6fca20539454262e29edcab"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b6f9be74ca7baaef11857ad002cad40419979516":["639c36565ce03aed5b0fce7c9e4448e53a1f7efd"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":["f0b9507caf22f292ac0e5e59f62db4275adf4511","ddc4c914be86e34b54f70023f45a60fa7f04e929"],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["0f3cee3d20b0c786e6fca20539454262e29edcab","1291e4568eb7d9463d751627596ef14baf4c1603"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["6267e1ce56c2eec111425690cd04e251b6f14952"],"1224a4027481acce15495b03bce9b48b93b42722":["a31c91eda919456f5f9237b086174385292f9935"],"7a54e23e03b47f3d568ab3020bdd386e4b2f0a05":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"a31c91eda919456f5f9237b086174385292f9935":["14ec33385f6fbb6ce172882d14605790418a5d31"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["d619839baa8ce5503e496b94a9e42ad6f079293f","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"3cc749c053615f5871f3b95715fe292f34e70a53":["06584e6e98d592b34e1329b384182f368d2025e8"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["4a69e5860d014751cc9329dfeb441a6d8fd1ed8e","4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"8dc26bfa5ebbc55b5a04fbec545dfcec647b046b":["6267e1ce56c2eec111425690cd04e251b6f14952"],"d8d3f45cdd3ff689aaf7a3aab99e2df31305ac10":["b48e4082e2f39f1eb6f935ea9a1203c5e8d830a9"],"60ba444201d2570214b6fcf1d15600dc1a01f548":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"1291e4568eb7d9463d751627596ef14baf4c1603":["d083e83f225b11e5fdd900e83d26ddb385b6955c","0f3cee3d20b0c786e6fca20539454262e29edcab"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["fd9325c7ff9928fabe81c28553b41fc7aa57dfab"],"7f6de6c702421cde625195435f521f7fdac46277":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"4122a26e1fd0457a340616673a3d3aada370f713":["3cc749c053615f5871f3b95715fe292f34e70a53"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","7a54e23e03b47f3d568ab3020bdd386e4b2f0a05"],"3bb13258feba31ab676502787ab2e1779f129b7a":["1f653cfcf159baeaafe5d01682a911e95bba4012","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["1f653cfcf159baeaafe5d01682a911e95bba4012"]},"commit2Childs":{"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"fd9325c7ff9928fabe81c28553b41fc7aa57dfab":["29ef99d61cda9641b6250bf9567329a6e65f901d","bb9b72f7c3d7827c64dd4ec580ded81778da361d","b0c7a8f7304b75b1528814c5820fa23a96816c27"],"14ec33385f6fbb6ce172882d14605790418a5d31":["a31c91eda919456f5f9237b086174385292f9935"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["3bb13258feba31ab676502787ab2e1779f129b7a","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["d619839baa8ce5503e496b94a9e42ad6f079293f"],"1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"a05409176bd65129d67a785ee70e881e238a9aef":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"b6242d3e8721ca39385f5aaf3c6b660f1d3c3992":["c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","7a54e23e03b47f3d568ab3020bdd386e4b2f0a05","ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["7ab99e8c71442b92c320e218141dee04a9b91ce8"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["2553b00f699380c64959ccb27991289aae87be2e"],"0f3cee3d20b0c786e6fca20539454262e29edcab":["f0b9507caf22f292ac0e5e59f62db4275adf4511","ddc4c914be86e34b54f70023f45a60fa7f04e929","1291e4568eb7d9463d751627596ef14baf4c1603"],"38a62612cfa4e104080d89d7751a8f1a258ac335":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"7ab99e8c71442b92c320e218141dee04a9b91ce8":["b48e4082e2f39f1eb6f935ea9a1203c5e8d830a9"],"06584e6e98d592b34e1329b384182f368d2025e8":["3cc749c053615f5871f3b95715fe292f34e70a53"],"0aab6e810b4b0d3743d6a048be0602801f4b3920":["0f3cee3d20b0c786e6fca20539454262e29edcab","2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"d572389229127c297dd1fa5ce4758e1cec41e799":["6267e1ce56c2eec111425690cd04e251b6f14952"],"962d04139994fce5193143ef35615499a9a96d78":[],"2553b00f699380c64959ccb27991289aae87be2e":["f0b9507caf22f292ac0e5e59f62db4275adf4511"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["1291e4568eb7d9463d751627596ef14baf4c1603"],"6267e1ce56c2eec111425690cd04e251b6f14952":["b21422ff1d1d56499dec481f193b402e5e8def5b","8dc26bfa5ebbc55b5a04fbec545dfcec647b046b"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":[],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","b6242d3e8721ca39385f5aaf3c6b660f1d3c3992","c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31","fd92b8bcc88e969302510acf77bd6970da3994c4"],"c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","fd92b8bcc88e969302510acf77bd6970da3994c4"],"67aadace85f701c87a4e0721eedcda25d8415a70":["06584e6e98d592b34e1329b384182f368d2025e8"],"75ec8c9aaa10ac00b30fd4c2465409770c838f7b":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"b48e4082e2f39f1eb6f935ea9a1203c5e8d830a9":["d8d3f45cdd3ff689aaf7a3aab99e2df31305ac10"],"fd92b8bcc88e969302510acf77bd6970da3994c4":[],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["b6f9be74ca7baaef11857ad002cad40419979516"],"4a69e5860d014751cc9329dfeb441a6d8fd1ed8e":["ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"bb9b72f7c3d7827c64dd4ec580ded81778da361d":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["962d04139994fce5193143ef35615499a9a96d78"],"69a923a22517eb7ff0bad9c6d1a7d45cc0696bd4":["d572389229127c297dd1fa5ce4758e1cec41e799"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["962d04139994fce5193143ef35615499a9a96d78","a3776dccca01c11e7046323cfad46a3b4a471233","135621f3a0670a9394eb563224a3b76cc4dddc0f","7f6de6c702421cde625195435f521f7fdac46277"],"f0b9507caf22f292ac0e5e59f62db4275adf4511":["5d004d0e0b3f65bb40da76d476d659d7888270e8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"b6f9be74ca7baaef11857ad002cad40419979516":["d083e83f225b11e5fdd900e83d26ddb385b6955c"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":[],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["5d004d0e0b3f65bb40da76d476d659d7888270e8","60ba444201d2570214b6fcf1d15600dc1a01f548"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c"],"1224a4027481acce15495b03bce9b48b93b42722":["d619839baa8ce5503e496b94a9e42ad6f079293f","a3776dccca01c11e7046323cfad46a3b4a471233","c0ef0193974807e4bddf5432a6b0287fe4d6c9df","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"7a54e23e03b47f3d568ab3020bdd386e4b2f0a05":["fd9325c7ff9928fabe81c28553b41fc7aa57dfab","868da859b43505d9d2a023bfeae6dd0c795f5295"],"a31c91eda919456f5f9237b086174385292f9935":["1224a4027481acce15495b03bce9b48b93b42722"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["a05409176bd65129d67a785ee70e881e238a9aef"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"3cc749c053615f5871f3b95715fe292f34e70a53":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","3615ce4a1f785ae1b779244de52c6a7d99227e60","4122a26e1fd0457a340616673a3d3aada370f713"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"8dc26bfa5ebbc55b5a04fbec545dfcec647b046b":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"d8d3f45cdd3ff689aaf7a3aab99e2df31305ac10":["69a923a22517eb7ff0bad9c6d1a7d45cc0696bd4"],"60ba444201d2570214b6fcf1d15600dc1a01f548":["67aadace85f701c87a4e0721eedcda25d8415a70"],"1291e4568eb7d9463d751627596ef14baf4c1603":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["14ec33385f6fbb6ce172882d14605790418a5d31","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"7f6de6c702421cde625195435f521f7fdac46277":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","0aab6e810b4b0d3743d6a048be0602801f4b3920","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","639c36565ce03aed5b0fce7c9e4448e53a1f7efd"],"4122a26e1fd0457a340616673a3d3aada370f713":["75ec8c9aaa10ac00b30fd4c2465409770c838f7b"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["bb9b72f7c3d7827c64dd4ec580ded81778da361d"],"3bb13258feba31ab676502787ab2e1779f129b7a":["4a69e5860d014751cc9329dfeb441a6d8fd1ed8e"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["38a62612cfa4e104080d89d7751a8f1a258ac335","3bb13258feba31ab676502787ab2e1779f129b7a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","962d04139994fce5193143ef35615499a9a96d78","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","fd92b8bcc88e969302510acf77bd6970da3994c4","5d004d0e0b3f65bb40da76d476d659d7888270e8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}