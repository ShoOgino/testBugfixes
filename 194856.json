{"path":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f08557cdb6c60ac7b88a9342c983a20cd236e74f","date":1330954480,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","date":1331075828,"type":3,"author":"Ryan McKinley","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bdb5e42b0cecd8dfb27767a02ada71899bf17917","date":1334100099,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5a238fc456663f685a9db1ed8d680e348bb45171","date":1334173266,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f6eee1e5a8555d83dd8f2f2e3c0a4ccec8e7cf9b","date":1337136355,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new MutableFieldInfos(new MutableFieldInfos.FieldNumberBiMap()), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"76923f6a33f2c4bec7f584e3f251261afe7ea276","date":1337149711,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new MutableFieldInfos(new MutableFieldInfos.FieldNumberBiMap()), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new MutableFieldInfos(new MutableFieldInfos.FieldNumberBiMap()), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d2dee33619431ada2a7a07f5fe2dbd94bac6a460","date":1337274029,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new MutableFieldInfos(new MutableFieldInfos.FieldNumberBiMap()), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, -1, mergedSegment,\n                                                                   false, null, false, 0, mergeState.fieldInfos.hasProx(), codec, null,\n                                                                   mergeState.fieldInfos.hasVectors(),\n                                                                   mergeState.fieldInfos.hasDocValues(),\n                                                                   mergeState.fieldInfos.hasNorms(),\n                                                                   mergeState.fieldInfos.hasFreq()),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new MutableFieldInfos(new MutableFieldInfos.FieldNumberBiMap()), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fc834f3412d287003cc04691da380b69ab983239","date":1337276089,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new MutableFieldInfos(new MutableFieldInfos.FieldNumberBiMap()), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, -1, mergedSegment,\n                                                                   false, null, false, 0, mergeState.fieldInfos.hasProx(), codec, null,\n                                                                   mergeState.fieldInfos.hasVectors()),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new MutableFieldInfos(new MutableFieldInfos.FieldNumberBiMap()), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, -1, mergedSegment,\n                                                                   false, null, false, 0, mergeState.fieldInfos.hasProx(), codec, null,\n                                                                   mergeState.fieldInfos.hasVectors(),\n                                                                   mergeState.fieldInfos.hasDocValues(),\n                                                                   mergeState.fieldInfos.hasNorms(),\n                                                                   mergeState.fieldInfos.hasFreq()),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4a8b14bc4241c302311422d5c6f7627f8febb86e","date":1337291675,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new MutableFieldInfos(new MutableFieldInfos.FieldNumberBiMap()), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, -1, mergedSegment,\n                                                                   false, null, false, 0, mergeState.fieldInfos.hasProx(), codec, null),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new MutableFieldInfos(new MutableFieldInfos.FieldNumberBiMap()), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, -1, mergedSegment,\n                                                                   false, null, false, 0, mergeState.fieldInfos.hasProx(), codec, null,\n                                                                   mergeState.fieldInfos.hasVectors()),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"dc97c61094c5498702b29cc2e8309beac50c23dc","date":1337293692,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new MutableFieldInfos(new MutableFieldInfos.FieldNumberBiMap()), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, -1, mergedSegment,\n                                                                   false, null, false, 0, codec, null),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new MutableFieldInfos(new MutableFieldInfos.FieldNumberBiMap()), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, -1, mergedSegment,\n                                                                   false, null, false, 0, mergeState.fieldInfos.hasProx(), codec, null),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a851824c09818632c94eba41e60ef5e72e323c8e","date":1337355760,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos.Builder(), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, -1, mergedSegment,\n                                                                   false, null, false, 0, codec, null),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new MutableFieldInfos(new MutableFieldInfos.FieldNumberBiMap()), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, -1, mergedSegment,\n                                                                   false, null, false, 0, codec, null),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4356000e349e38c9fb48034695b7c309abd54557","date":1337460341,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos.Builder(), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, mergedSegment,\n                                                                   false, null, false, 0, codec, null),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos.Builder(), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, -1, mergedSegment,\n                                                                   false, null, false, 0, codec, null),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9d153abcf92dc5329d98571a8c3035df9bd80648","date":1337702630,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos.Builder(), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, mergedSegment,\n                                                                         false, null, false, codec, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos.Builder(), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, mergedSegment,\n                                                                   false, null, false, 0, codec, null),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"63caed6eb28209e181e97822c4c8fdf808884c3b","date":1337712793,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos.Builder(), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, mergedSegment,\n                                                                         false, null, false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos.Builder(), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, mergedSegment,\n                                                                         false, null, false, codec, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"129c6e8ac0c0d9a110ba29e4b5f1889374f30076","date":1337725510,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, 10000, -1, mergedSegment, false, null, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos.Builder(), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, mergedSegment,\n                                                                         false, null, false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos.Builder(), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, mergedSegment,\n                                                                         false, null, false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"00f06a4178989089b29a77d6dce7c86dfb8b6931","date":1337729247,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, 10000, -1, mergedSegment, false, null, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.Builder(), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, mergedSegment,\n                                                                         false, null, false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, 10000, -1, mergedSegment, false, null, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos.Builder(), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, mergedSegment,\n                                                                         false, null, false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4e84d639980c2b2eb5d41330d5ff68d143239495","date":1337729749,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, 10000, -1, mergedSegment, false, null, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, mergedSegment,\n                                                                         false, null, false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, 10000, -1, mergedSegment, false, null, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.Builder(), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, mergedSegment,\n                                                                         false, null, false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0f6a6419266ce0a74e9f1501938a86a4c94d5af7","date":1337731230,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, 10000, -1, mergedSegment, false, null, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, mergedSegment,\n                                                                         false, null, false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, 10000, -1, mergedSegment, false, null, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, mergedSegment,\n                                                                         false, null, false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"16cbef32b882ec68df422af3f08845ec82620335","date":1337802266,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, -1, mergedSegment, false, null, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, mergedSegment,\n                                                                         false, null, false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, 10000, -1, mergedSegment, false, null, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, mergedSegment,\n                                                                         false, null, false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ff82b51516e4a8d24bb6182e5235be1c88b8ac2e","date":1337803615,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, -1, mergedSegment, false, null, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, mergedSegment,\n                                                                         false, null, false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, -1, mergedSegment, false, null, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, mergedSegment,\n                                                                         false, null, false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6a917aca07a305ab70118a83e84d931503441271","date":1337826487,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, null, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         null, false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, -1, mergedSegment, false, null, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged, -1, mergedSegment,\n                                                                         false, null, false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"764b942fd30efcae6e532c19771f32eeeb0037b2","date":1337868546,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, null, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         null, false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"615ddbd81799980d0fdd95e0238e1c498b6f47b0","date":1338233290,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4","date":1341839195,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"02331260bb246364779cb6f04919ca47900d01bb","date":1343749884,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":["872cff1d3a554e0cd64014cd97f88d3002b0f491"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bc124b3b129ef11a255212f3af482b771c5b3a6c","date":1344947616,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","date":1345029782,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3c188105a9aae04f56c24996f98f8333fc825d2e","date":1345031914,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1c93396a1df03720cb20e2c2f513a6fa59b21e4c","date":1345032673,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b05c56a41b733e02a189c48895922b5bd8c7f3d1","date":1345033322,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, null, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1d028314cced5858683a1bb4741423d0f934257b","date":1346596535,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad7de846867bd14c63f9dd19df082f72c5ea9c54","date":1355517454,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    Assume.assumeTrue(_TestUtil.canUseSimpleNorms());\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0837ab0472feecb3a54260729d845f839e1cbd72","date":1358283639,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    Assume.assumeTrue(_TestUtil.canUseSimpleNorms());\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad252c98ff183bc59bd0617be14fa46f9696d6fc","date":1363962178,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n                                             MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a45bec74b98f6fc05f52770cfb425739e6563960","date":1375119292,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e072d0b1fc19e0533d8ce432eed245196bca6fde","date":1379265112,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":["0567bdc5c86c94ced64201187cfcef2417d76dda"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"77f264c55cbf75404f8601ae7290d69157273a56","date":1380484282,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null),\n                                                         0, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null, null),\n                                                         0, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":["5f6bd27530a2846413fe2d00030493c0e2d3a072"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0","date":1383367127,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null),\n                                                         0, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfoPerCommit(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null),\n                                                         0, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null),\n                                                         0, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null),\n                                                         0, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random(), mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1f3b037cd083286b2af89f96e768f85dcd8072d6","date":1396337805,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()), true);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null),\n                                                         0, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null),\n                                                         0, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5eb2511ababf862ea11e10761c70ee560cd84510","date":1396607225,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()), true);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null),\n                                                         0, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null),\n                                                         0, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0567bdc5c86c94ced64201187cfcef2417d76dda","date":1400678298,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()), true);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null),\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()), true);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null),\n                                                         0, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":["e072d0b1fc19e0533d8ce432eed245196bca6fde"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a656b32c3aa151037a8c52e9b134acc3cbf482bc","date":1400688195,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()), true);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null),\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()), true);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null),\n                                                         0, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7605579001505896d48b07160075a5c8b8e128e","date":1400758727,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()), true);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null),\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()), true);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null),\n                                                         0, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"057a1793765d068ea9302f1a29e21734ee58d41e","date":1408130117,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()), true);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, docsMerged,\n                                                                         false, codec, null),\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()), true);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,\n                                                                         false, codec, null),\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":["5f6bd27530a2846413fe2d00030493c0e2d3a072"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<LeafReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()), true);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, docsMerged,\n                                                                         false, codec, null),\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()), true);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, docsMerged,\n                                                                         false, codec, null),\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":["505bff044e47a553f461b6f4484d1d08faf4ac85"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c65d2864d936ccf22dc7ec14dd48b4dff7bacceb","date":1411653326,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null, StringHelper.randomId());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<LeafReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()), true);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<LeafReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()), true);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, docsMerged,\n                                                                         false, codec, null),\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":["8521d944f9dfb45692ec28235dbf116d47ef69ba"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5f6bd27530a2846413fe2d00030493c0e2d3a072","date":1411811855,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null, StringHelper.randomId());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<LeafReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()), true);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<LeafReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()), true);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, docsMerged,\n                                                                         false, codec, null),\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":["057a1793765d068ea9302f1a29e21734ee58d41e","77f264c55cbf75404f8601ae7290d69157273a56"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2131047ecceac64b54ba70feec3d26bbd7e483d7","date":1411862069,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null, StringHelper.randomId());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<LeafReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null, StringHelper.randomId());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<LeafReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()), true);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9bb9a29a5e71a90295f175df8919802993142c9a","date":1412517673,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null, StringHelper.randomId());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<LeafReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null, StringHelper.randomId());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<LeafReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()), true);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8521d944f9dfb45692ec28235dbf116d47ef69ba","date":1417535150,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<LeafReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null, StringHelper.randomId());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<LeafReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":["c65d2864d936ccf22dc7ec14dd48b4dff7bacceb"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5faf65b6692f15cca0f87bf8666c87899afc619f","date":1420468108,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<LeafReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<LeafReader>asList(reader1, reader2),\n        si, InfoStream.getDefault(), mergedDir,\n        MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"505bff044e47a553f461b6f4484d1d08faf4ac85","date":1420728783,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<LeafReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"79700663e164dece87bed4adfd3e28bab6cb1385","date":1425241849,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"299a2348fa24151d150182211b6208a38e5e3450","date":1425304608,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0267c69e2456a3477a1ad785723f2135da3117e","date":1425317087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b06445ae1731e049327712db0454e5643ca9b7fe","date":1425329139,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.getDocCount();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        MultiFields.getLiveDocs(mergedReader),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","date":1453060490,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    StoredDocument newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    StoredDocument newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ceaef6cfc68c8ab22a684192e469a8280f9e6e70","date":1462354657,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3d33e731a93d4b57e662ff094f64f94a745422d4","date":1463128289,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ad30c6a479e764150a3316e57263319775f1df2","date":1463395403,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31741cf1390044e38a2ec3127cf302ba841bfd75","date":1491292636,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   Version.LATEST.major,\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"92212fd254551a0b1156aafc3a1a6ed1a43932ad","date":1491296431,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   Version.LATEST.major,\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eeba0a4d0845889a402dd225793d62f009d029c9","date":1527938093,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(null),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   Version.LATEST.major,\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   Version.LATEST.major,\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab548c8f96022b4780f7500a30b19b4f4a5feeb6","date":1527940044,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   Version.LATEST.major,\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(null),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   Version.LATEST.major,\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7e4ca6dc9612ff741d8713743e2bccfae5eadac","date":1528093718,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(null),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   Version.LATEST.major,\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   Version.LATEST.major,\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8f2203cb8ae87188877cfbf6ad170c5738a0aad5","date":1528117512,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(null),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, 0, -1L, -1L, -1L),\n                                                   Version.LATEST.major,\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(null),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   Version.LATEST.major,\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f592209545c71895260367152601e9200399776d","date":1528238935,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(null),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, 0, -1L, -1L, -1L),\n                                                   Version.LATEST.major,\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   Version.LATEST.major,\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b70042a8a492f7054d480ccdd2be9796510d4327","date":1528386658,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(null),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, 0, -1L, -1L, -1L),\n                                                   Version.LATEST.major,\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, -1L, -1L, -1L),\n                                                   Version.LATEST.major,\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"790693f23f4e88a59fbb25e47cc25f6d493b03cb","date":1553077690,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(null),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, 0, -1L, -1L, -1L),\n                                                   Version.LATEST.major,\n        false, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(null),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, 0, -1L, -1L, -1L),\n                                                   Version.LATEST.major,\n                                                   newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"763da4a9605e47013078edc323b9d4b608f0f9e0","date":1555353576,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(null),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, 0, -1L, -1L, -1L),\n                                                   Version.LATEST.major,\n        false, newIOContext(random()), Collections.emptyMap());\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(null),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, 0, -1L, -1L, -1L),\n                                                   Version.LATEST.major,\n        false, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a4e83191a3e02851a0b67e5335e6922f3e9ea86d","date":1583489709,"type":3,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(null),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n        mergeState.segmentInfo,\n        0, 0, -1L, -1L, -1L),\n        Version.LATEST.major,\n        false, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(null),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n                                                         mergeState.segmentInfo,\n                                                         0, 0, -1L, -1L, -1L),\n                                                   Version.LATEST.major,\n        false, newIOContext(random()), Collections.emptyMap());\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bec68e7c41fed133827595747d853cad504e481e","date":1583501052,"type":3,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(null),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n        mergeState.segmentInfo,\n        0, 0, -1L, -1L, -1L),\n        Version.LATEST.major,\n        newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(null),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n        mergeState.segmentInfo,\n        0, 0, -1L, -1L, -1L),\n        Version.LATEST.major,\n        false, newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"14654be3f7a82c9a3c52169e365baa55bfe64f66","date":1587212697,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(null),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n        mergeState.segmentInfo,\n        0, 0, -1L, -1L, -1L, StringHelper.randomId()),\n        Version.LATEST.major,\n        newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    final SegmentInfo si = new SegmentInfo(mergedDir, Version.LATEST, null, mergedSegment, -1, false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), null);\n\n    SegmentMerger merger = new SegmentMerger(Arrays.<CodecReader>asList(reader1, reader2),\n                                             si, InfoStream.getDefault(), mergedDir,\n                                             new FieldInfos.FieldNumbers(null),\n                                             newIOContext(random(), new IOContext(new MergeInfo(-1, -1, false, -1))));\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.segmentInfo.maxDoc();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentCommitInfo(\n        mergeState.segmentInfo,\n        0, 0, -1L, -1L, -1L),\n        Version.LATEST.major,\n        newIOContext(random()));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    PostingsEnum termDocs = TestUtil.docs(random(), mergedReader,\n        DocHelper.TEXT_FIELD_2_KEY,\n        new BytesRef(\"field\"),\n        null,\n        0);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.hasVectors()) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.size());\n    TermsEnum termsEnum = vector.iterator();\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["b0267c69e2456a3477a1ad785723f2135da3117e"],"4a8b14bc4241c302311422d5c6f7627f8febb86e":["fc834f3412d287003cc04691da380b69ab983239"],"1f3b037cd083286b2af89f96e768f85dcd8072d6":["6613659748fe4411a7dcf85266e55db1f95f7315"],"00f06a4178989089b29a77d6dce7c86dfb8b6931":["129c6e8ac0c0d9a110ba29e4b5f1889374f30076"],"8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":["1c93396a1df03720cb20e2c2f513a6fa59b21e4c","bc124b3b129ef11a255212f3af482b771c5b3a6c"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"63caed6eb28209e181e97822c4c8fdf808884c3b":["9d153abcf92dc5329d98571a8c3035df9bd80648"],"16cbef32b882ec68df422af3f08845ec82620335":["0f6a6419266ce0a74e9f1501938a86a4c94d5af7"],"f6eee1e5a8555d83dd8f2f2e3c0a4ccec8e7cf9b":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"a4e83191a3e02851a0b67e5335e6922f3e9ea86d":["763da4a9605e47013078edc323b9d4b608f0f9e0"],"ad252c98ff183bc59bd0617be14fa46f9696d6fc":["1d028314cced5858683a1bb4741423d0f934257b"],"129c6e8ac0c0d9a110ba29e4b5f1889374f30076":["63caed6eb28209e181e97822c4c8fdf808884c3b"],"ab548c8f96022b4780f7500a30b19b4f4a5feeb6":["eeba0a4d0845889a402dd225793d62f009d029c9"],"b70042a8a492f7054d480ccdd2be9796510d4327":["31741cf1390044e38a2ec3127cf302ba841bfd75","8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"79700663e164dece87bed4adfd3e28bab6cb1385":["51f5280f31484820499077f41fcdfe92d527d9dc"],"77f264c55cbf75404f8601ae7290d69157273a56":["e072d0b1fc19e0533d8ce432eed245196bca6fde"],"bc124b3b129ef11a255212f3af482b771c5b3a6c":["02331260bb246364779cb6f04919ca47900d01bb"],"299a2348fa24151d150182211b6208a38e5e3450":["51f5280f31484820499077f41fcdfe92d527d9dc","79700663e164dece87bed4adfd3e28bab6cb1385"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"790693f23f4e88a59fbb25e47cc25f6d493b03cb":["8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"a45bec74b98f6fc05f52770cfb425739e6563960":["ad252c98ff183bc59bd0617be14fa46f9696d6fc"],"b0267c69e2456a3477a1ad785723f2135da3117e":["79700663e164dece87bed4adfd3e28bab6cb1385"],"bec68e7c41fed133827595747d853cad504e481e":["a4e83191a3e02851a0b67e5335e6922f3e9ea86d"],"51f5280f31484820499077f41fcdfe92d527d9dc":["505bff044e47a553f461b6f4484d1d08faf4ac85"],"8521d944f9dfb45692ec28235dbf116d47ef69ba":["9bb9a29a5e71a90295f175df8919802993142c9a"],"d2dee33619431ada2a7a07f5fe2dbd94bac6a460":["76923f6a33f2c4bec7f584e3f251261afe7ea276"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"14654be3f7a82c9a3c52169e365baa55bfe64f66":["bec68e7c41fed133827595747d853cad504e481e"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4","02331260bb246364779cb6f04919ca47900d01bb"],"a656b32c3aa151037a8c52e9b134acc3cbf482bc":["1f3b037cd083286b2af89f96e768f85dcd8072d6","0567bdc5c86c94ced64201187cfcef2417d76dda"],"f592209545c71895260367152601e9200399776d":["ab548c8f96022b4780f7500a30b19b4f4a5feeb6","8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"02331260bb246364779cb6f04919ca47900d01bb":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"c65d2864d936ccf22dc7ec14dd48b4dff7bacceb":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["77f264c55cbf75404f8601ae7290d69157273a56"],"bdb5e42b0cecd8dfb27767a02ada71899bf17917":["f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"3c188105a9aae04f56c24996f98f8333fc825d2e":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f"],"6a917aca07a305ab70118a83e84d931503441271":["ff82b51516e4a8d24bb6182e5235be1c88b8ac2e"],"763da4a9605e47013078edc323b9d4b608f0f9e0":["790693f23f4e88a59fbb25e47cc25f6d493b03cb"],"764b942fd30efcae6e532c19771f32eeeb0037b2":["6a917aca07a305ab70118a83e84d931503441271"],"5f6bd27530a2846413fe2d00030493c0e2d3a072":["c9fb5f46e264daf5ba3860defe623a89d202dd87","c65d2864d936ccf22dc7ec14dd48b4dff7bacceb"],"ff82b51516e4a8d24bb6182e5235be1c88b8ac2e":["16cbef32b882ec68df422af3f08845ec82620335"],"a851824c09818632c94eba41e60ef5e72e323c8e":["dc97c61094c5498702b29cc2e8309beac50c23dc"],"0ad30c6a479e764150a3316e57263319775f1df2":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","3d33e731a93d4b57e662ff094f64f94a745422d4"],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":["d6f074e73200c07d54f242d3880a8da5a35ff97b","bc124b3b129ef11a255212f3af482b771c5b3a6c"],"b7605579001505896d48b07160075a5c8b8e128e":["1f3b037cd083286b2af89f96e768f85dcd8072d6","0567bdc5c86c94ced64201187cfcef2417d76dda"],"057a1793765d068ea9302f1a29e21734ee58d41e":["0567bdc5c86c94ced64201187cfcef2417d76dda"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["057a1793765d068ea9302f1a29e21734ee58d41e"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","0ad30c6a479e764150a3316e57263319775f1df2"],"76923f6a33f2c4bec7f584e3f251261afe7ea276":["f6eee1e5a8555d83dd8f2f2e3c0a4ccec8e7cf9b"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["51f5280f31484820499077f41fcdfe92d527d9dc","b0267c69e2456a3477a1ad785723f2135da3117e"],"1d028314cced5858683a1bb4741423d0f934257b":["bc124b3b129ef11a255212f3af482b771c5b3a6c","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198"],"4356000e349e38c9fb48034695b7c309abd54557":["a851824c09818632c94eba41e60ef5e72e323c8e"],"b06445ae1731e049327712db0454e5643ca9b7fe":["299a2348fa24151d150182211b6208a38e5e3450","b0267c69e2456a3477a1ad785723f2135da3117e"],"9bb9a29a5e71a90295f175df8919802993142c9a":["5f6bd27530a2846413fe2d00030493c0e2d3a072","2131047ecceac64b54ba70feec3d26bbd7e483d7"],"ad7de846867bd14c63f9dd19df082f72c5ea9c54":["1d028314cced5858683a1bb4741423d0f934257b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b7e4ca6dc9612ff741d8713743e2bccfae5eadac":["ab548c8f96022b4780f7500a30b19b4f4a5feeb6"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["ad252c98ff183bc59bd0617be14fa46f9696d6fc"],"0837ab0472feecb3a54260729d845f839e1cbd72":["ad7de846867bd14c63f9dd19df082f72c5ea9c54"],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"4e84d639980c2b2eb5d41330d5ff68d143239495":["00f06a4178989089b29a77d6dce7c86dfb8b6931"],"e072d0b1fc19e0533d8ce432eed245196bca6fde":["a45bec74b98f6fc05f52770cfb425739e6563960"],"5eb2511ababf862ea11e10761c70ee560cd84510":["6613659748fe4411a7dcf85266e55db1f95f7315","1f3b037cd083286b2af89f96e768f85dcd8072d6"],"0f6a6419266ce0a74e9f1501938a86a4c94d5af7":["4e84d639980c2b2eb5d41330d5ff68d143239495"],"5a238fc456663f685a9db1ed8d680e348bb45171":["f08557cdb6c60ac7b88a9342c983a20cd236e74f","bdb5e42b0cecd8dfb27767a02ada71899bf17917"],"6613659748fe4411a7dcf85266e55db1f95f7315":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"f08557cdb6c60ac7b88a9342c983a20cd236e74f":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"9d153abcf92dc5329d98571a8c3035df9bd80648":["4356000e349e38c9fb48034695b7c309abd54557"],"0567bdc5c86c94ced64201187cfcef2417d76dda":["1f3b037cd083286b2af89f96e768f85dcd8072d6"],"1c93396a1df03720cb20e2c2f513a6fa59b21e4c":["3c188105a9aae04f56c24996f98f8333fc825d2e"],"dc97c61094c5498702b29cc2e8309beac50c23dc":["4a8b14bc4241c302311422d5c6f7627f8febb86e"],"fc834f3412d287003cc04691da380b69ab983239":["d2dee33619431ada2a7a07f5fe2dbd94bac6a460"],"505bff044e47a553f461b6f4484d1d08faf4ac85":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","d470c8182e92b264680e34081b75e70a9f2b3c89"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["629c38c4ae4e303d0617e05fbfe508140b32f0a3","764b942fd30efcae6e532c19771f32eeeb0037b2"],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["615ddbd81799980d0fdd95e0238e1c498b6f47b0","02331260bb246364779cb6f04919ca47900d01bb"],"eeba0a4d0845889a402dd225793d62f009d029c9":["31741cf1390044e38a2ec3127cf302ba841bfd75"],"2131047ecceac64b54ba70feec3d26bbd7e483d7":["c65d2864d936ccf22dc7ec14dd48b4dff7bacceb"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["8521d944f9dfb45692ec28235dbf116d47ef69ba"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["bdb5e42b0cecd8dfb27767a02ada71899bf17917"],"8f2203cb8ae87188877cfbf6ad170c5738a0aad5":["b7e4ca6dc9612ff741d8713743e2bccfae5eadac"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","ceaef6cfc68c8ab22a684192e469a8280f9e6e70"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["14654be3f7a82c9a3c52169e365baa55bfe64f66"]},"commit2Childs":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"4a8b14bc4241c302311422d5c6f7627f8febb86e":["dc97c61094c5498702b29cc2e8309beac50c23dc"],"1f3b037cd083286b2af89f96e768f85dcd8072d6":["a656b32c3aa151037a8c52e9b134acc3cbf482bc","b7605579001505896d48b07160075a5c8b8e128e","5eb2511ababf862ea11e10761c70ee560cd84510","0567bdc5c86c94ced64201187cfcef2417d76dda"],"00f06a4178989089b29a77d6dce7c86dfb8b6931":["4e84d639980c2b2eb5d41330d5ff68d143239495"],"8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4":["d6f074e73200c07d54f242d3880a8da5a35ff97b"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":[],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"63caed6eb28209e181e97822c4c8fdf808884c3b":["129c6e8ac0c0d9a110ba29e4b5f1889374f30076"],"16cbef32b882ec68df422af3f08845ec82620335":["ff82b51516e4a8d24bb6182e5235be1c88b8ac2e"],"f6eee1e5a8555d83dd8f2f2e3c0a4ccec8e7cf9b":["76923f6a33f2c4bec7f584e3f251261afe7ea276"],"a4e83191a3e02851a0b67e5335e6922f3e9ea86d":["bec68e7c41fed133827595747d853cad504e481e"],"ad252c98ff183bc59bd0617be14fa46f9696d6fc":["a45bec74b98f6fc05f52770cfb425739e6563960","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee"],"129c6e8ac0c0d9a110ba29e4b5f1889374f30076":["00f06a4178989089b29a77d6dce7c86dfb8b6931"],"ab548c8f96022b4780f7500a30b19b4f4a5feeb6":["f592209545c71895260367152601e9200399776d","b7e4ca6dc9612ff741d8713743e2bccfae5eadac"],"b70042a8a492f7054d480ccdd2be9796510d4327":[],"79700663e164dece87bed4adfd3e28bab6cb1385":["299a2348fa24151d150182211b6208a38e5e3450","b0267c69e2456a3477a1ad785723f2135da3117e"],"77f264c55cbf75404f8601ae7290d69157273a56":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"bc124b3b129ef11a255212f3af482b771c5b3a6c":["b05c56a41b733e02a189c48895922b5bd8c7f3d1","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","1d028314cced5858683a1bb4741423d0f934257b"],"299a2348fa24151d150182211b6208a38e5e3450":["b06445ae1731e049327712db0454e5643ca9b7fe"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["0ad30c6a479e764150a3316e57263319775f1df2","d470c8182e92b264680e34081b75e70a9f2b3c89","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","ceaef6cfc68c8ab22a684192e469a8280f9e6e70","3d33e731a93d4b57e662ff094f64f94a745422d4"],"790693f23f4e88a59fbb25e47cc25f6d493b03cb":["763da4a9605e47013078edc323b9d4b608f0f9e0"],"a45bec74b98f6fc05f52770cfb425739e6563960":["e072d0b1fc19e0533d8ce432eed245196bca6fde"],"b0267c69e2456a3477a1ad785723f2135da3117e":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe"],"bec68e7c41fed133827595747d853cad504e481e":["14654be3f7a82c9a3c52169e365baa55bfe64f66"],"51f5280f31484820499077f41fcdfe92d527d9dc":["79700663e164dece87bed4adfd3e28bab6cb1385","299a2348fa24151d150182211b6208a38e5e3450","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"8521d944f9dfb45692ec28235dbf116d47ef69ba":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"d2dee33619431ada2a7a07f5fe2dbd94bac6a460":["fc834f3412d287003cc04691da380b69ab983239"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"14654be3f7a82c9a3c52169e365baa55bfe64f66":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198"],"a656b32c3aa151037a8c52e9b134acc3cbf482bc":[],"f592209545c71895260367152601e9200399776d":[],"02331260bb246364779cb6f04919ca47900d01bb":["bc124b3b129ef11a255212f3af482b771c5b3a6c","d6f074e73200c07d54f242d3880a8da5a35ff97b","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f"],"c65d2864d936ccf22dc7ec14dd48b4dff7bacceb":["5f6bd27530a2846413fe2d00030493c0e2d3a072","2131047ecceac64b54ba70feec3d26bbd7e483d7"],"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["6613659748fe4411a7dcf85266e55db1f95f7315"],"bdb5e42b0cecd8dfb27767a02ada71899bf17917":["5a238fc456663f685a9db1ed8d680e348bb45171","629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"3c188105a9aae04f56c24996f98f8333fc825d2e":["1c93396a1df03720cb20e2c2f513a6fa59b21e4c"],"6a917aca07a305ab70118a83e84d931503441271":["764b942fd30efcae6e532c19771f32eeeb0037b2"],"763da4a9605e47013078edc323b9d4b608f0f9e0":["a4e83191a3e02851a0b67e5335e6922f3e9ea86d"],"764b942fd30efcae6e532c19771f32eeeb0037b2":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"5f6bd27530a2846413fe2d00030493c0e2d3a072":["9bb9a29a5e71a90295f175df8919802993142c9a"],"ff82b51516e4a8d24bb6182e5235be1c88b8ac2e":["6a917aca07a305ab70118a83e84d931503441271"],"a851824c09818632c94eba41e60ef5e72e323c8e":["4356000e349e38c9fb48034695b7c309abd54557"],"0ad30c6a479e764150a3316e57263319775f1df2":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":["1d028314cced5858683a1bb4741423d0f934257b"],"b7605579001505896d48b07160075a5c8b8e128e":[],"057a1793765d068ea9302f1a29e21734ee58d41e":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["c65d2864d936ccf22dc7ec14dd48b4dff7bacceb","5f6bd27530a2846413fe2d00030493c0e2d3a072"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["b70042a8a492f7054d480ccdd2be9796510d4327","eeba0a4d0845889a402dd225793d62f009d029c9"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["31741cf1390044e38a2ec3127cf302ba841bfd75","92212fd254551a0b1156aafc3a1a6ed1a43932ad","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"76923f6a33f2c4bec7f584e3f251261afe7ea276":["d2dee33619431ada2a7a07f5fe2dbd94bac6a460"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"1d028314cced5858683a1bb4741423d0f934257b":["ad252c98ff183bc59bd0617be14fa46f9696d6fc","ad7de846867bd14c63f9dd19df082f72c5ea9c54"],"4356000e349e38c9fb48034695b7c309abd54557":["9d153abcf92dc5329d98571a8c3035df9bd80648"],"b06445ae1731e049327712db0454e5643ca9b7fe":[],"9bb9a29a5e71a90295f175df8919802993142c9a":["8521d944f9dfb45692ec28235dbf116d47ef69ba"],"ad7de846867bd14c63f9dd19df082f72c5ea9c54":["0837ab0472feecb3a54260729d845f839e1cbd72"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"b7e4ca6dc9612ff741d8713743e2bccfae5eadac":["8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":[],"0837ab0472feecb3a54260729d845f839e1cbd72":[],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":[],"4e84d639980c2b2eb5d41330d5ff68d143239495":["0f6a6419266ce0a74e9f1501938a86a4c94d5af7"],"e072d0b1fc19e0533d8ce432eed245196bca6fde":["77f264c55cbf75404f8601ae7290d69157273a56"],"5eb2511ababf862ea11e10761c70ee560cd84510":[],"0f6a6419266ce0a74e9f1501938a86a4c94d5af7":["16cbef32b882ec68df422af3f08845ec82620335"],"5a238fc456663f685a9db1ed8d680e348bb45171":[],"6613659748fe4411a7dcf85266e55db1f95f7315":["1f3b037cd083286b2af89f96e768f85dcd8072d6","5eb2511ababf862ea11e10761c70ee560cd84510"],"f08557cdb6c60ac7b88a9342c983a20cd236e74f":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","bdb5e42b0cecd8dfb27767a02ada71899bf17917","5a238fc456663f685a9db1ed8d680e348bb45171"],"9d153abcf92dc5329d98571a8c3035df9bd80648":["63caed6eb28209e181e97822c4c8fdf808884c3b"],"1c93396a1df03720cb20e2c2f513a6fa59b21e4c":["b05c56a41b733e02a189c48895922b5bd8c7f3d1"],"0567bdc5c86c94ced64201187cfcef2417d76dda":["a656b32c3aa151037a8c52e9b134acc3cbf482bc","b7605579001505896d48b07160075a5c8b8e128e","057a1793765d068ea9302f1a29e21734ee58d41e"],"dc97c61094c5498702b29cc2e8309beac50c23dc":["a851824c09818632c94eba41e60ef5e72e323c8e"],"fc834f3412d287003cc04691da380b69ab983239":["4a8b14bc4241c302311422d5c6f7627f8febb86e"],"505bff044e47a553f461b6f4484d1d08faf4ac85":["51f5280f31484820499077f41fcdfe92d527d9dc"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["8f0e28f2a7f0f3f0fca1a2ffedaa10c7ac9536c4","02331260bb246364779cb6f04919ca47900d01bb","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f"],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["3d33e731a93d4b57e662ff094f64f94a745422d4"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["3c188105a9aae04f56c24996f98f8333fc825d2e"],"eeba0a4d0845889a402dd225793d62f009d029c9":["ab548c8f96022b4780f7500a30b19b4f4a5feeb6"],"2131047ecceac64b54ba70feec3d26bbd7e483d7":["9bb9a29a5e71a90295f175df8919802993142c9a"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["505bff044e47a553f461b6f4484d1d08faf4ac85"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["f6eee1e5a8555d83dd8f2f2e3c0a4ccec8e7cf9b","615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"8f2203cb8ae87188877cfbf6ad170c5738a0aad5":["b70042a8a492f7054d480ccdd2be9796510d4327","790693f23f4e88a59fbb25e47cc25f6d493b03cb","f592209545c71895260367152601e9200399776d"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["0ad30c6a479e764150a3316e57263319775f1df2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","b05c56a41b733e02a189c48895922b5bd8c7f3d1","b70042a8a492f7054d480ccdd2be9796510d4327","a656b32c3aa151037a8c52e9b134acc3cbf482bc","f592209545c71895260367152601e9200399776d","b7605579001505896d48b07160075a5c8b8e128e","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","0837ab0472feecb3a54260729d845f839e1cbd72","92212fd254551a0b1156aafc3a1a6ed1a43932ad","5eb2511ababf862ea11e10761c70ee560cd84510","5a238fc456663f685a9db1ed8d680e348bb45171","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}