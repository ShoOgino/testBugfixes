{"path":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private Searcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new SimpleAnalyzer(\n        TEST_VERSION_CURRENT)).setMaxBufferedDocs(2));\n    ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(1000);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    //writer.optimize ();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (indexStore, true);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private Searcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new SimpleAnalyzer(\n        TEST_VERSION_CURRENT)).setMaxBufferedDocs(2));\n    ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(1000);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    //writer.optimize ();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (indexStore, true);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7ab99e8c71442b92c320e218141dee04a9b91ce8","date":1269203801,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private Searcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new SimpleAnalyzer(\n        TEST_VERSION_CURRENT)).setMaxBufferedDocs(2));\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(1000);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    //writer.optimize ();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (indexStore, true);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private Searcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new SimpleAnalyzer(\n        TEST_VERSION_CURRENT)).setMaxBufferedDocs(2));\n    ((LogMergePolicy) writer.getMergePolicy()).setMergeFactor(1000);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    //writer.optimize ();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (indexStore, true);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d572389229127c297dd1fa5ce4758e1cec41e799","date":1273610938,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private Searcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(2));\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(1000);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    //writer.optimize ();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (indexStore, true);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private Searcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new SimpleAnalyzer(\n        TEST_VERSION_CURRENT)).setMaxBufferedDocs(2));\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(1000);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    //writer.optimize ();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (indexStore, true);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c084e47df29de3330311d69dabf515ceaa989512","date":1279030906,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private Searcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    RAMDirectory indexStore = new RAMDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, \n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private Searcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(2));\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(1000);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    //writer.optimize ();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (indexStore, true);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private Searcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    RAMDirectory indexStore = new RAMDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, \n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private Searcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    RAMDirectory indexStore = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(2));\n    ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(1000);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    //writer.optimize ();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (indexStore, true);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"15bbd254c1506df5299c4df8c148262c7bd6301e","date":1279913113,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private Searcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    RAMDirectory indexStore = new RAMDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore);\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private Searcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    RAMDirectory indexStore = new RAMDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, \n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4b103252dee6afa1b6d7a622c773d178788eb85a","date":1280180143,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private Searcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    RAMDirectory indexStore = new RAMDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore);\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private Searcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    RAMDirectory indexStore = new RAMDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, \n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3242a09f703274d3b9283f2064a1a33064b53a1b","date":1280263474,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private Searcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    RAMDirectory indexStore = new RAMDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore);\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private Searcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    RAMDirectory indexStore = new RAMDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, \n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c","date":1281477834,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private Searcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    MockRAMDirectory indexStore = new MockRAMDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore);\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private Searcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    RAMDirectory indexStore = new RAMDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore);\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c","date":1281646583,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    MockRAMDirectory indexStore = newDirectory(random);\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore);\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private Searcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    MockRAMDirectory indexStore = new MockRAMDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore);\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a05409176bd65129d67a785ee70e881e238a9aef","date":1282582843,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory(random);\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore);\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    MockRAMDirectory indexStore = newDirectory(random);\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore);\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1f653cfcf159baeaafe5d01682a911e95bba4012","date":1284122058,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore);\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory(random);\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore);\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore);\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private Searcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    RAMDirectory indexStore = new RAMDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore);\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c19f985e36a65cc969e8e564fe337a0d41512075","date":1296330536,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore);\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"790e1fde4caa765b3faaad3fbcd25c6973450336","date":1296689245,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore);\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore);\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = new IndexSearcher (reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"01e5948db9a07144112d2f08f28ca2e3cd880348","date":1301759232,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"45669a651c970812a680841b97a77cce06af559f","date":1301922222,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9235b9d4454a46c066cda47fed7ca0a34e614529","date":1304414372,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) {\n          doc.add (DocValuesField.set(new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED), Type.INTS));\n        }\n        if (data[i][3] != null) {\n          doc.add (DocValuesField.set(new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED), Type.FLOAT_32));\n        }\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) { \n          doc.add (DocValuesField.set(new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED), Type.FLOAT_64));\n        }\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0cedee4c69436d5334e8f35463bf232084ff2fa3","date":1304587337,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) {\n          Field f = supportsDocValues ? \n              DocValuesField.set(new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED), Type.INTS)\n                               : new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = supportsDocValues ?\n              DocValuesField.set(new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED), Type.FLOAT_32)\n                              :  new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) {\n          Field f = supportsDocValues ?\n              DocValuesField.set(new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED), Type.FLOAT_64)\n                              :  new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) {\n          doc.add (DocValuesField.set(new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED), Type.INTS));\n        }\n        if (data[i][3] != null) {\n          doc.add (DocValuesField.set(new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED), Type.FLOAT_32));\n        }\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) { \n          doc.add (DocValuesField.set(new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED), Type.FLOAT_64));\n        }\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":["fa0f44f887719e97183771e977cfc4bfb485b766"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"39d51e9acac1e629cffe47855e85dc6dedc4754d","date":1305711497,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) {\n          Field f = supportsDocValues ? \n              DocValuesField.set(new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.INTS)\n                               : new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = supportsDocValues ?\n              DocValuesField.set(new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_32)\n                              :  new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) {\n          Field f = supportsDocValues ?\n              DocValuesField.set(new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_64)\n                              :  new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) {\n          Field f = supportsDocValues ? \n              DocValuesField.set(new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED), Type.INTS)\n                               : new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = supportsDocValues ?\n              DocValuesField.set(new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED), Type.FLOAT_32)\n                              :  new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) {\n          Field f = supportsDocValues ?\n              DocValuesField.set(new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED), Type.FLOAT_64)\n                              :  new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"605b061fadf71aa6458c0005eaae83a98350c403","date":1307117658,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) {\n          Field f = supportsDocValues ? \n              IndexDocValuesField.set(new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.INTS)\n                               : new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = supportsDocValues ?\n              IndexDocValuesField.set(new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_32)\n                              :  new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) {\n          Field f = supportsDocValues ?\n              IndexDocValuesField.set(new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_64)\n                              :  new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) {\n          Field f = supportsDocValues ? \n              DocValuesField.set(new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.INTS)\n                               : new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = supportsDocValues ?\n              DocValuesField.set(new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_32)\n                              :  new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) {\n          Field f = supportsDocValues ?\n              DocValuesField.set(new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_64)\n                              :  new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2e8d7ba2175f47e280231533f7d3016249cea88b","date":1307711934,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) {\n          Field f = supportsDocValues ? \n              IndexDocValuesField.set(new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.INTS)\n                               : new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = supportsDocValues ?\n              IndexDocValuesField.set(new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_32)\n                              :  new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) {\n          Field f = supportsDocValues ?\n              IndexDocValuesField.set(new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_64)\n                              :  new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","date":1307729864,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) {\n          Field f = supportsDocValues ? \n              IndexDocValuesField.set(new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.INTS)\n                               : new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = supportsDocValues ?\n              IndexDocValuesField.set(new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_32)\n                              :  new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) {\n          Field f = supportsDocValues ?\n              IndexDocValuesField.set(new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_64)\n                              :  new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) doc.add (new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][3] != null) doc.add (new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) doc.add (new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"93c59ad5034eecd863f267bd75e1df7b8a51e427","date":1309162882,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) {\n          Field f = supportsDocValues ? \n              IndexDocValuesField.set(new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.VAR_INTS)\n                               : new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = supportsDocValues ?\n              IndexDocValuesField.set(new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_32)\n                              :  new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) {\n          Field f = supportsDocValues ?\n              IndexDocValuesField.set(new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_64)\n                              :  new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) {\n          Field f = supportsDocValues ? \n              IndexDocValuesField.set(new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.INTS)\n                               : new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = supportsDocValues ?\n              IndexDocValuesField.set(new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_32)\n                              :  new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) {\n          Field f = supportsDocValues ?\n              IndexDocValuesField.set(new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_64)\n                              :  new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) {\n          Field f = supportsDocValues ? \n              IndexDocValuesField.set(new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.VAR_INTS)\n                               : new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = supportsDocValues ?\n              IndexDocValuesField.set(new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_32)\n                              :  new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) {\n          Field f = supportsDocValues ?\n              IndexDocValuesField.set(new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_64)\n                              :  new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) {\n          Field f = supportsDocValues ? \n              IndexDocValuesField.set(new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.INTS)\n                               : new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = supportsDocValues ?\n              IndexDocValuesField.set(new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_32)\n                              :  new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) {\n          Field f = supportsDocValues ?\n              IndexDocValuesField.set(new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_64)\n                              :  new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) {\n          Field f = supportsDocValues ? \n              IndexDocValuesField.set(new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.VAR_INTS)\n                               : new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = supportsDocValues ?\n              IndexDocValuesField.set(new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_32)\n                              :  new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) {\n          Field f = supportsDocValues ?\n              IndexDocValuesField.set(new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_64)\n                              :  new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) {\n          Field f = supportsDocValues ? \n              IndexDocValuesField.set(new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.INTS)\n                               : new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = supportsDocValues ?\n              IndexDocValuesField.set(new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_32)\n                              :  new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) {\n          Field f = supportsDocValues ?\n              IndexDocValuesField.set(new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_64)\n                              :  new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    FieldType ft1 = new FieldType();\n    ft1.setStored(true);\n    FieldType ft2 = new FieldType();\n    ft2.setIndexed(true);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\", ft1, data[i][0]));\n        doc.add (new TextField (\"contents\", data[i][1]));\n        if (data[i][2] != null) {\n          Field f = new StringField (\"int\", data[i][2]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.VAR_INTS);\n          };\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = new StringField (\"float\", data[i][3]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.FLOAT_32);\n          }\n          doc.add(f);\n        }\n        if (data[i][4] != null) doc.add (new StringField (\"string\",   data[i][4]));\n        if (data[i][5] != null) doc.add (new StringField (\"custom\",   data[i][5]));\n        if (data[i][6] != null) doc.add (new StringField (\"i18n\",     data[i][6]));\n        if (data[i][7] != null) doc.add (new StringField (\"long\",     data[i][7]));\n        if (data[i][8] != null) {\n          Field f = new StringField (\"double\", data[i][8]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.FLOAT_64);\n          }\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new StringField (\"short\",     data[i][9]));\n        if (data[i][10] != null) doc.add (new StringField (\"byte\",     data[i][10]));\n        if (data[i][11] != null) doc.add (new StringField (\"parser\",     data[i][11]));\n\n        for(IndexableField f : doc.getFields()) {\n          ((Field) f).setBoost(2.0f);\n        }\n\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\",   data[i][0], Field.Store.YES, Field.Index.NO));\n        doc.add (new Field (\"contents\", data[i][1], Field.Store.NO, Field.Index.ANALYZED));\n        if (data[i][2] != null) {\n          Field f = supportsDocValues ? \n              IndexDocValuesField.set(new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.VAR_INTS)\n                               : new Field (\"int\",      data[i][2], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = supportsDocValues ?\n              IndexDocValuesField.set(new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_32)\n                              :  new Field (\"float\",    data[i][3], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][4] != null) doc.add (new Field (\"string\",   data[i][4], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][5] != null) doc.add (new Field (\"custom\",   data[i][5], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][6] != null) doc.add (new Field (\"i18n\",     data[i][6], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][7] != null) doc.add (new Field (\"long\",     data[i][7], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][8] != null) {\n          Field f = supportsDocValues ?\n              IndexDocValuesField.set(new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED), ValueType.FLOAT_64)\n                              :  new Field (\"double\",     data[i][8], Field.Store.NO, Field.Index.NOT_ANALYZED);\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new Field (\"short\",     data[i][9], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][10] != null) doc.add (new Field (\"byte\",     data[i][10], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        if (data[i][11] != null) doc.add (new Field (\"parser\",     data[i][11], Field.Store.NO, Field.Index.NOT_ANALYZED));\n        doc.setBoost(2);  // produce some scores above 1.0\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2","d1336abe0899b2984e5652903556c1925fbdca9f","fa0f44f887719e97183771e977cfc4bfb485b766"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7e4db59c6b6c10e25322cfb41c4c19d78b4298bd","date":1317197236,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    FieldType ft1 = new FieldType();\n    ft1.setStored(true);\n    FieldType ft2 = new FieldType();\n    ft2.setIndexed(true);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\", data[i][0], ft1));\n        doc.add (new TextField (\"contents\", data[i][1]));\n        if (data[i][2] != null) {\n          Field f = new StringField (\"int\", data[i][2]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.VAR_INTS);\n          };\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = new StringField (\"float\", data[i][3]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.FLOAT_32);\n          }\n          doc.add(f);\n        }\n        if (data[i][4] != null) doc.add (new StringField (\"string\",   data[i][4]));\n        if (data[i][5] != null) doc.add (new StringField (\"custom\",   data[i][5]));\n        if (data[i][6] != null) doc.add (new StringField (\"i18n\",     data[i][6]));\n        if (data[i][7] != null) doc.add (new StringField (\"long\",     data[i][7]));\n        if (data[i][8] != null) {\n          Field f = new StringField (\"double\", data[i][8]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.FLOAT_64);\n          }\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new StringField (\"short\",     data[i][9]));\n        if (data[i][10] != null) doc.add (new StringField (\"byte\",     data[i][10]));\n        if (data[i][11] != null) doc.add (new StringField (\"parser\",     data[i][11]));\n\n        for(IndexableField f : doc.getFields()) {\n          ((Field) f).setBoost(2.0f);\n        }\n\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    FieldType ft1 = new FieldType();\n    ft1.setStored(true);\n    FieldType ft2 = new FieldType();\n    ft2.setIndexed(true);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\", ft1, data[i][0]));\n        doc.add (new TextField (\"contents\", data[i][1]));\n        if (data[i][2] != null) {\n          Field f = new StringField (\"int\", data[i][2]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.VAR_INTS);\n          };\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = new StringField (\"float\", data[i][3]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.FLOAT_32);\n          }\n          doc.add(f);\n        }\n        if (data[i][4] != null) doc.add (new StringField (\"string\",   data[i][4]));\n        if (data[i][5] != null) doc.add (new StringField (\"custom\",   data[i][5]));\n        if (data[i][6] != null) doc.add (new StringField (\"i18n\",     data[i][6]));\n        if (data[i][7] != null) doc.add (new StringField (\"long\",     data[i][7]));\n        if (data[i][8] != null) {\n          Field f = new StringField (\"double\", data[i][8]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.FLOAT_64);\n          }\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new StringField (\"short\",     data[i][9]));\n        if (data[i][10] != null) doc.add (new StringField (\"byte\",     data[i][10]));\n        if (data[i][11] != null) doc.add (new StringField (\"parser\",     data[i][11]));\n\n        for(IndexableField f : doc.getFields()) {\n          ((Field) f).setBoost(2.0f);\n        }\n\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"382fe3a6ca9745891afebda9b9a57cc158305545","date":1320952430,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    FieldType ft1 = new FieldType();\n    ft1.setStored(true);\n    FieldType ft2 = new FieldType();\n    ft2.setIndexed(true);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\", data[i][0], ft1));\n        doc.add (new TextField (\"contents\", data[i][1]));\n        if (data[i][2] != null) {\n          Field f = new StringField (\"int\", data[i][2]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.VAR_INTS);\n          }\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = new StringField (\"float\", data[i][3]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.FLOAT_32);\n          }\n          doc.add(f);\n        }\n        if (data[i][4] != null) doc.add (new StringField (\"string\",   data[i][4]));\n        if (data[i][5] != null) doc.add (new StringField (\"custom\",   data[i][5]));\n        if (data[i][6] != null) doc.add (new StringField (\"i18n\",     data[i][6]));\n        if (data[i][7] != null) doc.add (new StringField (\"long\",     data[i][7]));\n        if (data[i][8] != null) {\n          Field f = new StringField (\"double\", data[i][8]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.FLOAT_64);\n          }\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new StringField (\"short\",     data[i][9]));\n        if (data[i][10] != null) doc.add (new StringField (\"byte\",     data[i][10]));\n        if (data[i][11] != null) doc.add (new StringField (\"parser\",     data[i][11]));\n\n        for(IndexableField f : doc.getFields()) {\n          ((Field) f).setBoost(2.0f);\n        }\n\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    FieldType ft1 = new FieldType();\n    ft1.setStored(true);\n    FieldType ft2 = new FieldType();\n    ft2.setIndexed(true);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\", data[i][0], ft1));\n        doc.add (new TextField (\"contents\", data[i][1]));\n        if (data[i][2] != null) {\n          Field f = new StringField (\"int\", data[i][2]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.VAR_INTS);\n          };\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = new StringField (\"float\", data[i][3]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.FLOAT_32);\n          }\n          doc.add(f);\n        }\n        if (data[i][4] != null) doc.add (new StringField (\"string\",   data[i][4]));\n        if (data[i][5] != null) doc.add (new StringField (\"custom\",   data[i][5]));\n        if (data[i][6] != null) doc.add (new StringField (\"i18n\",     data[i][6]));\n        if (data[i][7] != null) doc.add (new StringField (\"long\",     data[i][7]));\n        if (data[i][8] != null) {\n          Field f = new StringField (\"double\", data[i][8]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.FLOAT_64);\n          }\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new StringField (\"short\",     data[i][9]));\n        if (data[i][10] != null) doc.add (new StringField (\"byte\",     data[i][10]));\n        if (data[i][11] != null) doc.add (new StringField (\"parser\",     data[i][11]));\n\n        for(IndexableField f : doc.getFields()) {\n          ((Field) f).setBoost(2.0f);\n        }\n\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"28088b0a688977b79dec2cc9119cff2fb4aab7ee","date":1321197996,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    final ValueType stringDVType;\n    if (dvStringSorted) {\n      // Index sorted\n      stringDVType = random.nextBoolean() ? ValueType.BYTES_VAR_SORTED : ValueType.BYTES_FIXED_SORTED;\n    } else {\n      // Index non-sorted\n      if (random.nextBoolean()) {\n        // Fixed\n        stringDVType = random.nextBoolean() ? ValueType.BYTES_FIXED_STRAIGHT : ValueType.BYTES_FIXED_DEREF;\n      } else {\n        // Var\n        stringDVType = random.nextBoolean() ? ValueType.BYTES_VAR_STRAIGHT : ValueType.BYTES_VAR_DEREF;\n      }\n    }\n\n    FieldType ft1 = new FieldType();\n    ft1.setStored(true);\n    FieldType ft2 = new FieldType();\n    ft2.setIndexed(true);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\", data[i][0], ft1));\n        doc.add (new TextField (\"contents\", data[i][1]));\n        if (data[i][2] != null) {\n          Field f = new StringField (\"int\", data[i][2]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.VAR_INTS);\n          }\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = new StringField (\"float\", data[i][3]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.FLOAT_32);\n          }\n          doc.add(f);\n        }\n        if (data[i][4] != null) {\n          Field f = new StringField (\"string\", data[i][4]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, stringDVType);\n          }\n          doc.add(f);\n        }\n        if (data[i][5] != null) doc.add (new StringField (\"custom\",   data[i][5]));\n        if (data[i][6] != null) doc.add (new StringField (\"i18n\",     data[i][6]));\n        if (data[i][7] != null) doc.add (new StringField (\"long\",     data[i][7]));\n        if (data[i][8] != null) {\n          Field f = new StringField (\"double\", data[i][8]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.FLOAT_64);\n          }\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new StringField (\"short\",     data[i][9]));\n        if (data[i][10] != null) doc.add (new StringField (\"byte\",     data[i][10]));\n        if (data[i][11] != null) doc.add (new StringField (\"parser\",     data[i][11]));\n\n        for(IndexableField f : doc.getFields()) {\n          ((Field) f).setBoost(2.0f);\n        }\n\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    FieldType ft1 = new FieldType();\n    ft1.setStored(true);\n    FieldType ft2 = new FieldType();\n    ft2.setIndexed(true);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\", data[i][0], ft1));\n        doc.add (new TextField (\"contents\", data[i][1]));\n        if (data[i][2] != null) {\n          Field f = new StringField (\"int\", data[i][2]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.VAR_INTS);\n          }\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = new StringField (\"float\", data[i][3]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.FLOAT_32);\n          }\n          doc.add(f);\n        }\n        if (data[i][4] != null) doc.add (new StringField (\"string\",   data[i][4]));\n        if (data[i][5] != null) doc.add (new StringField (\"custom\",   data[i][5]));\n        if (data[i][6] != null) doc.add (new StringField (\"i18n\",     data[i][6]));\n        if (data[i][7] != null) doc.add (new StringField (\"long\",     data[i][7]));\n        if (data[i][8] != null) {\n          Field f = new StringField (\"double\", data[i][8]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.FLOAT_64);\n          }\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new StringField (\"short\",     data[i][9]));\n        if (data[i][10] != null) doc.add (new StringField (\"byte\",     data[i][10]));\n        if (data[i][11] != null) doc.add (new StringField (\"parser\",     data[i][11]));\n\n        for(IndexableField f : doc.getFields()) {\n          ((Field) f).setBoost(2.0f);\n        }\n\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4","fa0f44f887719e97183771e977cfc4bfb485b766"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1e59c344a45b9502f40ec44f5fe4e20ed2291dbe","date":1323449025,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    final DocValues.Type stringDVType;\n    if (dvStringSorted) {\n      // Index sorted\n      stringDVType = random.nextBoolean() ? DocValues.Type.BYTES_VAR_SORTED : DocValues.Type.BYTES_FIXED_SORTED;\n    } else {\n      // Index non-sorted\n      if (random.nextBoolean()) {\n        // Fixed\n        stringDVType = random.nextBoolean() ? DocValues.Type.BYTES_FIXED_STRAIGHT : DocValues.Type.BYTES_FIXED_DEREF;\n      } else {\n        // Var\n        stringDVType = random.nextBoolean() ? DocValues.Type.BYTES_VAR_STRAIGHT : DocValues.Type.BYTES_VAR_DEREF;\n      }\n    }\n\n    FieldType ft1 = new FieldType();\n    ft1.setStored(true);\n    FieldType ft2 = new FieldType();\n    ft2.setIndexed(true);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\", data[i][0], ft1));\n        doc.add (new TextField (\"contents\", data[i][1]));\n        if (data[i][2] != null) {\n          Field f = new StringField (\"int\", data[i][2]);\n          if (supportsDocValues) {\n            f = DocValuesField.build(f, DocValues.Type.VAR_INTS);\n          }\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = new StringField (\"float\", data[i][3]);\n          if (supportsDocValues) {\n            f = DocValuesField.build(f, DocValues.Type.FLOAT_32);\n          }\n          doc.add(f);\n        }\n        if (data[i][4] != null) {\n          Field f = new StringField (\"string\", data[i][4]);\n          if (supportsDocValues) {\n            f = DocValuesField.build(f, stringDVType);\n          }\n          doc.add(f);\n        }\n        if (data[i][5] != null) doc.add (new StringField (\"custom\",   data[i][5]));\n        if (data[i][6] != null) doc.add (new StringField (\"i18n\",     data[i][6]));\n        if (data[i][7] != null) doc.add (new StringField (\"long\",     data[i][7]));\n        if (data[i][8] != null) {\n          Field f = new StringField (\"double\", data[i][8]);\n          if (supportsDocValues) {\n            f = DocValuesField.build(f, DocValues.Type.FLOAT_64);\n          }\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new StringField (\"short\",     data[i][9]));\n        if (data[i][10] != null) doc.add (new StringField (\"byte\",     data[i][10]));\n        if (data[i][11] != null) doc.add (new StringField (\"parser\",     data[i][11]));\n\n        for(IndexableField f : doc.getFields()) {\n          ((Field) f).setBoost(2.0f);\n        }\n\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    final ValueType stringDVType;\n    if (dvStringSorted) {\n      // Index sorted\n      stringDVType = random.nextBoolean() ? ValueType.BYTES_VAR_SORTED : ValueType.BYTES_FIXED_SORTED;\n    } else {\n      // Index non-sorted\n      if (random.nextBoolean()) {\n        // Fixed\n        stringDVType = random.nextBoolean() ? ValueType.BYTES_FIXED_STRAIGHT : ValueType.BYTES_FIXED_DEREF;\n      } else {\n        // Var\n        stringDVType = random.nextBoolean() ? ValueType.BYTES_VAR_STRAIGHT : ValueType.BYTES_VAR_DEREF;\n      }\n    }\n\n    FieldType ft1 = new FieldType();\n    ft1.setStored(true);\n    FieldType ft2 = new FieldType();\n    ft2.setIndexed(true);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\", data[i][0], ft1));\n        doc.add (new TextField (\"contents\", data[i][1]));\n        if (data[i][2] != null) {\n          Field f = new StringField (\"int\", data[i][2]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.VAR_INTS);\n          }\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = new StringField (\"float\", data[i][3]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.FLOAT_32);\n          }\n          doc.add(f);\n        }\n        if (data[i][4] != null) {\n          Field f = new StringField (\"string\", data[i][4]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, stringDVType);\n          }\n          doc.add(f);\n        }\n        if (data[i][5] != null) doc.add (new StringField (\"custom\",   data[i][5]));\n        if (data[i][6] != null) doc.add (new StringField (\"i18n\",     data[i][6]));\n        if (data[i][7] != null) doc.add (new StringField (\"long\",     data[i][7]));\n        if (data[i][8] != null) {\n          Field f = new StringField (\"double\", data[i][8]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.FLOAT_64);\n          }\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new StringField (\"short\",     data[i][9]));\n        if (data[i][10] != null) doc.add (new StringField (\"byte\",     data[i][10]));\n        if (data[i][11] != null) doc.add (new StringField (\"parser\",     data[i][11]));\n\n        for(IndexableField f : doc.getFields()) {\n          ((Field) f).setBoost(2.0f);\n        }\n\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4","fa0f44f887719e97183771e977cfc4bfb485b766"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d638301ad1cfcae567b681b893bc8781f0ee48a5","date":1323801546,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    final DocValues.Type stringDVType;\n    if (dvStringSorted) {\n      // Index sorted\n      stringDVType = random.nextBoolean() ? DocValues.Type.BYTES_VAR_SORTED : DocValues.Type.BYTES_FIXED_SORTED;\n    } else {\n      // Index non-sorted\n      if (random.nextBoolean()) {\n        // Fixed\n        stringDVType = random.nextBoolean() ? DocValues.Type.BYTES_FIXED_STRAIGHT : DocValues.Type.BYTES_FIXED_DEREF;\n      } else {\n        // Var\n        stringDVType = random.nextBoolean() ? DocValues.Type.BYTES_VAR_STRAIGHT : DocValues.Type.BYTES_VAR_DEREF;\n      }\n    }\n\n    FieldType ft1 = new FieldType();\n    ft1.setStored(true);\n    FieldType ft2 = new FieldType();\n    ft2.setIndexed(true);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\", data[i][0], ft1));\n        doc.add (new TextField (\"contents\", data[i][1]));\n        if (data[i][2] != null) {\n          Field f = new StringField (\"int\", data[i][2]);\n          if (supportsDocValues) {\n            f = DocValuesField.build(f, DocValues.Type.VAR_INTS);\n          }\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = new StringField (\"float\", data[i][3]);\n          if (supportsDocValues) {\n            f = DocValuesField.build(f, DocValues.Type.FLOAT_32);\n          }\n          doc.add(f);\n        }\n        if (data[i][4] != null) {\n          Field f = new StringField (\"string\", data[i][4]);\n          if (supportsDocValues) {\n            f = DocValuesField.build(f, stringDVType);\n          }\n          doc.add(f);\n        }\n        if (data[i][5] != null) doc.add (new StringField (\"custom\",   data[i][5]));\n        if (data[i][6] != null) doc.add (new StringField (\"i18n\",     data[i][6]));\n        if (data[i][7] != null) doc.add (new StringField (\"long\",     data[i][7]));\n        if (data[i][8] != null) {\n          Field f = new StringField (\"double\", data[i][8]);\n          if (supportsDocValues) {\n            f = DocValuesField.build(f, DocValues.Type.FLOAT_64);\n          }\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new StringField (\"short\",     data[i][9]));\n        if (data[i][10] != null) doc.add (new StringField (\"byte\",     data[i][10]));\n        if (data[i][11] != null) doc.add (new StringField (\"parser\",     data[i][11]));\n\n        for(IndexableField f : doc.getFields()) {\n          ((Field) f).setBoost(2.0f);\n        }\n\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    final ValueType stringDVType;\n    if (dvStringSorted) {\n      // Index sorted\n      stringDVType = random.nextBoolean() ? ValueType.BYTES_VAR_SORTED : ValueType.BYTES_FIXED_SORTED;\n    } else {\n      // Index non-sorted\n      if (random.nextBoolean()) {\n        // Fixed\n        stringDVType = random.nextBoolean() ? ValueType.BYTES_FIXED_STRAIGHT : ValueType.BYTES_FIXED_DEREF;\n      } else {\n        // Var\n        stringDVType = random.nextBoolean() ? ValueType.BYTES_VAR_STRAIGHT : ValueType.BYTES_VAR_DEREF;\n      }\n    }\n\n    FieldType ft1 = new FieldType();\n    ft1.setStored(true);\n    FieldType ft2 = new FieldType();\n    ft2.setIndexed(true);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\", data[i][0], ft1));\n        doc.add (new TextField (\"contents\", data[i][1]));\n        if (data[i][2] != null) {\n          Field f = new StringField (\"int\", data[i][2]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.VAR_INTS);\n          }\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = new StringField (\"float\", data[i][3]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.FLOAT_32);\n          }\n          doc.add(f);\n        }\n        if (data[i][4] != null) {\n          Field f = new StringField (\"string\", data[i][4]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, stringDVType);\n          }\n          doc.add(f);\n        }\n        if (data[i][5] != null) doc.add (new StringField (\"custom\",   data[i][5]));\n        if (data[i][6] != null) doc.add (new StringField (\"i18n\",     data[i][6]));\n        if (data[i][7] != null) doc.add (new StringField (\"long\",     data[i][7]));\n        if (data[i][8] != null) {\n          Field f = new StringField (\"double\", data[i][8]);\n          if (supportsDocValues) {\n            f = IndexDocValuesField.build(f, ValueType.FLOAT_64);\n          }\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new StringField (\"short\",     data[i][9]));\n        if (data[i][10] != null) doc.add (new StringField (\"byte\",     data[i][10]));\n        if (data[i][11] != null) doc.add (new StringField (\"parser\",     data[i][11]));\n\n        for(IndexableField f : doc.getFields()) {\n          ((Field) f).setBoost(2.0f);\n        }\n\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fa0f44f887719e97183771e977cfc4bfb485b766","date":1326668713,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    final DocValues.Type stringDVType;\n    if (dvStringSorted) {\n      // Index sorted\n      stringDVType = random.nextBoolean() ? DocValues.Type.BYTES_VAR_SORTED : DocValues.Type.BYTES_FIXED_SORTED;\n    } else {\n      // Index non-sorted\n      if (random.nextBoolean()) {\n        // Fixed\n        stringDVType = random.nextBoolean() ? DocValues.Type.BYTES_FIXED_STRAIGHT : DocValues.Type.BYTES_FIXED_DEREF;\n      } else {\n        // Var\n        stringDVType = random.nextBoolean() ? DocValues.Type.BYTES_VAR_STRAIGHT : DocValues.Type.BYTES_VAR_DEREF;\n      }\n    }\n\n    FieldType ft1 = new FieldType();\n    ft1.setStored(true);\n    FieldType ft2 = new FieldType();\n    ft2.setIndexed(true);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\", data[i][0], ft1));\n        doc.add (new TextField (\"contents\", data[i][1]));\n        if (data[i][2] != null) {\n          doc.add(new StringField (\"int\", data[i][2]));\n          if (supportsDocValues) {\n            doc.add(new DocValuesField(\"int\", Integer.parseInt(data[i][2]), DocValues.Type.VAR_INTS));\n          }\n        }\n        if (data[i][3] != null) {\n          doc.add(new StringField (\"float\", data[i][3]));\n          if (supportsDocValues) {\n            doc.add(new DocValuesField(\"float\", Float.parseFloat(data[i][3]), DocValues.Type.FLOAT_32));\n          }\n        }\n        if (data[i][4] != null) {\n          doc.add(new StringField (\"string\", data[i][4]));\n          if (supportsDocValues) {\n            doc.add(new DocValuesField(\"string\", new BytesRef(data[i][4]), stringDVType));\n          }\n        }\n        if (data[i][5] != null) doc.add (new StringField (\"custom\",   data[i][5]));\n        if (data[i][6] != null) doc.add (new StringField (\"i18n\",     data[i][6]));\n        if (data[i][7] != null) doc.add (new StringField (\"long\",     data[i][7]));\n        if (data[i][8] != null) {\n          doc.add(new StringField (\"double\", data[i][8]));\n          if (supportsDocValues) {\n            doc.add(new DocValuesField(\"double\", Double.parseDouble(data[i][8]), DocValues.Type.FLOAT_64));\n          }\n        }\n        if (data[i][9] != null) doc.add (new StringField (\"short\",     data[i][9]));\n        if (data[i][10] != null) doc.add (new StringField (\"byte\",     data[i][10]));\n        if (data[i][11] != null) doc.add (new StringField (\"parser\",     data[i][11]));\n\n        for(IndexableField f : doc.getFields()) {\n          ((Field) f).setBoost(2.0f);\n        }\n\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    final DocValues.Type stringDVType;\n    if (dvStringSorted) {\n      // Index sorted\n      stringDVType = random.nextBoolean() ? DocValues.Type.BYTES_VAR_SORTED : DocValues.Type.BYTES_FIXED_SORTED;\n    } else {\n      // Index non-sorted\n      if (random.nextBoolean()) {\n        // Fixed\n        stringDVType = random.nextBoolean() ? DocValues.Type.BYTES_FIXED_STRAIGHT : DocValues.Type.BYTES_FIXED_DEREF;\n      } else {\n        // Var\n        stringDVType = random.nextBoolean() ? DocValues.Type.BYTES_VAR_STRAIGHT : DocValues.Type.BYTES_VAR_DEREF;\n      }\n    }\n\n    FieldType ft1 = new FieldType();\n    ft1.setStored(true);\n    FieldType ft2 = new FieldType();\n    ft2.setIndexed(true);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\", data[i][0], ft1));\n        doc.add (new TextField (\"contents\", data[i][1]));\n        if (data[i][2] != null) {\n          Field f = new StringField (\"int\", data[i][2]);\n          if (supportsDocValues) {\n            f = DocValuesField.build(f, DocValues.Type.VAR_INTS);\n          }\n          doc.add(f);\n        }\n        if (data[i][3] != null) {\n          Field f = new StringField (\"float\", data[i][3]);\n          if (supportsDocValues) {\n            f = DocValuesField.build(f, DocValues.Type.FLOAT_32);\n          }\n          doc.add(f);\n        }\n        if (data[i][4] != null) {\n          Field f = new StringField (\"string\", data[i][4]);\n          if (supportsDocValues) {\n            f = DocValuesField.build(f, stringDVType);\n          }\n          doc.add(f);\n        }\n        if (data[i][5] != null) doc.add (new StringField (\"custom\",   data[i][5]));\n        if (data[i][6] != null) doc.add (new StringField (\"i18n\",     data[i][6]));\n        if (data[i][7] != null) doc.add (new StringField (\"long\",     data[i][7]));\n        if (data[i][8] != null) {\n          Field f = new StringField (\"double\", data[i][8]);\n          if (supportsDocValues) {\n            f = DocValuesField.build(f, DocValues.Type.FLOAT_64);\n          }\n          doc.add(f);\n        }\n        if (data[i][9] != null) doc.add (new StringField (\"short\",     data[i][9]));\n        if (data[i][10] != null) doc.add (new StringField (\"byte\",     data[i][10]));\n        if (data[i][11] != null) doc.add (new StringField (\"parser\",     data[i][11]));\n\n        for(IndexableField f : doc.getFields()) {\n          ((Field) f).setBoost(2.0f);\n        }\n\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":["0cedee4c69436d5334e8f35463bf232084ff2fa3","1e59c344a45b9502f40ec44f5fe4e20ed2291dbe","1509f151d7692d84fae414b2b799ac06ba60fcb4","28088b0a688977b79dec2cc9119cff2fb4aab7ee"],"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2","cd659803551ebd8ca09b9e4ad7abd18d3d558f9d"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestSort#getIndex(boolean,boolean).mjava","sourceNew":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    final DocValues.Type stringDVType;\n    if (dvStringSorted) {\n      // Index sorted\n      stringDVType = random.nextBoolean() ? DocValues.Type.BYTES_VAR_SORTED : DocValues.Type.BYTES_FIXED_SORTED;\n    } else {\n      // Index non-sorted\n      if (random.nextBoolean()) {\n        // Fixed\n        stringDVType = random.nextBoolean() ? DocValues.Type.BYTES_FIXED_STRAIGHT : DocValues.Type.BYTES_FIXED_DEREF;\n      } else {\n        // Var\n        stringDVType = random.nextBoolean() ? DocValues.Type.BYTES_VAR_STRAIGHT : DocValues.Type.BYTES_VAR_DEREF;\n      }\n    }\n\n    FieldType ft1 = new FieldType();\n    ft1.setStored(true);\n    FieldType ft2 = new FieldType();\n    ft2.setIndexed(true);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\", data[i][0], ft1));\n        doc.add (new TextField (\"contents\", data[i][1]));\n        if (data[i][2] != null) {\n          doc.add(new StringField (\"int\", data[i][2]));\n          if (supportsDocValues) {\n            doc.add(new DocValuesField(\"int\", Integer.parseInt(data[i][2]), DocValues.Type.VAR_INTS));\n          }\n        }\n        if (data[i][3] != null) {\n          doc.add(new StringField (\"float\", data[i][3]));\n          if (supportsDocValues) {\n            doc.add(new DocValuesField(\"float\", Float.parseFloat(data[i][3]), DocValues.Type.FLOAT_32));\n          }\n        }\n        if (data[i][4] != null) {\n          doc.add(new StringField (\"string\", data[i][4]));\n          if (supportsDocValues) {\n            doc.add(new DocValuesField(\"string\", new BytesRef(data[i][4]), stringDVType));\n          }\n        }\n        if (data[i][5] != null) doc.add (new StringField (\"custom\",   data[i][5]));\n        if (data[i][6] != null) doc.add (new StringField (\"i18n\",     data[i][6]));\n        if (data[i][7] != null) doc.add (new StringField (\"long\",     data[i][7]));\n        if (data[i][8] != null) {\n          doc.add(new StringField (\"double\", data[i][8]));\n          if (supportsDocValues) {\n            doc.add(new DocValuesField(\"double\", Double.parseDouble(data[i][8]), DocValues.Type.FLOAT_64));\n          }\n        }\n        if (data[i][9] != null) doc.add (new StringField (\"short\",     data[i][9]));\n        if (data[i][10] != null) doc.add (new StringField (\"byte\",     data[i][10]));\n        if (data[i][11] != null) doc.add (new StringField (\"parser\",     data[i][11]));\n\n        for(IndexableField f : doc.getFields()) {\n          ((Field) f).setBoost(2.0f);\n        }\n\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","sourceOld":"  // create an index of all the documents, or just the x, or just the y documents\n  private IndexSearcher getIndex (boolean even, boolean odd)\n  throws IOException {\n    Directory indexStore = newDirectory();\n    dirs.add(indexStore);\n    RandomIndexWriter writer = new RandomIndexWriter(random, indexStore, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n\n    final DocValues.Type stringDVType;\n    if (dvStringSorted) {\n      // Index sorted\n      stringDVType = random.nextBoolean() ? DocValues.Type.BYTES_VAR_SORTED : DocValues.Type.BYTES_FIXED_SORTED;\n    } else {\n      // Index non-sorted\n      if (random.nextBoolean()) {\n        // Fixed\n        stringDVType = random.nextBoolean() ? DocValues.Type.BYTES_FIXED_STRAIGHT : DocValues.Type.BYTES_FIXED_DEREF;\n      } else {\n        // Var\n        stringDVType = random.nextBoolean() ? DocValues.Type.BYTES_VAR_STRAIGHT : DocValues.Type.BYTES_VAR_DEREF;\n      }\n    }\n\n    FieldType ft1 = new FieldType();\n    ft1.setStored(true);\n    FieldType ft2 = new FieldType();\n    ft2.setIndexed(true);\n    for (int i=0; i<data.length; ++i) {\n      if (((i%2)==0 && even) || ((i%2)==1 && odd)) {\n        Document doc = new Document();\n        doc.add (new Field (\"tracer\", data[i][0], ft1));\n        doc.add (new TextField (\"contents\", data[i][1]));\n        if (data[i][2] != null) {\n          doc.add(new StringField (\"int\", data[i][2]));\n          if (supportsDocValues) {\n            doc.add(new DocValuesField(\"int\", Integer.parseInt(data[i][2]), DocValues.Type.VAR_INTS));\n          }\n        }\n        if (data[i][3] != null) {\n          doc.add(new StringField (\"float\", data[i][3]));\n          if (supportsDocValues) {\n            doc.add(new DocValuesField(\"float\", Float.parseFloat(data[i][3]), DocValues.Type.FLOAT_32));\n          }\n        }\n        if (data[i][4] != null) {\n          doc.add(new StringField (\"string\", data[i][4]));\n          if (supportsDocValues) {\n            doc.add(new DocValuesField(\"string\", new BytesRef(data[i][4]), stringDVType));\n          }\n        }\n        if (data[i][5] != null) doc.add (new StringField (\"custom\",   data[i][5]));\n        if (data[i][6] != null) doc.add (new StringField (\"i18n\",     data[i][6]));\n        if (data[i][7] != null) doc.add (new StringField (\"long\",     data[i][7]));\n        if (data[i][8] != null) {\n          doc.add(new StringField (\"double\", data[i][8]));\n          if (supportsDocValues) {\n            doc.add(new DocValuesField(\"double\", Double.parseDouble(data[i][8]), DocValues.Type.FLOAT_64));\n          }\n        }\n        if (data[i][9] != null) doc.add (new StringField (\"short\",     data[i][9]));\n        if (data[i][10] != null) doc.add (new StringField (\"byte\",     data[i][10]));\n        if (data[i][11] != null) doc.add (new StringField (\"parser\",     data[i][11]));\n\n        for(IndexableField f : doc.getFields()) {\n          ((Field) f).setBoost(2.0f);\n        }\n\n        writer.addDocument (doc);\n      }\n    }\n    IndexReader reader = writer.getReader();\n    writer.close ();\n    IndexSearcher s = newSearcher(reader);\n    s.setDefaultFieldSortScoring(true, true);\n    return s;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["fa0f44f887719e97183771e977cfc4bfb485b766"],"15bbd254c1506df5299c4df8c148262c7bd6301e":["c084e47df29de3330311d69dabf515ceaa989512"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["3242a09f703274d3b9283f2064a1a33064b53a1b","1f653cfcf159baeaafe5d01682a911e95bba4012"],"382fe3a6ca9745891afebda9b9a57cc158305545":["7e4db59c6b6c10e25322cfb41c4c19d78b4298bd"],"c19f985e36a65cc969e8e564fe337a0d41512075":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"d638301ad1cfcae567b681b893bc8781f0ee48a5":["28088b0a688977b79dec2cc9119cff2fb4aab7ee","1e59c344a45b9502f40ec44f5fe4e20ed2291dbe"],"01e5948db9a07144112d2f08f28ca2e3cd880348":["790e1fde4caa765b3faaad3fbcd25c6973450336"],"c084e47df29de3330311d69dabf515ceaa989512":["d572389229127c297dd1fa5ce4758e1cec41e799"],"9235b9d4454a46c066cda47fed7ca0a34e614529":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"2e8d7ba2175f47e280231533f7d3016249cea88b":["f2c5f0cb44df114db4228c8f77861714b5cabaea","605b061fadf71aa6458c0005eaae83a98350c403"],"39d51e9acac1e629cffe47855e85dc6dedc4754d":["0cedee4c69436d5334e8f35463bf232084ff2fa3"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["01e5948db9a07144112d2f08f28ca2e3cd880348"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["a05409176bd65129d67a785ee70e881e238a9aef"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["1f653cfcf159baeaafe5d01682a911e95bba4012","790e1fde4caa765b3faaad3fbcd25c6973450336"],"1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c":["4b103252dee6afa1b6d7a622c773d178788eb85a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a05409176bd65129d67a785ee70e881e238a9aef":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"fa0f44f887719e97183771e977cfc4bfb485b766":["d638301ad1cfcae567b681b893bc8781f0ee48a5"],"28088b0a688977b79dec2cc9119cff2fb4aab7ee":["382fe3a6ca9745891afebda9b9a57cc158305545"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["93c59ad5034eecd863f267bd75e1df7b8a51e427"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","790e1fde4caa765b3faaad3fbcd25c6973450336"],"7e4db59c6b6c10e25322cfb41c4c19d78b4298bd":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["a3776dccca01c11e7046323cfad46a3b4a471233","2e8d7ba2175f47e280231533f7d3016249cea88b"],"3242a09f703274d3b9283f2064a1a33064b53a1b":["5f4e87790277826a2aea119328600dfb07761f32","4b103252dee6afa1b6d7a622c773d178788eb85a"],"4b103252dee6afa1b6d7a622c773d178788eb85a":["c084e47df29de3330311d69dabf515ceaa989512","15bbd254c1506df5299c4df8c148262c7bd6301e"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c"],"7ab99e8c71442b92c320e218141dee04a9b91ce8":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"d572389229127c297dd1fa5ce4758e1cec41e799":["7ab99e8c71442b92c320e218141dee04a9b91ce8"],"5f4e87790277826a2aea119328600dfb07761f32":["d572389229127c297dd1fa5ce4758e1cec41e799","c084e47df29de3330311d69dabf515ceaa989512"],"962d04139994fce5193143ef35615499a9a96d78":["45669a651c970812a680841b97a77cce06af559f","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"2553b00f699380c64959ccb27991289aae87be2e":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","93c59ad5034eecd863f267bd75e1df7b8a51e427"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["2e8d7ba2175f47e280231533f7d3016249cea88b","93c59ad5034eecd863f267bd75e1df7b8a51e427"],"790e1fde4caa765b3faaad3fbcd25c6973450336":["c19f985e36a65cc969e8e564fe337a0d41512075"],"605b061fadf71aa6458c0005eaae83a98350c403":["39d51e9acac1e629cffe47855e85dc6dedc4754d"],"93c59ad5034eecd863f267bd75e1df7b8a51e427":["2e8d7ba2175f47e280231533f7d3016249cea88b"],"0cedee4c69436d5334e8f35463bf232084ff2fa3":["9235b9d4454a46c066cda47fed7ca0a34e614529"],"a3776dccca01c11e7046323cfad46a3b4a471233":["790e1fde4caa765b3faaad3fbcd25c6973450336","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"1e59c344a45b9502f40ec44f5fe4e20ed2291dbe":["28088b0a688977b79dec2cc9119cff2fb4aab7ee"],"45669a651c970812a680841b97a77cce06af559f":["bde51b089eb7f86171eb3406e38a274743f9b7ac","01e5948db9a07144112d2f08f28ca2e3cd880348"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"15bbd254c1506df5299c4df8c148262c7bd6301e":["4b103252dee6afa1b6d7a622c773d178788eb85a"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"382fe3a6ca9745891afebda9b9a57cc158305545":["28088b0a688977b79dec2cc9119cff2fb4aab7ee"],"c19f985e36a65cc969e8e564fe337a0d41512075":["790e1fde4caa765b3faaad3fbcd25c6973450336"],"d638301ad1cfcae567b681b893bc8781f0ee48a5":["fa0f44f887719e97183771e977cfc4bfb485b766"],"01e5948db9a07144112d2f08f28ca2e3cd880348":["f2c5f0cb44df114db4228c8f77861714b5cabaea","45669a651c970812a680841b97a77cce06af559f"],"c084e47df29de3330311d69dabf515ceaa989512":["15bbd254c1506df5299c4df8c148262c7bd6301e","4b103252dee6afa1b6d7a622c773d178788eb85a","5f4e87790277826a2aea119328600dfb07761f32"],"9235b9d4454a46c066cda47fed7ca0a34e614529":["0cedee4c69436d5334e8f35463bf232084ff2fa3"],"2e8d7ba2175f47e280231533f7d3016249cea88b":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","d083e83f225b11e5fdd900e83d26ddb385b6955c","93c59ad5034eecd863f267bd75e1df7b8a51e427"],"39d51e9acac1e629cffe47855e85dc6dedc4754d":["605b061fadf71aa6458c0005eaae83a98350c403"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["2e8d7ba2175f47e280231533f7d3016249cea88b","135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","a3776dccca01c11e7046323cfad46a3b4a471233"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","c19f985e36a65cc969e8e564fe337a0d41512075","29ef99d61cda9641b6250bf9567329a6e65f901d"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a05409176bd65129d67a785ee70e881e238a9aef":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"fa0f44f887719e97183771e977cfc4bfb485b766":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"28088b0a688977b79dec2cc9119cff2fb4aab7ee":["d638301ad1cfcae567b681b893bc8781f0ee48a5","1e59c344a45b9502f40ec44f5fe4e20ed2291dbe"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["7e4db59c6b6c10e25322cfb41c4c19d78b4298bd"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["45669a651c970812a680841b97a77cce06af559f"],"7e4db59c6b6c10e25322cfb41c4c19d78b4298bd":["382fe3a6ca9745891afebda9b9a57cc158305545"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["7ab99e8c71442b92c320e218141dee04a9b91ce8"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["2553b00f699380c64959ccb27991289aae87be2e"],"3242a09f703274d3b9283f2064a1a33064b53a1b":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"4b103252dee6afa1b6d7a622c773d178788eb85a":["1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c","3242a09f703274d3b9283f2064a1a33064b53a1b"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["a05409176bd65129d67a785ee70e881e238a9aef"],"7ab99e8c71442b92c320e218141dee04a9b91ce8":["d572389229127c297dd1fa5ce4758e1cec41e799"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["9235b9d4454a46c066cda47fed7ca0a34e614529"],"d572389229127c297dd1fa5ce4758e1cec41e799":["c084e47df29de3330311d69dabf515ceaa989512","5f4e87790277826a2aea119328600dfb07761f32"],"5f4e87790277826a2aea119328600dfb07761f32":["3242a09f703274d3b9283f2064a1a33064b53a1b"],"962d04139994fce5193143ef35615499a9a96d78":[],"2553b00f699380c64959ccb27991289aae87be2e":[],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"790e1fde4caa765b3faaad3fbcd25c6973450336":["01e5948db9a07144112d2f08f28ca2e3cd880348","29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac","a3776dccca01c11e7046323cfad46a3b4a471233"],"605b061fadf71aa6458c0005eaae83a98350c403":["2e8d7ba2175f47e280231533f7d3016249cea88b"],"93c59ad5034eecd863f267bd75e1df7b8a51e427":["1509f151d7692d84fae414b2b799ac06ba60fcb4","2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"0cedee4c69436d5334e8f35463bf232084ff2fa3":["39d51e9acac1e629cffe47855e85dc6dedc4754d"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655"],"1e59c344a45b9502f40ec44f5fe4e20ed2291dbe":["d638301ad1cfcae567b681b893bc8781f0ee48a5"],"45669a651c970812a680841b97a77cce06af559f":["962d04139994fce5193143ef35615499a9a96d78"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["962d04139994fce5193143ef35615499a9a96d78","2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}