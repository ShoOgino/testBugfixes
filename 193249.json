{"path":"lucene/core/src/java/org/apache/lucene/util/WAH8DocIdSet.WordBuilder#build().mjava","commits":[{"id":"794214a4691ccb0a156a9ea0dfd8663ab9f05bb4","date":1373484264,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/WAH8DocIdSet.WordBuilder#build().mjava","pathOld":"/dev/null","sourceNew":"    /** Build a new {@link WAH8DocIdSet}. */\n    public WAH8DocIdSet build() {\n      if (lastWordNum == -1) {\n        return EMPTY;\n      }\n      writeSequence(clean);\n      final byte[] data = Arrays.copyOf(out.bytes, out.length);\n\n      // Now build the index\n      final int valueCount = (numSequences - 1) / indexInterval + 1;\n      final PackedInts.Reader indexPositions;\n      final PackedInts.Reader indexWordNums;\n      if (valueCount <= 1) {\n        indexPositions = indexWordNums = EMPTY_READER;\n      } else {\n        // From the tests I ran, there is no need to expose acceptableOverheadRatio, these packed ints are never the bottleneck\n        final PackedInts.Mutable positions = PackedInts.getMutable(valueCount, PackedInts.bitsRequired(data.length - 1), PackedInts.COMPACT);\n        final PackedInts.Mutable wordNums = PackedInts.getMutable(valueCount, PackedInts.bitsRequired(lastWordNum), PackedInts.COMPACT);\n  \n        final Iterator it = new Iterator(data, null, null);\n        assert it.in.getPosition() == 0;\n        assert it.wordNum == -1;\n        for (int i = 1; i < valueCount; ++i) {\n          // skip indexInterval sequences\n          for (int j = 0; j < indexInterval; ++j) {\n            final boolean readSequence = it.readSequence();\n            assert readSequence;\n            it.skipDirtyBytes();\n          }\n          final int position = it.in.getPosition();\n          final int wordNum = it.wordNum;\n          positions.set(i, position);\n          wordNums.set(i, wordNum + 1);\n        }\n        indexPositions = positions;\n        indexWordNums = wordNums;\n      }\n\n      return new WAH8DocIdSet(data, indexPositions, indexWordNums);\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6713d909dc80e8c53878ff98bb5376dc1af95956","date":1373964521,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/WAH8DocIdSet.WordBuilder#build().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/WAH8DocIdSet.WordBuilder#build().mjava","sourceNew":"    /** Build a new {@link WAH8DocIdSet}. */\n    public WAH8DocIdSet build() {\n      if (cardinality == 0) {\n        assert lastWordNum == -1;\n        return EMPTY;\n      }\n      writeSequence(clean);\n      final byte[] data = Arrays.copyOf(out.bytes, out.length);\n\n      // Now build the index\n      final int valueCount = (numSequences - 1) / indexInterval + 1;\n      final MonotonicAppendingLongBuffer indexPositions, indexWordNums;\n      if (valueCount <= 1) {\n        indexPositions = indexWordNums = SINGLE_ZERO_BUFFER;\n      } else {\n        final int pageSize = 128;\n        final int initialPageCount = (valueCount + pageSize - 1) / pageSize;\n        final MonotonicAppendingLongBuffer positions = new MonotonicAppendingLongBuffer(initialPageCount, pageSize);\n        final MonotonicAppendingLongBuffer wordNums = new MonotonicAppendingLongBuffer(initialPageCount, pageSize);\n \n        positions.add(0L);\n        wordNums.add(0L);\n        final Iterator it = new Iterator(data, cardinality, Integer.MAX_VALUE, SINGLE_ZERO_BUFFER, SINGLE_ZERO_BUFFER);\n        assert it.in.getPosition() == 0;\n        assert it.wordNum == -1;\n        for (int i = 1; i < valueCount; ++i) {\n          // skip indexInterval sequences\n          for (int j = 0; j < indexInterval; ++j) {\n            final boolean readSequence = it.readSequence();\n            assert readSequence;\n            it.skipDirtyBytes();\n          }\n          final int position = it.in.getPosition();\n          final int wordNum = it.wordNum;\n          positions.add(position);\n          wordNums.add(wordNum + 1);\n        }\n        positions.freeze();\n        wordNums.freeze();\n        indexPositions = positions;\n        indexWordNums = wordNums;\n      }\n\n      return new WAH8DocIdSet(data, cardinality, indexInterval, indexPositions, indexWordNums);\n    }\n\n","sourceOld":"    /** Build a new {@link WAH8DocIdSet}. */\n    public WAH8DocIdSet build() {\n      if (lastWordNum == -1) {\n        return EMPTY;\n      }\n      writeSequence(clean);\n      final byte[] data = Arrays.copyOf(out.bytes, out.length);\n\n      // Now build the index\n      final int valueCount = (numSequences - 1) / indexInterval + 1;\n      final PackedInts.Reader indexPositions;\n      final PackedInts.Reader indexWordNums;\n      if (valueCount <= 1) {\n        indexPositions = indexWordNums = EMPTY_READER;\n      } else {\n        // From the tests I ran, there is no need to expose acceptableOverheadRatio, these packed ints are never the bottleneck\n        final PackedInts.Mutable positions = PackedInts.getMutable(valueCount, PackedInts.bitsRequired(data.length - 1), PackedInts.COMPACT);\n        final PackedInts.Mutable wordNums = PackedInts.getMutable(valueCount, PackedInts.bitsRequired(lastWordNum), PackedInts.COMPACT);\n  \n        final Iterator it = new Iterator(data, null, null);\n        assert it.in.getPosition() == 0;\n        assert it.wordNum == -1;\n        for (int i = 1; i < valueCount; ++i) {\n          // skip indexInterval sequences\n          for (int j = 0; j < indexInterval; ++j) {\n            final boolean readSequence = it.readSequence();\n            assert readSequence;\n            it.skipDirtyBytes();\n          }\n          final int position = it.in.getPosition();\n          final int wordNum = it.wordNum;\n          positions.set(i, position);\n          wordNums.set(i, wordNum + 1);\n        }\n        indexPositions = positions;\n        indexWordNums = wordNums;\n      }\n\n      return new WAH8DocIdSet(data, indexPositions, indexWordNums);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":0,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/WAH8DocIdSet.WordBuilder#build().mjava","pathOld":"/dev/null","sourceNew":"    /** Build a new {@link WAH8DocIdSet}. */\n    public WAH8DocIdSet build() {\n      if (cardinality == 0) {\n        assert lastWordNum == -1;\n        return EMPTY;\n      }\n      writeSequence(clean);\n      final byte[] data = Arrays.copyOf(out.bytes, out.length);\n\n      // Now build the index\n      final int valueCount = (numSequences - 1) / indexInterval + 1;\n      final MonotonicAppendingLongBuffer indexPositions, indexWordNums;\n      if (valueCount <= 1) {\n        indexPositions = indexWordNums = SINGLE_ZERO_BUFFER;\n      } else {\n        final int pageSize = 128;\n        final int initialPageCount = (valueCount + pageSize - 1) / pageSize;\n        final MonotonicAppendingLongBuffer positions = new MonotonicAppendingLongBuffer(initialPageCount, pageSize);\n        final MonotonicAppendingLongBuffer wordNums = new MonotonicAppendingLongBuffer(initialPageCount, pageSize);\n \n        positions.add(0L);\n        wordNums.add(0L);\n        final Iterator it = new Iterator(data, cardinality, Integer.MAX_VALUE, SINGLE_ZERO_BUFFER, SINGLE_ZERO_BUFFER);\n        assert it.in.getPosition() == 0;\n        assert it.wordNum == -1;\n        for (int i = 1; i < valueCount; ++i) {\n          // skip indexInterval sequences\n          for (int j = 0; j < indexInterval; ++j) {\n            final boolean readSequence = it.readSequence();\n            assert readSequence;\n            it.skipDirtyBytes();\n          }\n          final int position = it.in.getPosition();\n          final int wordNum = it.wordNum;\n          positions.add(position);\n          wordNums.add(wordNum + 1);\n        }\n        positions.freeze();\n        wordNums.freeze();\n        indexPositions = positions;\n        indexWordNums = wordNums;\n      }\n\n      return new WAH8DocIdSet(data, cardinality, indexInterval, indexPositions, indexWordNums);\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cb240aebd5a347d79f642127ad9255dd9a979f06","date":1375188159,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/WAH8DocIdSet.WordBuilder#build().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/WAH8DocIdSet.WordBuilder#build().mjava","sourceNew":"    /** Build a new {@link WAH8DocIdSet}. */\n    public WAH8DocIdSet build() {\n      if (cardinality == 0) {\n        assert lastWordNum == -1;\n        return EMPTY;\n      }\n      writeSequence(clean);\n      final byte[] data = Arrays.copyOf(out.bytes, out.length);\n\n      // Now build the index\n      final int valueCount = (numSequences - 1) / indexInterval + 1;\n      final MonotonicAppendingLongBuffer indexPositions, indexWordNums;\n      if (valueCount <= 1) {\n        indexPositions = indexWordNums = SINGLE_ZERO_BUFFER;\n      } else {\n        final int pageSize = 128;\n        final int initialPageCount = (valueCount + pageSize - 1) / pageSize;\n        final MonotonicAppendingLongBuffer positions = new MonotonicAppendingLongBuffer(initialPageCount, pageSize, PackedInts.COMPACT);\n        final MonotonicAppendingLongBuffer wordNums = new MonotonicAppendingLongBuffer(initialPageCount, pageSize, PackedInts.COMPACT);\n\n        positions.add(0L);\n        wordNums.add(0L);\n        final Iterator it = new Iterator(data, cardinality, Integer.MAX_VALUE, SINGLE_ZERO_BUFFER, SINGLE_ZERO_BUFFER);\n        assert it.in.getPosition() == 0;\n        assert it.wordNum == -1;\n        for (int i = 1; i < valueCount; ++i) {\n          // skip indexInterval sequences\n          for (int j = 0; j < indexInterval; ++j) {\n            final boolean readSequence = it.readSequence();\n            assert readSequence;\n            it.skipDirtyBytes();\n          }\n          final int position = it.in.getPosition();\n          final int wordNum = it.wordNum;\n          positions.add(position);\n          wordNums.add(wordNum + 1);\n        }\n        positions.freeze();\n        wordNums.freeze();\n        indexPositions = positions;\n        indexWordNums = wordNums;\n      }\n\n      return new WAH8DocIdSet(data, cardinality, indexInterval, indexPositions, indexWordNums);\n    }\n\n","sourceOld":"    /** Build a new {@link WAH8DocIdSet}. */\n    public WAH8DocIdSet build() {\n      if (cardinality == 0) {\n        assert lastWordNum == -1;\n        return EMPTY;\n      }\n      writeSequence(clean);\n      final byte[] data = Arrays.copyOf(out.bytes, out.length);\n\n      // Now build the index\n      final int valueCount = (numSequences - 1) / indexInterval + 1;\n      final MonotonicAppendingLongBuffer indexPositions, indexWordNums;\n      if (valueCount <= 1) {\n        indexPositions = indexWordNums = SINGLE_ZERO_BUFFER;\n      } else {\n        final int pageSize = 128;\n        final int initialPageCount = (valueCount + pageSize - 1) / pageSize;\n        final MonotonicAppendingLongBuffer positions = new MonotonicAppendingLongBuffer(initialPageCount, pageSize);\n        final MonotonicAppendingLongBuffer wordNums = new MonotonicAppendingLongBuffer(initialPageCount, pageSize);\n \n        positions.add(0L);\n        wordNums.add(0L);\n        final Iterator it = new Iterator(data, cardinality, Integer.MAX_VALUE, SINGLE_ZERO_BUFFER, SINGLE_ZERO_BUFFER);\n        assert it.in.getPosition() == 0;\n        assert it.wordNum == -1;\n        for (int i = 1; i < valueCount; ++i) {\n          // skip indexInterval sequences\n          for (int j = 0; j < indexInterval; ++j) {\n            final boolean readSequence = it.readSequence();\n            assert readSequence;\n            it.skipDirtyBytes();\n          }\n          final int position = it.in.getPosition();\n          final int wordNum = it.wordNum;\n          positions.add(position);\n          wordNums.add(wordNum + 1);\n        }\n        positions.freeze();\n        wordNums.freeze();\n        indexPositions = positions;\n        indexWordNums = wordNums;\n      }\n\n      return new WAH8DocIdSet(data, cardinality, indexInterval, indexPositions, indexWordNums);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"21fd82f9528d0f0c70205bd925dd0b0dce14fdab","date":1376071243,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/WAH8DocIdSet.WordBuilder#build().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/WAH8DocIdSet.WordBuilder#build().mjava","sourceNew":"    /** Build a new {@link WAH8DocIdSet}. */\n    public WAH8DocIdSet build() {\n      if (cardinality == 0) {\n        assert lastWordNum == -1;\n        return EMPTY;\n      }\n      writeSequence();\n      final byte[] data = Arrays.copyOf(out.bytes, out.length);\n\n      // Now build the index\n      final int valueCount = (numSequences - 1) / indexInterval + 1;\n      final MonotonicAppendingLongBuffer indexPositions, indexWordNums;\n      if (valueCount <= 1) {\n        indexPositions = indexWordNums = SINGLE_ZERO_BUFFER;\n      } else {\n        final int pageSize = 128;\n        final int initialPageCount = (valueCount + pageSize - 1) / pageSize;\n        final MonotonicAppendingLongBuffer positions = new MonotonicAppendingLongBuffer(initialPageCount, pageSize, PackedInts.COMPACT);\n        final MonotonicAppendingLongBuffer wordNums = new MonotonicAppendingLongBuffer(initialPageCount, pageSize, PackedInts.COMPACT);\n\n        positions.add(0L);\n        wordNums.add(0L);\n        final Iterator it = new Iterator(data, cardinality, Integer.MAX_VALUE, SINGLE_ZERO_BUFFER, SINGLE_ZERO_BUFFER);\n        assert it.in.getPosition() == 0;\n        assert it.wordNum == -1;\n        for (int i = 1; i < valueCount; ++i) {\n          // skip indexInterval sequences\n          for (int j = 0; j < indexInterval; ++j) {\n            final boolean readSequence = it.readSequence();\n            assert readSequence;\n            it.skipDirtyBytes();\n          }\n          final int position = it.in.getPosition();\n          final int wordNum = it.wordNum;\n          positions.add(position);\n          wordNums.add(wordNum + 1);\n        }\n        positions.freeze();\n        wordNums.freeze();\n        indexPositions = positions;\n        indexWordNums = wordNums;\n      }\n\n      return new WAH8DocIdSet(data, cardinality, indexInterval, indexPositions, indexWordNums);\n    }\n\n","sourceOld":"    /** Build a new {@link WAH8DocIdSet}. */\n    public WAH8DocIdSet build() {\n      if (cardinality == 0) {\n        assert lastWordNum == -1;\n        return EMPTY;\n      }\n      writeSequence(clean);\n      final byte[] data = Arrays.copyOf(out.bytes, out.length);\n\n      // Now build the index\n      final int valueCount = (numSequences - 1) / indexInterval + 1;\n      final MonotonicAppendingLongBuffer indexPositions, indexWordNums;\n      if (valueCount <= 1) {\n        indexPositions = indexWordNums = SINGLE_ZERO_BUFFER;\n      } else {\n        final int pageSize = 128;\n        final int initialPageCount = (valueCount + pageSize - 1) / pageSize;\n        final MonotonicAppendingLongBuffer positions = new MonotonicAppendingLongBuffer(initialPageCount, pageSize, PackedInts.COMPACT);\n        final MonotonicAppendingLongBuffer wordNums = new MonotonicAppendingLongBuffer(initialPageCount, pageSize, PackedInts.COMPACT);\n\n        positions.add(0L);\n        wordNums.add(0L);\n        final Iterator it = new Iterator(data, cardinality, Integer.MAX_VALUE, SINGLE_ZERO_BUFFER, SINGLE_ZERO_BUFFER);\n        assert it.in.getPosition() == 0;\n        assert it.wordNum == -1;\n        for (int i = 1; i < valueCount; ++i) {\n          // skip indexInterval sequences\n          for (int j = 0; j < indexInterval; ++j) {\n            final boolean readSequence = it.readSequence();\n            assert readSequence;\n            it.skipDirtyBytes();\n          }\n          final int position = it.in.getPosition();\n          final int wordNum = it.wordNum;\n          positions.add(position);\n          wordNums.add(wordNum + 1);\n        }\n        positions.freeze();\n        wordNums.freeze();\n        indexPositions = positions;\n        indexWordNums = wordNums;\n      }\n\n      return new WAH8DocIdSet(data, cardinality, indexInterval, indexPositions, indexWordNums);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/WAH8DocIdSet.WordBuilder#build().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/WAH8DocIdSet.WordBuilder#build().mjava","sourceNew":"    /** Build a new {@link WAH8DocIdSet}. */\n    public WAH8DocIdSet build() {\n      if (cardinality == 0) {\n        assert lastWordNum == -1;\n        return EMPTY;\n      }\n      writeSequence();\n      final byte[] data = Arrays.copyOf(out.bytes, out.length);\n\n      // Now build the index\n      final int valueCount = (numSequences - 1) / indexInterval + 1;\n      final MonotonicAppendingLongBuffer indexPositions, indexWordNums;\n      if (valueCount <= 1) {\n        indexPositions = indexWordNums = SINGLE_ZERO_BUFFER;\n      } else {\n        final int pageSize = 128;\n        final int initialPageCount = (valueCount + pageSize - 1) / pageSize;\n        final MonotonicAppendingLongBuffer positions = new MonotonicAppendingLongBuffer(initialPageCount, pageSize, PackedInts.COMPACT);\n        final MonotonicAppendingLongBuffer wordNums = new MonotonicAppendingLongBuffer(initialPageCount, pageSize, PackedInts.COMPACT);\n\n        positions.add(0L);\n        wordNums.add(0L);\n        final Iterator it = new Iterator(data, cardinality, Integer.MAX_VALUE, SINGLE_ZERO_BUFFER, SINGLE_ZERO_BUFFER);\n        assert it.in.getPosition() == 0;\n        assert it.wordNum == -1;\n        for (int i = 1; i < valueCount; ++i) {\n          // skip indexInterval sequences\n          for (int j = 0; j < indexInterval; ++j) {\n            final boolean readSequence = it.readSequence();\n            assert readSequence;\n            it.skipDirtyBytes();\n          }\n          final int position = it.in.getPosition();\n          final int wordNum = it.wordNum;\n          positions.add(position);\n          wordNums.add(wordNum + 1);\n        }\n        positions.freeze();\n        wordNums.freeze();\n        indexPositions = positions;\n        indexWordNums = wordNums;\n      }\n\n      return new WAH8DocIdSet(data, cardinality, indexInterval, indexPositions, indexWordNums);\n    }\n\n","sourceOld":"    /** Build a new {@link WAH8DocIdSet}. */\n    public WAH8DocIdSet build() {\n      if (cardinality == 0) {\n        assert lastWordNum == -1;\n        return EMPTY;\n      }\n      writeSequence(clean);\n      final byte[] data = Arrays.copyOf(out.bytes, out.length);\n\n      // Now build the index\n      final int valueCount = (numSequences - 1) / indexInterval + 1;\n      final MonotonicAppendingLongBuffer indexPositions, indexWordNums;\n      if (valueCount <= 1) {\n        indexPositions = indexWordNums = SINGLE_ZERO_BUFFER;\n      } else {\n        final int pageSize = 128;\n        final int initialPageCount = (valueCount + pageSize - 1) / pageSize;\n        final MonotonicAppendingLongBuffer positions = new MonotonicAppendingLongBuffer(initialPageCount, pageSize);\n        final MonotonicAppendingLongBuffer wordNums = new MonotonicAppendingLongBuffer(initialPageCount, pageSize);\n \n        positions.add(0L);\n        wordNums.add(0L);\n        final Iterator it = new Iterator(data, cardinality, Integer.MAX_VALUE, SINGLE_ZERO_BUFFER, SINGLE_ZERO_BUFFER);\n        assert it.in.getPosition() == 0;\n        assert it.wordNum == -1;\n        for (int i = 1; i < valueCount; ++i) {\n          // skip indexInterval sequences\n          for (int j = 0; j < indexInterval; ++j) {\n            final boolean readSequence = it.readSequence();\n            assert readSequence;\n            it.skipDirtyBytes();\n          }\n          final int position = it.in.getPosition();\n          final int wordNum = it.wordNum;\n          positions.add(position);\n          wordNums.add(wordNum + 1);\n        }\n        positions.freeze();\n        wordNums.freeze();\n        indexPositions = positions;\n        indexWordNums = wordNums;\n      }\n\n      return new WAH8DocIdSet(data, cardinality, indexInterval, indexPositions, indexWordNums);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aae6236deecc1bf344f9c22d8d9dd09ef6701dbd","date":1404226546,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/WAH8DocIdSet.WordBuilder#build().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/WAH8DocIdSet.WordBuilder#build().mjava","sourceNew":"    /** Build a new {@link WAH8DocIdSet}. */\n    public WAH8DocIdSet build() {\n      if (cardinality == 0) {\n        assert lastWordNum == -1;\n        return EMPTY;\n      }\n      writeSequence();\n      final byte[] data = Arrays.copyOf(out.bytes, out.length);\n\n      // Now build the index\n      final int valueCount = (numSequences - 1) / indexInterval + 1;\n      final PackedLongValues indexPositions, indexWordNums;\n      if (valueCount <= 1) {\n        indexPositions = indexWordNums = SINGLE_ZERO;\n      } else {\n        final int pageSize = 128;\n        final PackedLongValues.Builder positions = PackedLongValues.monotonicBuilder(pageSize, PackedInts.COMPACT);\n        final PackedLongValues.Builder wordNums = PackedLongValues.monotonicBuilder(pageSize, PackedInts.COMPACT);\n\n        positions.add(0L);\n        wordNums.add(0L);\n        final Iterator it = new Iterator(data, cardinality, Integer.MAX_VALUE, SINGLE_ZERO, SINGLE_ZERO);\n        assert it.in.getPosition() == 0;\n        assert it.wordNum == -1;\n        for (int i = 1; i < valueCount; ++i) {\n          // skip indexInterval sequences\n          for (int j = 0; j < indexInterval; ++j) {\n            final boolean readSequence = it.readSequence();\n            assert readSequence;\n            it.skipDirtyBytes();\n          }\n          final int position = it.in.getPosition();\n          final int wordNum = it.wordNum;\n          positions.add(position);\n          wordNums.add(wordNum + 1);\n        }\n        indexPositions = positions.build();\n        indexWordNums = wordNums.build();\n      }\n\n      return new WAH8DocIdSet(data, cardinality, indexInterval, indexPositions, indexWordNums);\n    }\n\n","sourceOld":"    /** Build a new {@link WAH8DocIdSet}. */\n    public WAH8DocIdSet build() {\n      if (cardinality == 0) {\n        assert lastWordNum == -1;\n        return EMPTY;\n      }\n      writeSequence();\n      final byte[] data = Arrays.copyOf(out.bytes, out.length);\n\n      // Now build the index\n      final int valueCount = (numSequences - 1) / indexInterval + 1;\n      final MonotonicAppendingLongBuffer indexPositions, indexWordNums;\n      if (valueCount <= 1) {\n        indexPositions = indexWordNums = SINGLE_ZERO_BUFFER;\n      } else {\n        final int pageSize = 128;\n        final int initialPageCount = (valueCount + pageSize - 1) / pageSize;\n        final MonotonicAppendingLongBuffer positions = new MonotonicAppendingLongBuffer(initialPageCount, pageSize, PackedInts.COMPACT);\n        final MonotonicAppendingLongBuffer wordNums = new MonotonicAppendingLongBuffer(initialPageCount, pageSize, PackedInts.COMPACT);\n\n        positions.add(0L);\n        wordNums.add(0L);\n        final Iterator it = new Iterator(data, cardinality, Integer.MAX_VALUE, SINGLE_ZERO_BUFFER, SINGLE_ZERO_BUFFER);\n        assert it.in.getPosition() == 0;\n        assert it.wordNum == -1;\n        for (int i = 1; i < valueCount; ++i) {\n          // skip indexInterval sequences\n          for (int j = 0; j < indexInterval; ++j) {\n            final boolean readSequence = it.readSequence();\n            assert readSequence;\n            it.skipDirtyBytes();\n          }\n          final int position = it.in.getPosition();\n          final int wordNum = it.wordNum;\n          positions.add(position);\n          wordNums.add(wordNum + 1);\n        }\n        positions.freeze();\n        wordNums.freeze();\n        indexPositions = positions;\n        indexWordNums = wordNums;\n      }\n\n      return new WAH8DocIdSet(data, cardinality, indexInterval, indexPositions, indexWordNums);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0557ea16725aaafcd3dd3d3ec90445ff1ce22eb8","date":1412674810,"type":4,"author":"Adrien Grand","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/util/WAH8DocIdSet.WordBuilder#build().mjava","sourceNew":null,"sourceOld":"    /** Build a new {@link WAH8DocIdSet}. */\n    public WAH8DocIdSet build() {\n      if (cardinality == 0) {\n        assert lastWordNum == -1;\n        return EMPTY;\n      }\n      writeSequence();\n      final byte[] data = Arrays.copyOf(out.bytes, out.length);\n\n      // Now build the index\n      final int valueCount = (numSequences - 1) / indexInterval + 1;\n      final PackedLongValues indexPositions, indexWordNums;\n      if (valueCount <= 1) {\n        indexPositions = indexWordNums = SINGLE_ZERO;\n      } else {\n        final int pageSize = 128;\n        final PackedLongValues.Builder positions = PackedLongValues.monotonicBuilder(pageSize, PackedInts.COMPACT);\n        final PackedLongValues.Builder wordNums = PackedLongValues.monotonicBuilder(pageSize, PackedInts.COMPACT);\n\n        positions.add(0L);\n        wordNums.add(0L);\n        final Iterator it = new Iterator(data, cardinality, Integer.MAX_VALUE, SINGLE_ZERO, SINGLE_ZERO);\n        assert it.in.getPosition() == 0;\n        assert it.wordNum == -1;\n        for (int i = 1; i < valueCount; ++i) {\n          // skip indexInterval sequences\n          for (int j = 0; j < indexInterval; ++j) {\n            final boolean readSequence = it.readSequence();\n            assert readSequence;\n            it.skipDirtyBytes();\n          }\n          final int position = it.in.getPosition();\n          final int wordNum = it.wordNum;\n          positions.add(position);\n          wordNums.add(wordNum + 1);\n        }\n        indexPositions = positions.build();\n        indexWordNums = wordNums.build();\n      }\n\n      return new WAH8DocIdSet(data, cardinality, indexInterval, indexPositions, indexWordNums);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55980207f1977bd1463465de1659b821347e2fa8","date":1413336386,"type":4,"author":"Robert Muir","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/util/WAH8DocIdSet.WordBuilder#build().mjava","sourceNew":null,"sourceOld":"    /** Build a new {@link WAH8DocIdSet}. */\n    public WAH8DocIdSet build() {\n      if (cardinality == 0) {\n        assert lastWordNum == -1;\n        return EMPTY;\n      }\n      writeSequence();\n      final byte[] data = Arrays.copyOf(out.bytes, out.length);\n\n      // Now build the index\n      final int valueCount = (numSequences - 1) / indexInterval + 1;\n      final PackedLongValues indexPositions, indexWordNums;\n      if (valueCount <= 1) {\n        indexPositions = indexWordNums = SINGLE_ZERO;\n      } else {\n        final int pageSize = 128;\n        final PackedLongValues.Builder positions = PackedLongValues.monotonicBuilder(pageSize, PackedInts.COMPACT);\n        final PackedLongValues.Builder wordNums = PackedLongValues.monotonicBuilder(pageSize, PackedInts.COMPACT);\n\n        positions.add(0L);\n        wordNums.add(0L);\n        final Iterator it = new Iterator(data, cardinality, Integer.MAX_VALUE, SINGLE_ZERO, SINGLE_ZERO);\n        assert it.in.getPosition() == 0;\n        assert it.wordNum == -1;\n        for (int i = 1; i < valueCount; ++i) {\n          // skip indexInterval sequences\n          for (int j = 0; j < indexInterval; ++j) {\n            final boolean readSequence = it.readSequence();\n            assert readSequence;\n            it.skipDirtyBytes();\n          }\n          final int position = it.in.getPosition();\n          final int wordNum = it.wordNum;\n          positions.add(position);\n          wordNums.add(wordNum + 1);\n        }\n        indexPositions = positions.build();\n        indexWordNums = wordNums.build();\n      }\n\n      return new WAH8DocIdSet(data, cardinality, indexInterval, indexPositions, indexWordNums);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"794214a4691ccb0a156a9ea0dfd8663ab9f05bb4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"55980207f1977bd1463465de1659b821347e2fa8":["aae6236deecc1bf344f9c22d8d9dd09ef6701dbd","0557ea16725aaafcd3dd3d3ec90445ff1ce22eb8"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","6713d909dc80e8c53878ff98bb5376dc1af95956"],"0557ea16725aaafcd3dd3d3ec90445ff1ce22eb8":["aae6236deecc1bf344f9c22d8d9dd09ef6701dbd"],"cb240aebd5a347d79f642127ad9255dd9a979f06":["6713d909dc80e8c53878ff98bb5376dc1af95956"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"21fd82f9528d0f0c70205bd925dd0b0dce14fdab":["cb240aebd5a347d79f642127ad9255dd9a979f06"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["37a0f60745e53927c4c876cfe5b5a58170f0646c"],"6713d909dc80e8c53878ff98bb5376dc1af95956":["794214a4691ccb0a156a9ea0dfd8663ab9f05bb4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0557ea16725aaafcd3dd3d3ec90445ff1ce22eb8"],"aae6236deecc1bf344f9c22d8d9dd09ef6701dbd":["21fd82f9528d0f0c70205bd925dd0b0dce14fdab"]},"commit2Childs":{"794214a4691ccb0a156a9ea0dfd8663ab9f05bb4":["6713d909dc80e8c53878ff98bb5376dc1af95956"],"55980207f1977bd1463465de1659b821347e2fa8":[],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee"],"0557ea16725aaafcd3dd3d3ec90445ff1ce22eb8":["55980207f1977bd1463465de1659b821347e2fa8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["794214a4691ccb0a156a9ea0dfd8663ab9f05bb4","37a0f60745e53927c4c876cfe5b5a58170f0646c"],"cb240aebd5a347d79f642127ad9255dd9a979f06":["21fd82f9528d0f0c70205bd925dd0b0dce14fdab"],"21fd82f9528d0f0c70205bd925dd0b0dce14fdab":["aae6236deecc1bf344f9c22d8d9dd09ef6701dbd"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":[],"6713d909dc80e8c53878ff98bb5376dc1af95956":["37a0f60745e53927c4c876cfe5b5a58170f0646c","cb240aebd5a347d79f642127ad9255dd9a979f06"],"aae6236deecc1bf344f9c22d8d9dd09ef6701dbd":["55980207f1977bd1463465de1659b821347e2fa8","0557ea16725aaafcd3dd3d3ec90445ff1ce22eb8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["55980207f1977bd1463465de1659b821347e2fa8","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}