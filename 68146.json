{"path":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try {\n      termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    } catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try {\n      termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    } catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30fd30bfbfa6b9e036bcd99c8339712e965d4a63","date":1351859294,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try {\n      termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    } catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try {\n      termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    } catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5","date":1379624229,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash); postings use\n  // this API.\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try {\n      termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    } catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash)\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try {\n      termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    } catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"426a4760316fc52cf79e191cadfcb328dfc2d1ca","date":1394042725,"type":3,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash); postings use\n  // this API.\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try {\n      termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    } catch (MaxBytesLengthExceededException e) {\n      // Term is too large; record this here (can't throw an\n      // exc because DocInverterPerField will then abort the\n      // entire segment) and then throw an exc later in\n      // DocInverterPerField.java.  LengthFilter can always be\n      // used to prune the term before indexing:\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash); postings use\n  // this API.\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try {\n      termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    } catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"96ea64d994d340044e0d57aeb6a5871539d10ca5","date":1394225445,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash); postings use\n  // this API.\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try {\n      termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    } catch (MaxBytesLengthExceededException e) {\n      // Term is too large; record this here (can't throw an\n      // exc because DocInverterPerField will then abort the\n      // entire segment) and then throw an exc later in\n      // DocInverterPerField.java.  LengthFilter can always be\n      // used to prune the term before indexing:\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash); postings use\n  // this API.\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try {\n      termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    } catch (MaxBytesLengthExceededException e) {\n      // Not enough room in current block\n      // Just skip this term, to remain as robust as\n      // possible during indexing.  A TokenFilter\n      // can be inserted into the analyzer chain if\n      // other behavior is wanted (pruning the term\n      // to a prefix, throwing an exception, etc).\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30fe9fa09df804ce770f1b667401a7a7647301ed","date":1397554534,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  // Primary entry point (for first TermsHash); postings use\n  // this API.\n  @Override\n  void add() throws IOException {\n\n    termAtt.fillBytesRef();\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try {\n      termID = bytesHash.add(termBytesRef);\n    } catch (MaxBytesLengthExceededException e) {\n      // Term is too large; record this here (can't throw an\n      // exc because DocInverterPerField will then abort the\n      // entire segment) and then throw an exc later in\n      // DocInverterPerField.java.  LengthFilter can always be\n      // used to prune the term before indexing:\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash); postings use\n  // this API.\n  @Override\n  void add() throws IOException {\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try {\n      termID = bytesHash.add(termBytesRef, termAtt.fillBytesRef());\n    } catch (MaxBytesLengthExceededException e) {\n      // Term is too large; record this here (can't throw an\n      // exc because DocInverterPerField will then abort the\n      // entire segment) and then throw an exc later in\n      // DocInverterPerField.java.  LengthFilter can always be\n      // used to prune the term before indexing:\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"52c7e49be259508735752fba88085255014a6ecf","date":1398706273,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  /** Called once per inverted token.  This is the primary\n   *  entry point (for first TermsHash); postings use this\n   *  API. */\n  void add() throws IOException {\n\n    termAtt.fillBytesRef();\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID = bytesHash.add(termBytesRef);\n      \n    //System.out.println(\"add term=\" + termBytesRef.utf8ToString() + \" doc=\" + docState.docID + \" termID=\" + termID);\n\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      addTerm(termID);\n    }\n\n    if (doNextCall) {\n      nextPerField.add(postingsArray.textStarts[termID]);\n    }\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash); postings use\n  // this API.\n  @Override\n  void add() throws IOException {\n\n    termAtt.fillBytesRef();\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try {\n      termID = bytesHash.add(termBytesRef);\n    } catch (MaxBytesLengthExceededException e) {\n      // Term is too large; record this here (can't throw an\n      // exc because DocInverterPerField will then abort the\n      // entire segment) and then throw an exc later in\n      // DocInverterPerField.java.  LengthFilter can always be\n      // used to prune the term before indexing:\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3394716f52b34ab259ad5247e7595d9f9db6e935","date":1398791921,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  /** Called once per inverted token.  This is the primary\n   *  entry point (for first TermsHash); postings use this\n   *  API. */\n  void add() throws IOException {\n\n    termAtt.fillBytesRef();\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID = bytesHash.add(termBytesRef);\n      \n    //System.out.println(\"add term=\" + termBytesRef.utf8ToString() + \" doc=\" + docState.docID + \" termID=\" + termID);\n\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      addTerm(termID);\n    }\n\n    if (doNextCall) {\n      nextPerField.add(postingsArray.textStarts[termID]);\n    }\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash); postings use\n  // this API.\n  @Override\n  void add() throws IOException {\n\n    termAtt.fillBytesRef();\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try {\n      termID = bytesHash.add(termBytesRef);\n    } catch (MaxBytesLengthExceededException e) {\n      // Term is too large; record this here (can't throw an\n      // exc because DocInverterPerField will then abort the\n      // entire segment) and then throw an exc later in\n      // DocInverterPerField.java.  LengthFilter can always be\n      // used to prune the term before indexing:\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","date":1398844771,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  /** Called once per inverted token.  This is the primary\n   *  entry point (for first TermsHash); postings use this\n   *  API. */\n  void add() throws IOException {\n\n    termAtt.fillBytesRef();\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID = bytesHash.add(termBytesRef);\n      \n    //System.out.println(\"add term=\" + termBytesRef.utf8ToString() + \" doc=\" + docState.docID + \" termID=\" + termID);\n\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      addTerm(termID);\n    }\n\n    if (doNextCall) {\n      nextPerField.add(postingsArray.textStarts[termID]);\n    }\n  }\n\n","sourceOld":"  // Primary entry point (for first TermsHash); postings use\n  // this API.\n  @Override\n  void add() throws IOException {\n\n    termAtt.fillBytesRef();\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID;\n    try {\n      termID = bytesHash.add(termBytesRef);\n    } catch (MaxBytesLengthExceededException e) {\n      // Term is too large; record this here (can't throw an\n      // exc because DocInverterPerField will then abort the\n      // entire segment) and then throw an exc later in\n      // DocInverterPerField.java.  LengthFilter can always be\n      // used to prune the term before indexing:\n      if (docState.maxTermPrefix == null) {\n        final int saved = termBytesRef.length;\n        try {\n          termBytesRef.length = Math.min(30, DocumentsWriterPerThread.MAX_TERM_LENGTH_UTF8);\n          docState.maxTermPrefix = termBytesRef.toString();\n        } finally {\n          termBytesRef.length = saved;\n        }\n      }\n      consumer.skippingLongTerm();\n      return;\n    }\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      final int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n\n    if (doNextCall)\n      nextPerField.add(postingsArray.textStarts[termID]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"804b857d1066ab5185b3b9101bde41b0b71426ec","date":1435846169,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":"  /** Called once per inverted token.  This is the primary\n   *  entry point (for first TermsHash); postings use this\n   *  API. */\n  void add() throws IOException {\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID = bytesHash.add(termAtt.getBytesRef());\n      \n    //System.out.println(\"add term=\" + termBytesRef.utf8ToString() + \" doc=\" + docState.docID + \" termID=\" + termID);\n\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      addTerm(termID);\n    }\n\n    if (doNextCall) {\n      nextPerField.add(postingsArray.textStarts[termID]);\n    }\n  }\n\n","sourceOld":"  /** Called once per inverted token.  This is the primary\n   *  entry point (for first TermsHash); postings use this\n   *  API. */\n  void add() throws IOException {\n\n    termAtt.fillBytesRef();\n\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID = bytesHash.add(termBytesRef);\n      \n    //System.out.println(\"add term=\" + termBytesRef.utf8ToString() + \" doc=\" + docState.docID + \" termID=\" + termID);\n\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      addTerm(termID);\n    }\n\n    if (doNextCall) {\n      nextPerField.add(postingsArray.textStarts[termID]);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3cc3fa1ecad75b99ec55169e44628808f9866ad","date":1592311545,"type":4,"author":"Simon Willnauer","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add().mjava","sourceNew":null,"sourceOld":"  /** Called once per inverted token.  This is the primary\n   *  entry point (for first TermsHash); postings use this\n   *  API. */\n  void add() throws IOException {\n    // We are first in the chain so we must \"intern\" the\n    // term text into textStart address\n    // Get the text & hash of this term.\n    int termID = bytesHash.add(termAtt.getBytesRef());\n      \n    //System.out.println(\"add term=\" + termBytesRef.utf8ToString() + \" doc=\" + docState.docID + \" termID=\" + termID);\n\n    if (termID >= 0) {// New posting\n      bytesHash.byteStart(termID);\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {\n        intPool.nextBuffer();\n      }\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & IntBlockPool.INT_BLOCK_MASK;\n      addTerm(termID);\n    }\n\n    if (doNextCall) {\n      nextPerField.add(postingsArray.textStarts[termID]);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"804b857d1066ab5185b3b9101bde41b0b71426ec":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"30fe9fa09df804ce770f1b667401a7a7647301ed":["426a4760316fc52cf79e191cadfcb328dfc2d1ca"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5","426a4760316fc52cf79e191cadfcb328dfc2d1ca"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":["30fe9fa09df804ce770f1b667401a7a7647301ed","3394716f52b34ab259ad5247e7595d9f9db6e935"],"426a4760316fc52cf79e191cadfcb328dfc2d1ca":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["30fe9fa09df804ce770f1b667401a7a7647301ed","52c7e49be259508735752fba88085255014a6ecf"],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["30fd30bfbfa6b9e036bcd99c8339712e965d4a63"],"30fd30bfbfa6b9e036bcd99c8339712e965d4a63":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"52c7e49be259508735752fba88085255014a6ecf":["30fe9fa09df804ce770f1b667401a7a7647301ed"],"d3cc3fa1ecad75b99ec55169e44628808f9866ad":["804b857d1066ab5185b3b9101bde41b0b71426ec"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d3cc3fa1ecad75b99ec55169e44628808f9866ad"]},"commit2Childs":{"804b857d1066ab5185b3b9101bde41b0b71426ec":["d3cc3fa1ecad75b99ec55169e44628808f9866ad"],"30fe9fa09df804ce770f1b667401a7a7647301ed":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","3394716f52b34ab259ad5247e7595d9f9db6e935","52c7e49be259508735752fba88085255014a6ecf"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":[],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":[],"426a4760316fc52cf79e191cadfcb328dfc2d1ca":["30fe9fa09df804ce770f1b667401a7a7647301ed","96ea64d994d340044e0d57aeb6a5871539d10ca5"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["30fd30bfbfa6b9e036bcd99c8339712e965d4a63"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["804b857d1066ab5185b3b9101bde41b0b71426ec","c0cd85fde84cb318b4dc97710dcf15e2959a1bbe"],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["96ea64d994d340044e0d57aeb6a5871539d10ca5","426a4760316fc52cf79e191cadfcb328dfc2d1ca"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"30fd30bfbfa6b9e036bcd99c8339712e965d4a63":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5"],"52c7e49be259508735752fba88085255014a6ecf":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"d3cc3fa1ecad75b99ec55169e44628808f9866ad":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["96ea64d994d340044e0d57aeb6a5871539d10ca5","c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}