{"path":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","pathOld":"src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","sourceNew":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream = analyzer.tokenStream(\"\", new StringReader(queryString));\n      if (stream != null)\n      {\n        List<String> terms = new ArrayList<String>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          TermAttribute termAtt = stream.addAttribute(TermAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          while (hasMoreTokens) {\n            terms.add(termAtt.term());\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new String[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","sourceOld":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream = analyzer.tokenStream(\"\", new StringReader(queryString));\n      if (stream != null)\n      {\n        List<String> terms = new ArrayList<String>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          TermAttribute termAtt = stream.addAttribute(TermAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          while (hasMoreTokens) {\n            terms.add(termAtt.term());\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new String[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a775c547c3519b47efd41c09cb47100ddb9604c7","date":1270914087,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","sourceNew":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream = analyzer.tokenStream(\"\", new StringReader(queryString));\n      if (stream != null)\n      {\n        List<String> terms = new ArrayList<String>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          final CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          while (hasMoreTokens) {\n            terms.add(termAtt.toString());\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new String[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","sourceOld":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream = analyzer.tokenStream(\"\", new StringReader(queryString));\n      if (stream != null)\n      {\n        List<String> terms = new ArrayList<String>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          TermAttribute termAtt = stream.addAttribute(TermAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          while (hasMoreTokens) {\n            terms.add(termAtt.term());\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new String[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4f29ba80b723649f5feb7e37afe1a558dd2c1304","date":1278318805,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","sourceNew":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream = analyzer.tokenStream(\"\", new StringReader(queryString));\n      if (stream != null)\n      {\n        List<BytesRef> terms = new ArrayList<BytesRef>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          final TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          while (hasMoreTokens) {\n            BytesRef bytes = new BytesRef();\n            termAtt.toBytesRef(bytes);\n            terms.add(bytes);\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new BytesRef[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","sourceOld":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream = analyzer.tokenStream(\"\", new StringReader(queryString));\n      if (stream != null)\n      {\n        List<String> terms = new ArrayList<String>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          final CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          while (hasMoreTokens) {\n            terms.add(termAtt.toString());\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new String[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","sourceNew":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream = analyzer.tokenStream(\"\", new StringReader(queryString));\n      if (stream != null)\n      {\n        List<BytesRef> terms = new ArrayList<BytesRef>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          final TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          while (hasMoreTokens) {\n            BytesRef bytes = new BytesRef();\n            termAtt.toBytesRef(bytes);\n            terms.add(bytes);\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new BytesRef[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","sourceOld":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream = analyzer.tokenStream(\"\", new StringReader(queryString));\n      if (stream != null)\n      {\n        List<String> terms = new ArrayList<String>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          final CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          while (hasMoreTokens) {\n            terms.add(termAtt.toString());\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new String[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3d07f1ae3b58102f36f3393c397d78ba4e547a4","date":1300715535,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","sourceNew":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream = analyzer.tokenStream(\"\", new StringReader(queryString));\n      if (stream != null)\n      {\n        List<BytesRef> terms = new ArrayList<BytesRef>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          final TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          BytesRef bytes = termAtt.getBytesRef();\n          while (hasMoreTokens) {\n            termAtt.fillBytesRef();\n            terms.add(new BytesRef(bytes));\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new BytesRef[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","sourceOld":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream = analyzer.tokenStream(\"\", new StringReader(queryString));\n      if (stream != null)\n      {\n        List<BytesRef> terms = new ArrayList<BytesRef>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          final TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          while (hasMoreTokens) {\n            BytesRef bytes = new BytesRef();\n            termAtt.toBytesRef(bytes);\n            terms.add(bytes);\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new BytesRef[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","sourceNew":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream = analyzer.tokenStream(\"\", new StringReader(queryString));\n      if (stream != null)\n      {\n        List<BytesRef> terms = new ArrayList<BytesRef>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          final TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          BytesRef bytes = termAtt.getBytesRef();\n          while (hasMoreTokens) {\n            termAtt.fillBytesRef();\n            terms.add(new BytesRef(bytes));\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new BytesRef[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","sourceOld":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream = analyzer.tokenStream(\"\", new StringReader(queryString));\n      if (stream != null)\n      {\n        List<BytesRef> terms = new ArrayList<BytesRef>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          final TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          while (hasMoreTokens) {\n            BytesRef bytes = new BytesRef();\n            termAtt.toBytesRef(bytes);\n            terms.add(bytes);\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new BytesRef[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0ef0193974807e4bddf5432a6b0287fe4d6c9df","date":1301476645,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","sourceNew":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream = analyzer.tokenStream(\"\", new StringReader(queryString));\n      if (stream != null)\n      {\n        List<BytesRef> terms = new ArrayList<BytesRef>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          final TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          BytesRef bytes = termAtt.getBytesRef();\n          while (hasMoreTokens) {\n            termAtt.fillBytesRef();\n            terms.add(new BytesRef(bytes));\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new BytesRef[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","sourceOld":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream = analyzer.tokenStream(\"\", new StringReader(queryString));\n      if (stream != null)\n      {\n        List<BytesRef> terms = new ArrayList<BytesRef>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          final TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          while (hasMoreTokens) {\n            BytesRef bytes = new BytesRef();\n            termAtt.toBytesRef(bytes);\n            terms.add(bytes);\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new BytesRef[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"00746ad002a54281629e3b6f3eb39833a33f093e","date":1305306799,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","sourceNew":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream;\n      try {\n        stream = analyzer.reusableTokenStream(\"\", new StringReader(queryString));\n      } catch (IOException e1) {\n        stream = null;\n      }\n      if (stream != null)\n      {\n        List<BytesRef> terms = new ArrayList<BytesRef>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          final TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          BytesRef bytes = termAtt.getBytesRef();\n          while (hasMoreTokens) {\n            termAtt.fillBytesRef();\n            terms.add(new BytesRef(bytes));\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new BytesRef[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","sourceOld":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream = analyzer.tokenStream(\"\", new StringReader(queryString));\n      if (stream != null)\n      {\n        List<BytesRef> terms = new ArrayList<BytesRef>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          final TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          BytesRef bytes = termAtt.getBytesRef();\n          while (hasMoreTokens) {\n            termAtt.fillBytesRef();\n            terms.add(new BytesRef(bytes));\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new BytesRef[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c3a8a449466c1ff7ce2274fe73dab487256964b4","date":1305735867,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","sourceNew":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream;\n      try {\n        stream = analyzer.reusableTokenStream(\"\", new StringReader(queryString));\n      } catch (IOException e1) {\n        stream = null;\n      }\n      if (stream != null)\n      {\n        List<BytesRef> terms = new ArrayList<BytesRef>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          final TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          BytesRef bytes = termAtt.getBytesRef();\n          while (hasMoreTokens) {\n            termAtt.fillBytesRef();\n            terms.add(new BytesRef(bytes));\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new BytesRef[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","sourceOld":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream = analyzer.tokenStream(\"\", new StringReader(queryString));\n      if (stream != null)\n      {\n        List<BytesRef> terms = new ArrayList<BytesRef>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          final TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          BytesRef bytes = termAtt.getBytesRef();\n          while (hasMoreTokens) {\n            termAtt.fillBytesRef();\n            terms.add(new BytesRef(bytes));\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new BytesRef[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","sourceNew":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream;\n      try {\n        stream = analyzer.reusableTokenStream(\"\", new StringReader(queryString));\n      } catch (IOException e1) {\n        stream = null;\n      }\n      if (stream != null)\n      {\n        List<BytesRef> terms = new ArrayList<BytesRef>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          final TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          BytesRef bytes = termAtt.getBytesRef();\n          while (hasMoreTokens) {\n            termAtt.fillBytesRef();\n            terms.add(new BytesRef(bytes));\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new BytesRef[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","sourceOld":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream = analyzer.tokenStream(\"\", new StringReader(queryString));\n      if (stream != null)\n      {\n        List<BytesRef> terms = new ArrayList<BytesRef>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          final TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          BytesRef bytes = termAtt.getBytesRef();\n          while (hasMoreTokens) {\n            termAtt.fillBytesRef();\n            terms.add(new BytesRef(bytes));\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new BytesRef[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"69e043c521d4e8db770cc140c63f5ef51f03426a","date":1317187614,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","sourceNew":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream;\n      try {\n        stream = analyzer.tokenStream(\"\", new StringReader(queryString));\n      } catch (IOException e1) {\n        stream = null;\n      }\n      if (stream != null)\n      {\n        List<BytesRef> terms = new ArrayList<BytesRef>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          final TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          BytesRef bytes = termAtt.getBytesRef();\n          while (hasMoreTokens) {\n            termAtt.fillBytesRef();\n            terms.add(new BytesRef(bytes));\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new BytesRef[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","sourceOld":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream;\n      try {\n        stream = analyzer.reusableTokenStream(\"\", new StringReader(queryString));\n      } catch (IOException e1) {\n        stream = null;\n      }\n      if (stream != null)\n      {\n        List<BytesRef> terms = new ArrayList<BytesRef>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          final TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          BytesRef bytes = termAtt.getBytesRef();\n          while (hasMoreTokens) {\n            termAtt.fillBytesRef();\n            terms.add(new BytesRef(bytes));\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new BytesRef[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/search/QueryTermVector#QueryTermVector(String,Analyzer).mjava","sourceNew":null,"sourceOld":"  public QueryTermVector(String queryString, Analyzer analyzer) {    \n    if (analyzer != null)\n    {\n      TokenStream stream;\n      try {\n        stream = analyzer.tokenStream(\"\", new StringReader(queryString));\n      } catch (IOException e1) {\n        stream = null;\n      }\n      if (stream != null)\n      {\n        List<BytesRef> terms = new ArrayList<BytesRef>();\n        try {\n          boolean hasMoreTokens = false;\n          \n          stream.reset(); \n          final TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n\n          hasMoreTokens = stream.incrementToken();\n          BytesRef bytes = termAtt.getBytesRef();\n          while (hasMoreTokens) {\n            termAtt.fillBytesRef();\n            terms.add(new BytesRef(bytes));\n            hasMoreTokens = stream.incrementToken();\n          }\n          processTerms(terms.toArray(new BytesRef[terms.size()]));\n        } catch (IOException e) {\n        }\n      }\n    }                                                              \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["a775c547c3519b47efd41c09cb47100ddb9604c7"],"3cc749c053615f5871f3b95715fe292f34e70a53":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"5f4e87790277826a2aea119328600dfb07761f32":["a775c547c3519b47efd41c09cb47100ddb9604c7","4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"a775c547c3519b47efd41c09cb47100ddb9604c7":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["5f4e87790277826a2aea119328600dfb07761f32","b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"c3a8a449466c1ff7ce2274fe73dab487256964b4":["d619839baa8ce5503e496b94a9e42ad6f079293f","00746ad002a54281629e3b6f3eb39833a33f093e"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["4f29ba80b723649f5feb7e37afe1a558dd2c1304","b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"a3776dccca01c11e7046323cfad46a3b4a471233":["b3d07f1ae3b58102f36f3393c397d78ba4e547a4","00746ad002a54281629e3b6f3eb39833a33f093e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"00746ad002a54281629e3b6f3eb39833a33f093e":["b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"b3d07f1ae3b58102f36f3393c397d78ba4e547a4":["4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"69e043c521d4e8db770cc140c63f5ef51f03426a":["00746ad002a54281629e3b6f3eb39833a33f093e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3cc749c053615f5871f3b95715fe292f34e70a53"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["5f4e87790277826a2aea119328600dfb07761f32","d619839baa8ce5503e496b94a9e42ad6f079293f","b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"3cc749c053615f5871f3b95715fe292f34e70a53":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5f4e87790277826a2aea119328600dfb07761f32":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"a775c547c3519b47efd41c09cb47100ddb9604c7":["4f29ba80b723649f5feb7e37afe1a558dd2c1304","5f4e87790277826a2aea119328600dfb07761f32"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":[],"c3a8a449466c1ff7ce2274fe73dab487256964b4":[],"d619839baa8ce5503e496b94a9e42ad6f079293f":["c3a8a449466c1ff7ce2274fe73dab487256964b4"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"b3d07f1ae3b58102f36f3393c397d78ba4e547a4":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","d619839baa8ce5503e496b94a9e42ad6f079293f","a3776dccca01c11e7046323cfad46a3b4a471233","00746ad002a54281629e3b6f3eb39833a33f093e"],"00746ad002a54281629e3b6f3eb39833a33f093e":["c3a8a449466c1ff7ce2274fe73dab487256964b4","a3776dccca01c11e7046323cfad46a3b4a471233","69e043c521d4e8db770cc140c63f5ef51f03426a"],"69e043c521d4e8db770cc140c63f5ef51f03426a":["3cc749c053615f5871f3b95715fe292f34e70a53"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a775c547c3519b47efd41c09cb47100ddb9604c7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","c3a8a449466c1ff7ce2274fe73dab487256964b4","a3776dccca01c11e7046323cfad46a3b4a471233","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}