{"path":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","commits":[{"id":"9532290bd66e70c1787e80aae9fb0427ce080194","date":1155764538,"type":0,"author":"Mark Harwood","isMerge":false,"pathNew":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"/dev/null","sourceNew":"\tpublic void testMaxSizeHighlightTruncates() throws IOException \n\t{\n\t    String goodWord=\"goodtoken\";\n\t    String stopWords[]={\"stoppedtoken\"};\n\t    \n\t    TermQuery query= new TermQuery( new Term( \"data\", goodWord ));\n\t    SimpleHTMLFormatter fm=new SimpleHTMLFormatter();\n\t    Highlighter hg = new Highlighter(fm, new QueryScorer( query ));\n\t    hg.setTextFragmenter( new NullFragmenter() );\n\n\t    String match = null;\n\t    StringBuffer sb=new StringBuffer();\n\t    sb.append(goodWord);\n\t    for(int i=0;i<10000;i++)\n\t    {\n\t    \tsb.append(\" \");\n\t    \tsb.append(stopWords[0]);\n\t    }\n\t    \t    \t\n\t    hg.setMaxDocBytesToAnalyze(100);\n\t    match = hg.getBestFragment( new StandardAnalyzer(stopWords), \"data\", sb.toString());\n\t    assertTrue(\"Matched text should be no more than 100 chars in length \", \n\t    \t\tmatch.length()<hg.getMaxDocBytesToAnalyze());\n\t    \n\t    //add another tokenized word to the overrall length - but set way beyond \n\t    //the length of text under consideration (after a large slug of stop words + whitespace)\n\t    sb.append(\" \");\n\t    sb.append(goodWord);\n\t    match = hg.getBestFragment( new StandardAnalyzer(stopWords), \"data\", sb.toString());\n\t    assertTrue(\"Matched text should be no more than 100 chars in length \", \n\t    \t\tmatch.length()<hg.getMaxDocBytesToAnalyze());\n\t    \n\t    \n\t} \t\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"296b8b38a87feb478921f77834a2302dfe77641c","date":1209506838,"type":3,"author":"Mark Harwood","isMerge":false,"pathNew":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        String stopWords[] = { \"stoppedtoken\" };\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuffer sb = new StringBuffer();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          sb.append(stopWords[0]);\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocBytesToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"\tpublic void testMaxSizeHighlightTruncates() throws IOException \n\t{\n\t    String goodWord=\"goodtoken\";\n\t    String stopWords[]={\"stoppedtoken\"};\n\t    \n\t    TermQuery query= new TermQuery( new Term( \"data\", goodWord ));\n\t    SimpleHTMLFormatter fm=new SimpleHTMLFormatter();\n\t    Highlighter hg = new Highlighter(fm, new QueryScorer( query ));\n\t    hg.setTextFragmenter( new NullFragmenter() );\n\n\t    String match = null;\n\t    StringBuffer sb=new StringBuffer();\n\t    sb.append(goodWord);\n\t    for(int i=0;i<10000;i++)\n\t    {\n\t    \tsb.append(\" \");\n\t    \tsb.append(stopWords[0]);\n\t    }\n\t    \t    \t\n\t    hg.setMaxDocBytesToAnalyze(100);\n\t    match = hg.getBestFragment( new StandardAnalyzer(stopWords), \"data\", sb.toString());\n\t    assertTrue(\"Matched text should be no more than 100 chars in length \", \n\t    \t\tmatch.length()<hg.getMaxDocBytesToAnalyze());\n\t    \n\t    //add another tokenized word to the overrall length - but set way beyond \n\t    //the length of text under consideration (after a large slug of stop words + whitespace)\n\t    sb.append(\" \");\n\t    sb.append(goodWord);\n\t    match = hg.getBestFragment( new StandardAnalyzer(stopWords), \"data\", sb.toString());\n\t    assertTrue(\"Matched text should be no more than 100 chars in length \", \n\t    \t\tmatch.length()<hg.getMaxDocBytesToAnalyze());\n\t    \n\t    \n\t} \t\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c881464041e282c06fdb34e91f883b83b8d97968","date":1247607562,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set stopWords = new HashSet(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuffer sb = new StringBuffer();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocBytesToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        String stopWords[] = { \"stoppedtoken\" };\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuffer sb = new StringBuffer();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          sb.append(stopWords[0]);\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocBytesToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"943c3f9cf96b8df37f4273d66a66182e2a669467","date":1249394171,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set stopWords = new HashSet(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuffer sb = new StringBuffer();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocBytesToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set stopWords = new HashSet(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuffer sb = new StringBuffer();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocBytesToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4625cb7ffd7c9caaf2d62b206ba9a382d68da82c","date":1254521470,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set stopWords = new HashSet(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocBytesToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set stopWords = new HashSet(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuffer sb = new StringBuffer();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocBytesToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4b41b991de69ba7b72d5e90cfcee25699a1a7fc9","date":1256127131,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set stopWords = new HashSet(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(TEST_VERSION, stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocBytesToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set stopWords = new HashSet(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocBytesToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d57eb7c98c08c03af6e4cd83509df31c81ac16af","date":1257684312,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set stopWords = new HashSet(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(TEST_VERSION, stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocBytesToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set stopWords = new HashSet(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(TEST_VERSION, stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocBytesToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2aa9553aad4bb588f33e036ce51485a850a2917","date":1257895368,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set stopWords = new HashSet(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(TEST_VERSION, stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set stopWords = new HashSet(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(TEST_VERSION, stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocBytesToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocBytesToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7848880b3c06f09f0f3ac50d0854b16efb0b815e","date":1260006234,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set<String> stopWords = new HashSet<String>(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(TEST_VERSION, stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set stopWords = new HashSet(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(TEST_VERSION, stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1cedb00d2dd44640194401179358a2e3ba6051bf","date":1268243626,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set<String> stopWords = new HashSet<String>(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set<String> stopWords = new HashSet<String>(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(TEST_VERSION, stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e52fea2c4081a1e552b98506691990be59503168","date":1268250331,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set<String> stopWords = new HashSet<String>(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(TEST_VERSION, stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set<String> stopWords = new HashSet<String>(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8","date":1268494368,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set<String> stopWords = new HashSet<String>(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set<String> stopWords = new HashSet<String>(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(TEST_VERSION, stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","pathOld":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlightTruncates().mjava","sourceNew":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set<String> stopWords = new HashSet<String>(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","sourceOld":"  public void testMaxSizeHighlightTruncates() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        String goodWord = \"goodtoken\";\n        Set<String> stopWords = new HashSet<String>(1);\n        stopWords.add(\"stoppedtoken\");\n\n        TermQuery query = new TermQuery(new Term(\"data\", goodWord));\n\n        String match = null;\n        StringBuilder sb = new StringBuilder();\n        sb.append(goodWord);\n        for (int i = 0; i < 10000; i++) {\n          sb.append(\" \");\n          // only one stopword\n          sb.append(stopWords.iterator().next());\n        }\n        SimpleHTMLFormatter fm = new SimpleHTMLFormatter();\n        Highlighter hg = getHighlighter(query, \"data\", new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords).tokenStream(\n            \"data\", new StringReader(sb.toString())), fm);// new Highlighter(fm,\n        // new\n        // QueryTermScorer(query));\n        hg.setTextFragmenter(new NullFragmenter());\n        hg.setMaxDocCharsToAnalyze(100);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n\n        // add another tokenized word to the overrall length - but set way\n        // beyond\n        // the length of text under consideration (after a large slug of stop\n        // words\n        // + whitespace)\n        sb.append(\" \");\n        sb.append(goodWord);\n        match = hg.getBestFragment(new StandardAnalyzer(TEST_VERSION_CURRENT, stopWords), \"data\", sb.toString());\n        assertTrue(\"Matched text should be no more than 100 chars in length \", match.length() < hg\n            .getMaxDocCharsToAnalyze());\n      }\n    };\n\n    helper.start();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"9532290bd66e70c1787e80aae9fb0427ce080194":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"1cedb00d2dd44640194401179358a2e3ba6051bf":["7848880b3c06f09f0f3ac50d0854b16efb0b815e"],"c881464041e282c06fdb34e91f883b83b8d97968":["296b8b38a87feb478921f77834a2302dfe77641c"],"d57eb7c98c08c03af6e4cd83509df31c81ac16af":["4b41b991de69ba7b72d5e90cfcee25699a1a7fc9"],"e52fea2c4081a1e552b98506691990be59503168":["1cedb00d2dd44640194401179358a2e3ba6051bf"],"7848880b3c06f09f0f3ac50d0854b16efb0b815e":["f2aa9553aad4bb588f33e036ce51485a850a2917"],"943c3f9cf96b8df37f4273d66a66182e2a669467":["c881464041e282c06fdb34e91f883b83b8d97968"],"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8":["e52fea2c4081a1e552b98506691990be59503168"],"f2aa9553aad4bb588f33e036ce51485a850a2917":["d57eb7c98c08c03af6e4cd83509df31c81ac16af"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4b41b991de69ba7b72d5e90cfcee25699a1a7fc9":["4625cb7ffd7c9caaf2d62b206ba9a382d68da82c"],"4625cb7ffd7c9caaf2d62b206ba9a382d68da82c":["943c3f9cf96b8df37f4273d66a66182e2a669467"],"296b8b38a87feb478921f77834a2302dfe77641c":["9532290bd66e70c1787e80aae9fb0427ce080194"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8"]},"commit2Childs":{"9532290bd66e70c1787e80aae9fb0427ce080194":["296b8b38a87feb478921f77834a2302dfe77641c"],"1cedb00d2dd44640194401179358a2e3ba6051bf":["e52fea2c4081a1e552b98506691990be59503168"],"c881464041e282c06fdb34e91f883b83b8d97968":["943c3f9cf96b8df37f4273d66a66182e2a669467"],"d57eb7c98c08c03af6e4cd83509df31c81ac16af":["f2aa9553aad4bb588f33e036ce51485a850a2917"],"e52fea2c4081a1e552b98506691990be59503168":["84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8"],"7848880b3c06f09f0f3ac50d0854b16efb0b815e":["1cedb00d2dd44640194401179358a2e3ba6051bf"],"943c3f9cf96b8df37f4273d66a66182e2a669467":["4625cb7ffd7c9caaf2d62b206ba9a382d68da82c"],"f2aa9553aad4bb588f33e036ce51485a850a2917":["7848880b3c06f09f0f3ac50d0854b16efb0b815e"],"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9532290bd66e70c1787e80aae9fb0427ce080194"],"4b41b991de69ba7b72d5e90cfcee25699a1a7fc9":["d57eb7c98c08c03af6e4cd83509df31c81ac16af"],"4625cb7ffd7c9caaf2d62b206ba9a382d68da82c":["4b41b991de69ba7b72d5e90cfcee25699a1a7fc9"],"296b8b38a87feb478921f77834a2302dfe77641c":["c881464041e282c06fdb34e91f883b83b8d97968"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}