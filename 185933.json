{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","sourceNew":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    Analyzer a = new LimitTokenCountAnalyzer(new WhitespaceAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2     3  4  5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, 4);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n    \n    a = new LimitTokenCountAnalyzer(new StandardAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n  }\n\n","sourceOld":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    Analyzer a = new LimitTokenCountAnalyzer(new WhitespaceAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2     3  4  5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, 4);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n    \n    a = new LimitTokenCountAnalyzer(new StandardAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5009d80b96d7223e4120e40035a8b8c1d48cb134","date":1353482392,"type":3,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","sourceNew":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    for (boolean consumeAll : new boolean[] { true, false }) {\n      MockAnalyzer mock = new MockAnalyzer(random());\n\n      // if we are consuming all tokens, we can use the checks, \n      // otherwise we can't\n      mock.setEnableChecks(consumeAll);\n      Analyzer a = new LimitTokenCountAnalyzer(mock, 2, consumeAll);\n    \n      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2     3  4  5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);\n      \n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  \")), new String[] { \"1\" }, new int[] { 0 }, new int[] { 1 }, consumeAll ? 3 : null);\n    \n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2  \")), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);\n    }\n  }\n\n","sourceOld":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    Analyzer a = new LimitTokenCountAnalyzer(new WhitespaceAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2     3  4  5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, 4);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n    \n    a = new LimitTokenCountAnalyzer(new StandardAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n  }\n\n","bugFix":null,"bugIntro":["c83d6c4335f31cae14f625a222bc842f20073dcd"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"407687e67faf6e1f02a211ca078d8e3eed631027","date":1355157407,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","sourceNew":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    for (boolean consumeAll : new boolean[] { true, false }) {\n      MockAnalyzer mock = new MockAnalyzer(random());\n\n      // if we are consuming all tokens, we can use the checks, \n      // otherwise we can't\n      mock.setEnableChecks(consumeAll);\n      Analyzer a = new LimitTokenCountAnalyzer(mock, 2, consumeAll);\n    \n      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2     3  4  5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);\n      \n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  \")), new String[] { \"1\" }, new int[] { 0 }, new int[] { 1 }, consumeAll ? 3 : null);\n    \n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2  \")), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);\n    }\n  }\n\n","sourceOld":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    Analyzer a = new LimitTokenCountAnalyzer(new WhitespaceAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2     3  4  5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, 4);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n    \n    a = new LimitTokenCountAnalyzer(new StandardAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c83d6c4335f31cae14f625a222bc842f20073dcd","date":1373306148,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","sourceNew":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    for (boolean consumeAll : new boolean[] { true, false }) {\n      MockAnalyzer mock = new MockAnalyzer(random());\n\n      // if we are consuming all tokens, we can use the checks, \n      // otherwise we can't\n      mock.setEnableChecks(consumeAll);\n      Analyzer a = new LimitTokenCountAnalyzer(mock, 2, consumeAll);\n    \n      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1 2 3 4 5\"), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);\n      \n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"), new String[] { \"1\" }, new int[] { 0 }, new int[] { 1 }, consumeAll ? 3 : null);\n    \n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);\n    }\n  }\n\n","sourceOld":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    for (boolean consumeAll : new boolean[] { true, false }) {\n      MockAnalyzer mock = new MockAnalyzer(random());\n\n      // if we are consuming all tokens, we can use the checks, \n      // otherwise we can't\n      mock.setEnableChecks(consumeAll);\n      Analyzer a = new LimitTokenCountAnalyzer(mock, 2, consumeAll);\n    \n      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2     3  4  5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);\n      \n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  \")), new String[] { \"1\" }, new int[] { 0 }, new int[] { 1 }, consumeAll ? 3 : null);\n    \n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2  \")), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);\n    }\n  }\n\n","bugFix":["5009d80b96d7223e4120e40035a8b8c1d48cb134"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","sourceNew":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    for (boolean consumeAll : new boolean[] { true, false }) {\n      MockAnalyzer mock = new MockAnalyzer(random());\n\n      // if we are consuming all tokens, we can use the checks, \n      // otherwise we can't\n      mock.setEnableChecks(consumeAll);\n      Analyzer a = new LimitTokenCountAnalyzer(mock, 2, consumeAll);\n    \n      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1 2 3 4 5\"), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);\n      \n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"), new String[] { \"1\" }, new int[] { 0 }, new int[] { 1 }, consumeAll ? 3 : null);\n    \n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);\n    }\n  }\n\n","sourceOld":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    for (boolean consumeAll : new boolean[] { true, false }) {\n      MockAnalyzer mock = new MockAnalyzer(random());\n\n      // if we are consuming all tokens, we can use the checks, \n      // otherwise we can't\n      mock.setEnableChecks(consumeAll);\n      Analyzer a = new LimitTokenCountAnalyzer(mock, 2, consumeAll);\n    \n      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2     3  4  5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);\n      \n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  \")), new String[] { \"1\" }, new int[] { 0 }, new int[] { 1 }, consumeAll ? 3 : null);\n    \n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2  \")), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1","date":1419400138,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","sourceNew":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    for (boolean consumeAll : new boolean[] { true, false }) {\n      MockAnalyzer mock = new MockAnalyzer(random());\n\n      // if we are consuming all tokens, we can use the checks, \n      // otherwise we can't\n      mock.setEnableChecks(consumeAll);\n      Analyzer a = new LimitTokenCountAnalyzer(mock, 2, consumeAll);\n    \n      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case it's correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1 2 3 4 5\"), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);\n      \n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"), new String[] { \"1\" }, new int[] { 0 }, new int[] { 1 }, consumeAll ? 3 : null);\n    \n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);\n    }\n  }\n\n","sourceOld":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    for (boolean consumeAll : new boolean[] { true, false }) {\n      MockAnalyzer mock = new MockAnalyzer(random());\n\n      // if we are consuming all tokens, we can use the checks, \n      // otherwise we can't\n      mock.setEnableChecks(consumeAll);\n      Analyzer a = new LimitTokenCountAnalyzer(mock, 2, consumeAll);\n    \n      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1 2 3 4 5\"), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);\n      \n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"), new String[] { \"1\" }, new int[] { 0 }, new int[] { 1 }, consumeAll ? 3 : null);\n    \n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a56958d7f71a28824f20031ffbb2e13502a0274e","date":1425573902,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","sourceNew":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    for (boolean consumeAll : new boolean[] { true, false }) {\n      MockAnalyzer mock = new MockAnalyzer(random());\n\n      // if we are consuming all tokens, we can use the checks, \n      // otherwise we can't\n      mock.setEnableChecks(consumeAll);\n      Analyzer a = new LimitTokenCountAnalyzer(mock, 2, consumeAll);\n    \n      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case it's correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1 2 3 4 5\"), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);\n      \n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"), new String[] { \"1\" }, new int[] { 0 }, new int[] { 1 }, consumeAll ? 3 : null);\n    \n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);\n      a.close();\n    }\n  }\n\n","sourceOld":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    for (boolean consumeAll : new boolean[] { true, false }) {\n      MockAnalyzer mock = new MockAnalyzer(random());\n\n      // if we are consuming all tokens, we can use the checks, \n      // otherwise we can't\n      mock.setEnableChecks(consumeAll);\n      Analyzer a = new LimitTokenCountAnalyzer(mock, 2, consumeAll);\n    \n      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case it's correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1 2 3 4 5\"), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);\n      \n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"), new String[] { \"1\" }, new int[] { 0 }, new int[] { 1 }, consumeAll ? 3 : null);\n    \n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","sourceNew":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    for (boolean consumeAll : new boolean[] { true, false }) {\n      MockAnalyzer mock = new MockAnalyzer(random());\n\n      // if we are consuming all tokens, we can use the checks, \n      // otherwise we can't\n      mock.setEnableChecks(consumeAll);\n      Analyzer a = new LimitTokenCountAnalyzer(mock, 2, consumeAll);\n    \n      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case it's correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1 2 3 4 5\"), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);\n      \n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"), new String[] { \"1\" }, new int[] { 0 }, new int[] { 1 }, consumeAll ? 3 : null);\n    \n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);\n      a.close();\n    }\n  }\n\n","sourceOld":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    for (boolean consumeAll : new boolean[] { true, false }) {\n      MockAnalyzer mock = new MockAnalyzer(random());\n\n      // if we are consuming all tokens, we can use the checks, \n      // otherwise we can't\n      mock.setEnableChecks(consumeAll);\n      Analyzer a = new LimitTokenCountAnalyzer(mock, 2, consumeAll);\n    \n      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case it's correct)!\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2     3  4  5\"), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1 2 3 4 5\"), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);\n      \n      // less than the limit, ensure we behave correctly\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  \"), new String[] { \"1\" }, new int[] { 0 }, new int[] { 1 }, consumeAll ? 3 : null);\n    \n      // equal to limit\n      assertTokenStreamContents(a.tokenStream(\"dummy\", \"1  2  \"), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 6 : null);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1","a56958d7f71a28824f20031ffbb2e13502a0274e"],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["c83d6c4335f31cae14f625a222bc842f20073dcd"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["5009d80b96d7223e4120e40035a8b8c1d48cb134","c83d6c4335f31cae14f625a222bc842f20073dcd"],"c83d6c4335f31cae14f625a222bc842f20073dcd":["5009d80b96d7223e4120e40035a8b8c1d48cb134"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"407687e67faf6e1f02a211ca078d8e3eed631027":["b89678825b68eccaf09e6ab71675fc0b0af1e099","5009d80b96d7223e4120e40035a8b8c1d48cb134"],"a56958d7f71a28824f20031ffbb2e13502a0274e":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a56958d7f71a28824f20031ffbb2e13502a0274e"],"5009d80b96d7223e4120e40035a8b8c1d48cb134":["b89678825b68eccaf09e6ab71675fc0b0af1e099"]},"commit2Childs":{"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","a56958d7f71a28824f20031ffbb2e13502a0274e"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["407687e67faf6e1f02a211ca078d8e3eed631027","5009d80b96d7223e4120e40035a8b8c1d48cb134"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"c83d6c4335f31cae14f625a222bc842f20073dcd":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1","37a0f60745e53927c4c876cfe5b5a58170f0646c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"407687e67faf6e1f02a211ca078d8e3eed631027":[],"a56958d7f71a28824f20031ffbb2e13502a0274e":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5009d80b96d7223e4120e40035a8b8c1d48cb134":["37a0f60745e53927c4c876cfe5b5a58170f0646c","c83d6c4335f31cae14f625a222bc842f20073dcd","407687e67faf6e1f02a211ca078d8e3eed631027"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","37a0f60745e53927c4c876cfe5b5a58170f0646c","407687e67faf6e1f02a211ca078d8e3eed631027","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}