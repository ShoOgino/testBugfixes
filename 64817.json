{"path":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","commits":[{"id":"193a8346fc41165af561d084bb5f40a62125a878","date":1315971230,"type":0,"author":"Christopher John Male","isMerge":false,"pathNew":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param matchVersion Version to be used in {@link StopFilter}\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Version matchVersion,\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    this.matchVersion = matchVersion;\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<String>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator();\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            stopWords.add(text.utf8ToChars(spare).toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","sourceNew":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param matchVersion Version to be used in {@link StopFilter}\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Version matchVersion,\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    this.matchVersion = matchVersion;\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<String>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            stopWords.add(text.utf8ToChars(spare).toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","sourceOld":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param matchVersion Version to be used in {@link StopFilter}\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Version matchVersion,\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    this.matchVersion = matchVersion;\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<String>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator();\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            stopWords.add(text.utf8ToChars(spare).toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"da6c07e76d701edbcc45c3e83ad8464a5e44a4c0","date":1322229341,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","sourceNew":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param matchVersion Version to be used in {@link StopFilter}\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Version matchVersion,\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    this.matchVersion = matchVersion;\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<String>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","sourceOld":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param matchVersion Version to be used in {@link StopFilter}\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Version matchVersion,\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    this.matchVersion = matchVersion;\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<String>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            stopWords.add(text.utf8ToChars(spare).toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#QueryAutoStopWordAnalyzer(Version,Analyzer,IndexReader,Collection[String],int).mjava","sourceNew":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param matchVersion Version to be used in {@link StopFilter}\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Version matchVersion,\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    this.matchVersion = matchVersion;\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<String>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","sourceOld":"  /**\n   * Creates a new QueryAutoStopWordAnalyzer with stopwords calculated for the\n   * given selection of fields from terms with a document frequency greater than\n   * the given maxDocFreq\n   *\n   * @param matchVersion Version to be used in {@link StopFilter}\n   * @param delegate Analyzer whose TokenStream will be filtered\n   * @param indexReader IndexReader to identify the stopwords from\n   * @param fields Selection of fields to calculate stopwords for\n   * @param maxDocFreq Document frequency terms should be above in order to be stopwords\n   * @throws IOException Can be thrown while reading from the IndexReader\n   */\n  public QueryAutoStopWordAnalyzer(\n      Version matchVersion,\n      Analyzer delegate,\n      IndexReader indexReader,\n      Collection<String> fields,\n      int maxDocFreq) throws IOException {\n    this.matchVersion = matchVersion;\n    this.delegate = delegate;\n    \n    for (String field : fields) {\n      Set<String> stopWords = new HashSet<String>();\n      Terms terms = MultiFields.getTerms(indexReader, field);\n      CharsRef spare = new CharsRef();\n      if (terms != null) {\n        TermsEnum te = terms.iterator(null);\n        BytesRef text;\n        while ((text = te.next()) != null) {\n          if (te.docFreq() > maxDocFreq) {\n            UnicodeUtil.UTF8toUTF16(text, spare);\n            stopWords.add(spare.toString());\n          }\n        }\n      }\n      stopWordsPerField.put(field, stopWords);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"da6c07e76d701edbcc45c3e83ad8464a5e44a4c0":["3cc749c053615f5871f3b95715fe292f34e70a53"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["da6c07e76d701edbcc45c3e83ad8464a5e44a4c0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"193a8346fc41165af561d084bb5f40a62125a878":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3cc749c053615f5871f3b95715fe292f34e70a53":["193a8346fc41165af561d084bb5f40a62125a878"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"]},"commit2Childs":{"da6c07e76d701edbcc45c3e83ad8464a5e44a4c0":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["193a8346fc41165af561d084bb5f40a62125a878"],"193a8346fc41165af561d084bb5f40a62125a878":["3cc749c053615f5871f3b95715fe292f34e70a53"],"3cc749c053615f5871f3b95715fe292f34e70a53":["da6c07e76d701edbcc45c3e83ad8464a5e44a4c0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}