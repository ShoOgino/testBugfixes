{"path":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","commits":[{"id":"4c835cc1a7b07477a469cdb1bf6c67bc05b85c07","date":1471849333,"type":0,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(Arrays.asList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(Arrays.asList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      Thread.sleep(15000); // sleep for a while for leader to change ...\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e1c5f7ce544a129550a8515f7f0eb5f1c0f4472a","date":1472163016,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(Arrays.asList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(Arrays.asList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      Thread.sleep(15000); // sleep for a while for leader to change ...\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fcc7eba0b32cbc7cc5b8fd388032bb833fa07786","date":1474482359,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      LeaderFailureAfterFreshStartTest.waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, 15);\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(Arrays.asList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(Arrays.asList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      Thread.sleep(15000); // sleep for a while for leader to change ...\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      LeaderFailureAfterFreshStartTest.waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, 15);\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(Arrays.asList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(Arrays.asList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      Thread.sleep(15000); // sleep for a while for leader to change ...\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"05569170a222447d3aec8fad773feedf4429fdd5","date":1476800012,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, 15);\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      LeaderFailureAfterFreshStartTest.waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, 15);\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b437b84da5e49daf1197c9cd533015490cdcb2c2","date":1476818430,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      LeaderFailureAfterFreshStartTest.waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, 15);\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, 15);\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      LeaderFailureAfterFreshStartTest.waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, 15);\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b11122ff28c20e6e4e02cb1366eaa029a92f69ce","date":1477307742,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, 15);\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      LeaderFailureAfterFreshStartTest.waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, 15);\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"80d0e6d59ae23f4a6f30eaf40bfb40742300287f","date":1477598926,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, 15);\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      LeaderFailureAfterFreshStartTest.waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, 15);\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7540b2d98e89684a4076a7e99ba2f8ec7983de7c","date":1483428128,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, SECONDS));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, 15);\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f03e4bed5023ec3ef93a771b8888cae991cf448d","date":1483469262,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, SECONDS));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, 15);\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"268f09ed3a9a9b77003b15a5ae30386dc4e3721f","date":1483992000,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, SECONDS));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // assert metrics\n      MetricRegistry registry = nodePeerSynced.jetty.getCoreContainer().getMetricManager().registry(\"solr.core.collection1\");\n      Map<String, Metric> metrics = registry.getMetrics();\n      assertTrue(\"REPLICATION.time present\", metrics.containsKey(\"REPLICATION.time\"));\n      assertTrue(\"REPLICATION.errors present\", metrics.containsKey(\"REPLICATION.errors\"));\n      Timer timer = (Timer)metrics.get(\"REPLICATION.time\");\n      assertEquals(1L, timer.getCount());\n      Counter counter = (Counter)metrics.get(\"REPLICATION.errors\");\n      assertEquals(0L, counter.getCount());\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, SECONDS));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"09ab8ee44ca898536770d0106a7c0ee4be4f0eb7","date":1484239864,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, SECONDS));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // assert metrics\n      MetricRegistry registry = nodePeerSynced.jetty.getCoreContainer().getMetricManager().registry(\"solr.core.collection1\");\n      Map<String, Metric> metrics = registry.getMetrics();\n      assertTrue(\"REPLICATION.time present\", metrics.containsKey(\"REPLICATION.time\"));\n      assertTrue(\"REPLICATION.errors present\", metrics.containsKey(\"REPLICATION.errors\"));\n      Timer timer = (Timer)metrics.get(\"REPLICATION.time\");\n      assertEquals(1L, timer.getCount());\n      Counter counter = (Counter)metrics.get(\"REPLICATION.errors\");\n      assertEquals(0L, counter.getCount());\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, SECONDS));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"746467918bcca1fc9791421f4a615b295c5e6da8","date":1502249179,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, SECONDS));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // assert metrics\n      SolrMetricManager manager = nodePeerSynced.jetty.getCoreContainer().getMetricManager();\n      MetricRegistry registry = null;\n      for (String name : manager.registryNames()) {\n        if (name.startsWith(\"solr.core.collection1\")) {\n          registry = manager.registry(name);\n          break;\n        }\n      }\n      assertNotNull(registry);\n      Map<String, Metric> metrics = registry.getMetrics();\n      assertTrue(\"REPLICATION.peerSync.time present\", metrics.containsKey(\"REPLICATION.peerSync.time\"));\n      assertTrue(\"REPLICATION.peerSync.errors present\", metrics.containsKey(\"REPLICATION.peerSync.errors\"));\n      Timer timer = (Timer)metrics.get(\"REPLICATION.peerSync.time\");\n      assertEquals(1L, timer.getCount());\n      Counter counter = (Counter)metrics.get(\"REPLICATION.peerSync.errors\");\n      assertEquals(0L, counter.getCount());\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, SECONDS));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // assert metrics\n      MetricRegistry registry = nodePeerSynced.jetty.getCoreContainer().getMetricManager().registry(\"solr.core.collection1\");\n      Map<String, Metric> metrics = registry.getMetrics();\n      assertTrue(\"REPLICATION.time present\", metrics.containsKey(\"REPLICATION.time\"));\n      assertTrue(\"REPLICATION.errors present\", metrics.containsKey(\"REPLICATION.errors\"));\n      Timer timer = (Timer)metrics.get(\"REPLICATION.time\");\n      assertEquals(1L, timer.getCount());\n      Counter counter = (Counter)metrics.get(\"REPLICATION.errors\");\n      assertEquals(0L, counter.getCount());\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"58884af1f68e9d61c217c753fbd6266d86a63b14","date":1502363401,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, SECONDS));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // assert metrics\n      SolrMetricManager manager = nodePeerSynced.jetty.getCoreContainer().getMetricManager();\n      MetricRegistry registry = null;\n      for (String name : manager.registryNames()) {\n        if (name.startsWith(\"solr.core.collection1\")) {\n          registry = manager.registry(name);\n          break;\n        }\n      }\n      assertNotNull(registry);\n      Map<String, Metric> metrics = registry.getMetrics();\n      assertTrue(\"REPLICATION.peerSync.time present\", metrics.containsKey(\"REPLICATION.peerSync.time\"));\n      assertTrue(\"REPLICATION.peerSync.errors present\", metrics.containsKey(\"REPLICATION.peerSync.errors\"));\n      Timer timer = (Timer)metrics.get(\"REPLICATION.peerSync.time\");\n      assertEquals(1L, timer.getCount());\n      Counter counter = (Counter)metrics.get(\"REPLICATION.peerSync.errors\");\n      assertEquals(0L, counter.getCount());\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, SECONDS));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // assert metrics\n      MetricRegistry registry = nodePeerSynced.jetty.getCoreContainer().getMetricManager().registry(\"solr.core.collection1\");\n      Map<String, Metric> metrics = registry.getMetrics();\n      assertTrue(\"REPLICATION.time present\", metrics.containsKey(\"REPLICATION.time\"));\n      assertTrue(\"REPLICATION.errors present\", metrics.containsKey(\"REPLICATION.errors\"));\n      Timer timer = (Timer)metrics.get(\"REPLICATION.time\");\n      assertEquals(1L, timer.getCount());\n      Counter counter = (Counter)metrics.get(\"REPLICATION.errors\");\n      assertEquals(0L, counter.getCount());\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"93d40a0287bd8a5b69a8df49a797dcd4a8b1a7be","date":1502692251,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, SECONDS));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // assert metrics\n      SolrMetricManager manager = nodePeerSynced.jetty.getCoreContainer().getMetricManager();\n      MetricRegistry registry = null;\n      for (String name : manager.registryNames()) {\n        if (name.startsWith(\"solr.core.collection1\")) {\n          registry = manager.registry(name);\n          break;\n        }\n      }\n      assertNotNull(registry);\n      Map<String, Metric> metrics = registry.getMetrics();\n      assertTrue(\"REPLICATION.peerSync.time present\", metrics.containsKey(\"REPLICATION.peerSync.time\"));\n      assertTrue(\"REPLICATION.peerSync.errors present\", metrics.containsKey(\"REPLICATION.peerSync.errors\"));\n      Timer timer = (Timer)metrics.get(\"REPLICATION.peerSync.time\");\n      assertEquals(1L, timer.getCount());\n      Counter counter = (Counter)metrics.get(\"REPLICATION.peerSync.errors\");\n      assertEquals(0L, counter.getCount());\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, SECONDS));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // assert metrics\n      MetricRegistry registry = nodePeerSynced.jetty.getCoreContainer().getMetricManager().registry(\"solr.core.collection1\");\n      Map<String, Metric> metrics = registry.getMetrics();\n      assertTrue(\"REPLICATION.time present\", metrics.containsKey(\"REPLICATION.time\"));\n      assertTrue(\"REPLICATION.errors present\", metrics.containsKey(\"REPLICATION.errors\"));\n      Timer timer = (Timer)metrics.get(\"REPLICATION.time\");\n      assertEquals(1L, timer.getCount());\n      Counter counter = (Counter)metrics.get(\"REPLICATION.errors\");\n      assertEquals(0L, counter.getCount());\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1d4bf9d5308dfef350829c28f2b3b2648df1e9b1","date":1513252583,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, SECONDS, TimeSource.NANO_TIME));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // assert metrics\n      SolrMetricManager manager = nodePeerSynced.jetty.getCoreContainer().getMetricManager();\n      MetricRegistry registry = null;\n      for (String name : manager.registryNames()) {\n        if (name.startsWith(\"solr.core.collection1\")) {\n          registry = manager.registry(name);\n          break;\n        }\n      }\n      assertNotNull(registry);\n      Map<String, Metric> metrics = registry.getMetrics();\n      assertTrue(\"REPLICATION.peerSync.time present\", metrics.containsKey(\"REPLICATION.peerSync.time\"));\n      assertTrue(\"REPLICATION.peerSync.errors present\", metrics.containsKey(\"REPLICATION.peerSync.errors\"));\n      Timer timer = (Timer)metrics.get(\"REPLICATION.peerSync.time\");\n      assertEquals(1L, timer.getCount());\n      Counter counter = (Counter)metrics.get(\"REPLICATION.peerSync.errors\");\n      assertEquals(0L, counter.getCount());\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, SECONDS));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // assert metrics\n      SolrMetricManager manager = nodePeerSynced.jetty.getCoreContainer().getMetricManager();\n      MetricRegistry registry = null;\n      for (String name : manager.registryNames()) {\n        if (name.startsWith(\"solr.core.collection1\")) {\n          registry = manager.registry(name);\n          break;\n        }\n      }\n      assertNotNull(registry);\n      Map<String, Metric> metrics = registry.getMetrics();\n      assertTrue(\"REPLICATION.peerSync.time present\", metrics.containsKey(\"REPLICATION.peerSync.time\"));\n      assertTrue(\"REPLICATION.peerSync.errors present\", metrics.containsKey(\"REPLICATION.peerSync.errors\"));\n      Timer timer = (Timer)metrics.get(\"REPLICATION.peerSync.time\");\n      assertEquals(1L, timer.getCount());\n      Counter counter = (Counter)metrics.get(\"REPLICATION.peerSync.errors\");\n      assertEquals(0L, counter.getCount());\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ff5e25fb60ccc8574bcbd65396786ae9163f0149","date":1519706112,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","sourceNew":"  @Test\n  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\")\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, SECONDS, TimeSource.NANO_TIME));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // assert metrics\n      SolrMetricManager manager = nodePeerSynced.jetty.getCoreContainer().getMetricManager();\n      MetricRegistry registry = null;\n      for (String name : manager.registryNames()) {\n        if (name.startsWith(\"solr.core.collection1\")) {\n          registry = manager.registry(name);\n          break;\n        }\n      }\n      assertNotNull(registry);\n      Map<String, Metric> metrics = registry.getMetrics();\n      assertTrue(\"REPLICATION.peerSync.time present\", metrics.containsKey(\"REPLICATION.peerSync.time\"));\n      assertTrue(\"REPLICATION.peerSync.errors present\", metrics.containsKey(\"REPLICATION.peerSync.errors\"));\n      Timer timer = (Timer)metrics.get(\"REPLICATION.peerSync.time\");\n      assertEquals(1L, timer.getCount());\n      Counter counter = (Counter)metrics.get(\"REPLICATION.peerSync.errors\");\n      assertEquals(0L, counter.getCount());\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, SECONDS, TimeSource.NANO_TIME));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // assert metrics\n      SolrMetricManager manager = nodePeerSynced.jetty.getCoreContainer().getMetricManager();\n      MetricRegistry registry = null;\n      for (String name : manager.registryNames()) {\n        if (name.startsWith(\"solr.core.collection1\")) {\n          registry = manager.registry(name);\n          break;\n        }\n      }\n      assertNotNull(registry);\n      Map<String, Metric> metrics = registry.getMetrics();\n      assertTrue(\"REPLICATION.peerSync.time present\", metrics.containsKey(\"REPLICATION.peerSync.time\"));\n      assertTrue(\"REPLICATION.peerSync.errors present\", metrics.containsKey(\"REPLICATION.peerSync.errors\"));\n      Timer timer = (Timer)metrics.get(\"REPLICATION.peerSync.time\");\n      assertEquals(1L, timer.getCount());\n      Counter counter = (Counter)metrics.get(\"REPLICATION.peerSync.errors\");\n      assertEquals(0L, counter.getCount());\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"05a3c9b5f1dfb39879069eb1dac3ca104d3e4108","date":1533256859,"type":3,"author":"Erick","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","sourceNew":"  @Test\n  //commented 2-Aug-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\")\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, SECONDS, TimeSource.NANO_TIME));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // assert metrics\n      SolrMetricManager manager = nodePeerSynced.jetty.getCoreContainer().getMetricManager();\n      MetricRegistry registry = null;\n      for (String name : manager.registryNames()) {\n        if (name.startsWith(\"solr.core.collection1\")) {\n          registry = manager.registry(name);\n          break;\n        }\n      }\n      assertNotNull(registry);\n      Map<String, Metric> metrics = registry.getMetrics();\n      assertTrue(\"REPLICATION.peerSync.time present\", metrics.containsKey(\"REPLICATION.peerSync.time\"));\n      assertTrue(\"REPLICATION.peerSync.errors present\", metrics.containsKey(\"REPLICATION.peerSync.errors\"));\n      Timer timer = (Timer)metrics.get(\"REPLICATION.peerSync.time\");\n      assertEquals(1L, timer.getCount());\n      Counter counter = (Counter)metrics.get(\"REPLICATION.peerSync.errors\");\n      assertEquals(0L, counter.getCount());\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","sourceOld":"  @Test\n  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\")\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, SECONDS, TimeSource.NANO_TIME));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // assert metrics\n      SolrMetricManager manager = nodePeerSynced.jetty.getCoreContainer().getMetricManager();\n      MetricRegistry registry = null;\n      for (String name : manager.registryNames()) {\n        if (name.startsWith(\"solr.core.collection1\")) {\n          registry = manager.registry(name);\n          break;\n        }\n      }\n      assertNotNull(registry);\n      Map<String, Metric> metrics = registry.getMetrics();\n      assertTrue(\"REPLICATION.peerSync.time present\", metrics.containsKey(\"REPLICATION.peerSync.time\"));\n      assertTrue(\"REPLICATION.peerSync.errors present\", metrics.containsKey(\"REPLICATION.peerSync.errors\"));\n      Timer timer = (Timer)metrics.get(\"REPLICATION.peerSync.time\");\n      assertEquals(1L, timer.getCount());\n      Counter counter = (Counter)metrics.get(\"REPLICATION.peerSync.errors\");\n      assertEquals(0L, counter.getCount());\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bb222a3f9d9421d5c95afce73013fbd8de07ea1f","date":1543514331,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","sourceNew":"  @Test\n  //commented 2-Aug-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\")\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, SECONDS, TimeSource.NANO_TIME));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // assert metrics\n      SolrMetricManager manager = nodePeerSynced.jetty.getCoreContainer().getMetricManager();\n      MetricRegistry registry = null;\n      for (String name : manager.registryNames()) {\n        if (name.startsWith(\"solr.core.collection1\")) {\n          registry = manager.registry(name);\n          break;\n        }\n      }\n      assertNotNull(registry);\n      Map<String, Metric> metrics = registry.getMetrics();\n      assertTrue(\"REPLICATION.peerSync.time present\", metrics.containsKey(\"REPLICATION.peerSync.time\"));\n      assertTrue(\"REPLICATION.peerSync.errors present\", metrics.containsKey(\"REPLICATION.peerSync.errors\"));\n\n      Counter counter = (Counter)metrics.get(\"REPLICATION.peerSync.errors\");\n      assertEquals(0L, counter.getCount());\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","sourceOld":"  @Test\n  //commented 2-Aug-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\")\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, SECONDS, TimeSource.NANO_TIME));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // assert metrics\n      SolrMetricManager manager = nodePeerSynced.jetty.getCoreContainer().getMetricManager();\n      MetricRegistry registry = null;\n      for (String name : manager.registryNames()) {\n        if (name.startsWith(\"solr.core.collection1\")) {\n          registry = manager.registry(name);\n          break;\n        }\n      }\n      assertNotNull(registry);\n      Map<String, Metric> metrics = registry.getMetrics();\n      assertTrue(\"REPLICATION.peerSync.time present\", metrics.containsKey(\"REPLICATION.peerSync.time\"));\n      assertTrue(\"REPLICATION.peerSync.errors present\", metrics.containsKey(\"REPLICATION.peerSync.errors\"));\n      Timer timer = (Timer)metrics.get(\"REPLICATION.peerSync.time\");\n      assertEquals(1L, timer.getCount());\n      Counter counter = (Counter)metrics.get(\"REPLICATION.peerSync.errors\");\n      assertEquals(0L, counter.getCount());\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","bugFix":["746467918bcca1fc9791421f4a615b295c5e6da8","268f09ed3a9a9b77003b15a5ae30386dc4e3721f"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"add1e7dd742ea533ff4318cea83ca0a1f669f662","date":1585262285,"type":3,"author":"Mike Drob","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/PeerSyncReplicationTest#test().mjava","sourceNew":"  @Test\n  //commented 2-Aug-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\")\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30, TimeUnit.SECONDS);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30, TimeUnit.SECONDS);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30, TimeUnit.SECONDS);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, TimeUnit.SECONDS, TimeSource.NANO_TIME));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // assert metrics\n      SolrMetricManager manager = nodePeerSynced.jetty.getCoreContainer().getMetricManager();\n      MetricRegistry registry = null;\n      for (String name : manager.registryNames()) {\n        if (name.startsWith(\"solr.core.collection1\")) {\n          registry = manager.registry(name);\n          break;\n        }\n      }\n      assertNotNull(registry);\n      Map<String, Metric> metrics = registry.getMetrics();\n      assertTrue(\"REPLICATION.peerSync.time present\", metrics.containsKey(\"REPLICATION.peerSync.time\"));\n      assertTrue(\"REPLICATION.peerSync.errors present\", metrics.containsKey(\"REPLICATION.peerSync.errors\"));\n\n      Counter counter = (Counter)metrics.get(\"REPLICATION.peerSync.errors\");\n      assertEquals(0L, counter.getCount());\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","sourceOld":"  @Test\n  //commented 2-Aug-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\")\n  public void test() throws Exception {\n    handle.clear();\n    handle.put(\"timestamp\", SKIPVAL);\n\n    waitForThingsToLevelOut(30);\n\n    del(\"*:*\");\n\n    // index enough docs and commit to establish frame of reference for PeerSync\n    for (int i = 0; i < 100; i++) {\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    try {\n      checkShardConsistency(false, true);\n\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      CloudJettyRunner initialLeaderJetty = shardToLeaderJetty.get(\"shard1\");\n      List<CloudJettyRunner> otherJetties = getOtherAvailableJetties(initialLeaderJetty);\n      CloudJettyRunner neverLeader = otherJetties.get(otherJetties.size() - 1);\n      otherJetties.remove(neverLeader) ;\n\n      // first shutdown a node that will never be a leader\n      forceNodeFailures(singletonList(neverLeader));\n\n      // node failure and recovery via PeerSync\n      log.info(\"Forcing PeerSync\");\n      CloudJettyRunner nodePeerSynced = forceNodeFailureAndDoPeerSync(false);\n\n      // add a few more docs\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      indexDoc(id, docId, i1, 50, tlong, 50, t1,\n          \"document number \" + docId++);\n      commit();\n\n      cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\")).getResults().getNumFound();\n      assertEquals(docId, cloudClientDocs);\n\n      // now shutdown all other nodes except for 'nodeShutDownForFailure'\n      otherJetties.remove(nodePeerSynced);\n      forceNodeFailures(otherJetties);\n      waitForThingsToLevelOut(30);\n      checkShardConsistency(false, true);\n\n      // now shutdown the original leader\n      log.info(\"Now shutting down initial leader\");\n      forceNodeFailures(singletonList(initialLeaderJetty));\n      log.info(\"Updating mappings from zk\");\n      waitForNewLeader(cloudClient, \"shard1\", (Replica) initialLeaderJetty.client.info, new TimeOut(15, SECONDS, TimeSource.NANO_TIME));\n      updateMappingsFromZk(jettys, clients, true);\n      assertEquals(\"PeerSynced node did not become leader\", nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // bring up node that was down all along, and let it PeerSync from the node that was forced to PeerSynce  \n      bringUpDeadNodeAndEnsureNoReplication(neverLeader, false);\n      waitTillNodesActive();\n\n      checkShardConsistency(false, true);\n\n      \n      // bring back all the nodes including initial leader \n      // (commented as reports Maximum concurrent create/delete watches above limit violation and reports thread leaks)\n      /*for(int i = 0 ; i < nodesDown.size(); i++) {\n        bringUpDeadNodeAndEnsureNoReplication(shardToLeaderJetty.get(\"shard1\"), neverLeader, false);\n      }\n      checkShardConsistency(false, true);*/\n\n      // make sure leader has not changed after bringing initial leader back\n      assertEquals(nodePeerSynced, shardToLeaderJetty.get(\"shard1\"));\n\n      // assert metrics\n      SolrMetricManager manager = nodePeerSynced.jetty.getCoreContainer().getMetricManager();\n      MetricRegistry registry = null;\n      for (String name : manager.registryNames()) {\n        if (name.startsWith(\"solr.core.collection1\")) {\n          registry = manager.registry(name);\n          break;\n        }\n      }\n      assertNotNull(registry);\n      Map<String, Metric> metrics = registry.getMetrics();\n      assertTrue(\"REPLICATION.peerSync.time present\", metrics.containsKey(\"REPLICATION.peerSync.time\"));\n      assertTrue(\"REPLICATION.peerSync.errors present\", metrics.containsKey(\"REPLICATION.peerSync.errors\"));\n\n      Counter counter = (Counter)metrics.get(\"REPLICATION.peerSync.errors\");\n      assertEquals(0L, counter.getCount());\n      success = true;\n    } finally {\n      System.clearProperty(\"solr.disableFingerprint\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"05a3c9b5f1dfb39879069eb1dac3ca104d3e4108":["ff5e25fb60ccc8574bcbd65396786ae9163f0149"],"ff5e25fb60ccc8574bcbd65396786ae9163f0149":["1d4bf9d5308dfef350829c28f2b3b2648df1e9b1"],"746467918bcca1fc9791421f4a615b295c5e6da8":["268f09ed3a9a9b77003b15a5ae30386dc4e3721f"],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["05a3c9b5f1dfb39879069eb1dac3ca104d3e4108"],"268f09ed3a9a9b77003b15a5ae30386dc4e3721f":["7540b2d98e89684a4076a7e99ba2f8ec7983de7c"],"fcc7eba0b32cbc7cc5b8fd388032bb833fa07786":["e1c5f7ce544a129550a8515f7f0eb5f1c0f4472a"],"09ab8ee44ca898536770d0106a7c0ee4be4f0eb7":["f03e4bed5023ec3ef93a771b8888cae991cf448d","268f09ed3a9a9b77003b15a5ae30386dc4e3721f"],"05569170a222447d3aec8fad773feedf4429fdd5":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","b437b84da5e49daf1197c9cd533015490cdcb2c2"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f","7540b2d98e89684a4076a7e99ba2f8ec7983de7c"],"58884af1f68e9d61c217c753fbd6266d86a63b14":["268f09ed3a9a9b77003b15a5ae30386dc4e3721f","746467918bcca1fc9791421f4a615b295c5e6da8"],"b11122ff28c20e6e4e02cb1366eaa029a92f69ce":["b437b84da5e49daf1197c9cd533015490cdcb2c2"],"b437b84da5e49daf1197c9cd533015490cdcb2c2":["05569170a222447d3aec8fad773feedf4429fdd5"],"4c835cc1a7b07477a469cdb1bf6c67bc05b85c07":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["e1c5f7ce544a129550a8515f7f0eb5f1c0f4472a","fcc7eba0b32cbc7cc5b8fd388032bb833fa07786"],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","b11122ff28c20e6e4e02cb1366eaa029a92f69ce"],"add1e7dd742ea533ff4318cea83ca0a1f669f662":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"93d40a0287bd8a5b69a8df49a797dcd4a8b1a7be":["268f09ed3a9a9b77003b15a5ae30386dc4e3721f","746467918bcca1fc9791421f4a615b295c5e6da8"],"1d4bf9d5308dfef350829c28f2b3b2648df1e9b1":["746467918bcca1fc9791421f4a615b295c5e6da8"],"e1c5f7ce544a129550a8515f7f0eb5f1c0f4472a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","4c835cc1a7b07477a469cdb1bf6c67bc05b85c07"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["add1e7dd742ea533ff4318cea83ca0a1f669f662"],"7540b2d98e89684a4076a7e99ba2f8ec7983de7c":["b11122ff28c20e6e4e02cb1366eaa029a92f69ce"]},"commit2Childs":{"05a3c9b5f1dfb39879069eb1dac3ca104d3e4108":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"ff5e25fb60ccc8574bcbd65396786ae9163f0149":["05a3c9b5f1dfb39879069eb1dac3ca104d3e4108"],"746467918bcca1fc9791421f4a615b295c5e6da8":["58884af1f68e9d61c217c753fbd6266d86a63b14","93d40a0287bd8a5b69a8df49a797dcd4a8b1a7be","1d4bf9d5308dfef350829c28f2b3b2648df1e9b1"],"268f09ed3a9a9b77003b15a5ae30386dc4e3721f":["746467918bcca1fc9791421f4a615b295c5e6da8","09ab8ee44ca898536770d0106a7c0ee4be4f0eb7","58884af1f68e9d61c217c753fbd6266d86a63b14","93d40a0287bd8a5b69a8df49a797dcd4a8b1a7be"],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["add1e7dd742ea533ff4318cea83ca0a1f669f662"],"fcc7eba0b32cbc7cc5b8fd388032bb833fa07786":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"09ab8ee44ca898536770d0106a7c0ee4be4f0eb7":[],"05569170a222447d3aec8fad773feedf4429fdd5":["b437b84da5e49daf1197c9cd533015490cdcb2c2"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":["09ab8ee44ca898536770d0106a7c0ee4be4f0eb7"],"58884af1f68e9d61c217c753fbd6266d86a63b14":[],"b11122ff28c20e6e4e02cb1366eaa029a92f69ce":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f","7540b2d98e89684a4076a7e99ba2f8ec7983de7c"],"b437b84da5e49daf1197c9cd533015490cdcb2c2":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","b11122ff28c20e6e4e02cb1366eaa029a92f69ce"],"4c835cc1a7b07477a469cdb1bf6c67bc05b85c07":["e1c5f7ce544a129550a8515f7f0eb5f1c0f4472a"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["05569170a222447d3aec8fad773feedf4429fdd5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","4c835cc1a7b07477a469cdb1bf6c67bc05b85c07","e1c5f7ce544a129550a8515f7f0eb5f1c0f4472a"],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":["f03e4bed5023ec3ef93a771b8888cae991cf448d"],"add1e7dd742ea533ff4318cea83ca0a1f669f662":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"93d40a0287bd8a5b69a8df49a797dcd4a8b1a7be":[],"1d4bf9d5308dfef350829c28f2b3b2648df1e9b1":["ff5e25fb60ccc8574bcbd65396786ae9163f0149"],"e1c5f7ce544a129550a8515f7f0eb5f1c0f4472a":["fcc7eba0b32cbc7cc5b8fd388032bb833fa07786","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"7540b2d98e89684a4076a7e99ba2f8ec7983de7c":["268f09ed3a9a9b77003b15a5ae30386dc4e3721f","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["09ab8ee44ca898536770d0106a7c0ee4be4f0eb7","58884af1f68e9d61c217c753fbd6266d86a63b14","93d40a0287bd8a5b69a8df49a797dcd4a8b1a7be","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}