{"path":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simUpdate(UpdateRequest).mjava","commits":[{"id":"9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5","date":1556572478,"type":1,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simUpdate(UpdateRequest).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simUpdate(UpdateRequest).mjava","sourceNew":"  /**\n   * Simulate an update by modifying replica metrics.\n   * The following core metrics are updated:\n   * <ul>\n   *   <li><code>SEARCHER.searcher.numDocs</code> - increased by added docs, decreased by deleteById and deleteByQuery</li>\n   *   <li><code>SEARCHER.searcher.deletedDocs</code> - decreased by deleteById and deleteByQuery by up to <code>numDocs</code></li>\n   *   <li><code>SEARCHER.searcher.maxDoc</code> - always increased by the number of added docs.</li>\n   * </ul>\n   * <p>IMPORTANT limitations:</p>\n   * <ul>\n   *   <li>document replacements are always counted as new docs</li>\n   *   <li>delete by ID always succeeds (unless numDocs == 0)</li>\n   *   <li>deleteByQuery is not supported unless the query is <code>*:*</code></li>\n   * </ul>\n   * @param req update request. This request MUST have the <code>collection</code> param set.\n   * @return {@link UpdateResponse}\n   * @throws SolrException on errors, such as nonexistent collection or unsupported deleteByQuery\n   */\n  public UpdateResponse simUpdate(UpdateRequest req) throws SolrException, InterruptedException, IOException {\n    ensureNotClosed();\n    String collection = req.getCollection();\n    if (collection == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection not set\");\n    }\n    ensureSystemCollection(collection);\n    \n    DocCollection coll = getClusterState().getCollection(collection);\n    DocRouter router = coll.getRouter();\n    List<String> deletes = req.getDeleteById();\n    if (deletes != null && !deletes.isEmpty()) {\n      for (String id : deletes) {\n        Slice s = router.getTargetSlice(id, null, null, req.getParams(), coll);\n        Replica leader = s.getLeader();\n        if (leader == null) {\n          log.debug(\"-- no leader in \" + s);\n          continue;\n        }\n        cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n        ReplicaInfo ri = getReplicaInfo(leader);\n        Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n        if (numDocs == null || numDocs.intValue() <= 0) {\n          log.debug(\"-- attempting to delete nonexistent doc \" + id + \" from \" + s.getLeader());\n          continue;\n        }\n        AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n        if (bufferedUpdates != null) {\n          if (bufferedUpdates.get() > 0) {\n            bufferedUpdates.decrementAndGet();\n          } else {\n            log.debug(\"-- attempting to delete nonexistent buffered doc \" + id + \" from \" + s.getLeader());\n          }\n          continue;\n        }\n        lock.lockInterruptibly();\n        try {\n          simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.deletedDocs\", 1, true, false);\n          simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.numDocs\", -1, true, false);\n          Number indexSize = (Number)ri.getVariable(Type.CORE_IDX.metricsAttribute);\n          if (indexSize != null && indexSize.longValue() > SimCloudManager.DEFAULT_IDX_SIZE_BYTES) {\n            indexSize = indexSize.longValue() - DEFAULT_DOC_SIZE_BYTES;\n            simSetShardValue(collection, s.getName(), Type.CORE_IDX.metricsAttribute,\n                new AtomicLong(indexSize.longValue()), false, false);\n            simSetShardValue(collection, s.getName(), Variable.coreidxsize,\n                new AtomicDouble((Double)Type.CORE_IDX.convertVal(indexSize)), false, false);\n          } else {\n            throw new Exception(\"unexpected indexSize ri=\" + ri);\n          }\n        } catch (Exception e) {\n          throw new IOException(e);\n        } finally {\n          lock.unlock();\n        }\n      }\n    }\n    deletes = req.getDeleteQuery();\n    if (deletes != null && !deletes.isEmpty()) {\n      for (String q : deletes) {\n        if (!\"*:*\".equals(q)) {\n          throw new UnsupportedOperationException(\"Only '*:*' query is supported in deleteByQuery\");\n        }\n        for (Slice s : coll.getSlices()) {\n          Replica leader = s.getLeader();\n          if (leader == null) {\n            log.debug(\"-- no leader in \" + s);\n            continue;\n          }\n\n          cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n          ReplicaInfo ri = getReplicaInfo(leader);\n          Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n          if (numDocs == null || numDocs.intValue() == 0) {\n            continue;\n          }\n          lock.lockInterruptibly();\n          try {\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.deletedDocs\", new AtomicLong(numDocs.longValue()), false, false);\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.numDocs\", new AtomicLong(0), false, false);\n            simSetShardValue(collection, s.getName(), Type.CORE_IDX.metricsAttribute,\n                new AtomicLong(SimCloudManager.DEFAULT_IDX_SIZE_BYTES), false, false);\n            simSetShardValue(collection, s.getName(), Variable.coreidxsize,\n                new AtomicDouble((Double)Type.CORE_IDX.convertVal(SimCloudManager.DEFAULT_IDX_SIZE_BYTES)), false, false);\n          } catch (Exception e) {\n            throw new IOException(e);\n          } finally {\n            lock.unlock();\n          }\n        }\n      }\n    }\n    List<SolrInputDocument> docs = req.getDocuments();\n    int docCount = 0;\n    Iterator<SolrInputDocument> it = null;\n    if (docs != null) {\n      docCount = docs.size();\n    } else {\n      it = req.getDocIterator();\n      if (it != null) {\n        while (it.hasNext()) {\n          it.next();\n          docCount++;\n        }\n      }\n    }\n    if (docCount > 0) {\n      // this approach to updating counters and metrics drastically increases performance\n      // of bulk updates, because simSetShardValue is relatively costly\n\n      Map<String, AtomicLong> docUpdates = new HashMap<>();\n      Map<String, Map<String, AtomicLong>> metricUpdates = new HashMap<>();\n\n      // XXX don't add more than 2bln docs in one request\n      boolean modified = false;\n      lock.lockInterruptibly();\n      try {\n        coll = getClusterState().getCollection(collection);\n        Slice[] slices = coll.getActiveSlicesArr();\n        if (slices.length == 0) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection without slices\");\n        }\n        int[] perSlice = new int[slices.length];\n\n        if (it != null) {\n          // BULK UPDATE: simulate random doc assignment without actually calling DocRouter,\n          // which adds significant overhead\n\n          int totalAdded = 0;\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            long count = (long) docCount * ((long) s.getRange().max - (long) s.getRange().min) / 0x100000000L;\n            perSlice[i] = (int) count;\n            totalAdded += perSlice[i];\n          }\n          // loss of precision due to integer math\n          int diff = docCount - totalAdded;\n          if (diff > 0) {\n            // spread the remainder more or less equally\n            int perRemain = diff / slices.length;\n            int remainder = diff % slices.length;\n            int remainderSlice = slices.length > 1 ? bulkUpdateRandom.nextInt(slices.length) : 0;\n            for (int i = 0; i < slices.length; i++) {\n              perSlice[i] += perRemain;\n              if (i == remainderSlice) {\n                perSlice[i] += remainder;\n              }\n            }\n          }\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              log.debug(\"-- no leader in \" + s);\n              continue;\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n            modified = true;\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.addAndGet(perSlice[i]);\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n          }\n        } else {\n          // SMALL UPDATE: use exact assignment via DocRouter\n          for (SolrInputDocument doc : docs) {\n            String id = (String) doc.getFieldValue(\"id\");\n            if (id == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Document without id: \" + doc);\n            }\n            Slice s = coll.getRouter().getTargetSlice(id, doc, null, null, coll);\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              log.debug(\"-- no leader in \" + s);\n              continue;\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .incrementAndGet();\n            modified = true;\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.incrementAndGet();\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .incrementAndGet();\n          }\n        }\n\n        if (modified) {\n          docUpdates.forEach((sh, count) -> {\n            try {\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.numDocs\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.maxDoc\", count.get(), true, false);\n              // for each new document increase the size by DEFAULT_DOC_SIZE_BYTES\n              simSetShardValue(collection, sh, Type.CORE_IDX.metricsAttribute,\n                  DEFAULT_DOC_SIZE_BYTES * count.get(), true, false);\n              simSetShardValue(collection, sh, Variable.coreidxsize,\n                  Type.CORE_IDX.convertVal(DEFAULT_DOC_SIZE_BYTES * count.get()), true, false);\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          });\n          metricUpdates.forEach((sh, cores) -> {\n            cores.forEach((core, count) -> {\n              String registry = SolrMetricManager.getRegistryName(SolrInfoBean.Group.core, collection, sh,\n                  Utils.parseMetricsReplicaName(collection, core));\n              cloudManager.getMetricManager().registry(registry).counter(\"UPDATE./update.requests\").inc(count.get());\n            });\n          });\n        }\n      } finally {\n        lock.unlock();\n      }\n    }\n    return new UpdateResponse();\n  }\n\n","sourceOld":"  /**\n   * Simulate an update by modifying replica metrics.\n   * The following core metrics are updated:\n   * <ul>\n   *   <li><code>SEARCHER.searcher.numDocs</code> - increased by added docs, decreased by deleteById and deleteByQuery</li>\n   *   <li><code>SEARCHER.searcher.deletedDocs</code> - decreased by deleteById and deleteByQuery by up to <code>numDocs</code></li>\n   *   <li><code>SEARCHER.searcher.maxDoc</code> - always increased by the number of added docs.</li>\n   * </ul>\n   * <p>IMPORTANT limitations:</p>\n   * <ul>\n   *   <li>document replacements are always counted as new docs</li>\n   *   <li>delete by ID always succeeds (unless numDocs == 0)</li>\n   *   <li>deleteByQuery is not supported unless the query is <code>*:*</code></li>\n   * </ul>\n   * @param req update request. This request MUST have the <code>collection</code> param set.\n   * @return {@link UpdateResponse}\n   * @throws SolrException on errors, such as nonexistent collection or unsupported deleteByQuery\n   */\n  public UpdateResponse simUpdate(UpdateRequest req) throws SolrException, InterruptedException, IOException {\n    ensureNotClosed();\n    String collection = req.getCollection();\n    if (collection == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection not set\");\n    }\n    ensureSystemCollection(collection);\n    \n    DocCollection coll = getClusterState().getCollection(collection);\n    DocRouter router = coll.getRouter();\n    List<String> deletes = req.getDeleteById();\n    if (deletes != null && !deletes.isEmpty()) {\n      for (String id : deletes) {\n        Slice s = router.getTargetSlice(id, null, null, req.getParams(), coll);\n        Replica leader = s.getLeader();\n        if (leader == null) {\n          log.debug(\"-- no leader in \" + s);\n          continue;\n        }\n        cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n        ReplicaInfo ri = getReplicaInfo(leader);\n        Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n        if (numDocs == null || numDocs.intValue() <= 0) {\n          log.debug(\"-- attempting to delete nonexistent doc \" + id + \" from \" + s.getLeader());\n          continue;\n        }\n        AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n        if (bufferedUpdates != null) {\n          if (bufferedUpdates.get() > 0) {\n            bufferedUpdates.decrementAndGet();\n          } else {\n            log.debug(\"-- attempting to delete nonexistent buffered doc \" + id + \" from \" + s.getLeader());\n          }\n          continue;\n        }\n        lock.lockInterruptibly();\n        try {\n          simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.deletedDocs\", 1, true, false);\n          simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.numDocs\", -1, true, false);\n          Number indexSize = (Number)ri.getVariable(Type.CORE_IDX.metricsAttribute);\n          if (indexSize != null && indexSize.longValue() > SimCloudManager.DEFAULT_IDX_SIZE_BYTES) {\n            indexSize = indexSize.longValue() - DEFAULT_DOC_SIZE_BYTES;\n            simSetShardValue(collection, s.getName(), Type.CORE_IDX.metricsAttribute,\n                new AtomicLong(indexSize.longValue()), false, false);\n            simSetShardValue(collection, s.getName(), Variable.coreidxsize,\n                new AtomicDouble((Double)Type.CORE_IDX.convertVal(indexSize)), false, false);\n          } else {\n            throw new Exception(\"unexpected indexSize ri=\" + ri);\n          }\n        } catch (Exception e) {\n          throw new IOException(e);\n        } finally {\n          lock.unlock();\n        }\n      }\n    }\n    deletes = req.getDeleteQuery();\n    if (deletes != null && !deletes.isEmpty()) {\n      for (String q : deletes) {\n        if (!\"*:*\".equals(q)) {\n          throw new UnsupportedOperationException(\"Only '*:*' query is supported in deleteByQuery\");\n        }\n        for (Slice s : coll.getSlices()) {\n          Replica leader = s.getLeader();\n          if (leader == null) {\n            log.debug(\"-- no leader in \" + s);\n            continue;\n          }\n\n          cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n          ReplicaInfo ri = getReplicaInfo(leader);\n          Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n          if (numDocs == null || numDocs.intValue() == 0) {\n            continue;\n          }\n          lock.lockInterruptibly();\n          try {\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.deletedDocs\", new AtomicLong(numDocs.longValue()), false, false);\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.numDocs\", new AtomicLong(0), false, false);\n            simSetShardValue(collection, s.getName(), Type.CORE_IDX.metricsAttribute,\n                new AtomicLong(SimCloudManager.DEFAULT_IDX_SIZE_BYTES), false, false);\n            simSetShardValue(collection, s.getName(), Variable.coreidxsize,\n                new AtomicDouble((Double)Type.CORE_IDX.convertVal(SimCloudManager.DEFAULT_IDX_SIZE_BYTES)), false, false);\n          } catch (Exception e) {\n            throw new IOException(e);\n          } finally {\n            lock.unlock();\n          }\n        }\n      }\n    }\n    List<SolrInputDocument> docs = req.getDocuments();\n    int docCount = 0;\n    Iterator<SolrInputDocument> it = null;\n    if (docs != null) {\n      docCount = docs.size();\n    } else {\n      it = req.getDocIterator();\n      if (it != null) {\n        while (it.hasNext()) {\n          it.next();\n          docCount++;\n        }\n      }\n    }\n    if (docCount > 0) {\n      // this approach to updating counters and metrics drastically increases performance\n      // of bulk updates, because simSetShardValue is relatively costly\n\n      Map<String, AtomicLong> docUpdates = new HashMap<>();\n      Map<String, Map<String, AtomicLong>> metricUpdates = new HashMap<>();\n\n      // XXX don't add more than 2bln docs in one request\n      boolean modified = false;\n      lock.lockInterruptibly();\n      try {\n        coll = getClusterState().getCollection(collection);\n        Slice[] slices = coll.getActiveSlicesArr();\n        if (slices.length == 0) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection without slices\");\n        }\n        int[] perSlice = new int[slices.length];\n\n        if (it != null) {\n          // BULK UPDATE: simulate random doc assignment without actually calling DocRouter,\n          // which adds significant overhead\n\n          int totalAdded = 0;\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            long count = (long) docCount * ((long) s.getRange().max - (long) s.getRange().min) / 0x100000000L;\n            perSlice[i] = (int) count;\n            totalAdded += perSlice[i];\n          }\n          // loss of precision due to integer math\n          int diff = docCount - totalAdded;\n          if (diff > 0) {\n            // spread the remainder more or less equally\n            int perRemain = diff / slices.length;\n            int remainder = diff % slices.length;\n            int remainderSlice = slices.length > 1 ? bulkUpdateRandom.nextInt(slices.length) : 0;\n            for (int i = 0; i < slices.length; i++) {\n              perSlice[i] += perRemain;\n              if (i == remainderSlice) {\n                perSlice[i] += remainder;\n              }\n            }\n          }\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              log.debug(\"-- no leader in \" + s);\n              continue;\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n            modified = true;\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.addAndGet(perSlice[i]);\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n          }\n        } else {\n          // SMALL UPDATE: use exact assignment via DocRouter\n          for (SolrInputDocument doc : docs) {\n            String id = (String) doc.getFieldValue(\"id\");\n            if (id == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Document without id: \" + doc);\n            }\n            Slice s = coll.getRouter().getTargetSlice(id, doc, null, null, coll);\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              log.debug(\"-- no leader in \" + s);\n              continue;\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .incrementAndGet();\n            modified = true;\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.incrementAndGet();\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .incrementAndGet();\n          }\n        }\n\n        if (modified) {\n          docUpdates.forEach((sh, count) -> {\n            try {\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.numDocs\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.maxDoc\", count.get(), true, false);\n              // for each new document increase the size by DEFAULT_DOC_SIZE_BYTES\n              simSetShardValue(collection, sh, Type.CORE_IDX.metricsAttribute,\n                  DEFAULT_DOC_SIZE_BYTES * count.get(), true, false);\n              simSetShardValue(collection, sh, Variable.coreidxsize,\n                  Type.CORE_IDX.convertVal(DEFAULT_DOC_SIZE_BYTES * count.get()), true, false);\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          });\n          metricUpdates.forEach((sh, cores) -> {\n            cores.forEach((core, count) -> {\n              String registry = SolrMetricManager.getRegistryName(SolrInfoBean.Group.core, collection, sh,\n                  Utils.parseMetricsReplicaName(collection, core));\n              cloudManager.getMetricManager().registry(registry).counter(\"UPDATE./update.requests\").inc(count.get());\n            });\n          });\n        }\n      } finally {\n        lock.unlock();\n      }\n    }\n    return new UpdateResponse();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cb2411226bebe23191dc34a8e8252a936a7261a8","date":1557853382,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simUpdate(UpdateRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simUpdate(UpdateRequest).mjava","sourceNew":"  /**\n   * Simulate an update by modifying replica metrics.\n   * The following core metrics are updated:\n   * <ul>\n   *   <li><code>SEARCHER.searcher.numDocs</code> - increased by added docs, decreased by deleteById and deleteByQuery</li>\n   *   <li><code>SEARCHER.searcher.deletedDocs</code> - decreased by deleteById and deleteByQuery by up to <code>numDocs</code></li>\n   *   <li><code>SEARCHER.searcher.maxDoc</code> - always increased by the number of added docs.</li>\n   * </ul>\n   * <p>IMPORTANT limitations:</p>\n   * <ul>\n   *   <li>document replacements are always counted as new docs</li>\n   *   <li>delete by ID always succeeds (unless numDocs == 0)</li>\n   *   <li>deleteByQuery is not supported unless the query is <code>*:*</code></li>\n   * </ul>\n   * @param req update request. This request MUST have the <code>collection</code> param set.\n   * @return {@link UpdateResponse}\n   * @throws SolrException on errors, such as nonexistent collection or unsupported deleteByQuery\n   */\n  public UpdateResponse simUpdate(UpdateRequest req) throws SolrException, InterruptedException, IOException {\n    ensureNotClosed();\n    String collection = req.getCollection();\n    if (collection == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection not set\");\n    }\n    ensureSystemCollection(collection);\n    \n    DocCollection coll = getClusterState().getCollection(collection);\n    DocRouter router = coll.getRouter();\n    List<String> deletes = req.getDeleteById();\n    if (deletes != null && !deletes.isEmpty()) {\n      for (String id : deletes) {\n        Slice s = router.getTargetSlice(id, null, null, req.getParams(), coll);\n        Replica leader = s.getLeader();\n        if (leader == null) {\n          log.debug(\"-- no leader in \" + s);\n          continue;\n        }\n        cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n        ReplicaInfo ri = getReplicaInfo(leader);\n        Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n        if (numDocs == null || numDocs.intValue() <= 0) {\n          log.debug(\"-- attempting to delete nonexistent doc \" + id + \" from \" + s.getLeader());\n          continue;\n        }\n        AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n        if (bufferedUpdates != null) {\n          if (bufferedUpdates.get() > 0) {\n            bufferedUpdates.decrementAndGet();\n          } else {\n            log.debug(\"-- attempting to delete nonexistent buffered doc \" + id + \" from \" + s.getLeader());\n          }\n          continue;\n        }\n        lock.lockInterruptibly();\n        try {\n          simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.deletedDocs\", 1, true, false);\n          simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.numDocs\", -1, true, false);\n          Number indexSize = (Number)ri.getVariable(Type.CORE_IDX.metricsAttribute);\n          if (indexSize != null && indexSize.longValue() > SimCloudManager.DEFAULT_IDX_SIZE_BYTES) {\n            indexSize = indexSize.longValue() - DEFAULT_DOC_SIZE_BYTES;\n            simSetShardValue(collection, s.getName(), Type.CORE_IDX.metricsAttribute,\n                new AtomicLong(indexSize.longValue()), false, false);\n            simSetShardValue(collection, s.getName(), Variable.coreidxsize,\n                new AtomicDouble((Double)Type.CORE_IDX.convertVal(indexSize)), false, false);\n          } else {\n            throw new Exception(\"unexpected indexSize ri=\" + ri);\n          }\n        } catch (Exception e) {\n          throw new IOException(e);\n        } finally {\n          lock.unlock();\n        }\n      }\n    }\n    deletes = req.getDeleteQuery();\n    if (deletes != null && !deletes.isEmpty()) {\n      for (String q : deletes) {\n        if (!\"*:*\".equals(q)) {\n          throw new UnsupportedOperationException(\"Only '*:*' query is supported in deleteByQuery\");\n        }\n        for (Slice s : coll.getSlices()) {\n          Replica leader = s.getLeader();\n          if (leader == null) {\n            log.debug(\"-- no leader in \" + s);\n            continue;\n          }\n\n          cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n          ReplicaInfo ri = getReplicaInfo(leader);\n          Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n          if (numDocs == null || numDocs.intValue() == 0) {\n            continue;\n          }\n          lock.lockInterruptibly();\n          try {\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.deletedDocs\", new AtomicLong(numDocs.longValue()), false, false);\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.numDocs\", new AtomicLong(0), false, false);\n            simSetShardValue(collection, s.getName(), Type.CORE_IDX.metricsAttribute,\n                new AtomicLong(SimCloudManager.DEFAULT_IDX_SIZE_BYTES), false, false);\n            simSetShardValue(collection, s.getName(), Variable.coreidxsize,\n                new AtomicDouble((Double)Type.CORE_IDX.convertVal(SimCloudManager.DEFAULT_IDX_SIZE_BYTES)), false, false);\n          } catch (Exception e) {\n            throw new IOException(e);\n          } finally {\n            lock.unlock();\n          }\n        }\n      }\n    }\n    List<SolrInputDocument> docs = req.getDocuments();\n    int docCount = 0;\n    Iterator<SolrInputDocument> it = null;\n    if (docs != null) {\n      docCount = docs.size();\n    } else {\n      it = req.getDocIterator();\n      if (it != null) {\n        while (it.hasNext()) {\n          it.next();\n          docCount++;\n        }\n      }\n    }\n    if (docCount > 0) {\n      // this approach to updating counters and metrics drastically increases performance\n      // of bulk updates, because simSetShardValue is relatively costly\n\n      Map<String, AtomicLong> docUpdates = new HashMap<>();\n      Map<String, Map<String, AtomicLong>> metricUpdates = new HashMap<>();\n\n      // XXX don't add more than 2bln docs in one request\n      boolean modified = false;\n      lock.lockInterruptibly();\n      try {\n        coll = getClusterState().getCollection(collection);\n        Slice[] slices = coll.getActiveSlicesArr();\n        if (slices.length == 0) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection without slices\");\n        }\n        int[] perSlice = new int[slices.length];\n\n        if (it != null) {\n          // BULK UPDATE: simulate random doc assignment without actually calling DocRouter,\n          // which adds significant overhead\n\n          int totalAdded = 0;\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            long count = (long) docCount * ((long) s.getRange().max - (long) s.getRange().min) / 0x100000000L;\n            perSlice[i] = (int) count;\n            totalAdded += perSlice[i];\n          }\n          // loss of precision due to integer math\n          int diff = docCount - totalAdded;\n          if (diff > 0) {\n            // spread the remainder more or less equally\n            int perRemain = diff / slices.length;\n            int remainder = diff % slices.length;\n            int remainderSlice = slices.length > 1 ? bulkUpdateRandom.nextInt(slices.length) : 0;\n            for (int i = 0; i < slices.length; i++) {\n              perSlice[i] += perRemain;\n              if (i == remainderSlice) {\n                perSlice[i] += remainder;\n              }\n            }\n          }\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              log.debug(\"-- no leader in \" + s);\n              continue;\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n            modified = true;\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.addAndGet(perSlice[i]);\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n          }\n        } else {\n          // SMALL UPDATE: use exact assignment via DocRouter\n          for (SolrInputDocument doc : docs) {\n            String id = (String) doc.getFieldValue(\"id\");\n            if (id == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Document without id: \" + doc);\n            }\n            Slice s = coll.getRouter().getTargetSlice(id, doc, null, null, coll);\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              log.debug(\"-- no leader in \" + s);\n              continue;\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .incrementAndGet();\n            modified = true;\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.incrementAndGet();\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .incrementAndGet();\n          }\n        }\n\n        if (modified) {\n          docUpdates.forEach((sh, count) -> {\n            try {\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.numDocs\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.maxDoc\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"UPDATE./update.requests\", count.get(), true, false);\n              // for each new document increase the size by DEFAULT_DOC_SIZE_BYTES\n              simSetShardValue(collection, sh, Type.CORE_IDX.metricsAttribute,\n                  DEFAULT_DOC_SIZE_BYTES * count.get(), true, false);\n              simSetShardValue(collection, sh, Variable.coreidxsize,\n                  Type.CORE_IDX.convertVal(DEFAULT_DOC_SIZE_BYTES * count.get()), true, false);\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          });\n          metricUpdates.forEach((sh, cores) -> {\n            cores.forEach((core, count) -> {\n              String registry = SolrMetricManager.getRegistryName(SolrInfoBean.Group.core, collection, sh,\n                  Utils.parseMetricsReplicaName(collection, core));\n              cloudManager.getMetricManager().registry(registry).counter(\"UPDATE./update.requests\").inc(count.get());\n            });\n          });\n        }\n      } finally {\n        lock.unlock();\n      }\n    }\n    return new UpdateResponse();\n  }\n\n","sourceOld":"  /**\n   * Simulate an update by modifying replica metrics.\n   * The following core metrics are updated:\n   * <ul>\n   *   <li><code>SEARCHER.searcher.numDocs</code> - increased by added docs, decreased by deleteById and deleteByQuery</li>\n   *   <li><code>SEARCHER.searcher.deletedDocs</code> - decreased by deleteById and deleteByQuery by up to <code>numDocs</code></li>\n   *   <li><code>SEARCHER.searcher.maxDoc</code> - always increased by the number of added docs.</li>\n   * </ul>\n   * <p>IMPORTANT limitations:</p>\n   * <ul>\n   *   <li>document replacements are always counted as new docs</li>\n   *   <li>delete by ID always succeeds (unless numDocs == 0)</li>\n   *   <li>deleteByQuery is not supported unless the query is <code>*:*</code></li>\n   * </ul>\n   * @param req update request. This request MUST have the <code>collection</code> param set.\n   * @return {@link UpdateResponse}\n   * @throws SolrException on errors, such as nonexistent collection or unsupported deleteByQuery\n   */\n  public UpdateResponse simUpdate(UpdateRequest req) throws SolrException, InterruptedException, IOException {\n    ensureNotClosed();\n    String collection = req.getCollection();\n    if (collection == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection not set\");\n    }\n    ensureSystemCollection(collection);\n    \n    DocCollection coll = getClusterState().getCollection(collection);\n    DocRouter router = coll.getRouter();\n    List<String> deletes = req.getDeleteById();\n    if (deletes != null && !deletes.isEmpty()) {\n      for (String id : deletes) {\n        Slice s = router.getTargetSlice(id, null, null, req.getParams(), coll);\n        Replica leader = s.getLeader();\n        if (leader == null) {\n          log.debug(\"-- no leader in \" + s);\n          continue;\n        }\n        cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n        ReplicaInfo ri = getReplicaInfo(leader);\n        Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n        if (numDocs == null || numDocs.intValue() <= 0) {\n          log.debug(\"-- attempting to delete nonexistent doc \" + id + \" from \" + s.getLeader());\n          continue;\n        }\n        AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n        if (bufferedUpdates != null) {\n          if (bufferedUpdates.get() > 0) {\n            bufferedUpdates.decrementAndGet();\n          } else {\n            log.debug(\"-- attempting to delete nonexistent buffered doc \" + id + \" from \" + s.getLeader());\n          }\n          continue;\n        }\n        lock.lockInterruptibly();\n        try {\n          simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.deletedDocs\", 1, true, false);\n          simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.numDocs\", -1, true, false);\n          Number indexSize = (Number)ri.getVariable(Type.CORE_IDX.metricsAttribute);\n          if (indexSize != null && indexSize.longValue() > SimCloudManager.DEFAULT_IDX_SIZE_BYTES) {\n            indexSize = indexSize.longValue() - DEFAULT_DOC_SIZE_BYTES;\n            simSetShardValue(collection, s.getName(), Type.CORE_IDX.metricsAttribute,\n                new AtomicLong(indexSize.longValue()), false, false);\n            simSetShardValue(collection, s.getName(), Variable.coreidxsize,\n                new AtomicDouble((Double)Type.CORE_IDX.convertVal(indexSize)), false, false);\n          } else {\n            throw new Exception(\"unexpected indexSize ri=\" + ri);\n          }\n        } catch (Exception e) {\n          throw new IOException(e);\n        } finally {\n          lock.unlock();\n        }\n      }\n    }\n    deletes = req.getDeleteQuery();\n    if (deletes != null && !deletes.isEmpty()) {\n      for (String q : deletes) {\n        if (!\"*:*\".equals(q)) {\n          throw new UnsupportedOperationException(\"Only '*:*' query is supported in deleteByQuery\");\n        }\n        for (Slice s : coll.getSlices()) {\n          Replica leader = s.getLeader();\n          if (leader == null) {\n            log.debug(\"-- no leader in \" + s);\n            continue;\n          }\n\n          cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n          ReplicaInfo ri = getReplicaInfo(leader);\n          Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n          if (numDocs == null || numDocs.intValue() == 0) {\n            continue;\n          }\n          lock.lockInterruptibly();\n          try {\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.deletedDocs\", new AtomicLong(numDocs.longValue()), false, false);\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.numDocs\", new AtomicLong(0), false, false);\n            simSetShardValue(collection, s.getName(), Type.CORE_IDX.metricsAttribute,\n                new AtomicLong(SimCloudManager.DEFAULT_IDX_SIZE_BYTES), false, false);\n            simSetShardValue(collection, s.getName(), Variable.coreidxsize,\n                new AtomicDouble((Double)Type.CORE_IDX.convertVal(SimCloudManager.DEFAULT_IDX_SIZE_BYTES)), false, false);\n          } catch (Exception e) {\n            throw new IOException(e);\n          } finally {\n            lock.unlock();\n          }\n        }\n      }\n    }\n    List<SolrInputDocument> docs = req.getDocuments();\n    int docCount = 0;\n    Iterator<SolrInputDocument> it = null;\n    if (docs != null) {\n      docCount = docs.size();\n    } else {\n      it = req.getDocIterator();\n      if (it != null) {\n        while (it.hasNext()) {\n          it.next();\n          docCount++;\n        }\n      }\n    }\n    if (docCount > 0) {\n      // this approach to updating counters and metrics drastically increases performance\n      // of bulk updates, because simSetShardValue is relatively costly\n\n      Map<String, AtomicLong> docUpdates = new HashMap<>();\n      Map<String, Map<String, AtomicLong>> metricUpdates = new HashMap<>();\n\n      // XXX don't add more than 2bln docs in one request\n      boolean modified = false;\n      lock.lockInterruptibly();\n      try {\n        coll = getClusterState().getCollection(collection);\n        Slice[] slices = coll.getActiveSlicesArr();\n        if (slices.length == 0) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection without slices\");\n        }\n        int[] perSlice = new int[slices.length];\n\n        if (it != null) {\n          // BULK UPDATE: simulate random doc assignment without actually calling DocRouter,\n          // which adds significant overhead\n\n          int totalAdded = 0;\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            long count = (long) docCount * ((long) s.getRange().max - (long) s.getRange().min) / 0x100000000L;\n            perSlice[i] = (int) count;\n            totalAdded += perSlice[i];\n          }\n          // loss of precision due to integer math\n          int diff = docCount - totalAdded;\n          if (diff > 0) {\n            // spread the remainder more or less equally\n            int perRemain = diff / slices.length;\n            int remainder = diff % slices.length;\n            int remainderSlice = slices.length > 1 ? bulkUpdateRandom.nextInt(slices.length) : 0;\n            for (int i = 0; i < slices.length; i++) {\n              perSlice[i] += perRemain;\n              if (i == remainderSlice) {\n                perSlice[i] += remainder;\n              }\n            }\n          }\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              log.debug(\"-- no leader in \" + s);\n              continue;\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n            modified = true;\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.addAndGet(perSlice[i]);\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n          }\n        } else {\n          // SMALL UPDATE: use exact assignment via DocRouter\n          for (SolrInputDocument doc : docs) {\n            String id = (String) doc.getFieldValue(\"id\");\n            if (id == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Document without id: \" + doc);\n            }\n            Slice s = coll.getRouter().getTargetSlice(id, doc, null, null, coll);\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              log.debug(\"-- no leader in \" + s);\n              continue;\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .incrementAndGet();\n            modified = true;\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.incrementAndGet();\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .incrementAndGet();\n          }\n        }\n\n        if (modified) {\n          docUpdates.forEach((sh, count) -> {\n            try {\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.numDocs\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.maxDoc\", count.get(), true, false);\n              // for each new document increase the size by DEFAULT_DOC_SIZE_BYTES\n              simSetShardValue(collection, sh, Type.CORE_IDX.metricsAttribute,\n                  DEFAULT_DOC_SIZE_BYTES * count.get(), true, false);\n              simSetShardValue(collection, sh, Variable.coreidxsize,\n                  Type.CORE_IDX.convertVal(DEFAULT_DOC_SIZE_BYTES * count.get()), true, false);\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          });\n          metricUpdates.forEach((sh, cores) -> {\n            cores.forEach((core, count) -> {\n              String registry = SolrMetricManager.getRegistryName(SolrInfoBean.Group.core, collection, sh,\n                  Utils.parseMetricsReplicaName(collection, core));\n              cloudManager.getMetricManager().registry(registry).counter(\"UPDATE./update.requests\").inc(count.get());\n            });\n          });\n        }\n      } finally {\n        lock.unlock();\n      }\n    }\n    return new UpdateResponse();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2d80c1ad9241ae005a167d7ee8ac473601b0e57c","date":1559036097,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simUpdate(UpdateRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simUpdate(UpdateRequest).mjava","sourceNew":"  /**\n   * Simulate an update by modifying replica metrics.\n   * The following core metrics are updated:\n   * <ul>\n   *   <li><code>SEARCHER.searcher.numDocs</code> - increased by added docs, decreased by deleteById and deleteByQuery</li>\n   *   <li><code>SEARCHER.searcher.deletedDocs</code> - decreased by deleteById and deleteByQuery by up to <code>numDocs</code></li>\n   *   <li><code>SEARCHER.searcher.maxDoc</code> - always increased by the number of added docs.</li>\n   * </ul>\n   * <p>IMPORTANT limitations:</p>\n   * <ul>\n   *   <li>document replacements are always counted as new docs</li>\n   *   <li>delete by ID always succeeds (unless numDocs == 0)</li>\n   *   <li>deleteByQuery is not supported unless the query is <code>*:*</code></li>\n   * </ul>\n   * @param req update request. This request MUST have the <code>collection</code> param set.\n   * @return {@link UpdateResponse}\n   * @throws SolrException on errors, such as nonexistent collection or unsupported deleteByQuery\n   */\n  public UpdateResponse simUpdate(UpdateRequest req) throws SolrException, InterruptedException, IOException {\n    ensureNotClosed();\n    String collection = req.getCollection();\n    if (collection == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection not set\");\n    }\n    ensureSystemCollection(collection);\n    \n    DocCollection coll = getClusterState().getCollection(collection);\n    DocRouter router = coll.getRouter();\n    List<String> deletes = req.getDeleteById();\n    if (deletes != null && !deletes.isEmpty()) {\n      for (String id : deletes) {\n        Slice s = router.getTargetSlice(id, null, null, req.getParams(), coll);\n        Replica leader = s.getLeader();\n        if (leader == null) {\n          log.debug(\"-- no leader in \" + s);\n          continue;\n        }\n        cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n        ReplicaInfo ri = getReplicaInfo(leader);\n        Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n        if (numDocs == null || numDocs.intValue() <= 0) {\n          log.debug(\"-- attempting to delete nonexistent doc \" + id + \" from \" + s.getLeader());\n          continue;\n        }\n        AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n        if (bufferedUpdates != null) {\n          if (bufferedUpdates.get() > 0) {\n            bufferedUpdates.decrementAndGet();\n          } else {\n            log.debug(\"-- attempting to delete nonexistent buffered doc \" + id + \" from \" + s.getLeader());\n          }\n          continue;\n        }\n        lock.lockInterruptibly();\n        try {\n          simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.deletedDocs\", 1, true, false);\n          simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.numDocs\", -1, true, false);\n          Number indexSize = (Number)ri.getVariable(Type.CORE_IDX.metricsAttribute);\n          if (indexSize != null && indexSize.longValue() > SimCloudManager.DEFAULT_IDX_SIZE_BYTES) {\n            indexSize = indexSize.longValue() - DEFAULT_DOC_SIZE_BYTES;\n            simSetShardValue(collection, s.getName(), Type.CORE_IDX.metricsAttribute,\n                new AtomicLong(indexSize.longValue()), false, false);\n            simSetShardValue(collection, s.getName(), Variable.coreidxsize,\n                new AtomicDouble((Double)Type.CORE_IDX.convertVal(indexSize)), false, false);\n          } else {\n            throw new Exception(\"unexpected indexSize ri=\" + ri);\n          }\n        } catch (Exception e) {\n          throw new IOException(e);\n        } finally {\n          lock.unlock();\n        }\n      }\n    }\n    deletes = req.getDeleteQuery();\n    if (deletes != null && !deletes.isEmpty()) {\n      for (String q : deletes) {\n        if (!\"*:*\".equals(q)) {\n          throw new UnsupportedOperationException(\"Only '*:*' query is supported in deleteByQuery\");\n        }\n        for (Slice s : coll.getSlices()) {\n          Replica leader = s.getLeader();\n          if (leader == null) {\n            log.debug(\"-- no leader in \" + s);\n            continue;\n          }\n\n          cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n          ReplicaInfo ri = getReplicaInfo(leader);\n          Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n          if (numDocs == null || numDocs.intValue() == 0) {\n            continue;\n          }\n          lock.lockInterruptibly();\n          try {\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.deletedDocs\", new AtomicLong(numDocs.longValue()), false, false);\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.numDocs\", new AtomicLong(0), false, false);\n            simSetShardValue(collection, s.getName(), Type.CORE_IDX.metricsAttribute,\n                new AtomicLong(SimCloudManager.DEFAULT_IDX_SIZE_BYTES), false, false);\n            simSetShardValue(collection, s.getName(), Variable.coreidxsize,\n                new AtomicDouble((Double)Type.CORE_IDX.convertVal(SimCloudManager.DEFAULT_IDX_SIZE_BYTES)), false, false);\n          } catch (Exception e) {\n            throw new IOException(e);\n          } finally {\n            lock.unlock();\n          }\n        }\n      }\n    }\n    List<SolrInputDocument> docs = req.getDocuments();\n    int docCount = 0;\n    Iterator<SolrInputDocument> it = null;\n    if (docs != null) {\n      docCount = docs.size();\n    } else {\n      it = req.getDocIterator();\n      if (it != null) {\n        while (it.hasNext()) {\n          it.next();\n          docCount++;\n        }\n      }\n    }\n    if (docCount > 0) {\n      // this approach to updating counters and metrics drastically increases performance\n      // of bulk updates, because simSetShardValue is relatively costly\n\n      Map<String, AtomicLong> docUpdates = new HashMap<>();\n      Map<String, Map<String, AtomicLong>> metricUpdates = new HashMap<>();\n\n      // XXX don't add more than 2bln docs in one request\n      boolean modified = false;\n      lock.lockInterruptibly();\n      try {\n        coll = getClusterState().getCollection(collection);\n        Slice[] slices = coll.getActiveSlicesArr();\n        if (slices.length == 0) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection without slices\");\n        }\n        int[] perSlice = new int[slices.length];\n\n        if (it != null) {\n          // BULK UPDATE: simulate random doc assignment without actually calling DocRouter,\n          // which adds significant overhead\n\n          int totalAdded = 0;\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            long count = (long) docCount * ((long) s.getRange().max - (long) s.getRange().min) / 0x100000000L;\n            perSlice[i] = (int) count;\n            totalAdded += perSlice[i];\n          }\n          // loss of precision due to integer math\n          int diff = docCount - totalAdded;\n          if (diff > 0) {\n            // spread the remainder more or less equally\n            int perRemain = diff / slices.length;\n            int remainder = diff % slices.length;\n            int remainderSlice = slices.length > 1 ? bulkUpdateRandom.nextInt(slices.length) : 0;\n            for (int i = 0; i < slices.length; i++) {\n              perSlice[i] += perRemain;\n              if (i == remainderSlice) {\n                perSlice[i] += remainder;\n              }\n            }\n          }\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              log.debug(\"-- no leader in \" + s);\n              continue;\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n            modified = true;\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.addAndGet(perSlice[i]);\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n          }\n        } else {\n          // SMALL UPDATE: use exact assignment via DocRouter\n          for (SolrInputDocument doc : docs) {\n            String id = (String) doc.getFieldValue(\"id\");\n            if (id == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Document without id: \" + doc);\n            }\n            Slice s = coll.getRouter().getTargetSlice(id, doc, null, null, coll);\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              log.debug(\"-- no leader in \" + s);\n              continue;\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .incrementAndGet();\n            modified = true;\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.incrementAndGet();\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .incrementAndGet();\n          }\n        }\n\n        if (modified) {\n          docUpdates.forEach((sh, count) -> {\n            try {\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.numDocs\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.maxDoc\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"UPDATE./update.requests\", count.get(), true, false);\n              // for each new document increase the size by DEFAULT_DOC_SIZE_BYTES\n              simSetShardValue(collection, sh, Type.CORE_IDX.metricsAttribute,\n                  DEFAULT_DOC_SIZE_BYTES * count.get(), true, false);\n              simSetShardValue(collection, sh, Variable.coreidxsize,\n                  Type.CORE_IDX.convertVal(DEFAULT_DOC_SIZE_BYTES * count.get()), true, false);\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          });\n          metricUpdates.forEach((sh, cores) -> {\n            cores.forEach((core, count) -> {\n              String registry = SolrMetricManager.getRegistryName(SolrInfoBean.Group.core, collection, sh,\n                  Utils.parseMetricsReplicaName(collection, core));\n              cloudManager.getMetricManager().registry(registry).counter(\"UPDATE./update.requests\").inc(count.get());\n            });\n          });\n        }\n      } finally {\n        lock.unlock();\n      }\n    }\n    SolrParams params = req.getParams();\n    if (params != null && (params.getBool(UpdateParams.OPTIMIZE, false) || params.getBool(UpdateParams.EXPUNGE_DELETES, false))) {\n      lock.lockInterruptibly();\n      try {\n        coll.getSlices().forEach(s -> {\n          Replica leader = s.getLeader();\n          ReplicaInfo ri = getReplicaInfo(leader);\n          Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n          if (numDocs == null || numDocs.intValue() == 0) {\n            numDocs = 0;\n          }\n          try {\n            simSetShardValue(ri.getCollection(), ri.getShard(), \"SEARCHER.searcher.maxDoc\", numDocs, false, false);\n            simSetShardValue(ri.getCollection(), ri.getShard(), \"SEARCHER.searcher.deletedDocs\", 0, false, false);\n          } catch (Exception e) {\n            throw new RuntimeException(e);\n          }\n        });\n      } finally {\n        lock.unlock();\n      }\n    }\n    return new UpdateResponse();\n  }\n\n","sourceOld":"  /**\n   * Simulate an update by modifying replica metrics.\n   * The following core metrics are updated:\n   * <ul>\n   *   <li><code>SEARCHER.searcher.numDocs</code> - increased by added docs, decreased by deleteById and deleteByQuery</li>\n   *   <li><code>SEARCHER.searcher.deletedDocs</code> - decreased by deleteById and deleteByQuery by up to <code>numDocs</code></li>\n   *   <li><code>SEARCHER.searcher.maxDoc</code> - always increased by the number of added docs.</li>\n   * </ul>\n   * <p>IMPORTANT limitations:</p>\n   * <ul>\n   *   <li>document replacements are always counted as new docs</li>\n   *   <li>delete by ID always succeeds (unless numDocs == 0)</li>\n   *   <li>deleteByQuery is not supported unless the query is <code>*:*</code></li>\n   * </ul>\n   * @param req update request. This request MUST have the <code>collection</code> param set.\n   * @return {@link UpdateResponse}\n   * @throws SolrException on errors, such as nonexistent collection or unsupported deleteByQuery\n   */\n  public UpdateResponse simUpdate(UpdateRequest req) throws SolrException, InterruptedException, IOException {\n    ensureNotClosed();\n    String collection = req.getCollection();\n    if (collection == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection not set\");\n    }\n    ensureSystemCollection(collection);\n    \n    DocCollection coll = getClusterState().getCollection(collection);\n    DocRouter router = coll.getRouter();\n    List<String> deletes = req.getDeleteById();\n    if (deletes != null && !deletes.isEmpty()) {\n      for (String id : deletes) {\n        Slice s = router.getTargetSlice(id, null, null, req.getParams(), coll);\n        Replica leader = s.getLeader();\n        if (leader == null) {\n          log.debug(\"-- no leader in \" + s);\n          continue;\n        }\n        cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n        ReplicaInfo ri = getReplicaInfo(leader);\n        Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n        if (numDocs == null || numDocs.intValue() <= 0) {\n          log.debug(\"-- attempting to delete nonexistent doc \" + id + \" from \" + s.getLeader());\n          continue;\n        }\n        AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n        if (bufferedUpdates != null) {\n          if (bufferedUpdates.get() > 0) {\n            bufferedUpdates.decrementAndGet();\n          } else {\n            log.debug(\"-- attempting to delete nonexistent buffered doc \" + id + \" from \" + s.getLeader());\n          }\n          continue;\n        }\n        lock.lockInterruptibly();\n        try {\n          simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.deletedDocs\", 1, true, false);\n          simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.numDocs\", -1, true, false);\n          Number indexSize = (Number)ri.getVariable(Type.CORE_IDX.metricsAttribute);\n          if (indexSize != null && indexSize.longValue() > SimCloudManager.DEFAULT_IDX_SIZE_BYTES) {\n            indexSize = indexSize.longValue() - DEFAULT_DOC_SIZE_BYTES;\n            simSetShardValue(collection, s.getName(), Type.CORE_IDX.metricsAttribute,\n                new AtomicLong(indexSize.longValue()), false, false);\n            simSetShardValue(collection, s.getName(), Variable.coreidxsize,\n                new AtomicDouble((Double)Type.CORE_IDX.convertVal(indexSize)), false, false);\n          } else {\n            throw new Exception(\"unexpected indexSize ri=\" + ri);\n          }\n        } catch (Exception e) {\n          throw new IOException(e);\n        } finally {\n          lock.unlock();\n        }\n      }\n    }\n    deletes = req.getDeleteQuery();\n    if (deletes != null && !deletes.isEmpty()) {\n      for (String q : deletes) {\n        if (!\"*:*\".equals(q)) {\n          throw new UnsupportedOperationException(\"Only '*:*' query is supported in deleteByQuery\");\n        }\n        for (Slice s : coll.getSlices()) {\n          Replica leader = s.getLeader();\n          if (leader == null) {\n            log.debug(\"-- no leader in \" + s);\n            continue;\n          }\n\n          cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n          ReplicaInfo ri = getReplicaInfo(leader);\n          Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n          if (numDocs == null || numDocs.intValue() == 0) {\n            continue;\n          }\n          lock.lockInterruptibly();\n          try {\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.deletedDocs\", new AtomicLong(numDocs.longValue()), false, false);\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.numDocs\", new AtomicLong(0), false, false);\n            simSetShardValue(collection, s.getName(), Type.CORE_IDX.metricsAttribute,\n                new AtomicLong(SimCloudManager.DEFAULT_IDX_SIZE_BYTES), false, false);\n            simSetShardValue(collection, s.getName(), Variable.coreidxsize,\n                new AtomicDouble((Double)Type.CORE_IDX.convertVal(SimCloudManager.DEFAULT_IDX_SIZE_BYTES)), false, false);\n          } catch (Exception e) {\n            throw new IOException(e);\n          } finally {\n            lock.unlock();\n          }\n        }\n      }\n    }\n    List<SolrInputDocument> docs = req.getDocuments();\n    int docCount = 0;\n    Iterator<SolrInputDocument> it = null;\n    if (docs != null) {\n      docCount = docs.size();\n    } else {\n      it = req.getDocIterator();\n      if (it != null) {\n        while (it.hasNext()) {\n          it.next();\n          docCount++;\n        }\n      }\n    }\n    if (docCount > 0) {\n      // this approach to updating counters and metrics drastically increases performance\n      // of bulk updates, because simSetShardValue is relatively costly\n\n      Map<String, AtomicLong> docUpdates = new HashMap<>();\n      Map<String, Map<String, AtomicLong>> metricUpdates = new HashMap<>();\n\n      // XXX don't add more than 2bln docs in one request\n      boolean modified = false;\n      lock.lockInterruptibly();\n      try {\n        coll = getClusterState().getCollection(collection);\n        Slice[] slices = coll.getActiveSlicesArr();\n        if (slices.length == 0) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection without slices\");\n        }\n        int[] perSlice = new int[slices.length];\n\n        if (it != null) {\n          // BULK UPDATE: simulate random doc assignment without actually calling DocRouter,\n          // which adds significant overhead\n\n          int totalAdded = 0;\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            long count = (long) docCount * ((long) s.getRange().max - (long) s.getRange().min) / 0x100000000L;\n            perSlice[i] = (int) count;\n            totalAdded += perSlice[i];\n          }\n          // loss of precision due to integer math\n          int diff = docCount - totalAdded;\n          if (diff > 0) {\n            // spread the remainder more or less equally\n            int perRemain = diff / slices.length;\n            int remainder = diff % slices.length;\n            int remainderSlice = slices.length > 1 ? bulkUpdateRandom.nextInt(slices.length) : 0;\n            for (int i = 0; i < slices.length; i++) {\n              perSlice[i] += perRemain;\n              if (i == remainderSlice) {\n                perSlice[i] += remainder;\n              }\n            }\n          }\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              log.debug(\"-- no leader in \" + s);\n              continue;\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n            modified = true;\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.addAndGet(perSlice[i]);\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n          }\n        } else {\n          // SMALL UPDATE: use exact assignment via DocRouter\n          for (SolrInputDocument doc : docs) {\n            String id = (String) doc.getFieldValue(\"id\");\n            if (id == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Document without id: \" + doc);\n            }\n            Slice s = coll.getRouter().getTargetSlice(id, doc, null, null, coll);\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              log.debug(\"-- no leader in \" + s);\n              continue;\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .incrementAndGet();\n            modified = true;\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.incrementAndGet();\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .incrementAndGet();\n          }\n        }\n\n        if (modified) {\n          docUpdates.forEach((sh, count) -> {\n            try {\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.numDocs\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.maxDoc\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"UPDATE./update.requests\", count.get(), true, false);\n              // for each new document increase the size by DEFAULT_DOC_SIZE_BYTES\n              simSetShardValue(collection, sh, Type.CORE_IDX.metricsAttribute,\n                  DEFAULT_DOC_SIZE_BYTES * count.get(), true, false);\n              simSetShardValue(collection, sh, Variable.coreidxsize,\n                  Type.CORE_IDX.convertVal(DEFAULT_DOC_SIZE_BYTES * count.get()), true, false);\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          });\n          metricUpdates.forEach((sh, cores) -> {\n            cores.forEach((core, count) -> {\n              String registry = SolrMetricManager.getRegistryName(SolrInfoBean.Group.core, collection, sh,\n                  Utils.parseMetricsReplicaName(collection, core));\n              cloudManager.getMetricManager().registry(registry).counter(\"UPDATE./update.requests\").inc(count.get());\n            });\n          });\n        }\n      } finally {\n        lock.unlock();\n      }\n    }\n    return new UpdateResponse();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bffd02b7c57b27d76ece244beb098f61c974b9d9","date":1568827127,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simUpdate(UpdateRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simUpdate(UpdateRequest).mjava","sourceNew":"  /**\n   * Simulate an update by modifying replica metrics.\n   * The following core metrics are updated:\n   * <ul>\n   *   <li><code>SEARCHER.searcher.numDocs</code> - increased by added docs, decreased by deleteById and deleteByQuery</li>\n   *   <li><code>SEARCHER.searcher.deletedDocs</code> - decreased by deleteById and deleteByQuery by up to <code>numDocs</code></li>\n   *   <li><code>SEARCHER.searcher.maxDoc</code> - always increased by the number of added docs.</li>\n   * </ul>\n   * <p>IMPORTANT limitations:</p>\n   * <ul>\n   *   <li>document replacements are always counted as new docs</li>\n   *   <li>delete by ID always succeeds (unless numDocs == 0)</li>\n   *   <li>deleteByQuery is not supported unless the query is <code>*:*</code></li>\n   * </ul>\n   * @param req update request. This request MUST have the <code>collection</code> param set.\n   * @return {@link UpdateResponse}\n   * @throws SolrException on errors, such as nonexistent collection or unsupported deleteByQuery\n   */\n  public UpdateResponse simUpdate(UpdateRequest req) throws SolrException, InterruptedException, IOException {\n    ensureNotClosed();\n    String collection = req.getCollection();\n    if (collection == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection not set\");\n    }\n    ensureSystemCollection(collection);\n    DocCollection coll = getClusterState().getCollection(collection);\n    DocRouter router = coll.getRouter();\n    List<String> deletes = req.getDeleteById();\n    Map<String, AtomicLong> freediskDeltaPerNode = new HashMap<>();\n    if (deletes != null && !deletes.isEmpty()) {\n      Map<String, AtomicLong> deletesPerShard = new HashMap<>();\n      Map<String, Number> indexSizePerShard = new HashMap<>();\n      for (String id : deletes) {\n        Slice s = router.getTargetSlice(id, null, null, req.getParams(), coll);\n        Replica leader = s.getLeader();\n        if (leader == null) {\n          throw new IOException(\"-- no leader in \" + s);\n        }\n        cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n        ReplicaInfo ri = getReplicaInfo(leader);\n        Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n        if (numDocs == null || numDocs.intValue() <= 0) {\n          log.debug(\"-- attempting to delete nonexistent doc \" + id + \" from \" + s.getLeader());\n          continue;\n        }\n\n        // this is somewhat wrong - we should wait until buffered updates are applied\n        // but this way the freedisk changes are much easier to track\n        s.getReplicas().forEach(r ->\n            freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong(0))\n                .addAndGet(DEFAULT_DOC_SIZE_BYTES));\n\n        AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n        if (bufferedUpdates != null) {\n          if (bufferedUpdates.get() > 0) {\n            bufferedUpdates.decrementAndGet();\n          } else {\n            log.debug(\"-- attempting to delete nonexistent buffered doc \" + id + \" from \" + s.getLeader());\n          }\n          continue;\n        }\n        deletesPerShard.computeIfAbsent(s.getName(), slice -> new AtomicLong(0)).incrementAndGet();\n        Number indexSize = (Number)ri.getVariable(Type.CORE_IDX.metricsAttribute);\n        if (indexSize != null) {\n          indexSizePerShard.put(s.getName(), indexSize);\n        }\n      }\n      if (!deletesPerShard.isEmpty()) {\n        lock.lockInterruptibly();\n        try {\n          for (Map.Entry<String, AtomicLong> entry : deletesPerShard.entrySet()) {\n            String shard = entry.getKey();\n            simSetShardValue(collection, shard, \"SEARCHER.searcher.deletedDocs\", entry.getValue().get(), true, false);\n            simSetShardValue(collection, shard, \"SEARCHER.searcher.numDocs\", -entry.getValue().get(), true, false);\n            Number indexSize = indexSizePerShard.get(shard);\n            long delSize = DEFAULT_DOC_SIZE_BYTES * entry.getValue().get();\n            if (indexSize != null) {\n              indexSize = indexSize.longValue() - delSize;\n              if (indexSize.longValue() < SimCloudManager.DEFAULT_IDX_SIZE_BYTES) {\n                indexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES;\n              }\n              simSetShardValue(collection, shard, Type.CORE_IDX.metricsAttribute,\n                  new AtomicLong(indexSize.longValue()), false, false);\n              simSetShardValue(collection, shard, Variable.coreidxsize,\n                  new AtomicDouble((Double)Type.CORE_IDX.convertVal(indexSize)), false, false);\n            } else {\n              throw new Exception(\"unexpected indexSize for collection=\" + collection + \", shard=\" + shard + \": \" + indexSize);\n            }\n          }\n        } catch (Exception e) {\n          throw new IOException(e);\n        } finally {\n          lock.unlock();\n        }\n      }\n    }\n    deletes = req.getDeleteQuery();\n    if (deletes != null && !deletes.isEmpty()) {\n      for (String q : deletes) {\n        if (!\"*:*\".equals(q)) {\n          throw new UnsupportedOperationException(\"Only '*:*' query is supported in deleteByQuery\");\n        }\n        //log.debug(\"-- req delByQ \" + collection);\n        for (Slice s : coll.getSlices()) {\n          Replica leader = s.getLeader();\n          if (leader == null) {\n            throw new IOException(\"-- no leader in \" + s);\n          }\n\n          cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n          ReplicaInfo ri = getReplicaInfo(leader);\n          Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n          if (numDocs == null || numDocs.intValue() == 0) {\n            continue;\n          }\n          lock.lockInterruptibly();\n          try {\n            Number indexSize = (Number)ri.getVariable(Type.CORE_IDX.metricsAttribute);\n            if (indexSize != null) {\n              long delta = indexSize.longValue() < SimCloudManager.DEFAULT_IDX_SIZE_BYTES ? 0 :\n                  indexSize.longValue() - SimCloudManager.DEFAULT_IDX_SIZE_BYTES;\n              s.getReplicas().forEach(r ->\n                  freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong(0))\n                  .addAndGet(delta));\n            } else {\n              throw new RuntimeException(\"Missing index size in \" + ri);\n            }\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.deletedDocs\", new AtomicLong(numDocs.longValue()), false, false);\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.numDocs\", new AtomicLong(0), false, false);\n            simSetShardValue(collection, s.getName(), Type.CORE_IDX.metricsAttribute,\n                new AtomicLong(SimCloudManager.DEFAULT_IDX_SIZE_BYTES), false, false);\n            simSetShardValue(collection, s.getName(), Variable.coreidxsize,\n                new AtomicDouble((Double)Type.CORE_IDX.convertVal(SimCloudManager.DEFAULT_IDX_SIZE_BYTES)), false, false);\n          } catch (Exception e) {\n            throw new IOException(e);\n          } finally {\n            lock.unlock();\n          }\n        }\n      }\n    }\n    List<SolrInputDocument> docs = req.getDocuments();\n    int docCount = 0;\n    Iterator<SolrInputDocument> it = null;\n    if (docs != null) {\n      docCount = docs.size();\n    } else {\n      it = req.getDocIterator();\n      if (it != null) {\n        while (it.hasNext()) {\n          it.next();\n          docCount++;\n        }\n      }\n    }\n    if (docCount > 0) {\n      //log.debug(\"-- req update \" + collection + \" / \" + docCount);\n      // this approach to updating counters and metrics drastically increases performance\n      // of bulk updates, because simSetShardValue is relatively costly\n\n      Map<String, AtomicLong> docUpdates = new HashMap<>();\n      Map<String, Map<String, AtomicLong>> metricUpdates = new HashMap<>();\n\n      // XXX don't add more than 2bln docs in one request\n      boolean modified = false;\n      lock.lockInterruptibly();\n      try {\n        coll = getClusterState().getCollection(collection);\n        Slice[] slices = coll.getActiveSlicesArr();\n        if (slices.length == 0) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection without slices\");\n        }\n        int[] perSlice = new int[slices.length];\n\n        if (it != null) {\n          // BULK UPDATE: simulate random doc assignment without actually calling DocRouter,\n          // which adds significant overhead\n\n          int totalAdded = 0;\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            long count = (long) docCount * ((long) s.getRange().max - (long) s.getRange().min) / 0x100000000L;\n            perSlice[i] = (int) count;\n            totalAdded += perSlice[i];\n          }\n          // loss of precision due to integer math\n          int diff = docCount - totalAdded;\n          if (diff > 0) {\n            // spread the remainder more or less equally\n            int perRemain = diff / slices.length;\n            int remainder = diff % slices.length;\n            int remainderSlice = slices.length > 1 ? bulkUpdateRandom.nextInt(slices.length) : 0;\n            for (int i = 0; i < slices.length; i++) {\n              perSlice[i] += perRemain;\n              if (i == remainderSlice) {\n                perSlice[i] += remainder;\n              }\n            }\n          }\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              throw new IOException(\"-- no leader in \" + s);\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n            modified = true;\n            long perSliceCount = perSlice[i];\n            s.getReplicas().forEach(r ->\n                freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong(0))\n                    .addAndGet(-perSliceCount * DEFAULT_DOC_SIZE_BYTES));\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.addAndGet(perSlice[i]);\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n          }\n        } else {\n          // SMALL UPDATE: use exact assignment via DocRouter\n          for (SolrInputDocument doc : docs) {\n            String id = (String) doc.getFieldValue(\"id\");\n            if (id == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Document without id: \" + doc);\n            }\n            Slice s = coll.getRouter().getTargetSlice(id, doc, null, null, coll);\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              throw new IOException(\"-- no leader in \" + s);\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .incrementAndGet();\n            modified = true;\n            s.getReplicas().forEach(r ->\n                freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong())\n                    .addAndGet(-DEFAULT_DOC_SIZE_BYTES));\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.incrementAndGet();\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .incrementAndGet();\n          }\n        }\n\n        if (modified) {\n          docUpdates.forEach((sh, count) -> {\n            try {\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.numDocs\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.maxDoc\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"UPDATE./update.requests\", count.get(), true, false);\n              // for each new document increase the size by DEFAULT_DOC_SIZE_BYTES\n              simSetShardValue(collection, sh, Type.CORE_IDX.metricsAttribute,\n                  DEFAULT_DOC_SIZE_BYTES * count.get(), true, false);\n              simSetShardValue(collection, sh, Variable.coreidxsize,\n                  Type.CORE_IDX.convertVal(DEFAULT_DOC_SIZE_BYTES * count.get()), true, false);\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          });\n          metricUpdates.forEach((sh, cores) -> {\n            cores.forEach((core, count) -> {\n              String registry = SolrMetricManager.getRegistryName(SolrInfoBean.Group.core, collection, sh,\n                  Utils.parseMetricsReplicaName(collection, core));\n              cloudManager.getMetricManager().registry(registry).counter(\"UPDATE./update.requests\").inc(count.get());\n            });\n          });\n        }\n      } finally {\n        lock.unlock();\n      }\n    }\n    if (!freediskDeltaPerNode.isEmpty()) {\n      SimNodeStateProvider nodeStateProvider = cloudManager.getSimNodeStateProvider();\n      freediskDeltaPerNode.forEach((node, delta) -> {\n        if (delta.get() == 0) {\n          return;\n        }\n        try {\n          // this method does its own locking to prevent races\n          nodeStateProvider.simUpdateNodeValue(node, Type.FREEDISK.tagName, val -> {\n            if (val == null) {\n              throw new RuntimeException(\"no freedisk for node \" + node);\n            }\n            double freedisk = ((Number) val).doubleValue();\n            double deltaGB = (Double) Type.FREEDISK.convertVal(delta.get());\n            freedisk += deltaGB;\n            if (freedisk < 0) {\n              log.warn(\"-- freedisk=\" + freedisk + \" - ran out of disk space on node \" + node);\n              freedisk = 0;\n            }\n            return freedisk;\n          });\n        } catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      });\n    }\n    SolrParams params = req.getParams();\n    if (params != null && (params.getBool(UpdateParams.OPTIMIZE, false) || params.getBool(UpdateParams.EXPUNGE_DELETES, false))) {\n      lock.lockInterruptibly();\n      try {\n        coll.getSlices().forEach(s -> {\n          Replica leader = s.getLeader();\n          ReplicaInfo ri = getReplicaInfo(leader);\n          Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n          if (numDocs == null || numDocs.intValue() == 0) {\n            numDocs = 0;\n          }\n          try {\n            simSetShardValue(ri.getCollection(), ri.getShard(), \"SEARCHER.searcher.maxDoc\", numDocs, false, false);\n            simSetShardValue(ri.getCollection(), ri.getShard(), \"SEARCHER.searcher.deletedDocs\", 0, false, false);\n          } catch (Exception e) {\n            throw new RuntimeException(e);\n          }\n        });\n      } finally {\n        lock.unlock();\n      }\n    }\n    return new UpdateResponse();\n  }\n\n","sourceOld":"  /**\n   * Simulate an update by modifying replica metrics.\n   * The following core metrics are updated:\n   * <ul>\n   *   <li><code>SEARCHER.searcher.numDocs</code> - increased by added docs, decreased by deleteById and deleteByQuery</li>\n   *   <li><code>SEARCHER.searcher.deletedDocs</code> - decreased by deleteById and deleteByQuery by up to <code>numDocs</code></li>\n   *   <li><code>SEARCHER.searcher.maxDoc</code> - always increased by the number of added docs.</li>\n   * </ul>\n   * <p>IMPORTANT limitations:</p>\n   * <ul>\n   *   <li>document replacements are always counted as new docs</li>\n   *   <li>delete by ID always succeeds (unless numDocs == 0)</li>\n   *   <li>deleteByQuery is not supported unless the query is <code>*:*</code></li>\n   * </ul>\n   * @param req update request. This request MUST have the <code>collection</code> param set.\n   * @return {@link UpdateResponse}\n   * @throws SolrException on errors, such as nonexistent collection or unsupported deleteByQuery\n   */\n  public UpdateResponse simUpdate(UpdateRequest req) throws SolrException, InterruptedException, IOException {\n    ensureNotClosed();\n    String collection = req.getCollection();\n    if (collection == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection not set\");\n    }\n    ensureSystemCollection(collection);\n    \n    DocCollection coll = getClusterState().getCollection(collection);\n    DocRouter router = coll.getRouter();\n    List<String> deletes = req.getDeleteById();\n    if (deletes != null && !deletes.isEmpty()) {\n      for (String id : deletes) {\n        Slice s = router.getTargetSlice(id, null, null, req.getParams(), coll);\n        Replica leader = s.getLeader();\n        if (leader == null) {\n          log.debug(\"-- no leader in \" + s);\n          continue;\n        }\n        cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n        ReplicaInfo ri = getReplicaInfo(leader);\n        Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n        if (numDocs == null || numDocs.intValue() <= 0) {\n          log.debug(\"-- attempting to delete nonexistent doc \" + id + \" from \" + s.getLeader());\n          continue;\n        }\n        AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n        if (bufferedUpdates != null) {\n          if (bufferedUpdates.get() > 0) {\n            bufferedUpdates.decrementAndGet();\n          } else {\n            log.debug(\"-- attempting to delete nonexistent buffered doc \" + id + \" from \" + s.getLeader());\n          }\n          continue;\n        }\n        lock.lockInterruptibly();\n        try {\n          simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.deletedDocs\", 1, true, false);\n          simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.numDocs\", -1, true, false);\n          Number indexSize = (Number)ri.getVariable(Type.CORE_IDX.metricsAttribute);\n          if (indexSize != null && indexSize.longValue() > SimCloudManager.DEFAULT_IDX_SIZE_BYTES) {\n            indexSize = indexSize.longValue() - DEFAULT_DOC_SIZE_BYTES;\n            simSetShardValue(collection, s.getName(), Type.CORE_IDX.metricsAttribute,\n                new AtomicLong(indexSize.longValue()), false, false);\n            simSetShardValue(collection, s.getName(), Variable.coreidxsize,\n                new AtomicDouble((Double)Type.CORE_IDX.convertVal(indexSize)), false, false);\n          } else {\n            throw new Exception(\"unexpected indexSize ri=\" + ri);\n          }\n        } catch (Exception e) {\n          throw new IOException(e);\n        } finally {\n          lock.unlock();\n        }\n      }\n    }\n    deletes = req.getDeleteQuery();\n    if (deletes != null && !deletes.isEmpty()) {\n      for (String q : deletes) {\n        if (!\"*:*\".equals(q)) {\n          throw new UnsupportedOperationException(\"Only '*:*' query is supported in deleteByQuery\");\n        }\n        for (Slice s : coll.getSlices()) {\n          Replica leader = s.getLeader();\n          if (leader == null) {\n            log.debug(\"-- no leader in \" + s);\n            continue;\n          }\n\n          cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n          ReplicaInfo ri = getReplicaInfo(leader);\n          Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n          if (numDocs == null || numDocs.intValue() == 0) {\n            continue;\n          }\n          lock.lockInterruptibly();\n          try {\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.deletedDocs\", new AtomicLong(numDocs.longValue()), false, false);\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.numDocs\", new AtomicLong(0), false, false);\n            simSetShardValue(collection, s.getName(), Type.CORE_IDX.metricsAttribute,\n                new AtomicLong(SimCloudManager.DEFAULT_IDX_SIZE_BYTES), false, false);\n            simSetShardValue(collection, s.getName(), Variable.coreidxsize,\n                new AtomicDouble((Double)Type.CORE_IDX.convertVal(SimCloudManager.DEFAULT_IDX_SIZE_BYTES)), false, false);\n          } catch (Exception e) {\n            throw new IOException(e);\n          } finally {\n            lock.unlock();\n          }\n        }\n      }\n    }\n    List<SolrInputDocument> docs = req.getDocuments();\n    int docCount = 0;\n    Iterator<SolrInputDocument> it = null;\n    if (docs != null) {\n      docCount = docs.size();\n    } else {\n      it = req.getDocIterator();\n      if (it != null) {\n        while (it.hasNext()) {\n          it.next();\n          docCount++;\n        }\n      }\n    }\n    if (docCount > 0) {\n      // this approach to updating counters and metrics drastically increases performance\n      // of bulk updates, because simSetShardValue is relatively costly\n\n      Map<String, AtomicLong> docUpdates = new HashMap<>();\n      Map<String, Map<String, AtomicLong>> metricUpdates = new HashMap<>();\n\n      // XXX don't add more than 2bln docs in one request\n      boolean modified = false;\n      lock.lockInterruptibly();\n      try {\n        coll = getClusterState().getCollection(collection);\n        Slice[] slices = coll.getActiveSlicesArr();\n        if (slices.length == 0) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection without slices\");\n        }\n        int[] perSlice = new int[slices.length];\n\n        if (it != null) {\n          // BULK UPDATE: simulate random doc assignment without actually calling DocRouter,\n          // which adds significant overhead\n\n          int totalAdded = 0;\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            long count = (long) docCount * ((long) s.getRange().max - (long) s.getRange().min) / 0x100000000L;\n            perSlice[i] = (int) count;\n            totalAdded += perSlice[i];\n          }\n          // loss of precision due to integer math\n          int diff = docCount - totalAdded;\n          if (diff > 0) {\n            // spread the remainder more or less equally\n            int perRemain = diff / slices.length;\n            int remainder = diff % slices.length;\n            int remainderSlice = slices.length > 1 ? bulkUpdateRandom.nextInt(slices.length) : 0;\n            for (int i = 0; i < slices.length; i++) {\n              perSlice[i] += perRemain;\n              if (i == remainderSlice) {\n                perSlice[i] += remainder;\n              }\n            }\n          }\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              log.debug(\"-- no leader in \" + s);\n              continue;\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n            modified = true;\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.addAndGet(perSlice[i]);\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n          }\n        } else {\n          // SMALL UPDATE: use exact assignment via DocRouter\n          for (SolrInputDocument doc : docs) {\n            String id = (String) doc.getFieldValue(\"id\");\n            if (id == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Document without id: \" + doc);\n            }\n            Slice s = coll.getRouter().getTargetSlice(id, doc, null, null, coll);\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              log.debug(\"-- no leader in \" + s);\n              continue;\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .incrementAndGet();\n            modified = true;\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.incrementAndGet();\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .incrementAndGet();\n          }\n        }\n\n        if (modified) {\n          docUpdates.forEach((sh, count) -> {\n            try {\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.numDocs\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.maxDoc\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"UPDATE./update.requests\", count.get(), true, false);\n              // for each new document increase the size by DEFAULT_DOC_SIZE_BYTES\n              simSetShardValue(collection, sh, Type.CORE_IDX.metricsAttribute,\n                  DEFAULT_DOC_SIZE_BYTES * count.get(), true, false);\n              simSetShardValue(collection, sh, Variable.coreidxsize,\n                  Type.CORE_IDX.convertVal(DEFAULT_DOC_SIZE_BYTES * count.get()), true, false);\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          });\n          metricUpdates.forEach((sh, cores) -> {\n            cores.forEach((core, count) -> {\n              String registry = SolrMetricManager.getRegistryName(SolrInfoBean.Group.core, collection, sh,\n                  Utils.parseMetricsReplicaName(collection, core));\n              cloudManager.getMetricManager().registry(registry).counter(\"UPDATE./update.requests\").inc(count.get());\n            });\n          });\n        }\n      } finally {\n        lock.unlock();\n      }\n    }\n    SolrParams params = req.getParams();\n    if (params != null && (params.getBool(UpdateParams.OPTIMIZE, false) || params.getBool(UpdateParams.EXPUNGE_DELETES, false))) {\n      lock.lockInterruptibly();\n      try {\n        coll.getSlices().forEach(s -> {\n          Replica leader = s.getLeader();\n          ReplicaInfo ri = getReplicaInfo(leader);\n          Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n          if (numDocs == null || numDocs.intValue() == 0) {\n            numDocs = 0;\n          }\n          try {\n            simSetShardValue(ri.getCollection(), ri.getShard(), \"SEARCHER.searcher.maxDoc\", numDocs, false, false);\n            simSetShardValue(ri.getCollection(), ri.getShard(), \"SEARCHER.searcher.deletedDocs\", 0, false, false);\n          } catch (Exception e) {\n            throw new RuntimeException(e);\n          }\n        });\n      } finally {\n        lock.unlock();\n      }\n    }\n    return new UpdateResponse();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e35f2dde06b35aa9904949a3a93fabd090371077","date":1587906921,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simUpdate(UpdateRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simUpdate(UpdateRequest).mjava","sourceNew":"  /**\n   * Simulate an update by modifying replica metrics.\n   * The following core metrics are updated:\n   * <ul>\n   *   <li><code>SEARCHER.searcher.numDocs</code> - increased by added docs, decreased by deleteById and deleteByQuery</li>\n   *   <li><code>SEARCHER.searcher.deletedDocs</code> - decreased by deleteById and deleteByQuery by up to <code>numDocs</code></li>\n   *   <li><code>SEARCHER.searcher.maxDoc</code> - always increased by the number of added docs.</li>\n   * </ul>\n   * <p>IMPORTANT limitations:</p>\n   * <ul>\n   *   <li>document replacements are always counted as new docs</li>\n   *   <li>delete by ID always succeeds (unless numDocs == 0)</li>\n   *   <li>deleteByQuery is not supported unless the query is <code>*:*</code></li>\n   * </ul>\n   * @param req update request. This request MUST have the <code>collection</code> param set.\n   * @return {@link UpdateResponse}\n   * @throws SolrException on errors, such as nonexistent collection or unsupported deleteByQuery\n   */\n  public UpdateResponse simUpdate(UpdateRequest req) throws SolrException, InterruptedException, IOException {\n    ensureNotClosed();\n    String collection = req.getCollection();\n    if (collection == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection not set\");\n    }\n    ensureSystemCollection(collection);\n    DocCollection coll = getClusterState().getCollection(collection);\n    DocRouter router = coll.getRouter();\n    List<String> deletes = req.getDeleteById();\n    Map<String, AtomicLong> freediskDeltaPerNode = new HashMap<>();\n    if (deletes != null && !deletes.isEmpty()) {\n      Map<String, AtomicLong> deletesPerShard = new HashMap<>();\n      Map<String, Number> indexSizePerShard = new HashMap<>();\n      for (String id : deletes) {\n        Slice s = router.getTargetSlice(id, null, null, req.getParams(), coll);\n        Replica leader = s.getLeader();\n        if (leader == null) {\n          throw new IOException(\"-- no leader in \" + s);\n        }\n        cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n        ReplicaInfo ri = getReplicaInfo(leader);\n        Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n        if (numDocs == null || numDocs.intValue() <= 0) {\n          if (log.isDebugEnabled()) {\n            log.debug(\"-- attempting to delete nonexistent doc {} from {}\", id, s.getLeader());\n          }\n          continue;\n        }\n\n        // this is somewhat wrong - we should wait until buffered updates are applied\n        // but this way the freedisk changes are much easier to track\n        s.getReplicas().forEach(r ->\n            freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong(0))\n                .addAndGet(DEFAULT_DOC_SIZE_BYTES));\n\n        AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n        if (bufferedUpdates != null) {\n          if (bufferedUpdates.get() > 0) {\n            bufferedUpdates.decrementAndGet();\n          } else {\n            if (log.isDebugEnabled()) {\n              log.debug(\"-- attempting to delete nonexistent buffered doc {} from {}\", id, s.getLeader());\n            }\n          }\n          continue;\n        }\n        deletesPerShard.computeIfAbsent(s.getName(), slice -> new AtomicLong(0)).incrementAndGet();\n        Number indexSize = (Number)ri.getVariable(Type.CORE_IDX.metricsAttribute);\n        if (indexSize != null) {\n          indexSizePerShard.put(s.getName(), indexSize);\n        }\n      }\n      if (!deletesPerShard.isEmpty()) {\n        lock.lockInterruptibly();\n        try {\n          for (Map.Entry<String, AtomicLong> entry : deletesPerShard.entrySet()) {\n            String shard = entry.getKey();\n            simSetShardValue(collection, shard, \"SEARCHER.searcher.deletedDocs\", entry.getValue().get(), true, false);\n            simSetShardValue(collection, shard, \"SEARCHER.searcher.numDocs\", -entry.getValue().get(), true, false);\n            Number indexSize = indexSizePerShard.get(shard);\n            long delSize = DEFAULT_DOC_SIZE_BYTES * entry.getValue().get();\n            if (indexSize != null) {\n              indexSize = indexSize.longValue() - delSize;\n              if (indexSize.longValue() < SimCloudManager.DEFAULT_IDX_SIZE_BYTES) {\n                indexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES;\n              }\n              simSetShardValue(collection, shard, Type.CORE_IDX.metricsAttribute,\n                  new AtomicLong(indexSize.longValue()), false, false);\n              simSetShardValue(collection, shard, Variable.coreidxsize,\n                  new AtomicDouble((Double)Type.CORE_IDX.convertVal(indexSize)), false, false);\n            } else {\n              throw new Exception(\"unexpected indexSize for collection=\" + collection + \", shard=\" + shard + \": \" + indexSize);\n            }\n          }\n        } catch (Exception e) {\n          throw new IOException(e);\n        } finally {\n          lock.unlock();\n        }\n      }\n    }\n    deletes = req.getDeleteQuery();\n    if (deletes != null && !deletes.isEmpty()) {\n      for (String q : deletes) {\n        if (!\"*:*\".equals(q)) {\n          throw new UnsupportedOperationException(\"Only '*:*' query is supported in deleteByQuery\");\n        }\n        //log.debug(\"-- req delByQ {}\", collection);\n        for (Slice s : coll.getSlices()) {\n          Replica leader = s.getLeader();\n          if (leader == null) {\n            throw new IOException(\"-- no leader in \" + s);\n          }\n\n          cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n          ReplicaInfo ri = getReplicaInfo(leader);\n          Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n          if (numDocs == null || numDocs.intValue() == 0) {\n            continue;\n          }\n          lock.lockInterruptibly();\n          try {\n            Number indexSize = (Number)ri.getVariable(Type.CORE_IDX.metricsAttribute);\n            if (indexSize != null) {\n              long delta = indexSize.longValue() < SimCloudManager.DEFAULT_IDX_SIZE_BYTES ? 0 :\n                  indexSize.longValue() - SimCloudManager.DEFAULT_IDX_SIZE_BYTES;\n              s.getReplicas().forEach(r ->\n                  freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong(0))\n                  .addAndGet(delta));\n            } else {\n              throw new RuntimeException(\"Missing index size in \" + ri);\n            }\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.deletedDocs\", new AtomicLong(numDocs.longValue()), false, false);\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.numDocs\", new AtomicLong(0), false, false);\n            simSetShardValue(collection, s.getName(), Type.CORE_IDX.metricsAttribute,\n                new AtomicLong(SimCloudManager.DEFAULT_IDX_SIZE_BYTES), false, false);\n            simSetShardValue(collection, s.getName(), Variable.coreidxsize,\n                new AtomicDouble((Double)Type.CORE_IDX.convertVal(SimCloudManager.DEFAULT_IDX_SIZE_BYTES)), false, false);\n          } catch (Exception e) {\n            throw new IOException(e);\n          } finally {\n            lock.unlock();\n          }\n        }\n      }\n    }\n    List<SolrInputDocument> docs = req.getDocuments();\n    int docCount = 0;\n    Iterator<SolrInputDocument> it = null;\n    if (docs != null) {\n      docCount = docs.size();\n    } else {\n      it = req.getDocIterator();\n      if (it != null) {\n        while (it.hasNext()) {\n          it.next();\n          docCount++;\n        }\n      }\n    }\n    if (docCount > 0) {\n      //log.debug(\"-- req update {}/{}\", collection, docCount);\n      // this approach to updating counters and metrics drastically increases performance\n      // of bulk updates, because simSetShardValue is relatively costly\n\n      Map<String, AtomicLong> docUpdates = new HashMap<>();\n      Map<String, Map<String, AtomicLong>> metricUpdates = new HashMap<>();\n\n      // XXX don't add more than 2bln docs in one request\n      boolean modified = false;\n      lock.lockInterruptibly();\n      try {\n        coll = getClusterState().getCollection(collection);\n        Slice[] slices = coll.getActiveSlicesArr();\n        if (slices.length == 0) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection without slices\");\n        }\n        int[] perSlice = new int[slices.length];\n\n        if (it != null) {\n          // BULK UPDATE: simulate random doc assignment without actually calling DocRouter,\n          // which adds significant overhead\n\n          int totalAdded = 0;\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            long count = (long) docCount * ((long) s.getRange().max - (long) s.getRange().min) / 0x100000000L;\n            perSlice[i] = (int) count;\n            totalAdded += perSlice[i];\n          }\n          // loss of precision due to integer math\n          int diff = docCount - totalAdded;\n          if (diff > 0) {\n            // spread the remainder more or less equally\n            int perRemain = diff / slices.length;\n            int remainder = diff % slices.length;\n            int remainderSlice = slices.length > 1 ? bulkUpdateRandom.nextInt(slices.length) : 0;\n            for (int i = 0; i < slices.length; i++) {\n              perSlice[i] += perRemain;\n              if (i == remainderSlice) {\n                perSlice[i] += remainder;\n              }\n            }\n          }\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              throw new IOException(\"-- no leader in \" + s);\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n            modified = true;\n            long perSliceCount = perSlice[i];\n            s.getReplicas().forEach(r ->\n                freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong(0))\n                    .addAndGet(-perSliceCount * DEFAULT_DOC_SIZE_BYTES));\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.addAndGet(perSlice[i]);\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n          }\n        } else {\n          // SMALL UPDATE: use exact assignment via DocRouter\n          for (SolrInputDocument doc : docs) {\n            String id = (String) doc.getFieldValue(\"id\");\n            if (id == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Document without id: \" + doc);\n            }\n            Slice s = coll.getRouter().getTargetSlice(id, doc, null, null, coll);\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              throw new IOException(\"-- no leader in \" + s);\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .incrementAndGet();\n            modified = true;\n            s.getReplicas().forEach(r ->\n                freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong())\n                    .addAndGet(-DEFAULT_DOC_SIZE_BYTES));\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.incrementAndGet();\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .incrementAndGet();\n          }\n        }\n\n        if (modified) {\n          docUpdates.forEach((sh, count) -> {\n            try {\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.numDocs\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.maxDoc\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"UPDATE./update.requests\", count.get(), true, false);\n              // for each new document increase the size by DEFAULT_DOC_SIZE_BYTES\n              simSetShardValue(collection, sh, Type.CORE_IDX.metricsAttribute,\n                  DEFAULT_DOC_SIZE_BYTES * count.get(), true, false);\n              simSetShardValue(collection, sh, Variable.coreidxsize,\n                  Type.CORE_IDX.convertVal(DEFAULT_DOC_SIZE_BYTES * count.get()), true, false);\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          });\n          metricUpdates.forEach((sh, cores) -> {\n            cores.forEach((core, count) -> {\n              String registry = SolrMetricManager.getRegistryName(SolrInfoBean.Group.core, collection, sh,\n                  Utils.parseMetricsReplicaName(collection, core));\n              cloudManager.getMetricManager().registry(registry).counter(\"UPDATE./update.requests\").inc(count.get());\n            });\n          });\n        }\n      } finally {\n        lock.unlock();\n      }\n    }\n    if (!freediskDeltaPerNode.isEmpty()) {\n      SimNodeStateProvider nodeStateProvider = cloudManager.getSimNodeStateProvider();\n      freediskDeltaPerNode.forEach((node, delta) -> {\n        if (delta.get() == 0) {\n          return;\n        }\n        try {\n          // this method does its own locking to prevent races\n          nodeStateProvider.simUpdateNodeValue(node, Type.FREEDISK.tagName, val -> {\n            if (val == null) {\n              throw new RuntimeException(\"no freedisk for node \" + node);\n            }\n            double freedisk = ((Number) val).doubleValue();\n            double deltaGB = (Double) Type.FREEDISK.convertVal(delta.get());\n            freedisk += deltaGB;\n            if (freedisk < 0) {\n              log.warn(\"-- freedisk={} - ran out of disk space on node {}\", freedisk, node);\n              freedisk = 0;\n            }\n            return freedisk;\n          });\n        } catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      });\n    }\n    SolrParams params = req.getParams();\n    if (params != null && (params.getBool(UpdateParams.OPTIMIZE, false) || params.getBool(UpdateParams.EXPUNGE_DELETES, false))) {\n      lock.lockInterruptibly();\n      try {\n        coll.getSlices().forEach(s -> {\n          Replica leader = s.getLeader();\n          ReplicaInfo ri = getReplicaInfo(leader);\n          Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n          if (numDocs == null || numDocs.intValue() == 0) {\n            numDocs = 0;\n          }\n          try {\n            simSetShardValue(ri.getCollection(), ri.getShard(), \"SEARCHER.searcher.maxDoc\", numDocs, false, false);\n            simSetShardValue(ri.getCollection(), ri.getShard(), \"SEARCHER.searcher.deletedDocs\", 0, false, false);\n          } catch (Exception e) {\n            throw new RuntimeException(e);\n          }\n        });\n      } finally {\n        lock.unlock();\n      }\n    }\n    return new UpdateResponse();\n  }\n\n","sourceOld":"  /**\n   * Simulate an update by modifying replica metrics.\n   * The following core metrics are updated:\n   * <ul>\n   *   <li><code>SEARCHER.searcher.numDocs</code> - increased by added docs, decreased by deleteById and deleteByQuery</li>\n   *   <li><code>SEARCHER.searcher.deletedDocs</code> - decreased by deleteById and deleteByQuery by up to <code>numDocs</code></li>\n   *   <li><code>SEARCHER.searcher.maxDoc</code> - always increased by the number of added docs.</li>\n   * </ul>\n   * <p>IMPORTANT limitations:</p>\n   * <ul>\n   *   <li>document replacements are always counted as new docs</li>\n   *   <li>delete by ID always succeeds (unless numDocs == 0)</li>\n   *   <li>deleteByQuery is not supported unless the query is <code>*:*</code></li>\n   * </ul>\n   * @param req update request. This request MUST have the <code>collection</code> param set.\n   * @return {@link UpdateResponse}\n   * @throws SolrException on errors, such as nonexistent collection or unsupported deleteByQuery\n   */\n  public UpdateResponse simUpdate(UpdateRequest req) throws SolrException, InterruptedException, IOException {\n    ensureNotClosed();\n    String collection = req.getCollection();\n    if (collection == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection not set\");\n    }\n    ensureSystemCollection(collection);\n    DocCollection coll = getClusterState().getCollection(collection);\n    DocRouter router = coll.getRouter();\n    List<String> deletes = req.getDeleteById();\n    Map<String, AtomicLong> freediskDeltaPerNode = new HashMap<>();\n    if (deletes != null && !deletes.isEmpty()) {\n      Map<String, AtomicLong> deletesPerShard = new HashMap<>();\n      Map<String, Number> indexSizePerShard = new HashMap<>();\n      for (String id : deletes) {\n        Slice s = router.getTargetSlice(id, null, null, req.getParams(), coll);\n        Replica leader = s.getLeader();\n        if (leader == null) {\n          throw new IOException(\"-- no leader in \" + s);\n        }\n        cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n        ReplicaInfo ri = getReplicaInfo(leader);\n        Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n        if (numDocs == null || numDocs.intValue() <= 0) {\n          log.debug(\"-- attempting to delete nonexistent doc \" + id + \" from \" + s.getLeader());\n          continue;\n        }\n\n        // this is somewhat wrong - we should wait until buffered updates are applied\n        // but this way the freedisk changes are much easier to track\n        s.getReplicas().forEach(r ->\n            freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong(0))\n                .addAndGet(DEFAULT_DOC_SIZE_BYTES));\n\n        AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n        if (bufferedUpdates != null) {\n          if (bufferedUpdates.get() > 0) {\n            bufferedUpdates.decrementAndGet();\n          } else {\n            log.debug(\"-- attempting to delete nonexistent buffered doc \" + id + \" from \" + s.getLeader());\n          }\n          continue;\n        }\n        deletesPerShard.computeIfAbsent(s.getName(), slice -> new AtomicLong(0)).incrementAndGet();\n        Number indexSize = (Number)ri.getVariable(Type.CORE_IDX.metricsAttribute);\n        if (indexSize != null) {\n          indexSizePerShard.put(s.getName(), indexSize);\n        }\n      }\n      if (!deletesPerShard.isEmpty()) {\n        lock.lockInterruptibly();\n        try {\n          for (Map.Entry<String, AtomicLong> entry : deletesPerShard.entrySet()) {\n            String shard = entry.getKey();\n            simSetShardValue(collection, shard, \"SEARCHER.searcher.deletedDocs\", entry.getValue().get(), true, false);\n            simSetShardValue(collection, shard, \"SEARCHER.searcher.numDocs\", -entry.getValue().get(), true, false);\n            Number indexSize = indexSizePerShard.get(shard);\n            long delSize = DEFAULT_DOC_SIZE_BYTES * entry.getValue().get();\n            if (indexSize != null) {\n              indexSize = indexSize.longValue() - delSize;\n              if (indexSize.longValue() < SimCloudManager.DEFAULT_IDX_SIZE_BYTES) {\n                indexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES;\n              }\n              simSetShardValue(collection, shard, Type.CORE_IDX.metricsAttribute,\n                  new AtomicLong(indexSize.longValue()), false, false);\n              simSetShardValue(collection, shard, Variable.coreidxsize,\n                  new AtomicDouble((Double)Type.CORE_IDX.convertVal(indexSize)), false, false);\n            } else {\n              throw new Exception(\"unexpected indexSize for collection=\" + collection + \", shard=\" + shard + \": \" + indexSize);\n            }\n          }\n        } catch (Exception e) {\n          throw new IOException(e);\n        } finally {\n          lock.unlock();\n        }\n      }\n    }\n    deletes = req.getDeleteQuery();\n    if (deletes != null && !deletes.isEmpty()) {\n      for (String q : deletes) {\n        if (!\"*:*\".equals(q)) {\n          throw new UnsupportedOperationException(\"Only '*:*' query is supported in deleteByQuery\");\n        }\n        //log.debug(\"-- req delByQ \" + collection);\n        for (Slice s : coll.getSlices()) {\n          Replica leader = s.getLeader();\n          if (leader == null) {\n            throw new IOException(\"-- no leader in \" + s);\n          }\n\n          cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n          ReplicaInfo ri = getReplicaInfo(leader);\n          Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n          if (numDocs == null || numDocs.intValue() == 0) {\n            continue;\n          }\n          lock.lockInterruptibly();\n          try {\n            Number indexSize = (Number)ri.getVariable(Type.CORE_IDX.metricsAttribute);\n            if (indexSize != null) {\n              long delta = indexSize.longValue() < SimCloudManager.DEFAULT_IDX_SIZE_BYTES ? 0 :\n                  indexSize.longValue() - SimCloudManager.DEFAULT_IDX_SIZE_BYTES;\n              s.getReplicas().forEach(r ->\n                  freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong(0))\n                  .addAndGet(delta));\n            } else {\n              throw new RuntimeException(\"Missing index size in \" + ri);\n            }\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.deletedDocs\", new AtomicLong(numDocs.longValue()), false, false);\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.numDocs\", new AtomicLong(0), false, false);\n            simSetShardValue(collection, s.getName(), Type.CORE_IDX.metricsAttribute,\n                new AtomicLong(SimCloudManager.DEFAULT_IDX_SIZE_BYTES), false, false);\n            simSetShardValue(collection, s.getName(), Variable.coreidxsize,\n                new AtomicDouble((Double)Type.CORE_IDX.convertVal(SimCloudManager.DEFAULT_IDX_SIZE_BYTES)), false, false);\n          } catch (Exception e) {\n            throw new IOException(e);\n          } finally {\n            lock.unlock();\n          }\n        }\n      }\n    }\n    List<SolrInputDocument> docs = req.getDocuments();\n    int docCount = 0;\n    Iterator<SolrInputDocument> it = null;\n    if (docs != null) {\n      docCount = docs.size();\n    } else {\n      it = req.getDocIterator();\n      if (it != null) {\n        while (it.hasNext()) {\n          it.next();\n          docCount++;\n        }\n      }\n    }\n    if (docCount > 0) {\n      //log.debug(\"-- req update \" + collection + \" / \" + docCount);\n      // this approach to updating counters and metrics drastically increases performance\n      // of bulk updates, because simSetShardValue is relatively costly\n\n      Map<String, AtomicLong> docUpdates = new HashMap<>();\n      Map<String, Map<String, AtomicLong>> metricUpdates = new HashMap<>();\n\n      // XXX don't add more than 2bln docs in one request\n      boolean modified = false;\n      lock.lockInterruptibly();\n      try {\n        coll = getClusterState().getCollection(collection);\n        Slice[] slices = coll.getActiveSlicesArr();\n        if (slices.length == 0) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection without slices\");\n        }\n        int[] perSlice = new int[slices.length];\n\n        if (it != null) {\n          // BULK UPDATE: simulate random doc assignment without actually calling DocRouter,\n          // which adds significant overhead\n\n          int totalAdded = 0;\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            long count = (long) docCount * ((long) s.getRange().max - (long) s.getRange().min) / 0x100000000L;\n            perSlice[i] = (int) count;\n            totalAdded += perSlice[i];\n          }\n          // loss of precision due to integer math\n          int diff = docCount - totalAdded;\n          if (diff > 0) {\n            // spread the remainder more or less equally\n            int perRemain = diff / slices.length;\n            int remainder = diff % slices.length;\n            int remainderSlice = slices.length > 1 ? bulkUpdateRandom.nextInt(slices.length) : 0;\n            for (int i = 0; i < slices.length; i++) {\n              perSlice[i] += perRemain;\n              if (i == remainderSlice) {\n                perSlice[i] += remainder;\n              }\n            }\n          }\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              throw new IOException(\"-- no leader in \" + s);\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n            modified = true;\n            long perSliceCount = perSlice[i];\n            s.getReplicas().forEach(r ->\n                freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong(0))\n                    .addAndGet(-perSliceCount * DEFAULT_DOC_SIZE_BYTES));\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.addAndGet(perSlice[i]);\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n          }\n        } else {\n          // SMALL UPDATE: use exact assignment via DocRouter\n          for (SolrInputDocument doc : docs) {\n            String id = (String) doc.getFieldValue(\"id\");\n            if (id == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Document without id: \" + doc);\n            }\n            Slice s = coll.getRouter().getTargetSlice(id, doc, null, null, coll);\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              throw new IOException(\"-- no leader in \" + s);\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .incrementAndGet();\n            modified = true;\n            s.getReplicas().forEach(r ->\n                freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong())\n                    .addAndGet(-DEFAULT_DOC_SIZE_BYTES));\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.incrementAndGet();\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .incrementAndGet();\n          }\n        }\n\n        if (modified) {\n          docUpdates.forEach((sh, count) -> {\n            try {\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.numDocs\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.maxDoc\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"UPDATE./update.requests\", count.get(), true, false);\n              // for each new document increase the size by DEFAULT_DOC_SIZE_BYTES\n              simSetShardValue(collection, sh, Type.CORE_IDX.metricsAttribute,\n                  DEFAULT_DOC_SIZE_BYTES * count.get(), true, false);\n              simSetShardValue(collection, sh, Variable.coreidxsize,\n                  Type.CORE_IDX.convertVal(DEFAULT_DOC_SIZE_BYTES * count.get()), true, false);\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          });\n          metricUpdates.forEach((sh, cores) -> {\n            cores.forEach((core, count) -> {\n              String registry = SolrMetricManager.getRegistryName(SolrInfoBean.Group.core, collection, sh,\n                  Utils.parseMetricsReplicaName(collection, core));\n              cloudManager.getMetricManager().registry(registry).counter(\"UPDATE./update.requests\").inc(count.get());\n            });\n          });\n        }\n      } finally {\n        lock.unlock();\n      }\n    }\n    if (!freediskDeltaPerNode.isEmpty()) {\n      SimNodeStateProvider nodeStateProvider = cloudManager.getSimNodeStateProvider();\n      freediskDeltaPerNode.forEach((node, delta) -> {\n        if (delta.get() == 0) {\n          return;\n        }\n        try {\n          // this method does its own locking to prevent races\n          nodeStateProvider.simUpdateNodeValue(node, Type.FREEDISK.tagName, val -> {\n            if (val == null) {\n              throw new RuntimeException(\"no freedisk for node \" + node);\n            }\n            double freedisk = ((Number) val).doubleValue();\n            double deltaGB = (Double) Type.FREEDISK.convertVal(delta.get());\n            freedisk += deltaGB;\n            if (freedisk < 0) {\n              log.warn(\"-- freedisk=\" + freedisk + \" - ran out of disk space on node \" + node);\n              freedisk = 0;\n            }\n            return freedisk;\n          });\n        } catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      });\n    }\n    SolrParams params = req.getParams();\n    if (params != null && (params.getBool(UpdateParams.OPTIMIZE, false) || params.getBool(UpdateParams.EXPUNGE_DELETES, false))) {\n      lock.lockInterruptibly();\n      try {\n        coll.getSlices().forEach(s -> {\n          Replica leader = s.getLeader();\n          ReplicaInfo ri = getReplicaInfo(leader);\n          Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n          if (numDocs == null || numDocs.intValue() == 0) {\n            numDocs = 0;\n          }\n          try {\n            simSetShardValue(ri.getCollection(), ri.getShard(), \"SEARCHER.searcher.maxDoc\", numDocs, false, false);\n            simSetShardValue(ri.getCollection(), ri.getShard(), \"SEARCHER.searcher.deletedDocs\", 0, false, false);\n          } catch (Exception e) {\n            throw new RuntimeException(e);\n          }\n        });\n      } finally {\n        lock.unlock();\n      }\n    }\n    return new UpdateResponse();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd","date":1594731683,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simUpdate(UpdateRequest).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simUpdate(UpdateRequest).mjava","sourceNew":"  /**\n   * Simulate an update by modifying replica metrics.\n   * The following core metrics are updated:\n   * <ul>\n   *   <li><code>SEARCHER.searcher.numDocs</code> - increased by added docs, decreased by deleteById and deleteByQuery</li>\n   *   <li><code>SEARCHER.searcher.deletedDocs</code> - decreased by deleteById and deleteByQuery by up to <code>numDocs</code></li>\n   *   <li><code>SEARCHER.searcher.maxDoc</code> - always increased by the number of added docs.</li>\n   * </ul>\n   * <p>IMPORTANT limitations:</p>\n   * <ul>\n   *   <li>document replacements are always counted as new docs</li>\n   *   <li>delete by ID always succeeds (unless numDocs == 0)</li>\n   *   <li>deleteByQuery is not supported unless the query is <code>*:*</code></li>\n   * </ul>\n   * @param req update request. This request MUST have the <code>collection</code> param set.\n   * @return {@link UpdateResponse}\n   * @throws SolrException on errors, such as nonexistent collection or unsupported deleteByQuery\n   */\n  public UpdateResponse simUpdate(UpdateRequest req) throws SolrException, InterruptedException, IOException {\n    ensureNotClosed();\n    String collection = req.getCollection();\n    if (collection == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection not set\");\n    }\n    ensureSystemCollection(collection);\n    DocCollection coll = getClusterState().getCollection(collection);\n    DocRouter router = coll.getRouter();\n    List<String> deletes = req.getDeleteById();\n    Map<String, AtomicLong> freediskDeltaPerNode = new HashMap<>();\n    if (deletes != null && !deletes.isEmpty()) {\n      Map<String, AtomicLong> deletesPerShard = new HashMap<>();\n      Map<String, Number> indexSizePerShard = new HashMap<>();\n      for (String id : deletes) {\n        Slice s = router.getTargetSlice(id, null, null, req.getParams(), coll);\n        Replica leader = s.getLeader();\n        if (leader == null) {\n          throw new IOException(\"-- no leader in \" + s);\n        }\n        cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n        Replica ri = getReplicaInfo(leader);\n        Number numDocs = (Number)ri.get(\"SEARCHER.searcher.numDocs\");\n        if (numDocs == null || numDocs.intValue() <= 0) {\n          if (log.isDebugEnabled()) {\n            log.debug(\"-- attempting to delete nonexistent doc {} from {}\", id, s.getLeader());\n          }\n          continue;\n        }\n\n        // this is somewhat wrong - we should wait until buffered updates are applied\n        // but this way the freedisk changes are much easier to track\n        s.getReplicas().forEach(r ->\n            freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong(0))\n                .addAndGet(DEFAULT_DOC_SIZE_BYTES));\n\n        AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n        if (bufferedUpdates != null) {\n          if (bufferedUpdates.get() > 0) {\n            bufferedUpdates.decrementAndGet();\n          } else {\n            if (log.isDebugEnabled()) {\n              log.debug(\"-- attempting to delete nonexistent buffered doc {} from {}\", id, s.getLeader());\n            }\n          }\n          continue;\n        }\n        deletesPerShard.computeIfAbsent(s.getName(), slice -> new AtomicLong(0)).incrementAndGet();\n        Number indexSize = (Number)ri.get(Type.CORE_IDX.metricsAttribute);\n        if (indexSize != null) {\n          indexSizePerShard.put(s.getName(), indexSize);\n        }\n      }\n      if (!deletesPerShard.isEmpty()) {\n        lock.lockInterruptibly();\n        try {\n          for (Map.Entry<String, AtomicLong> entry : deletesPerShard.entrySet()) {\n            String shard = entry.getKey();\n            simSetShardValue(collection, shard, \"SEARCHER.searcher.deletedDocs\", entry.getValue().get(), true, false);\n            simSetShardValue(collection, shard, \"SEARCHER.searcher.numDocs\", -entry.getValue().get(), true, false);\n            Number indexSize = indexSizePerShard.get(shard);\n            long delSize = DEFAULT_DOC_SIZE_BYTES * entry.getValue().get();\n            if (indexSize != null) {\n              indexSize = indexSize.longValue() - delSize;\n              if (indexSize.longValue() < SimCloudManager.DEFAULT_IDX_SIZE_BYTES) {\n                indexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES;\n              }\n              simSetShardValue(collection, shard, Type.CORE_IDX.metricsAttribute,\n                  new AtomicLong(indexSize.longValue()), false, false);\n              simSetShardValue(collection, shard, Variable.coreidxsize,\n                  new AtomicDouble((Double)Type.CORE_IDX.convertVal(indexSize)), false, false);\n            } else {\n              throw new Exception(\"unexpected indexSize for collection=\" + collection + \", shard=\" + shard + \": \" + indexSize);\n            }\n          }\n        } catch (Exception e) {\n          throw new IOException(e);\n        } finally {\n          lock.unlock();\n        }\n      }\n    }\n    deletes = req.getDeleteQuery();\n    if (deletes != null && !deletes.isEmpty()) {\n      for (String q : deletes) {\n        if (!\"*:*\".equals(q)) {\n          throw new UnsupportedOperationException(\"Only '*:*' query is supported in deleteByQuery\");\n        }\n        //log.debug(\"-- req delByQ {}\", collection);\n        for (Slice s : coll.getSlices()) {\n          Replica leader = s.getLeader();\n          if (leader == null) {\n            throw new IOException(\"-- no leader in \" + s);\n          }\n\n          cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n          Replica ri = getReplicaInfo(leader);\n          Number numDocs = (Number)ri.get(\"SEARCHER.searcher.numDocs\");\n          if (numDocs == null || numDocs.intValue() == 0) {\n            continue;\n          }\n          lock.lockInterruptibly();\n          try {\n            Number indexSize = (Number)ri.get(Type.CORE_IDX.metricsAttribute);\n            if (indexSize != null) {\n              long delta = indexSize.longValue() < SimCloudManager.DEFAULT_IDX_SIZE_BYTES ? 0 :\n                  indexSize.longValue() - SimCloudManager.DEFAULT_IDX_SIZE_BYTES;\n              s.getReplicas().forEach(r ->\n                  freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong(0))\n                  .addAndGet(delta));\n            } else {\n              throw new RuntimeException(\"Missing index size in \" + ri);\n            }\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.deletedDocs\", new AtomicLong(numDocs.longValue()), false, false);\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.numDocs\", new AtomicLong(0), false, false);\n            simSetShardValue(collection, s.getName(), Type.CORE_IDX.metricsAttribute,\n                new AtomicLong(SimCloudManager.DEFAULT_IDX_SIZE_BYTES), false, false);\n            simSetShardValue(collection, s.getName(), Variable.coreidxsize,\n                new AtomicDouble((Double)Type.CORE_IDX.convertVal(SimCloudManager.DEFAULT_IDX_SIZE_BYTES)), false, false);\n          } catch (Exception e) {\n            throw new IOException(e);\n          } finally {\n            lock.unlock();\n          }\n        }\n      }\n    }\n    List<SolrInputDocument> docs = req.getDocuments();\n    int docCount = 0;\n    Iterator<SolrInputDocument> it = null;\n    if (docs != null) {\n      docCount = docs.size();\n    } else {\n      it = req.getDocIterator();\n      if (it != null) {\n        while (it.hasNext()) {\n          it.next();\n          docCount++;\n        }\n      }\n    }\n    if (docCount > 0) {\n      //log.debug(\"-- req update {}/{}\", collection, docCount);\n      // this approach to updating counters and metrics drastically increases performance\n      // of bulk updates, because simSetShardValue is relatively costly\n\n      Map<String, AtomicLong> docUpdates = new HashMap<>();\n      Map<String, Map<String, AtomicLong>> metricUpdates = new HashMap<>();\n\n      // XXX don't add more than 2bln docs in one request\n      boolean modified = false;\n      lock.lockInterruptibly();\n      try {\n        coll = getClusterState().getCollection(collection);\n        Slice[] slices = coll.getActiveSlicesArr();\n        if (slices.length == 0) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection without slices\");\n        }\n        int[] perSlice = new int[slices.length];\n\n        if (it != null) {\n          // BULK UPDATE: simulate random doc assignment without actually calling DocRouter,\n          // which adds significant overhead\n\n          int totalAdded = 0;\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            long count = (long) docCount * ((long) s.getRange().max - (long) s.getRange().min) / 0x100000000L;\n            perSlice[i] = (int) count;\n            totalAdded += perSlice[i];\n          }\n          // loss of precision due to integer math\n          int diff = docCount - totalAdded;\n          if (diff > 0) {\n            // spread the remainder more or less equally\n            int perRemain = diff / slices.length;\n            int remainder = diff % slices.length;\n            int remainderSlice = slices.length > 1 ? bulkUpdateRandom.nextInt(slices.length) : 0;\n            for (int i = 0; i < slices.length; i++) {\n              perSlice[i] += perRemain;\n              if (i == remainderSlice) {\n                perSlice[i] += remainder;\n              }\n            }\n          }\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              throw new IOException(\"-- no leader in \" + s);\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n            modified = true;\n            long perSliceCount = perSlice[i];\n            s.getReplicas().forEach(r ->\n                freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong(0))\n                    .addAndGet(-perSliceCount * DEFAULT_DOC_SIZE_BYTES));\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.addAndGet(perSlice[i]);\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n          }\n        } else {\n          // SMALL UPDATE: use exact assignment via DocRouter\n          for (SolrInputDocument doc : docs) {\n            String id = (String) doc.getFieldValue(\"id\");\n            if (id == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Document without id: \" + doc);\n            }\n            Slice s = coll.getRouter().getTargetSlice(id, doc, null, null, coll);\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              throw new IOException(\"-- no leader in \" + s);\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .incrementAndGet();\n            modified = true;\n            s.getReplicas().forEach(r ->\n                freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong())\n                    .addAndGet(-DEFAULT_DOC_SIZE_BYTES));\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.incrementAndGet();\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .incrementAndGet();\n          }\n        }\n\n        if (modified) {\n          docUpdates.forEach((sh, count) -> {\n            try {\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.numDocs\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.maxDoc\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"UPDATE./update.requests\", count.get(), true, false);\n              // for each new document increase the size by DEFAULT_DOC_SIZE_BYTES\n              simSetShardValue(collection, sh, Type.CORE_IDX.metricsAttribute,\n                  DEFAULT_DOC_SIZE_BYTES * count.get(), true, false);\n              simSetShardValue(collection, sh, Variable.coreidxsize,\n                  Type.CORE_IDX.convertVal(DEFAULT_DOC_SIZE_BYTES * count.get()), true, false);\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          });\n          metricUpdates.forEach((sh, cores) -> {\n            cores.forEach((core, count) -> {\n              String registry = SolrMetricManager.getRegistryName(SolrInfoBean.Group.core, collection, sh,\n                  Utils.parseMetricsReplicaName(collection, core));\n              cloudManager.getMetricManager().registry(registry).counter(\"UPDATE./update.requests\").inc(count.get());\n            });\n          });\n        }\n      } finally {\n        lock.unlock();\n      }\n    }\n    if (!freediskDeltaPerNode.isEmpty()) {\n      SimNodeStateProvider nodeStateProvider = cloudManager.getSimNodeStateProvider();\n      freediskDeltaPerNode.forEach((node, delta) -> {\n        if (delta.get() == 0) {\n          return;\n        }\n        try {\n          // this method does its own locking to prevent races\n          nodeStateProvider.simUpdateNodeValue(node, Type.FREEDISK.tagName, val -> {\n            if (val == null) {\n              throw new RuntimeException(\"no freedisk for node \" + node);\n            }\n            double freedisk = ((Number) val).doubleValue();\n            double deltaGB = (Double) Type.FREEDISK.convertVal(delta.get());\n            freedisk += deltaGB;\n            if (freedisk < 0) {\n              log.warn(\"-- freedisk={} - ran out of disk space on node {}\", freedisk, node);\n              freedisk = 0;\n            }\n            return freedisk;\n          });\n        } catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      });\n    }\n    SolrParams params = req.getParams();\n    if (params != null && (params.getBool(UpdateParams.OPTIMIZE, false) || params.getBool(UpdateParams.EXPUNGE_DELETES, false))) {\n      lock.lockInterruptibly();\n      try {\n        coll.getSlices().forEach(s -> {\n          Replica leader = s.getLeader();\n          Replica ri = getReplicaInfo(leader);\n          Number numDocs = (Number)ri.get(\"SEARCHER.searcher.numDocs\");\n          if (numDocs == null || numDocs.intValue() == 0) {\n            numDocs = 0;\n          }\n          try {\n            simSetShardValue(ri.getCollection(), ri.getShard(), \"SEARCHER.searcher.maxDoc\", numDocs, false, false);\n            simSetShardValue(ri.getCollection(), ri.getShard(), \"SEARCHER.searcher.deletedDocs\", 0, false, false);\n          } catch (Exception e) {\n            throw new RuntimeException(e);\n          }\n        });\n      } finally {\n        lock.unlock();\n      }\n    }\n    return new UpdateResponse();\n  }\n\n","sourceOld":"  /**\n   * Simulate an update by modifying replica metrics.\n   * The following core metrics are updated:\n   * <ul>\n   *   <li><code>SEARCHER.searcher.numDocs</code> - increased by added docs, decreased by deleteById and deleteByQuery</li>\n   *   <li><code>SEARCHER.searcher.deletedDocs</code> - decreased by deleteById and deleteByQuery by up to <code>numDocs</code></li>\n   *   <li><code>SEARCHER.searcher.maxDoc</code> - always increased by the number of added docs.</li>\n   * </ul>\n   * <p>IMPORTANT limitations:</p>\n   * <ul>\n   *   <li>document replacements are always counted as new docs</li>\n   *   <li>delete by ID always succeeds (unless numDocs == 0)</li>\n   *   <li>deleteByQuery is not supported unless the query is <code>*:*</code></li>\n   * </ul>\n   * @param req update request. This request MUST have the <code>collection</code> param set.\n   * @return {@link UpdateResponse}\n   * @throws SolrException on errors, such as nonexistent collection or unsupported deleteByQuery\n   */\n  public UpdateResponse simUpdate(UpdateRequest req) throws SolrException, InterruptedException, IOException {\n    ensureNotClosed();\n    String collection = req.getCollection();\n    if (collection == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection not set\");\n    }\n    ensureSystemCollection(collection);\n    DocCollection coll = getClusterState().getCollection(collection);\n    DocRouter router = coll.getRouter();\n    List<String> deletes = req.getDeleteById();\n    Map<String, AtomicLong> freediskDeltaPerNode = new HashMap<>();\n    if (deletes != null && !deletes.isEmpty()) {\n      Map<String, AtomicLong> deletesPerShard = new HashMap<>();\n      Map<String, Number> indexSizePerShard = new HashMap<>();\n      for (String id : deletes) {\n        Slice s = router.getTargetSlice(id, null, null, req.getParams(), coll);\n        Replica leader = s.getLeader();\n        if (leader == null) {\n          throw new IOException(\"-- no leader in \" + s);\n        }\n        cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n        ReplicaInfo ri = getReplicaInfo(leader);\n        Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n        if (numDocs == null || numDocs.intValue() <= 0) {\n          if (log.isDebugEnabled()) {\n            log.debug(\"-- attempting to delete nonexistent doc {} from {}\", id, s.getLeader());\n          }\n          continue;\n        }\n\n        // this is somewhat wrong - we should wait until buffered updates are applied\n        // but this way the freedisk changes are much easier to track\n        s.getReplicas().forEach(r ->\n            freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong(0))\n                .addAndGet(DEFAULT_DOC_SIZE_BYTES));\n\n        AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n        if (bufferedUpdates != null) {\n          if (bufferedUpdates.get() > 0) {\n            bufferedUpdates.decrementAndGet();\n          } else {\n            if (log.isDebugEnabled()) {\n              log.debug(\"-- attempting to delete nonexistent buffered doc {} from {}\", id, s.getLeader());\n            }\n          }\n          continue;\n        }\n        deletesPerShard.computeIfAbsent(s.getName(), slice -> new AtomicLong(0)).incrementAndGet();\n        Number indexSize = (Number)ri.getVariable(Type.CORE_IDX.metricsAttribute);\n        if (indexSize != null) {\n          indexSizePerShard.put(s.getName(), indexSize);\n        }\n      }\n      if (!deletesPerShard.isEmpty()) {\n        lock.lockInterruptibly();\n        try {\n          for (Map.Entry<String, AtomicLong> entry : deletesPerShard.entrySet()) {\n            String shard = entry.getKey();\n            simSetShardValue(collection, shard, \"SEARCHER.searcher.deletedDocs\", entry.getValue().get(), true, false);\n            simSetShardValue(collection, shard, \"SEARCHER.searcher.numDocs\", -entry.getValue().get(), true, false);\n            Number indexSize = indexSizePerShard.get(shard);\n            long delSize = DEFAULT_DOC_SIZE_BYTES * entry.getValue().get();\n            if (indexSize != null) {\n              indexSize = indexSize.longValue() - delSize;\n              if (indexSize.longValue() < SimCloudManager.DEFAULT_IDX_SIZE_BYTES) {\n                indexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES;\n              }\n              simSetShardValue(collection, shard, Type.CORE_IDX.metricsAttribute,\n                  new AtomicLong(indexSize.longValue()), false, false);\n              simSetShardValue(collection, shard, Variable.coreidxsize,\n                  new AtomicDouble((Double)Type.CORE_IDX.convertVal(indexSize)), false, false);\n            } else {\n              throw new Exception(\"unexpected indexSize for collection=\" + collection + \", shard=\" + shard + \": \" + indexSize);\n            }\n          }\n        } catch (Exception e) {\n          throw new IOException(e);\n        } finally {\n          lock.unlock();\n        }\n      }\n    }\n    deletes = req.getDeleteQuery();\n    if (deletes != null && !deletes.isEmpty()) {\n      for (String q : deletes) {\n        if (!\"*:*\".equals(q)) {\n          throw new UnsupportedOperationException(\"Only '*:*' query is supported in deleteByQuery\");\n        }\n        //log.debug(\"-- req delByQ {}\", collection);\n        for (Slice s : coll.getSlices()) {\n          Replica leader = s.getLeader();\n          if (leader == null) {\n            throw new IOException(\"-- no leader in \" + s);\n          }\n\n          cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n          ReplicaInfo ri = getReplicaInfo(leader);\n          Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n          if (numDocs == null || numDocs.intValue() == 0) {\n            continue;\n          }\n          lock.lockInterruptibly();\n          try {\n            Number indexSize = (Number)ri.getVariable(Type.CORE_IDX.metricsAttribute);\n            if (indexSize != null) {\n              long delta = indexSize.longValue() < SimCloudManager.DEFAULT_IDX_SIZE_BYTES ? 0 :\n                  indexSize.longValue() - SimCloudManager.DEFAULT_IDX_SIZE_BYTES;\n              s.getReplicas().forEach(r ->\n                  freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong(0))\n                  .addAndGet(delta));\n            } else {\n              throw new RuntimeException(\"Missing index size in \" + ri);\n            }\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.deletedDocs\", new AtomicLong(numDocs.longValue()), false, false);\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.numDocs\", new AtomicLong(0), false, false);\n            simSetShardValue(collection, s.getName(), Type.CORE_IDX.metricsAttribute,\n                new AtomicLong(SimCloudManager.DEFAULT_IDX_SIZE_BYTES), false, false);\n            simSetShardValue(collection, s.getName(), Variable.coreidxsize,\n                new AtomicDouble((Double)Type.CORE_IDX.convertVal(SimCloudManager.DEFAULT_IDX_SIZE_BYTES)), false, false);\n          } catch (Exception e) {\n            throw new IOException(e);\n          } finally {\n            lock.unlock();\n          }\n        }\n      }\n    }\n    List<SolrInputDocument> docs = req.getDocuments();\n    int docCount = 0;\n    Iterator<SolrInputDocument> it = null;\n    if (docs != null) {\n      docCount = docs.size();\n    } else {\n      it = req.getDocIterator();\n      if (it != null) {\n        while (it.hasNext()) {\n          it.next();\n          docCount++;\n        }\n      }\n    }\n    if (docCount > 0) {\n      //log.debug(\"-- req update {}/{}\", collection, docCount);\n      // this approach to updating counters and metrics drastically increases performance\n      // of bulk updates, because simSetShardValue is relatively costly\n\n      Map<String, AtomicLong> docUpdates = new HashMap<>();\n      Map<String, Map<String, AtomicLong>> metricUpdates = new HashMap<>();\n\n      // XXX don't add more than 2bln docs in one request\n      boolean modified = false;\n      lock.lockInterruptibly();\n      try {\n        coll = getClusterState().getCollection(collection);\n        Slice[] slices = coll.getActiveSlicesArr();\n        if (slices.length == 0) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection without slices\");\n        }\n        int[] perSlice = new int[slices.length];\n\n        if (it != null) {\n          // BULK UPDATE: simulate random doc assignment without actually calling DocRouter,\n          // which adds significant overhead\n\n          int totalAdded = 0;\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            long count = (long) docCount * ((long) s.getRange().max - (long) s.getRange().min) / 0x100000000L;\n            perSlice[i] = (int) count;\n            totalAdded += perSlice[i];\n          }\n          // loss of precision due to integer math\n          int diff = docCount - totalAdded;\n          if (diff > 0) {\n            // spread the remainder more or less equally\n            int perRemain = diff / slices.length;\n            int remainder = diff % slices.length;\n            int remainderSlice = slices.length > 1 ? bulkUpdateRandom.nextInt(slices.length) : 0;\n            for (int i = 0; i < slices.length; i++) {\n              perSlice[i] += perRemain;\n              if (i == remainderSlice) {\n                perSlice[i] += remainder;\n              }\n            }\n          }\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              throw new IOException(\"-- no leader in \" + s);\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n            modified = true;\n            long perSliceCount = perSlice[i];\n            s.getReplicas().forEach(r ->\n                freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong(0))\n                    .addAndGet(-perSliceCount * DEFAULT_DOC_SIZE_BYTES));\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.addAndGet(perSlice[i]);\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n          }\n        } else {\n          // SMALL UPDATE: use exact assignment via DocRouter\n          for (SolrInputDocument doc : docs) {\n            String id = (String) doc.getFieldValue(\"id\");\n            if (id == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Document without id: \" + doc);\n            }\n            Slice s = coll.getRouter().getTargetSlice(id, doc, null, null, coll);\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              throw new IOException(\"-- no leader in \" + s);\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .incrementAndGet();\n            modified = true;\n            s.getReplicas().forEach(r ->\n                freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong())\n                    .addAndGet(-DEFAULT_DOC_SIZE_BYTES));\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.incrementAndGet();\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .incrementAndGet();\n          }\n        }\n\n        if (modified) {\n          docUpdates.forEach((sh, count) -> {\n            try {\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.numDocs\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.maxDoc\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"UPDATE./update.requests\", count.get(), true, false);\n              // for each new document increase the size by DEFAULT_DOC_SIZE_BYTES\n              simSetShardValue(collection, sh, Type.CORE_IDX.metricsAttribute,\n                  DEFAULT_DOC_SIZE_BYTES * count.get(), true, false);\n              simSetShardValue(collection, sh, Variable.coreidxsize,\n                  Type.CORE_IDX.convertVal(DEFAULT_DOC_SIZE_BYTES * count.get()), true, false);\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          });\n          metricUpdates.forEach((sh, cores) -> {\n            cores.forEach((core, count) -> {\n              String registry = SolrMetricManager.getRegistryName(SolrInfoBean.Group.core, collection, sh,\n                  Utils.parseMetricsReplicaName(collection, core));\n              cloudManager.getMetricManager().registry(registry).counter(\"UPDATE./update.requests\").inc(count.get());\n            });\n          });\n        }\n      } finally {\n        lock.unlock();\n      }\n    }\n    if (!freediskDeltaPerNode.isEmpty()) {\n      SimNodeStateProvider nodeStateProvider = cloudManager.getSimNodeStateProvider();\n      freediskDeltaPerNode.forEach((node, delta) -> {\n        if (delta.get() == 0) {\n          return;\n        }\n        try {\n          // this method does its own locking to prevent races\n          nodeStateProvider.simUpdateNodeValue(node, Type.FREEDISK.tagName, val -> {\n            if (val == null) {\n              throw new RuntimeException(\"no freedisk for node \" + node);\n            }\n            double freedisk = ((Number) val).doubleValue();\n            double deltaGB = (Double) Type.FREEDISK.convertVal(delta.get());\n            freedisk += deltaGB;\n            if (freedisk < 0) {\n              log.warn(\"-- freedisk={} - ran out of disk space on node {}\", freedisk, node);\n              freedisk = 0;\n            }\n            return freedisk;\n          });\n        } catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      });\n    }\n    SolrParams params = req.getParams();\n    if (params != null && (params.getBool(UpdateParams.OPTIMIZE, false) || params.getBool(UpdateParams.EXPUNGE_DELETES, false))) {\n      lock.lockInterruptibly();\n      try {\n        coll.getSlices().forEach(s -> {\n          Replica leader = s.getLeader();\n          ReplicaInfo ri = getReplicaInfo(leader);\n          Number numDocs = (Number)ri.getVariable(\"SEARCHER.searcher.numDocs\");\n          if (numDocs == null || numDocs.intValue() == 0) {\n            numDocs = 0;\n          }\n          try {\n            simSetShardValue(ri.getCollection(), ri.getShard(), \"SEARCHER.searcher.maxDoc\", numDocs, false, false);\n            simSetShardValue(ri.getCollection(), ri.getShard(), \"SEARCHER.searcher.deletedDocs\", 0, false, false);\n          } catch (Exception e) {\n            throw new RuntimeException(e);\n          }\n        });\n      } finally {\n        lock.unlock();\n      }\n    }\n    return new UpdateResponse();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3f504512a03d978990cbff30db0522b354e846db","date":1595247421,"type":4,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimClusterStateProvider#simUpdate(UpdateRequest).mjava","sourceNew":null,"sourceOld":"  /**\n   * Simulate an update by modifying replica metrics.\n   * The following core metrics are updated:\n   * <ul>\n   *   <li><code>SEARCHER.searcher.numDocs</code> - increased by added docs, decreased by deleteById and deleteByQuery</li>\n   *   <li><code>SEARCHER.searcher.deletedDocs</code> - decreased by deleteById and deleteByQuery by up to <code>numDocs</code></li>\n   *   <li><code>SEARCHER.searcher.maxDoc</code> - always increased by the number of added docs.</li>\n   * </ul>\n   * <p>IMPORTANT limitations:</p>\n   * <ul>\n   *   <li>document replacements are always counted as new docs</li>\n   *   <li>delete by ID always succeeds (unless numDocs == 0)</li>\n   *   <li>deleteByQuery is not supported unless the query is <code>*:*</code></li>\n   * </ul>\n   * @param req update request. This request MUST have the <code>collection</code> param set.\n   * @return {@link UpdateResponse}\n   * @throws SolrException on errors, such as nonexistent collection or unsupported deleteByQuery\n   */\n  public UpdateResponse simUpdate(UpdateRequest req) throws SolrException, InterruptedException, IOException {\n    ensureNotClosed();\n    String collection = req.getCollection();\n    if (collection == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection not set\");\n    }\n    ensureSystemCollection(collection);\n    DocCollection coll = getClusterState().getCollection(collection);\n    DocRouter router = coll.getRouter();\n    List<String> deletes = req.getDeleteById();\n    Map<String, AtomicLong> freediskDeltaPerNode = new HashMap<>();\n    if (deletes != null && !deletes.isEmpty()) {\n      Map<String, AtomicLong> deletesPerShard = new HashMap<>();\n      Map<String, Number> indexSizePerShard = new HashMap<>();\n      for (String id : deletes) {\n        Slice s = router.getTargetSlice(id, null, null, req.getParams(), coll);\n        Replica leader = s.getLeader();\n        if (leader == null) {\n          throw new IOException(\"-- no leader in \" + s);\n        }\n        cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n        Replica ri = getReplicaInfo(leader);\n        Number numDocs = (Number)ri.get(\"SEARCHER.searcher.numDocs\");\n        if (numDocs == null || numDocs.intValue() <= 0) {\n          if (log.isDebugEnabled()) {\n            log.debug(\"-- attempting to delete nonexistent doc {} from {}\", id, s.getLeader());\n          }\n          continue;\n        }\n\n        // this is somewhat wrong - we should wait until buffered updates are applied\n        // but this way the freedisk changes are much easier to track\n        s.getReplicas().forEach(r ->\n            freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong(0))\n                .addAndGet(DEFAULT_DOC_SIZE_BYTES));\n\n        AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n        if (bufferedUpdates != null) {\n          if (bufferedUpdates.get() > 0) {\n            bufferedUpdates.decrementAndGet();\n          } else {\n            if (log.isDebugEnabled()) {\n              log.debug(\"-- attempting to delete nonexistent buffered doc {} from {}\", id, s.getLeader());\n            }\n          }\n          continue;\n        }\n        deletesPerShard.computeIfAbsent(s.getName(), slice -> new AtomicLong(0)).incrementAndGet();\n        Number indexSize = (Number)ri.get(Type.CORE_IDX.metricsAttribute);\n        if (indexSize != null) {\n          indexSizePerShard.put(s.getName(), indexSize);\n        }\n      }\n      if (!deletesPerShard.isEmpty()) {\n        lock.lockInterruptibly();\n        try {\n          for (Map.Entry<String, AtomicLong> entry : deletesPerShard.entrySet()) {\n            String shard = entry.getKey();\n            simSetShardValue(collection, shard, \"SEARCHER.searcher.deletedDocs\", entry.getValue().get(), true, false);\n            simSetShardValue(collection, shard, \"SEARCHER.searcher.numDocs\", -entry.getValue().get(), true, false);\n            Number indexSize = indexSizePerShard.get(shard);\n            long delSize = DEFAULT_DOC_SIZE_BYTES * entry.getValue().get();\n            if (indexSize != null) {\n              indexSize = indexSize.longValue() - delSize;\n              if (indexSize.longValue() < SimCloudManager.DEFAULT_IDX_SIZE_BYTES) {\n                indexSize = SimCloudManager.DEFAULT_IDX_SIZE_BYTES;\n              }\n              simSetShardValue(collection, shard, Type.CORE_IDX.metricsAttribute,\n                  new AtomicLong(indexSize.longValue()), false, false);\n              simSetShardValue(collection, shard, Variable.coreidxsize,\n                  new AtomicDouble((Double)Type.CORE_IDX.convertVal(indexSize)), false, false);\n            } else {\n              throw new Exception(\"unexpected indexSize for collection=\" + collection + \", shard=\" + shard + \": \" + indexSize);\n            }\n          }\n        } catch (Exception e) {\n          throw new IOException(e);\n        } finally {\n          lock.unlock();\n        }\n      }\n    }\n    deletes = req.getDeleteQuery();\n    if (deletes != null && !deletes.isEmpty()) {\n      for (String q : deletes) {\n        if (!\"*:*\".equals(q)) {\n          throw new UnsupportedOperationException(\"Only '*:*' query is supported in deleteByQuery\");\n        }\n        //log.debug(\"-- req delByQ {}\", collection);\n        for (Slice s : coll.getSlices()) {\n          Replica leader = s.getLeader();\n          if (leader == null) {\n            throw new IOException(\"-- no leader in \" + s);\n          }\n\n          cloudManager.getMetricManager().registry(createRegistryName(collection, s.getName(), leader)).counter(\"UPDATE./update.requests\").inc();\n          Replica ri = getReplicaInfo(leader);\n          Number numDocs = (Number)ri.get(\"SEARCHER.searcher.numDocs\");\n          if (numDocs == null || numDocs.intValue() == 0) {\n            continue;\n          }\n          lock.lockInterruptibly();\n          try {\n            Number indexSize = (Number)ri.get(Type.CORE_IDX.metricsAttribute);\n            if (indexSize != null) {\n              long delta = indexSize.longValue() < SimCloudManager.DEFAULT_IDX_SIZE_BYTES ? 0 :\n                  indexSize.longValue() - SimCloudManager.DEFAULT_IDX_SIZE_BYTES;\n              s.getReplicas().forEach(r ->\n                  freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong(0))\n                  .addAndGet(delta));\n            } else {\n              throw new RuntimeException(\"Missing index size in \" + ri);\n            }\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.deletedDocs\", new AtomicLong(numDocs.longValue()), false, false);\n            simSetShardValue(collection, s.getName(), \"SEARCHER.searcher.numDocs\", new AtomicLong(0), false, false);\n            simSetShardValue(collection, s.getName(), Type.CORE_IDX.metricsAttribute,\n                new AtomicLong(SimCloudManager.DEFAULT_IDX_SIZE_BYTES), false, false);\n            simSetShardValue(collection, s.getName(), Variable.coreidxsize,\n                new AtomicDouble((Double)Type.CORE_IDX.convertVal(SimCloudManager.DEFAULT_IDX_SIZE_BYTES)), false, false);\n          } catch (Exception e) {\n            throw new IOException(e);\n          } finally {\n            lock.unlock();\n          }\n        }\n      }\n    }\n    List<SolrInputDocument> docs = req.getDocuments();\n    int docCount = 0;\n    Iterator<SolrInputDocument> it = null;\n    if (docs != null) {\n      docCount = docs.size();\n    } else {\n      it = req.getDocIterator();\n      if (it != null) {\n        while (it.hasNext()) {\n          it.next();\n          docCount++;\n        }\n      }\n    }\n    if (docCount > 0) {\n      //log.debug(\"-- req update {}/{}\", collection, docCount);\n      // this approach to updating counters and metrics drastically increases performance\n      // of bulk updates, because simSetShardValue is relatively costly\n\n      Map<String, AtomicLong> docUpdates = new HashMap<>();\n      Map<String, Map<String, AtomicLong>> metricUpdates = new HashMap<>();\n\n      // XXX don't add more than 2bln docs in one request\n      boolean modified = false;\n      lock.lockInterruptibly();\n      try {\n        coll = getClusterState().getCollection(collection);\n        Slice[] slices = coll.getActiveSlicesArr();\n        if (slices.length == 0) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Collection without slices\");\n        }\n        int[] perSlice = new int[slices.length];\n\n        if (it != null) {\n          // BULK UPDATE: simulate random doc assignment without actually calling DocRouter,\n          // which adds significant overhead\n\n          int totalAdded = 0;\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            long count = (long) docCount * ((long) s.getRange().max - (long) s.getRange().min) / 0x100000000L;\n            perSlice[i] = (int) count;\n            totalAdded += perSlice[i];\n          }\n          // loss of precision due to integer math\n          int diff = docCount - totalAdded;\n          if (diff > 0) {\n            // spread the remainder more or less equally\n            int perRemain = diff / slices.length;\n            int remainder = diff % slices.length;\n            int remainderSlice = slices.length > 1 ? bulkUpdateRandom.nextInt(slices.length) : 0;\n            for (int i = 0; i < slices.length; i++) {\n              perSlice[i] += perRemain;\n              if (i == remainderSlice) {\n                perSlice[i] += remainder;\n              }\n            }\n          }\n          for (int i = 0; i < slices.length; i++) {\n            Slice s = slices[i];\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              throw new IOException(\"-- no leader in \" + s);\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n            modified = true;\n            long perSliceCount = perSlice[i];\n            s.getReplicas().forEach(r ->\n                freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong(0))\n                    .addAndGet(-perSliceCount * DEFAULT_DOC_SIZE_BYTES));\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.addAndGet(perSlice[i]);\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .addAndGet(perSlice[i]);\n          }\n        } else {\n          // SMALL UPDATE: use exact assignment via DocRouter\n          for (SolrInputDocument doc : docs) {\n            String id = (String) doc.getFieldValue(\"id\");\n            if (id == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Document without id: \" + doc);\n            }\n            Slice s = coll.getRouter().getTargetSlice(id, doc, null, null, coll);\n            Replica leader = s.getLeader();\n            if (leader == null) {\n              throw new IOException(\"-- no leader in \" + s);\n            }\n            metricUpdates.computeIfAbsent(s.getName(), sh -> new HashMap<>())\n                .computeIfAbsent(leader.getCoreName(), cn -> new AtomicLong())\n                .incrementAndGet();\n            modified = true;\n            s.getReplicas().forEach(r ->\n                freediskDeltaPerNode.computeIfAbsent(r.getNodeName(), node -> new AtomicLong())\n                    .addAndGet(-DEFAULT_DOC_SIZE_BYTES));\n            AtomicLong bufferedUpdates = (AtomicLong)sliceProperties.get(collection).get(s.getName()).get(BUFFERED_UPDATES);\n            if (bufferedUpdates != null) {\n              bufferedUpdates.incrementAndGet();\n              continue;\n            }\n            docUpdates.computeIfAbsent(s.getName(), sh -> new AtomicLong())\n                .incrementAndGet();\n          }\n        }\n\n        if (modified) {\n          docUpdates.forEach((sh, count) -> {\n            try {\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.numDocs\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"SEARCHER.searcher.maxDoc\", count.get(), true, false);\n              simSetShardValue(collection, sh, \"UPDATE./update.requests\", count.get(), true, false);\n              // for each new document increase the size by DEFAULT_DOC_SIZE_BYTES\n              simSetShardValue(collection, sh, Type.CORE_IDX.metricsAttribute,\n                  DEFAULT_DOC_SIZE_BYTES * count.get(), true, false);\n              simSetShardValue(collection, sh, Variable.coreidxsize,\n                  Type.CORE_IDX.convertVal(DEFAULT_DOC_SIZE_BYTES * count.get()), true, false);\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          });\n          metricUpdates.forEach((sh, cores) -> {\n            cores.forEach((core, count) -> {\n              String registry = SolrMetricManager.getRegistryName(SolrInfoBean.Group.core, collection, sh,\n                  Utils.parseMetricsReplicaName(collection, core));\n              cloudManager.getMetricManager().registry(registry).counter(\"UPDATE./update.requests\").inc(count.get());\n            });\n          });\n        }\n      } finally {\n        lock.unlock();\n      }\n    }\n    if (!freediskDeltaPerNode.isEmpty()) {\n      SimNodeStateProvider nodeStateProvider = cloudManager.getSimNodeStateProvider();\n      freediskDeltaPerNode.forEach((node, delta) -> {\n        if (delta.get() == 0) {\n          return;\n        }\n        try {\n          // this method does its own locking to prevent races\n          nodeStateProvider.simUpdateNodeValue(node, Type.FREEDISK.tagName, val -> {\n            if (val == null) {\n              throw new RuntimeException(\"no freedisk for node \" + node);\n            }\n            double freedisk = ((Number) val).doubleValue();\n            double deltaGB = (Double) Type.FREEDISK.convertVal(delta.get());\n            freedisk += deltaGB;\n            if (freedisk < 0) {\n              log.warn(\"-- freedisk={} - ran out of disk space on node {}\", freedisk, node);\n              freedisk = 0;\n            }\n            return freedisk;\n          });\n        } catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      });\n    }\n    SolrParams params = req.getParams();\n    if (params != null && (params.getBool(UpdateParams.OPTIMIZE, false) || params.getBool(UpdateParams.EXPUNGE_DELETES, false))) {\n      lock.lockInterruptibly();\n      try {\n        coll.getSlices().forEach(s -> {\n          Replica leader = s.getLeader();\n          Replica ri = getReplicaInfo(leader);\n          Number numDocs = (Number)ri.get(\"SEARCHER.searcher.numDocs\");\n          if (numDocs == null || numDocs.intValue() == 0) {\n            numDocs = 0;\n          }\n          try {\n            simSetShardValue(ri.getCollection(), ri.getShard(), \"SEARCHER.searcher.maxDoc\", numDocs, false, false);\n            simSetShardValue(ri.getCollection(), ri.getShard(), \"SEARCHER.searcher.deletedDocs\", 0, false, false);\n          } catch (Exception e) {\n            throw new RuntimeException(e);\n          }\n        });\n      } finally {\n        lock.unlock();\n      }\n    }\n    return new UpdateResponse();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd":["e35f2dde06b35aa9904949a3a93fabd090371077"],"cb2411226bebe23191dc34a8e8252a936a7261a8":["9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5"],"3f504512a03d978990cbff30db0522b354e846db":["7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e35f2dde06b35aa9904949a3a93fabd090371077":["bffd02b7c57b27d76ece244beb098f61c974b9d9"],"2d80c1ad9241ae005a167d7ee8ac473601b0e57c":["cb2411226bebe23191dc34a8e8252a936a7261a8"],"9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3f504512a03d978990cbff30db0522b354e846db"],"bffd02b7c57b27d76ece244beb098f61c974b9d9":["2d80c1ad9241ae005a167d7ee8ac473601b0e57c"]},"commit2Childs":{"7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd":["3f504512a03d978990cbff30db0522b354e846db"],"cb2411226bebe23191dc34a8e8252a936a7261a8":["2d80c1ad9241ae005a167d7ee8ac473601b0e57c"],"3f504512a03d978990cbff30db0522b354e846db":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5"],"e35f2dde06b35aa9904949a3a93fabd090371077":["7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd"],"9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5":["cb2411226bebe23191dc34a8e8252a936a7261a8"],"2d80c1ad9241ae005a167d7ee8ac473601b0e57c":["bffd02b7c57b27d76ece244beb098f61c974b9d9"],"bffd02b7c57b27d76ece244beb098f61c974b9d9":["e35f2dde06b35aa9904949a3a93fabd090371077"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}