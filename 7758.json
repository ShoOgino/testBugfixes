{"path":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","commits":[{"id":"893269407f5c988a4b2ee13c7ffc01ec43136c1d","date":1268598046,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<Token> analyzeTokenStream(TokenStream tokenStream) {\n    List<Token> tokens = new ArrayList<Token>();\n    Token reusableToken = new Token();\n    Token token = null;\n\n    try {\n      while ((token = tokenStream.next(reusableToken)) != null) {\n        tokens.add((Token) token.clone());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","date":1268599006,"type":4,"author":"Mark Robert Miller","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","sourceNew":null,"sourceOld":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<Token> analyzeTokenStream(TokenStream tokenStream) {\n    List<Token> tokens = new ArrayList<Token>();\n    Token reusableToken = new Token();\n    Token token = null;\n\n    try {\n      while ((token = tokenStream.next(reusableToken)) != null) {\n        tokens.add((Token) token.clone());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","pathOld":"src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","sourceNew":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<Token> analyzeTokenStream(TokenStream tokenStream) {\n    List<Token> tokens = new ArrayList<Token>();\n\n    // TODO change this API to support custom attributes\n    TermAttribute termAtt = (TermAttribute) \n      tokenStream.addAttribute(TermAttribute.class);\n    OffsetAttribute offsetAtt = (OffsetAttribute) \n      tokenStream.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = (TypeAttribute) \n      tokenStream.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = (FlagsAttribute) \n      tokenStream.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = (PayloadAttribute) \n      tokenStream.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) \n      tokenStream.addAttribute(PositionIncrementAttribute.class);\n    \n    try {\n      while (tokenStream.incrementToken()) {\n        Token token = new Token();\n        token.setTermBuffer(termAtt.termBuffer(), 0, termAtt.termLength());\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tokens.add((Token) token.clone());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<Token> analyzeTokenStream(TokenStream tokenStream) {\n    List<Token> tokens = new ArrayList<Token>();\n\n    // TODO change this API to support custom attributes\n    TermAttribute termAtt = (TermAttribute) \n      tokenStream.addAttribute(TermAttribute.class);\n    OffsetAttribute offsetAtt = (OffsetAttribute) \n      tokenStream.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = (TypeAttribute) \n      tokenStream.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = (FlagsAttribute) \n      tokenStream.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = (PayloadAttribute) \n      tokenStream.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) \n      tokenStream.addAttribute(PositionIncrementAttribute.class);\n    \n    try {\n      while (tokenStream.incrementToken()) {\n        Token token = new Token();\n        token.setTermBuffer(termAtt.termBuffer(), 0, termAtt.termLength());\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tokens.add((Token) token.clone());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1da8d55113b689b06716246649de6f62430f15c0","date":1453508340,"type":0,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<Token> analyzeTokenStream(TokenStream tokenStream) {\n    List<Token> tokens = new ArrayList<Token>();\n\n    // TODO change this API to support custom attributes\n    TermAttribute termAtt = (TermAttribute) \n      tokenStream.addAttribute(TermAttribute.class);\n    OffsetAttribute offsetAtt = (OffsetAttribute) \n      tokenStream.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = (TypeAttribute) \n      tokenStream.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = (FlagsAttribute) \n      tokenStream.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = (PayloadAttribute) \n      tokenStream.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) \n      tokenStream.addAttribute(PositionIncrementAttribute.class);\n    \n    try {\n      while (tokenStream.incrementToken()) {\n        Token token = new Token();\n        token.setTermBuffer(termAtt.termBuffer(), 0, termAtt.termLength());\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tokens.add((Token) token.clone());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","sourceNew":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<Token> analyzeTokenStream(TokenStream tokenStream) {\n    List<Token> tokens = new ArrayList<Token>();\n    \n    // TODO change this API to support custom attributes\n    TermAttribute termAtt = null;\n    TermToBytesRefAttribute bytesAtt = null;\n    if (tokenStream.hasAttribute(TermAttribute.class)) {\n      termAtt = tokenStream.getAttribute(TermAttribute.class);\n    } else if (tokenStream.hasAttribute(TermToBytesRefAttribute.class)) {\n      bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    }\n    final OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n    final TypeAttribute typeAtt = tokenStream.addAttribute(TypeAttribute.class);\n    final PositionIncrementAttribute posIncAtt = tokenStream.addAttribute(PositionIncrementAttribute.class);\n    final FlagsAttribute flagsAtt = tokenStream.addAttribute(FlagsAttribute.class);\n    final PayloadAttribute payloadAtt = tokenStream.addAttribute(PayloadAttribute.class);\n    \n    final BytesRef bytes = new BytesRef();\n    try {\n      while (tokenStream.incrementToken()) {\n        Token token = new Token();\n        if (termAtt != null) {\n          token.setTermBuffer(termAtt.term());\n        }\n        if (bytesAtt != null) {\n          bytesAtt.toBytesRef(bytes);\n          // TODO: This is incorrect when numeric fields change in later lucene versions. It should use BytesRef directly!\n          token.setTermBuffer(bytes.utf8ToString());\n        }\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tokens.add((Token) token.clone());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<Token> analyzeTokenStream(TokenStream tokenStream) {\n    List<Token> tokens = new ArrayList<Token>();\n\n    // TODO change this API to support custom attributes\n    TermAttribute termAtt = (TermAttribute) \n      tokenStream.addAttribute(TermAttribute.class);\n    OffsetAttribute offsetAtt = (OffsetAttribute) \n      tokenStream.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = (TypeAttribute) \n      tokenStream.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = (FlagsAttribute) \n      tokenStream.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = (PayloadAttribute) \n      tokenStream.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) \n      tokenStream.addAttribute(PositionIncrementAttribute.class);\n    \n    try {\n      while (tokenStream.incrementToken()) {\n        Token token = new Token();\n        token.setTermBuffer(termAtt.termBuffer(), 0, termAtt.termLength());\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tokens.add((Token) token.clone());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":["72b952811367f6e21added5f7306dc6bf2aa1e9c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d085fb336a7208eea2214e5ffcc803960819b60b","date":1270981894,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","sourceNew":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<Token> analyzeTokenStream(TokenStream tokenStream) {\n    List<Token> tokens = new ArrayList<Token>();\n    \n    // TODO change this API to support custom attributes\n    CharTermAttribute termAtt = null;\n    TermToBytesRefAttribute bytesAtt = null;\n    if (tokenStream.hasAttribute(CharTermAttribute.class)) {\n      termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n    } else if (tokenStream.hasAttribute(TermToBytesRefAttribute.class)) {\n      bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    }\n    final OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n    final TypeAttribute typeAtt = tokenStream.addAttribute(TypeAttribute.class);\n    final PositionIncrementAttribute posIncAtt = tokenStream.addAttribute(PositionIncrementAttribute.class);\n    final FlagsAttribute flagsAtt = tokenStream.addAttribute(FlagsAttribute.class);\n    final PayloadAttribute payloadAtt = tokenStream.addAttribute(PayloadAttribute.class);\n    \n    final BytesRef bytes = new BytesRef();\n    try {\n      while (tokenStream.incrementToken()) {\n        Token token = new Token();\n        if (termAtt != null) {\n          token.setTermBuffer(termAtt.toString());\n        }\n        if (bytesAtt != null) {\n          bytesAtt.toBytesRef(bytes);\n          // TODO: This is incorrect when numeric fields change in later lucene versions. It should use BytesRef directly!\n          token.setTermBuffer(bytes.utf8ToString());\n        }\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tokens.add((Token) token.clone());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<Token> analyzeTokenStream(TokenStream tokenStream) {\n    List<Token> tokens = new ArrayList<Token>();\n    \n    // TODO change this API to support custom attributes\n    TermAttribute termAtt = null;\n    TermToBytesRefAttribute bytesAtt = null;\n    if (tokenStream.hasAttribute(TermAttribute.class)) {\n      termAtt = tokenStream.getAttribute(TermAttribute.class);\n    } else if (tokenStream.hasAttribute(TermToBytesRefAttribute.class)) {\n      bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    }\n    final OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n    final TypeAttribute typeAtt = tokenStream.addAttribute(TypeAttribute.class);\n    final PositionIncrementAttribute posIncAtt = tokenStream.addAttribute(PositionIncrementAttribute.class);\n    final FlagsAttribute flagsAtt = tokenStream.addAttribute(FlagsAttribute.class);\n    final PayloadAttribute payloadAtt = tokenStream.addAttribute(PayloadAttribute.class);\n    \n    final BytesRef bytes = new BytesRef();\n    try {\n      while (tokenStream.incrementToken()) {\n        Token token = new Token();\n        if (termAtt != null) {\n          token.setTermBuffer(termAtt.term());\n        }\n        if (bytesAtt != null) {\n          bytesAtt.toBytesRef(bytes);\n          // TODO: This is incorrect when numeric fields change in later lucene versions. It should use BytesRef directly!\n          token.setTermBuffer(bytes.utf8ToString());\n        }\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tokens.add((Token) token.clone());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a7347509fad0711ac30cb15a746e9a3830a38ebd","date":1275388513,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","sourceNew":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<Token> analyzeTokenStream(TokenStream tokenStream) {\n    List<Token> tokens = new ArrayList<Token>();\n    \n    // TODO change this API to support custom attributes\n    CharTermAttribute termAtt = null;\n    TermToBytesRefAttribute bytesAtt = null;\n    if (tokenStream.hasAttribute(CharTermAttribute.class)) {\n      termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n    } else if (tokenStream.hasAttribute(TermToBytesRefAttribute.class)) {\n      bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    }\n    final OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n    final TypeAttribute typeAtt = tokenStream.addAttribute(TypeAttribute.class);\n    final PositionIncrementAttribute posIncAtt = tokenStream.addAttribute(PositionIncrementAttribute.class);\n    final FlagsAttribute flagsAtt = tokenStream.addAttribute(FlagsAttribute.class);\n    final PayloadAttribute payloadAtt = tokenStream.addAttribute(PayloadAttribute.class);\n    \n    final BytesRef bytes = new BytesRef();\n    try {\n      while (tokenStream.incrementToken()) {\n        Token token = new Token();\n        if (termAtt != null) {\n          token.setEmpty().append(termAtt);\n        }\n        if (bytesAtt != null) {\n          bytesAtt.toBytesRef(bytes);\n          // TODO: This is incorrect when numeric fields change in later lucene versions. It should use BytesRef directly!\n          token.setEmpty().append(bytes.utf8ToString());\n        }\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tokens.add((Token) token.clone());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<Token> analyzeTokenStream(TokenStream tokenStream) {\n    List<Token> tokens = new ArrayList<Token>();\n    \n    // TODO change this API to support custom attributes\n    CharTermAttribute termAtt = null;\n    TermToBytesRefAttribute bytesAtt = null;\n    if (tokenStream.hasAttribute(CharTermAttribute.class)) {\n      termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n    } else if (tokenStream.hasAttribute(TermToBytesRefAttribute.class)) {\n      bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    }\n    final OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n    final TypeAttribute typeAtt = tokenStream.addAttribute(TypeAttribute.class);\n    final PositionIncrementAttribute posIncAtt = tokenStream.addAttribute(PositionIncrementAttribute.class);\n    final FlagsAttribute flagsAtt = tokenStream.addAttribute(FlagsAttribute.class);\n    final PayloadAttribute payloadAtt = tokenStream.addAttribute(PayloadAttribute.class);\n    \n    final BytesRef bytes = new BytesRef();\n    try {\n      while (tokenStream.incrementToken()) {\n        Token token = new Token();\n        if (termAtt != null) {\n          token.setTermBuffer(termAtt.toString());\n        }\n        if (bytesAtt != null) {\n          bytesAtt.toBytesRef(bytes);\n          // TODO: This is incorrect when numeric fields change in later lucene versions. It should use BytesRef directly!\n          token.setTermBuffer(bytes.utf8ToString());\n        }\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tokens.add((Token) token.clone());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ec58fb7921964848d01bea54f8ec4a2ac813eaeb","date":1295476876,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","sourceNew":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<AttributeSource> analyzeTokenStream(TokenStream tokenStream) {\n    List<AttributeSource> tokens = new ArrayList<AttributeSource>();\n    // for backwards compatibility, add all \"common\" attributes\n    tokenStream.addAttribute(PositionIncrementAttribute.class);\n    tokenStream.addAttribute(OffsetAttribute.class);\n    tokenStream.addAttribute(TypeAttribute.class);\n    final BytesRef bytes = new BytesRef();\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        tokens.add(tokenStream.cloneAttributes());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<Token> analyzeTokenStream(TokenStream tokenStream) {\n    List<Token> tokens = new ArrayList<Token>();\n    \n    // TODO change this API to support custom attributes\n    CharTermAttribute termAtt = null;\n    TermToBytesRefAttribute bytesAtt = null;\n    if (tokenStream.hasAttribute(CharTermAttribute.class)) {\n      termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n    } else if (tokenStream.hasAttribute(TermToBytesRefAttribute.class)) {\n      bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    }\n    final OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n    final TypeAttribute typeAtt = tokenStream.addAttribute(TypeAttribute.class);\n    final PositionIncrementAttribute posIncAtt = tokenStream.addAttribute(PositionIncrementAttribute.class);\n    final FlagsAttribute flagsAtt = tokenStream.addAttribute(FlagsAttribute.class);\n    final PayloadAttribute payloadAtt = tokenStream.addAttribute(PayloadAttribute.class);\n    \n    final BytesRef bytes = new BytesRef();\n    try {\n      while (tokenStream.incrementToken()) {\n        Token token = new Token();\n        if (termAtt != null) {\n          token.setEmpty().append(termAtt);\n        }\n        if (bytesAtt != null) {\n          bytesAtt.toBytesRef(bytes);\n          // TODO: This is incorrect when numeric fields change in later lucene versions. It should use BytesRef directly!\n          token.setEmpty().append(bytes.utf8ToString());\n        }\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tokens.add((Token) token.clone());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e79a6d080bdd5b2a8f56342cf571b5476de04180","date":1295638686,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","sourceNew":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<AttributeSource> analyzeTokenStream(TokenStream tokenStream) {\n    List<AttributeSource> tokens = new ArrayList<AttributeSource>();\n    // for backwards compatibility, add all \"common\" attributes\n    tokenStream.addAttribute(PositionIncrementAttribute.class);\n    tokenStream.addAttribute(OffsetAttribute.class);\n    tokenStream.addAttribute(TypeAttribute.class);\n    final BytesRef bytes = new BytesRef();\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        tokens.add(tokenStream.cloneAttributes());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<Token> analyzeTokenStream(TokenStream tokenStream) {\n    List<Token> tokens = new ArrayList<Token>();\n    \n    // TODO change this API to support custom attributes\n    CharTermAttribute termAtt = null;\n    TermToBytesRefAttribute bytesAtt = null;\n    if (tokenStream.hasAttribute(CharTermAttribute.class)) {\n      termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n    } else if (tokenStream.hasAttribute(TermToBytesRefAttribute.class)) {\n      bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    }\n    final OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n    final TypeAttribute typeAtt = tokenStream.addAttribute(TypeAttribute.class);\n    final PositionIncrementAttribute posIncAtt = tokenStream.addAttribute(PositionIncrementAttribute.class);\n    final FlagsAttribute flagsAtt = tokenStream.addAttribute(FlagsAttribute.class);\n    final PayloadAttribute payloadAtt = tokenStream.addAttribute(PayloadAttribute.class);\n    \n    final BytesRef bytes = new BytesRef();\n    try {\n      while (tokenStream.incrementToken()) {\n        Token token = new Token();\n        if (termAtt != null) {\n          token.setEmpty().append(termAtt);\n        }\n        if (bytesAtt != null) {\n          bytesAtt.toBytesRef(bytes);\n          // TODO: This is incorrect when numeric fields change in later lucene versions. It should use BytesRef directly!\n          token.setEmpty().append(bytes.utf8ToString());\n        }\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tokens.add((Token) token.clone());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","sourceNew":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<AttributeSource> analyzeTokenStream(TokenStream tokenStream) {\n    List<AttributeSource> tokens = new ArrayList<AttributeSource>();\n    // for backwards compatibility, add all \"common\" attributes\n    tokenStream.addAttribute(PositionIncrementAttribute.class);\n    tokenStream.addAttribute(OffsetAttribute.class);\n    tokenStream.addAttribute(TypeAttribute.class);\n    final BytesRef bytes = new BytesRef();\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        tokens.add(tokenStream.cloneAttributes());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<Token> analyzeTokenStream(TokenStream tokenStream) {\n    List<Token> tokens = new ArrayList<Token>();\n    \n    // TODO change this API to support custom attributes\n    CharTermAttribute termAtt = null;\n    TermToBytesRefAttribute bytesAtt = null;\n    if (tokenStream.hasAttribute(CharTermAttribute.class)) {\n      termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n    } else if (tokenStream.hasAttribute(TermToBytesRefAttribute.class)) {\n      bytesAtt = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n    }\n    final OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n    final TypeAttribute typeAtt = tokenStream.addAttribute(TypeAttribute.class);\n    final PositionIncrementAttribute posIncAtt = tokenStream.addAttribute(PositionIncrementAttribute.class);\n    final FlagsAttribute flagsAtt = tokenStream.addAttribute(FlagsAttribute.class);\n    final PayloadAttribute payloadAtt = tokenStream.addAttribute(PayloadAttribute.class);\n    \n    final BytesRef bytes = new BytesRef();\n    try {\n      while (tokenStream.incrementToken()) {\n        Token token = new Token();\n        if (termAtt != null) {\n          token.setEmpty().append(termAtt);\n        }\n        if (bytesAtt != null) {\n          bytesAtt.toBytesRef(bytes);\n          // TODO: This is incorrect when numeric fields change in later lucene versions. It should use BytesRef directly!\n          token.setEmpty().append(bytes.utf8ToString());\n        }\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tokens.add((Token) token.clone());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0487f900016b7da69f089f740e28192189ef3972","date":1307810819,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","sourceNew":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<AttributeSource> analyzeTokenStream(TokenStream tokenStream) {\n    final List<AttributeSource> tokens = new ArrayList<AttributeSource>();\n    final PositionIncrementAttribute posIncrAtt = tokenStream.addAttribute(PositionIncrementAttribute.class);\n    final TokenTrackingAttribute trackerAtt = tokenStream.addAttribute(TokenTrackingAttribute.class);\n    // for backwards compatibility, add all \"common\" attributes\n    tokenStream.addAttribute(OffsetAttribute.class);\n    tokenStream.addAttribute(TypeAttribute.class);\n    final BytesRef bytes = new BytesRef();\n    try {\n      tokenStream.reset();\n      int position = 0;\n      while (tokenStream.incrementToken()) {\n        position += posIncrAtt.getPositionIncrement();\n        trackerAtt.setActPosition(position);\n        tokens.add(tokenStream.cloneAttributes());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<AttributeSource> analyzeTokenStream(TokenStream tokenStream) {\n    List<AttributeSource> tokens = new ArrayList<AttributeSource>();\n    // for backwards compatibility, add all \"common\" attributes\n    tokenStream.addAttribute(PositionIncrementAttribute.class);\n    tokenStream.addAttribute(OffsetAttribute.class);\n    tokenStream.addAttribute(TypeAttribute.class);\n    final BytesRef bytes = new BytesRef();\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        tokens.add(tokenStream.cloneAttributes());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9ed208afa1e7aa98899ddb1dedfddedddf898253","date":1308079587,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","sourceNew":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<AttributeSource> analyzeTokenStream(TokenStream tokenStream) {\n    final List<AttributeSource> tokens = new ArrayList<AttributeSource>();\n    final PositionIncrementAttribute posIncrAtt = tokenStream.addAttribute(PositionIncrementAttribute.class);\n    final TokenTrackingAttribute trackerAtt = tokenStream.addAttribute(TokenTrackingAttribute.class);\n    // for backwards compatibility, add all \"common\" attributes\n    tokenStream.addAttribute(OffsetAttribute.class);\n    tokenStream.addAttribute(TypeAttribute.class);\n    final BytesRef bytes = new BytesRef();\n    try {\n      tokenStream.reset();\n      int position = 0;\n      while (tokenStream.incrementToken()) {\n        position += posIncrAtt.getPositionIncrement();\n        trackerAtt.setActPosition(position);\n        tokens.add(tokenStream.cloneAttributes());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<AttributeSource> analyzeTokenStream(TokenStream tokenStream) {\n    List<AttributeSource> tokens = new ArrayList<AttributeSource>();\n    // for backwards compatibility, add all \"common\" attributes\n    tokenStream.addAttribute(PositionIncrementAttribute.class);\n    tokenStream.addAttribute(OffsetAttribute.class);\n    tokenStream.addAttribute(TypeAttribute.class);\n    final BytesRef bytes = new BytesRef();\n    try {\n      tokenStream.reset();\n      while (tokenStream.incrementToken()) {\n        tokens.add(tokenStream.cloneAttributes());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","sourceNew":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<AttributeSource> analyzeTokenStream(TokenStream tokenStream) {\n    final List<AttributeSource> tokens = new ArrayList<AttributeSource>();\n    final PositionIncrementAttribute posIncrAtt = tokenStream.addAttribute(PositionIncrementAttribute.class);\n    final TokenTrackingAttribute trackerAtt = tokenStream.addAttribute(TokenTrackingAttribute.class);\n    // for backwards compatibility, add all \"common\" attributes\n    tokenStream.addAttribute(OffsetAttribute.class);\n    tokenStream.addAttribute(TypeAttribute.class);\n    final BytesRef bytes = new BytesRef();\n    try {\n      tokenStream.reset();\n      int position = 0;\n      while (tokenStream.incrementToken()) {\n        position += posIncrAtt.getPositionIncrement();\n        trackerAtt.setActPosition(position);\n        tokens.add(tokenStream.cloneAttributes());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<AttributeSource> analyzeTokenStream(TokenStream tokenStream) {\n    final List<AttributeSource> tokens = new ArrayList<AttributeSource>();\n    final PositionIncrementAttribute posIncrAtt = tokenStream.addAttribute(PositionIncrementAttribute.class);\n    final TokenTrackingAttribute trackerAtt = tokenStream.addAttribute(TokenTrackingAttribute.class);\n    // for backwards compatibility, add all \"common\" attributes\n    tokenStream.addAttribute(OffsetAttribute.class);\n    tokenStream.addAttribute(TypeAttribute.class);\n    final BytesRef bytes = new BytesRef();\n    try {\n      tokenStream.reset();\n      int position = 0;\n      while (tokenStream.incrementToken()) {\n        position += posIncrAtt.getPositionIncrement();\n        trackerAtt.setActPosition(position);\n        tokens.add(tokenStream.cloneAttributes());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","sourceNew":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<AttributeSource> analyzeTokenStream(TokenStream tokenStream) {\n    final List<AttributeSource> tokens = new ArrayList<AttributeSource>();\n    final PositionIncrementAttribute posIncrAtt = tokenStream.addAttribute(PositionIncrementAttribute.class);\n    final TokenTrackingAttribute trackerAtt = tokenStream.addAttribute(TokenTrackingAttribute.class);\n    // for backwards compatibility, add all \"common\" attributes\n    tokenStream.addAttribute(OffsetAttribute.class);\n    tokenStream.addAttribute(TypeAttribute.class);\n    final BytesRef bytes = new BytesRef();\n    try {\n      tokenStream.reset();\n      int position = 0;\n      while (tokenStream.incrementToken()) {\n        position += posIncrAtt.getPositionIncrement();\n        trackerAtt.setActPosition(position);\n        tokens.add(tokenStream.cloneAttributes());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<AttributeSource> analyzeTokenStream(TokenStream tokenStream) {\n    final List<AttributeSource> tokens = new ArrayList<AttributeSource>();\n    final PositionIncrementAttribute posIncrAtt = tokenStream.addAttribute(PositionIncrementAttribute.class);\n    final TokenTrackingAttribute trackerAtt = tokenStream.addAttribute(TokenTrackingAttribute.class);\n    // for backwards compatibility, add all \"common\" attributes\n    tokenStream.addAttribute(OffsetAttribute.class);\n    tokenStream.addAttribute(TypeAttribute.class);\n    final BytesRef bytes = new BytesRef();\n    try {\n      tokenStream.reset();\n      int position = 0;\n      while (tokenStream.incrementToken()) {\n        position += posIncrAtt.getPositionIncrement();\n        trackerAtt.setActPosition(position);\n        tokens.add(tokenStream.cloneAttributes());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"72b952811367f6e21added5f7306dc6bf2aa1e9c","date":1310127304,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","sourceNew":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<AttributeSource> analyzeTokenStream(TokenStream tokenStream) {\n    final List<AttributeSource> tokens = new ArrayList<AttributeSource>();\n    final PositionIncrementAttribute posIncrAtt = tokenStream.addAttribute(PositionIncrementAttribute.class);\n    final TokenTrackingAttribute trackerAtt = tokenStream.addAttribute(TokenTrackingAttribute.class);\n    // for backwards compatibility, add all \"common\" attributes\n    tokenStream.addAttribute(OffsetAttribute.class);\n    tokenStream.addAttribute(TypeAttribute.class);\n    try {\n      tokenStream.reset();\n      int position = 0;\n      while (tokenStream.incrementToken()) {\n        position += posIncrAtt.getPositionIncrement();\n        trackerAtt.setActPosition(position);\n        tokens.add(tokenStream.cloneAttributes());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<AttributeSource> analyzeTokenStream(TokenStream tokenStream) {\n    final List<AttributeSource> tokens = new ArrayList<AttributeSource>();\n    final PositionIncrementAttribute posIncrAtt = tokenStream.addAttribute(PositionIncrementAttribute.class);\n    final TokenTrackingAttribute trackerAtt = tokenStream.addAttribute(TokenTrackingAttribute.class);\n    // for backwards compatibility, add all \"common\" attributes\n    tokenStream.addAttribute(OffsetAttribute.class);\n    tokenStream.addAttribute(TypeAttribute.class);\n    final BytesRef bytes = new BytesRef();\n    try {\n      tokenStream.reset();\n      int position = 0;\n      while (tokenStream.incrementToken()) {\n        position += posIncrAtt.getPositionIncrement();\n        trackerAtt.setActPosition(position);\n        tokens.add(tokenStream.cloneAttributes());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","bugFix":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":5,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","pathOld":"solr/src/java/org/apache/solr/handler/AnalysisRequestHandlerBase#analyzeTokenStream(TokenStream).mjava","sourceNew":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<AttributeSource> analyzeTokenStream(TokenStream tokenStream) {\n    final List<AttributeSource> tokens = new ArrayList<AttributeSource>();\n    final PositionIncrementAttribute posIncrAtt = tokenStream.addAttribute(PositionIncrementAttribute.class);\n    final TokenTrackingAttribute trackerAtt = tokenStream.addAttribute(TokenTrackingAttribute.class);\n    // for backwards compatibility, add all \"common\" attributes\n    tokenStream.addAttribute(OffsetAttribute.class);\n    tokenStream.addAttribute(TypeAttribute.class);\n    try {\n      tokenStream.reset();\n      int position = 0;\n      while (tokenStream.incrementToken()) {\n        position += posIncrAtt.getPositionIncrement();\n        trackerAtt.setActPosition(position);\n        tokens.add(tokenStream.cloneAttributes());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","sourceOld":"  /**\n   * Analyzes the given TokenStream, collecting the Tokens it produces.\n   *\n   * @param tokenStream TokenStream to analyze\n   *\n   * @return List of tokens produced from the TokenStream\n   */\n  private List<AttributeSource> analyzeTokenStream(TokenStream tokenStream) {\n    final List<AttributeSource> tokens = new ArrayList<AttributeSource>();\n    final PositionIncrementAttribute posIncrAtt = tokenStream.addAttribute(PositionIncrementAttribute.class);\n    final TokenTrackingAttribute trackerAtt = tokenStream.addAttribute(TokenTrackingAttribute.class);\n    // for backwards compatibility, add all \"common\" attributes\n    tokenStream.addAttribute(OffsetAttribute.class);\n    tokenStream.addAttribute(TypeAttribute.class);\n    try {\n      tokenStream.reset();\n      int position = 0;\n      while (tokenStream.incrementToken()) {\n        position += posIncrAtt.getPositionIncrement();\n        trackerAtt.setActPosition(position);\n        tokens.add(tokenStream.cloneAttributes());\n      }\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Error occured while iterating over tokenstream\", ioe);\n    }\n\n    return tokens;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"c26f00b574427b55127e869b935845554afde1fa":["72b952811367f6e21added5f7306dc6bf2aa1e9c","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"72b952811367f6e21added5f7306dc6bf2aa1e9c":["0487f900016b7da69f089f740e28192189ef3972"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["1da8d55113b689b06716246649de6f62430f15c0"],"0487f900016b7da69f089f740e28192189ef3972":["ec58fb7921964848d01bea54f8ec4a2ac813eaeb"],"ec58fb7921964848d01bea54f8ec4a2ac813eaeb":["a7347509fad0711ac30cb15a746e9a3830a38ebd"],"9ed208afa1e7aa98899ddb1dedfddedddf898253":["ec58fb7921964848d01bea54f8ec4a2ac813eaeb","0487f900016b7da69f089f740e28192189ef3972"],"e79a6d080bdd5b2a8f56342cf571b5476de04180":["a7347509fad0711ac30cb15a746e9a3830a38ebd","ec58fb7921964848d01bea54f8ec4a2ac813eaeb"],"1da8d55113b689b06716246649de6f62430f15c0":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","ad94625fb8d088209f46650c8097196fec67f00c"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["9ed208afa1e7aa98899ddb1dedfddedddf898253"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["a7347509fad0711ac30cb15a746e9a3830a38ebd","ec58fb7921964848d01bea54f8ec4a2ac813eaeb"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a258fbb26824fd104ed795e5d9033d2d040049ee":["0487f900016b7da69f089f740e28192189ef3972"],"ad94625fb8d088209f46650c8097196fec67f00c":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"d085fb336a7208eea2214e5ffcc803960819b60b":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"a7347509fad0711ac30cb15a746e9a3830a38ebd":["d085fb336a7208eea2214e5ffcc803960819b60b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c26f00b574427b55127e869b935845554afde1fa"]},"commit2Childs":{"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["1da8d55113b689b06716246649de6f62430f15c0"],"c26f00b574427b55127e869b935845554afde1fa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["ad94625fb8d088209f46650c8097196fec67f00c"],"72b952811367f6e21added5f7306dc6bf2aa1e9c":["c26f00b574427b55127e869b935845554afde1fa"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["d085fb336a7208eea2214e5ffcc803960819b60b"],"0487f900016b7da69f089f740e28192189ef3972":["72b952811367f6e21added5f7306dc6bf2aa1e9c","9ed208afa1e7aa98899ddb1dedfddedddf898253","a258fbb26824fd104ed795e5d9033d2d040049ee"],"ec58fb7921964848d01bea54f8ec4a2ac813eaeb":["0487f900016b7da69f089f740e28192189ef3972","9ed208afa1e7aa98899ddb1dedfddedddf898253","e79a6d080bdd5b2a8f56342cf571b5476de04180","29ef99d61cda9641b6250bf9567329a6e65f901d"],"9ed208afa1e7aa98899ddb1dedfddedddf898253":["c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"e79a6d080bdd5b2a8f56342cf571b5476de04180":[],"1da8d55113b689b06716246649de6f62430f15c0":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"29ef99d61cda9641b6250bf9567329a6e65f901d":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"ad94625fb8d088209f46650c8097196fec67f00c":["1da8d55113b689b06716246649de6f62430f15c0"],"d085fb336a7208eea2214e5ffcc803960819b60b":["a7347509fad0711ac30cb15a746e9a3830a38ebd"],"a7347509fad0711ac30cb15a746e9a3830a38ebd":["ec58fb7921964848d01bea54f8ec4a2ac813eaeb","e79a6d080bdd5b2a8f56342cf571b5476de04180","29ef99d61cda9641b6250bf9567329a6e65f901d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["e79a6d080bdd5b2a8f56342cf571b5476de04180","29ef99d61cda9641b6250bf9567329a6e65f901d","a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}