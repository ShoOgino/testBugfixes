{"path":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    AtomicReaderContext[] children = topReaderContext.leaves();\n    DocValues docValues = children[0].reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    AtomicReaderContext[] children = topReaderContext.leaves();\n    DocValues docValues = children[0].reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cd0f953fbccd59aa346f280fe7e30a698f5ecb04","date":1331511349,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    AtomicReaderContext[] children = topReaderContext.leaves();\n    DocValues docValues = children[0].reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    AtomicReaderContext[] children = topReaderContext.leaves();\n    DocValues docValues = children[0].reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"38e3b736c7ca086d61b7dbb841c905ee115490da","date":1331657018,"type":3,"author":"Ryan McKinley","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    AtomicReaderContext[] children = topReaderContext.leaves();\n    DocValues docValues = children[0].reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    AtomicReaderContext[] children = topReaderContext.leaves();\n    DocValues docValues = children[0].reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random()), values, 0, num_1);\n    writer.commit();\n    \n    if (random().nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      index(writer_2,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random().nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    AtomicReaderContext[] children = topReaderContext.leaves();\n    DocValues docValues = children[0].reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    AtomicReaderContext[] children = topReaderContext.leaves();\n    DocValues docValues = children[0].reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random()), values, 0, num_1);\n    writer.commit();\n    \n    if (random().nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      index(writer_2,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random().nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = DirectoryReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    AtomicReaderContext[] children = topReaderContext.leaves();\n    DocValues docValues = children[0].reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random()), values, 0, num_1);\n    writer.commit();\n    \n    if (random().nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      index(writer_2,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random().nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    AtomicReaderContext[] children = topReaderContext.leaves();\n    DocValues docValues = children[0].reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cc8f931c07d7930ebee666cf6d69b1b6d9f9cd18","date":1339188570,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    Type[] sourceType = new Type[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random()), values, sourceType, 0, num_1);\n    writer.commit();\n    \n    if (random().nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      index(writer_2,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random().nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = DirectoryReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    AtomicReaderContext[] children = topReaderContext.leaves();\n    DocValues docValues = children[0].reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values, sourceType);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random()), values, 0, num_1);\n    writer.commit();\n    \n    if (random().nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      index(writer_2,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random().nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = DirectoryReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    AtomicReaderContext[] children = topReaderContext.leaves();\n    DocValues docValues = children[0].reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4c5ae929ce8aa0c4856f0d6bfd4c196bc2d3eb9c","date":1340090669,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    Type[] sourceType = new Type[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random()), values, sourceType, 0, num_1);\n    writer.commit();\n    \n    if (random().nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      index(writer_2,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random().nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = DirectoryReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().size());\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    List<AtomicReaderContext> leaves = topReaderContext.leaves();\n    DocValues docValues = leaves.get(0).reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values, sourceType);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    Type[] sourceType = new Type[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random()), values, sourceType, 0, num_1);\n    writer.commit();\n    \n    if (random().nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      index(writer_2,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random().nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = DirectoryReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    AtomicReaderContext[] children = topReaderContext.leaves();\n    DocValues docValues = children[0].reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values, sourceType);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ced66195b26fdb1f77ee00e2a77ec6918dedd766","date":1344948886,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    Type[] sourceType = new Type[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random()), values, sourceType, 0, num_1);\n    writer.commit();\n    \n    if (random().nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      index(writer_2,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random().nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = DirectoryReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.leaves().size());\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    List<AtomicReaderContext> leaves = topReaderContext.leaves();\n    DocValues docValues = leaves.get(0).reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values, sourceType);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    Type[] sourceType = new Type[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random()), values, sourceType, 0, num_1);\n    writer.commit();\n    \n    if (random().nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      index(writer_2,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random().nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = DirectoryReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().size());\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    List<AtomicReaderContext> leaves = topReaderContext.leaves();\n    DocValues docValues = leaves.get(0).reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values, sourceType);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["4c5ae929ce8aa0c4856f0d6bfd4c196bc2d3eb9c"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"211b1506e56f7860762fbd4698f6d1d1b57f672c","date":1344976996,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    Type[] sourceType = new Type[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random()), values, sourceType, 0, num_1);\n    writer.commit();\n    \n    if (random().nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      index(writer_2,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random().nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = DirectoryReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.leaves().size());\n    IndexReaderContext topReaderContext = reader.getContext();\n    List<AtomicReaderContext> leaves = topReaderContext.leaves();\n    DocValues docValues = leaves.get(0).reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values, sourceType);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    Type[] sourceType = new Type[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random()), values, sourceType, 0, num_1);\n    writer.commit();\n    \n    if (random().nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      index(writer_2,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random().nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = DirectoryReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.leaves().size());\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    List<AtomicReaderContext> leaves = topReaderContext.leaves();\n    DocValues docValues = leaves.get(0).reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values, sourceType);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","date":1345029782,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    Type[] sourceType = new Type[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random()), values, sourceType, 0, num_1);\n    writer.commit();\n    \n    if (random().nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      index(writer_2,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random().nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = DirectoryReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.leaves().size());\n    IndexReaderContext topReaderContext = reader.getContext();\n    List<AtomicReaderContext> leaves = topReaderContext.leaves();\n    DocValues docValues = leaves.get(0).reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values, sourceType);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    Type[] sourceType = new Type[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random()), values, sourceType, 0, num_1);\n    writer.commit();\n    \n    if (random().nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      index(writer_2,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random().nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = DirectoryReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().size());\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    List<AtomicReaderContext> leaves = topReaderContext.leaves();\n    DocValues docValues = leaves.get(0).reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values, sourceType);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3c188105a9aae04f56c24996f98f8333fc825d2e","date":1345031914,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    Type[] sourceType = new Type[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random()), values, sourceType, 0, num_1);\n    writer.commit();\n    \n    if (random().nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      index(writer_2,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random().nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = DirectoryReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.leaves().size());\n    IndexReaderContext topReaderContext = reader.getContext();\n    List<AtomicReaderContext> leaves = topReaderContext.leaves();\n    DocValues docValues = leaves.get(0).reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values, sourceType);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    Type[] sourceType = new Type[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random()), values, sourceType, 0, num_1);\n    writer.commit();\n    \n    if (random().nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      index(writer_2,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random().nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = DirectoryReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().size());\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    List<AtomicReaderContext> leaves = topReaderContext.leaves();\n    DocValues docValues = leaves.get(0).reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values, sourceType);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1c93396a1df03720cb20e2c2f513a6fa59b21e4c","date":1345032673,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    Type[] sourceType = new Type[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random()), values, sourceType, 0, num_1);\n    writer.commit();\n    \n    if (random().nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      index(writer_2,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random().nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = DirectoryReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().size());\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    List<AtomicReaderContext> leaves = topReaderContext.leaves();\n    DocValues docValues = leaves.get(0).reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values, sourceType);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    Type[] sourceType = new Type[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random()), values, sourceType, 0, num_1);\n    writer.commit();\n    \n    if (random().nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      index(writer_2,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random().nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = DirectoryReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.leaves().size());\n    IndexReaderContext topReaderContext = reader.getContext();\n    List<AtomicReaderContext> leaves = topReaderContext.leaves();\n    DocValues docValues = leaves.get(0).reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values, sourceType);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b05c56a41b733e02a189c48895922b5bd8c7f3d1","date":1345033322,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    Type[] sourceType = new Type[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random()), values, sourceType, 0, num_1);\n    writer.commit();\n    \n    if (random().nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      index(writer_2,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random().nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = DirectoryReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.leaves().size());\n    IndexReaderContext topReaderContext = reader.getContext();\n    List<AtomicReaderContext> leaves = topReaderContext.leaves();\n    DocValues docValues = leaves.get(0).reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values, sourceType);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    Type[] sourceType = new Type[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random()), values, sourceType, 0, num_1);\n    writer.commit();\n    \n    if (random().nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      index(writer_2,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random().nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = DirectoryReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().size());\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    List<AtomicReaderContext> leaves = topReaderContext.leaves();\n    DocValues docValues = leaves.get(0).reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values, sourceType);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6e2893fd5349134af382d33ccc3d84840394c6c1","date":1353682567,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":null,"sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    Type[] sourceType = new Type[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random()), values, sourceType, 0, num_1);\n    writer.commit();\n    \n    if (random().nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      index(writer_2,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random().nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = DirectoryReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.leaves().size());\n    IndexReaderContext topReaderContext = reader.getContext();\n    List<AtomicReaderContext> leaves = topReaderContext.leaves();\n    DocValues docValues = leaves.get(0).reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values, sourceType);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":4,"author":"Robert Muir","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":null,"sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    Type[] sourceType = new Type[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random()), values, sourceType, 0, num_1);\n    writer.commit();\n    \n    if (random().nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));\n      index(writer_2,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random().nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = DirectoryReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random().nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random()), values, sourceType, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.leaves().size());\n    IndexReaderContext topReaderContext = reader.getContext();\n    List<AtomicReaderContext> leaves = topReaderContext.leaves();\n    DocValues docValues = leaves.get(0).reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values, sourceType);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.getType());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"38e3b736c7ca086d61b7dbb841c905ee115490da":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","cd0f953fbccd59aa346f280fe7e30a698f5ecb04"],"3c188105a9aae04f56c24996f98f8333fc825d2e":["4c5ae929ce8aa0c4856f0d6bfd4c196bc2d3eb9c"],"4c5ae929ce8aa0c4856f0d6bfd4c196bc2d3eb9c":["cc8f931c07d7930ebee666cf6d69b1b6d9f9cd18"],"6e2893fd5349134af382d33ccc3d84840394c6c1":["211b1506e56f7860762fbd4698f6d1d1b57f672c"],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":["1c93396a1df03720cb20e2c2f513a6fa59b21e4c","211b1506e56f7860762fbd4698f6d1d1b57f672c"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"1c93396a1df03720cb20e2c2f513a6fa59b21e4c":["3c188105a9aae04f56c24996f98f8333fc825d2e"],"211b1506e56f7860762fbd4698f6d1d1b57f672c":["ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":["4c5ae929ce8aa0c4856f0d6bfd4c196bc2d3eb9c","211b1506e56f7860762fbd4698f6d1d1b57f672c"],"cc8f931c07d7930ebee666cf6d69b1b6d9f9cd18":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["211b1506e56f7860762fbd4698f6d1d1b57f672c","6e2893fd5349134af382d33ccc3d84840394c6c1"],"cd0f953fbccd59aa346f280fe7e30a698f5ecb04":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["cd0f953fbccd59aa346f280fe7e30a698f5ecb04"],"ced66195b26fdb1f77ee00e2a77ec6918dedd766":["4c5ae929ce8aa0c4856f0d6bfd4c196bc2d3eb9c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d4d69c535930b5cce125cff868d40f6373dc27d4"]},"commit2Childs":{"38e3b736c7ca086d61b7dbb841c905ee115490da":[],"3c188105a9aae04f56c24996f98f8333fc825d2e":["1c93396a1df03720cb20e2c2f513a6fa59b21e4c"],"4c5ae929ce8aa0c4856f0d6bfd4c196bc2d3eb9c":["3c188105a9aae04f56c24996f98f8333fc825d2e","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","ced66195b26fdb1f77ee00e2a77ec6918dedd766"],"6e2893fd5349134af382d33ccc3d84840394c6c1":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["38e3b736c7ca086d61b7dbb841c905ee115490da","cd0f953fbccd59aa346f280fe7e30a698f5ecb04"],"1c93396a1df03720cb20e2c2f513a6fa59b21e4c":["b05c56a41b733e02a189c48895922b5bd8c7f3d1"],"211b1506e56f7860762fbd4698f6d1d1b57f672c":["6e2893fd5349134af382d33ccc3d84840394c6c1","b05c56a41b733e02a189c48895922b5bd8c7f3d1","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","d4d69c535930b5cce125cff868d40f6373dc27d4"],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":[],"cc8f931c07d7930ebee666cf6d69b1b6d9f9cd18":["4c5ae929ce8aa0c4856f0d6bfd4c196bc2d3eb9c"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["cc8f931c07d7930ebee666cf6d69b1b6d9f9cd18"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd0f953fbccd59aa346f280fe7e30a698f5ecb04":["38e3b736c7ca086d61b7dbb841c905ee115490da","629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"ced66195b26fdb1f77ee00e2a77ec6918dedd766":["211b1506e56f7860762fbd4698f6d1d1b57f672c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["38e3b736c7ca086d61b7dbb841c905ee115490da","b05c56a41b733e02a189c48895922b5bd8c7f3d1","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}