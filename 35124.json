{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","commits":[{"id":"925e350e1b939d6381c12c2d73ce20aa438d482c","date":1399678506,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"/dev/null","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = Codec.forName(\"Lucene46\");\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = new Lucene46SegmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["0567bdc5c86c94ced64201187cfcef2417d76dda","8106bc60c7452250f84c65cdb43ab6b1d8eb1534"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29937417e80326ef8c86754923bab3c82314d592","date":1399976804,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newFSDirectory(createTempDir());\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = Codec.forName(\"Lucene46\");\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = new Lucene46SegmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = Codec.forName(\"Lucene46\");\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = new Lucene46SegmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8b57d7226a882c9e7d8fdaee52a51c4c04813dd6","date":1399976975,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = Codec.forName(\"Lucene46\");\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = new Lucene46SegmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newFSDirectory(createTempDir());\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = Codec.forName(\"Lucene46\");\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = new Lucene46SegmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0567bdc5c86c94ced64201187cfcef2417d76dda","date":1400678298,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = Codec.forName(\"Lucene46\");\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = new Lucene46SegmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = Codec.forName(\"Lucene46\");\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = new Lucene46SegmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":["925e350e1b939d6381c12c2d73ce20aa438d482c"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a656b32c3aa151037a8c52e9b134acc3cbf482bc","date":1400688195,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = Codec.forName(\"Lucene46\");\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = new Lucene46SegmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = Codec.forName(\"Lucene46\");\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = new Lucene46SegmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7605579001505896d48b07160075a5c8b8e128e","date":1400758727,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = Codec.forName(\"Lucene46\");\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = new Lucene46SegmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = Codec.forName(\"Lucene46\");\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = new Lucene46SegmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8106bc60c7452250f84c65cdb43ab6b1d8eb1534","date":1401906364,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = Codec.forName(\"Lucene49\");\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = new Lucene46SegmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = Codec.forName(\"Lucene46\");\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = new Lucene46SegmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":["925e350e1b939d6381c12c2d73ce20aa438d482c"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = Codec.forName(\"Lucene49\");\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = new Lucene46SegmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = Codec.forName(\"Lucene49\");\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = new Lucene46SegmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ce5e7f280a7b3f0f96f2623d9f0ce70f742b793","date":1408030244,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = Codec.forName(\"Lucene410\");\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = new Lucene46SegmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = Codec.forName(\"Lucene49\");\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = new Lucene46SegmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"941b7027a51547b0a38d711bc08ec354f9e2e4a7","date":1411394279,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = new Lucene46SegmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = Codec.forName(\"Lucene410\");\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = new Lucene46SegmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e49088db00ea6cb232fbde9c8c646c721d4d049f","date":1411433559,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = new Lucene46SegmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e2fe60a17a7a0cfd101b1169acf089221bc6c166","date":1412767493,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55980207f1977bd1463465de1659b821347e2fa8","date":1413336386,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().getSegmentInfoReader().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3384e6013a93e4d11b7d75388693f8d0388602bf","date":1413951663,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            byte id[] = readSegmentInfoID(dir, fileName);\n            SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"db68c63cbfaa8698b9c4475f75ed2b9c9696d238","date":1414118621,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            byte id[] = readSegmentInfoID(dir, fileName);\n            SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c958ba8636e86fd54ee6ae00a51d8a21ebb78a14","date":1417031875,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    long counter = 0;\n    long checkAt = 100;\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n      if (counter++ == checkAt) {\n        for(String fileName : dir.listAll()) {\n          if (fileName.endsWith(\".si\")) {\n            String segName = IndexFileNames.parseSegmentName(fileName);\n            if (segSeen.contains(segName) == false) {\n              segSeen.add(segName);\n              byte id[] = readSegmentInfoID(dir, fileName);\n              SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n              si.setCodec(codec);\n              SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n              SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n              try {\n                thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n                thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n              } finally {\n                sr.close();\n              }\n            }\n          }\n        }\n\n        checkAt = (long) (checkAt * 1.25);\n        counter = 0;\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n\n      for(String fileName : dir.listAll()) {\n        if (fileName.endsWith(\".si\")) {\n          String segName = IndexFileNames.parseSegmentName(fileName);\n          if (segSeen.contains(segName) == false) {\n            segSeen.add(segName);\n            byte id[] = readSegmentInfoID(dir, fileName);\n            SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n            si.setCodec(codec);\n            SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n            SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n            try {\n              thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n              thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n            } finally {\n              sr.close();\n            }\n          }\n        }\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31741cf1390044e38a2ec3127cf302ba841bfd75","date":1491292636,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    long counter = 0;\n    long checkAt = 100;\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n      if (counter++ == checkAt) {\n        for(String fileName : dir.listAll()) {\n          if (fileName.endsWith(\".si\")) {\n            String segName = IndexFileNames.parseSegmentName(fileName);\n            if (segSeen.contains(segName) == false) {\n              segSeen.add(segName);\n              byte id[] = readSegmentInfoID(dir, fileName);\n              SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n              si.setCodec(codec);\n              SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n              SegmentReader sr = new SegmentReader(sci, Version.LATEST.major, IOContext.DEFAULT);\n              try {\n                thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n                thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n              } finally {\n                sr.close();\n              }\n            }\n          }\n        }\n\n        checkAt = (long) (checkAt * 1.25);\n        counter = 0;\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    long counter = 0;\n    long checkAt = 100;\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n      if (counter++ == checkAt) {\n        for(String fileName : dir.listAll()) {\n          if (fileName.endsWith(\".si\")) {\n            String segName = IndexFileNames.parseSegmentName(fileName);\n            if (segSeen.contains(segName) == false) {\n              segSeen.add(segName);\n              byte id[] = readSegmentInfoID(dir, fileName);\n              SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n              si.setCodec(codec);\n              SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n              SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n              try {\n                thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n                thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n              } finally {\n                sr.close();\n              }\n            }\n          }\n        }\n\n        checkAt = (long) (checkAt * 1.25);\n        counter = 0;\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"92212fd254551a0b1156aafc3a1a6ed1a43932ad","date":1491296431,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    long counter = 0;\n    long checkAt = 100;\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n      if (counter++ == checkAt) {\n        for(String fileName : dir.listAll()) {\n          if (fileName.endsWith(\".si\")) {\n            String segName = IndexFileNames.parseSegmentName(fileName);\n            if (segSeen.contains(segName) == false) {\n              segSeen.add(segName);\n              byte id[] = readSegmentInfoID(dir, fileName);\n              SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n              si.setCodec(codec);\n              SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n              SegmentReader sr = new SegmentReader(sci, Version.LATEST.major, IOContext.DEFAULT);\n              try {\n                thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n                thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n              } finally {\n                sr.close();\n              }\n            }\n          }\n        }\n\n        checkAt = (long) (checkAt * 1.25);\n        counter = 0;\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    long counter = 0;\n    long checkAt = 100;\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n      if (counter++ == checkAt) {\n        for(String fileName : dir.listAll()) {\n          if (fileName.endsWith(\".si\")) {\n            String segName = IndexFileNames.parseSegmentName(fileName);\n            if (segSeen.contains(segName) == false) {\n              segSeen.add(segName);\n              byte id[] = readSegmentInfoID(dir, fileName);\n              SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n              si.setCodec(codec);\n              SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n              SegmentReader sr = new SegmentReader(sci, IOContext.DEFAULT);\n              try {\n                thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n                thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n              } finally {\n                sr.close();\n              }\n            }\n          }\n        }\n\n        checkAt = (long) (checkAt * 1.25);\n        counter = 0;\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8f2203cb8ae87188877cfbf6ad170c5738a0aad5","date":1528117512,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    long counter = 0;\n    long checkAt = 100;\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n      if (counter++ == checkAt) {\n        for(String fileName : dir.listAll()) {\n          if (fileName.endsWith(\".si\")) {\n            String segName = IndexFileNames.parseSegmentName(fileName);\n            if (segSeen.contains(segName) == false) {\n              segSeen.add(segName);\n              byte id[] = readSegmentInfoID(dir, fileName);\n              SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n              si.setCodec(codec);\n              SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, 0, -1, -1, -1);\n              SegmentReader sr = new SegmentReader(sci, Version.LATEST.major, IOContext.DEFAULT);\n              try {\n                thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n                thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n              } finally {\n                sr.close();\n              }\n            }\n          }\n        }\n\n        checkAt = (long) (checkAt * 1.25);\n        counter = 0;\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    long counter = 0;\n    long checkAt = 100;\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n      if (counter++ == checkAt) {\n        for(String fileName : dir.listAll()) {\n          if (fileName.endsWith(\".si\")) {\n            String segName = IndexFileNames.parseSegmentName(fileName);\n            if (segSeen.contains(segName) == false) {\n              segSeen.add(segName);\n              byte id[] = readSegmentInfoID(dir, fileName);\n              SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n              si.setCodec(codec);\n              SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n              SegmentReader sr = new SegmentReader(sci, Version.LATEST.major, IOContext.DEFAULT);\n              try {\n                thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n                thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n              } finally {\n                sr.close();\n              }\n            }\n          }\n        }\n\n        checkAt = (long) (checkAt * 1.25);\n        counter = 0;\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f592209545c71895260367152601e9200399776d","date":1528238935,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    long counter = 0;\n    long checkAt = 100;\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n      if (counter++ == checkAt) {\n        for(String fileName : dir.listAll()) {\n          if (fileName.endsWith(\".si\")) {\n            String segName = IndexFileNames.parseSegmentName(fileName);\n            if (segSeen.contains(segName) == false) {\n              segSeen.add(segName);\n              byte id[] = readSegmentInfoID(dir, fileName);\n              SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n              si.setCodec(codec);\n              SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, 0, -1, -1, -1);\n              SegmentReader sr = new SegmentReader(sci, Version.LATEST.major, IOContext.DEFAULT);\n              try {\n                thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n                thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n              } finally {\n                sr.close();\n              }\n            }\n          }\n        }\n\n        checkAt = (long) (checkAt * 1.25);\n        counter = 0;\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    long counter = 0;\n    long checkAt = 100;\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n      if (counter++ == checkAt) {\n        for(String fileName : dir.listAll()) {\n          if (fileName.endsWith(\".si\")) {\n            String segName = IndexFileNames.parseSegmentName(fileName);\n            if (segSeen.contains(segName) == false) {\n              segSeen.add(segName);\n              byte id[] = readSegmentInfoID(dir, fileName);\n              SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n              si.setCodec(codec);\n              SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n              SegmentReader sr = new SegmentReader(sci, Version.LATEST.major, IOContext.DEFAULT);\n              try {\n                thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n                thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n              } finally {\n                sr.close();\n              }\n            }\n          }\n        }\n\n        checkAt = (long) (checkAt * 1.25);\n        counter = 0;\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b70042a8a492f7054d480ccdd2be9796510d4327","date":1528386658,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    long counter = 0;\n    long checkAt = 100;\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n      if (counter++ == checkAt) {\n        for(String fileName : dir.listAll()) {\n          if (fileName.endsWith(\".si\")) {\n            String segName = IndexFileNames.parseSegmentName(fileName);\n            if (segSeen.contains(segName) == false) {\n              segSeen.add(segName);\n              byte id[] = readSegmentInfoID(dir, fileName);\n              SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n              si.setCodec(codec);\n              SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, 0, -1, -1, -1);\n              SegmentReader sr = new SegmentReader(sci, Version.LATEST.major, IOContext.DEFAULT);\n              try {\n                thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n                thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n              } finally {\n                sr.close();\n              }\n            }\n          }\n        }\n\n        checkAt = (long) (checkAt * 1.25);\n        counter = 0;\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    long counter = 0;\n    long checkAt = 100;\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n      if (counter++ == checkAt) {\n        for(String fileName : dir.listAll()) {\n          if (fileName.endsWith(\".si\")) {\n            String segName = IndexFileNames.parseSegmentName(fileName);\n            if (segSeen.contains(segName) == false) {\n              segSeen.add(segName);\n              byte id[] = readSegmentInfoID(dir, fileName);\n              SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n              si.setCodec(codec);\n              SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, -1, -1, -1);\n              SegmentReader sr = new SegmentReader(sci, Version.LATEST.major, IOContext.DEFAULT);\n              try {\n                thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n                thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n              } finally {\n                sr.close();\n              }\n            }\n          }\n        }\n\n        checkAt = (long) (checkAt * 1.25);\n        counter = 0;\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"790693f23f4e88a59fbb25e47cc25f6d493b03cb","date":1553077690,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    long counter = 0;\n    long checkAt = 100;\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n      if (counter++ == checkAt) {\n        for(String fileName : dir.listAll()) {\n          if (fileName.endsWith(\".si\")) {\n            String segName = IndexFileNames.parseSegmentName(fileName);\n            if (segSeen.contains(segName) == false) {\n              segSeen.add(segName);\n              byte id[] = readSegmentInfoID(dir, fileName);\n              SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n              si.setCodec(codec);\n              SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, 0, -1, -1, -1);\n              SegmentReader sr = new SegmentReader(sci, Version.LATEST.major, false, IOContext.DEFAULT);\n              try {\n                thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n                thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n              } finally {\n                sr.close();\n              }\n            }\n          }\n        }\n\n        checkAt = (long) (checkAt * 1.25);\n        counter = 0;\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    long counter = 0;\n    long checkAt = 100;\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n      if (counter++ == checkAt) {\n        for(String fileName : dir.listAll()) {\n          if (fileName.endsWith(\".si\")) {\n            String segName = IndexFileNames.parseSegmentName(fileName);\n            if (segSeen.contains(segName) == false) {\n              segSeen.add(segName);\n              byte id[] = readSegmentInfoID(dir, fileName);\n              SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n              si.setCodec(codec);\n              SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, 0, -1, -1, -1);\n              SegmentReader sr = new SegmentReader(sci, Version.LATEST.major, IOContext.DEFAULT);\n              try {\n                thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n                thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n              } finally {\n                sr.close();\n              }\n            }\n          }\n        }\n\n        checkAt = (long) (checkAt * 1.25);\n        counter = 0;\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"763da4a9605e47013078edc323b9d4b608f0f9e0","date":1555353576,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    long counter = 0;\n    long checkAt = 100;\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n      if (counter++ == checkAt) {\n        for(String fileName : dir.listAll()) {\n          if (fileName.endsWith(\".si\")) {\n            String segName = IndexFileNames.parseSegmentName(fileName);\n            if (segSeen.contains(segName) == false) {\n              segSeen.add(segName);\n              byte id[] = readSegmentInfoID(dir, fileName);\n              SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n              si.setCodec(codec);\n              SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, 0, -1, -1, -1);\n              SegmentReader sr = new SegmentReader(sci, Version.LATEST.major, false, IOContext.DEFAULT, Collections.emptyMap());\n              try {\n                thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n                thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n              } finally {\n                sr.close();\n              }\n            }\n          }\n        }\n\n        checkAt = (long) (checkAt * 1.25);\n        counter = 0;\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    long counter = 0;\n    long checkAt = 100;\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n      if (counter++ == checkAt) {\n        for(String fileName : dir.listAll()) {\n          if (fileName.endsWith(\".si\")) {\n            String segName = IndexFileNames.parseSegmentName(fileName);\n            if (segSeen.contains(segName) == false) {\n              segSeen.add(segName);\n              byte id[] = readSegmentInfoID(dir, fileName);\n              SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n              si.setCodec(codec);\n              SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, 0, -1, -1, -1);\n              SegmentReader sr = new SegmentReader(sci, Version.LATEST.major, false, IOContext.DEFAULT);\n              try {\n                thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n                thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n              } finally {\n                sr.close();\n              }\n            }\n          }\n        }\n\n        checkAt = (long) (checkAt * 1.25);\n        counter = 0;\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a4e83191a3e02851a0b67e5335e6922f3e9ea86d","date":1583489709,"type":3,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    long counter = 0;\n    long checkAt = 100;\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n      if (counter++ == checkAt) {\n        for(String fileName : dir.listAll()) {\n          if (fileName.endsWith(\".si\")) {\n            String segName = IndexFileNames.parseSegmentName(fileName);\n            if (segSeen.contains(segName) == false) {\n              segSeen.add(segName);\n              byte id[] = readSegmentInfoID(dir, fileName);\n              SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n              si.setCodec(codec);\n              SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, 0, -1, -1, -1);\n              SegmentReader sr = new SegmentReader(sci, Version.LATEST.major, false, IOContext.DEFAULT);\n              try {\n                thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n                thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n              } finally {\n                sr.close();\n              }\n            }\n          }\n        }\n\n        checkAt = (long) (checkAt * 1.25);\n        counter = 0;\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    long counter = 0;\n    long checkAt = 100;\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n      if (counter++ == checkAt) {\n        for(String fileName : dir.listAll()) {\n          if (fileName.endsWith(\".si\")) {\n            String segName = IndexFileNames.parseSegmentName(fileName);\n            if (segSeen.contains(segName) == false) {\n              segSeen.add(segName);\n              byte id[] = readSegmentInfoID(dir, fileName);\n              SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n              si.setCodec(codec);\n              SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, 0, -1, -1, -1);\n              SegmentReader sr = new SegmentReader(sci, Version.LATEST.major, false, IOContext.DEFAULT, Collections.emptyMap());\n              try {\n                thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n                thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n              } finally {\n                sr.close();\n              }\n            }\n          }\n        }\n\n        checkAt = (long) (checkAt * 1.25);\n        counter = 0;\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bec68e7c41fed133827595747d853cad504e481e","date":1583501052,"type":3,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    long counter = 0;\n    long checkAt = 100;\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n      if (counter++ == checkAt) {\n        for(String fileName : dir.listAll()) {\n          if (fileName.endsWith(\".si\")) {\n            String segName = IndexFileNames.parseSegmentName(fileName);\n            if (segSeen.contains(segName) == false) {\n              segSeen.add(segName);\n              byte id[] = readSegmentInfoID(dir, fileName);\n              SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n              si.setCodec(codec);\n              SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, 0, -1, -1, -1);\n              SegmentReader sr = new SegmentReader(sci, Version.LATEST.major, IOContext.DEFAULT);\n              try {\n                thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n                thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n              } finally {\n                sr.close();\n              }\n            }\n          }\n        }\n\n        checkAt = (long) (checkAt * 1.25);\n        counter = 0;\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    long counter = 0;\n    long checkAt = 100;\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n      if (counter++ == checkAt) {\n        for(String fileName : dir.listAll()) {\n          if (fileName.endsWith(\".si\")) {\n            String segName = IndexFileNames.parseSegmentName(fileName);\n            if (segSeen.contains(segName) == false) {\n              segSeen.add(segName);\n              byte id[] = readSegmentInfoID(dir, fileName);\n              SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n              si.setCodec(codec);\n              SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, 0, -1, -1, -1);\n              SegmentReader sr = new SegmentReader(sci, Version.LATEST.major, false, IOContext.DEFAULT);\n              try {\n                thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n                thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n              } finally {\n                sr.close();\n              }\n            }\n          }\n        }\n\n        checkAt = (long) (checkAt * 1.25);\n        counter = 0;\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"14654be3f7a82c9a3c52169e365baa55bfe64f66","date":1587212697,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterThreadsToSegments#testDocsStuckInRAMForever().mjava","sourceNew":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    long counter = 0;\n    long checkAt = 100;\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n      if (counter++ == checkAt) {\n        for(String fileName : dir.listAll()) {\n          if (fileName.endsWith(\".si\")) {\n            String segName = IndexFileNames.parseSegmentName(fileName);\n            if (segSeen.contains(segName) == false) {\n              segSeen.add(segName);\n              byte id[] = readSegmentInfoID(dir, fileName);\n              SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n              si.setCodec(codec);\n              SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, 0, -1, -1, -1, StringHelper.randomId());\n              SegmentReader sr = new SegmentReader(sci, Version.LATEST.major, IOContext.DEFAULT);\n              try {\n                thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n                thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n              } finally {\n                sr.close();\n              }\n            }\n          }\n        }\n\n        checkAt = (long) (checkAt * 1.25);\n        counter = 0;\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocsStuckInRAMForever() throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setRAMBufferSizeMB(.2);\n    Codec codec = TestUtil.getDefaultCodec();\n    iwc.setCodec(codec);\n    iwc.setMergePolicy(NoMergePolicy.INSTANCE);\n    final IndexWriter w = new IndexWriter(dir, iwc);\n    final CountDownLatch startingGun = new CountDownLatch(1);\n    Thread[] threads = new Thread[2];\n    for(int i=0;i<threads.length;i++) {\n      final int threadID = i;\n      threads[i] = new Thread() {\n          @Override\n          public void run() {\n            try {\n              startingGun.await();\n              for(int j=0;j<1000;j++) {\n                Document doc = new Document();\n                doc.add(newStringField(\"field\", \"threadID\" + threadID, Field.Store.NO));\n                w.addDocument(doc);\n              }\n            } catch (Exception e) {\n              throw new RuntimeException(e);\n            }\n          }\n        };\n      threads[i].start();\n    }\n\n    startingGun.countDown();\n    for(Thread t : threads) {\n      t.join();\n    }\n\n    Set<String> segSeen = new HashSet<>();\n    int thread0Count = 0;\n    int thread1Count = 0;\n\n    // At this point the writer should have 2 thread states w/ docs; now we index with only 1 thread until we see all 1000 thread0 & thread1\n    // docs flushed.  If the writer incorrectly holds onto previously indexed docs forever then this will run forever:\n    long counter = 0;\n    long checkAt = 100;\n    while (thread0Count < 1000 || thread1Count < 1000) {\n      Document doc = new Document();\n      doc.add(newStringField(\"field\", \"threadIDmain\", Field.Store.NO));\n      w.addDocument(doc);\n      if (counter++ == checkAt) {\n        for(String fileName : dir.listAll()) {\n          if (fileName.endsWith(\".si\")) {\n            String segName = IndexFileNames.parseSegmentName(fileName);\n            if (segSeen.contains(segName) == false) {\n              segSeen.add(segName);\n              byte id[] = readSegmentInfoID(dir, fileName);\n              SegmentInfo si = TestUtil.getDefaultCodec().segmentInfoFormat().read(dir, segName, id, IOContext.DEFAULT);\n              si.setCodec(codec);\n              SegmentCommitInfo sci = new SegmentCommitInfo(si, 0, 0, -1, -1, -1);\n              SegmentReader sr = new SegmentReader(sci, Version.LATEST.major, IOContext.DEFAULT);\n              try {\n                thread0Count += sr.docFreq(new Term(\"field\", \"threadID0\"));\n                thread1Count += sr.docFreq(new Term(\"field\", \"threadID1\"));\n              } finally {\n                sr.close();\n              }\n            }\n          }\n        }\n\n        checkAt = (long) (checkAt * 1.25);\n        counter = 0;\n      }\n    }\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e49088db00ea6cb232fbde9c8c646c721d4d049f":["941b7027a51547b0a38d711bc08ec354f9e2e4a7"],"8b57d7226a882c9e7d8fdaee52a51c4c04813dd6":["29937417e80326ef8c86754923bab3c82314d592"],"763da4a9605e47013078edc323b9d4b608f0f9e0":["790693f23f4e88a59fbb25e47cc25f6d493b03cb"],"b7605579001505896d48b07160075a5c8b8e128e":["8b57d7226a882c9e7d8fdaee52a51c4c04813dd6","0567bdc5c86c94ced64201187cfcef2417d76dda"],"29937417e80326ef8c86754923bab3c82314d592":["925e350e1b939d6381c12c2d73ce20aa438d482c"],"a4e83191a3e02851a0b67e5335e6922f3e9ea86d":["763da4a9605e47013078edc323b9d4b608f0f9e0"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["c958ba8636e86fd54ee6ae00a51d8a21ebb78a14"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b70042a8a492f7054d480ccdd2be9796510d4327":["31741cf1390044e38a2ec3127cf302ba841bfd75","8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"925e350e1b939d6381c12c2d73ce20aa438d482c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":["c958ba8636e86fd54ee6ae00a51d8a21ebb78a14"],"0ce5e7f280a7b3f0f96f2623d9f0ce70f742b793":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"941b7027a51547b0a38d711bc08ec354f9e2e4a7":["0ce5e7f280a7b3f0f96f2623d9f0ce70f742b793"],"790693f23f4e88a59fbb25e47cc25f6d493b03cb":["8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"e2fe60a17a7a0cfd101b1169acf089221bc6c166":["e49088db00ea6cb232fbde9c8c646c721d4d049f"],"55980207f1977bd1463465de1659b821347e2fa8":["e49088db00ea6cb232fbde9c8c646c721d4d049f","e2fe60a17a7a0cfd101b1169acf089221bc6c166"],"c958ba8636e86fd54ee6ae00a51d8a21ebb78a14":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"0567bdc5c86c94ced64201187cfcef2417d76dda":["8b57d7226a882c9e7d8fdaee52a51c4c04813dd6"],"bec68e7c41fed133827595747d853cad504e481e":["a4e83191a3e02851a0b67e5335e6922f3e9ea86d"],"3384e6013a93e4d11b7d75388693f8d0388602bf":["55980207f1977bd1463465de1659b821347e2fa8"],"14654be3f7a82c9a3c52169e365baa55bfe64f66":["bec68e7c41fed133827595747d853cad504e481e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["8106bc60c7452250f84c65cdb43ab6b1d8eb1534"],"a656b32c3aa151037a8c52e9b134acc3cbf482bc":["8b57d7226a882c9e7d8fdaee52a51c4c04813dd6","0567bdc5c86c94ced64201187cfcef2417d76dda"],"8106bc60c7452250f84c65cdb43ab6b1d8eb1534":["0567bdc5c86c94ced64201187cfcef2417d76dda"],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["e2fe60a17a7a0cfd101b1169acf089221bc6c166","3384e6013a93e4d11b7d75388693f8d0388602bf"],"8f2203cb8ae87188877cfbf6ad170c5738a0aad5":["31741cf1390044e38a2ec3127cf302ba841bfd75"],"f592209545c71895260367152601e9200399776d":["31741cf1390044e38a2ec3127cf302ba841bfd75","8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["14654be3f7a82c9a3c52169e365baa55bfe64f66"]},"commit2Childs":{"e49088db00ea6cb232fbde9c8c646c721d4d049f":["e2fe60a17a7a0cfd101b1169acf089221bc6c166","55980207f1977bd1463465de1659b821347e2fa8"],"8b57d7226a882c9e7d8fdaee52a51c4c04813dd6":["b7605579001505896d48b07160075a5c8b8e128e","0567bdc5c86c94ced64201187cfcef2417d76dda","a656b32c3aa151037a8c52e9b134acc3cbf482bc"],"763da4a9605e47013078edc323b9d4b608f0f9e0":["a4e83191a3e02851a0b67e5335e6922f3e9ea86d"],"b7605579001505896d48b07160075a5c8b8e128e":[],"29937417e80326ef8c86754923bab3c82314d592":["8b57d7226a882c9e7d8fdaee52a51c4c04813dd6"],"a4e83191a3e02851a0b67e5335e6922f3e9ea86d":["bec68e7c41fed133827595747d853cad504e481e"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["b70042a8a492f7054d480ccdd2be9796510d4327","8f2203cb8ae87188877cfbf6ad170c5738a0aad5","f592209545c71895260367152601e9200399776d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["925e350e1b939d6381c12c2d73ce20aa438d482c"],"b70042a8a492f7054d480ccdd2be9796510d4327":[],"925e350e1b939d6381c12c2d73ce20aa438d482c":["29937417e80326ef8c86754923bab3c82314d592"],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":[],"0ce5e7f280a7b3f0f96f2623d9f0ce70f742b793":["941b7027a51547b0a38d711bc08ec354f9e2e4a7"],"941b7027a51547b0a38d711bc08ec354f9e2e4a7":["e49088db00ea6cb232fbde9c8c646c721d4d049f"],"790693f23f4e88a59fbb25e47cc25f6d493b03cb":["763da4a9605e47013078edc323b9d4b608f0f9e0"],"e2fe60a17a7a0cfd101b1169acf089221bc6c166":["55980207f1977bd1463465de1659b821347e2fa8","db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"55980207f1977bd1463465de1659b821347e2fa8":["3384e6013a93e4d11b7d75388693f8d0388602bf"],"c958ba8636e86fd54ee6ae00a51d8a21ebb78a14":["31741cf1390044e38a2ec3127cf302ba841bfd75","92212fd254551a0b1156aafc3a1a6ed1a43932ad"],"0567bdc5c86c94ced64201187cfcef2417d76dda":["b7605579001505896d48b07160075a5c8b8e128e","a656b32c3aa151037a8c52e9b134acc3cbf482bc","8106bc60c7452250f84c65cdb43ab6b1d8eb1534"],"bec68e7c41fed133827595747d853cad504e481e":["14654be3f7a82c9a3c52169e365baa55bfe64f66"],"3384e6013a93e4d11b7d75388693f8d0388602bf":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"14654be3f7a82c9a3c52169e365baa55bfe64f66":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["0ce5e7f280a7b3f0f96f2623d9f0ce70f742b793"],"a656b32c3aa151037a8c52e9b134acc3cbf482bc":[],"8106bc60c7452250f84c65cdb43ab6b1d8eb1534":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["c958ba8636e86fd54ee6ae00a51d8a21ebb78a14"],"8f2203cb8ae87188877cfbf6ad170c5738a0aad5":["b70042a8a492f7054d480ccdd2be9796510d4327","790693f23f4e88a59fbb25e47cc25f6d493b03cb","f592209545c71895260367152601e9200399776d"],"f592209545c71895260367152601e9200399776d":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7605579001505896d48b07160075a5c8b8e128e","b70042a8a492f7054d480ccdd2be9796510d4327","92212fd254551a0b1156aafc3a1a6ed1a43932ad","a656b32c3aa151037a8c52e9b134acc3cbf482bc","f592209545c71895260367152601e9200399776d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}