{"path":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(CoreContainer,String,String,ZkCoreNodeProps,CoreDescriptor,boolean).mjava","commits":[{"id":"3e0300d1df37d7e9662d491269e91b6f66dca8bd","date":1443011762,"type":1,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(CoreContainer,String,String,ZkCoreNodeProps,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(CoreContainer,String,String,ZkCoreNodeProps,String,boolean).mjava","sourceNew":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * <p>\n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final CoreContainer container,\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      CoreDescriptor leaderCd, boolean forcePublishState)\n      throws KeeperException, InterruptedException {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica) replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \" + replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {\n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // we only really need to try to start the LIR process if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n\n        LeaderInitiatedRecoveryThread lirThread =\n            new LeaderInitiatedRecoveryThread(this,\n                container,\n                collection,\n                shardId,\n                replicaCoreProps,\n                120,\n                leaderCd);\n        ExecutorService executor = container.getUpdateShardHandler().getUpdateExecutor();\n        try {\n          MDC.put(\"DistributedUpdateProcessor.replicaUrlToRecover\", replicaCoreProps.getCoreUrl());\n          executor.execute(lirThread);\n        } finally {\n          MDC.remove(\"DistributedUpdateProcessor.replicaUrlToRecover\");\n        }\n\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }\n    }\n\n    return nodeIsLive;\n  }\n\n","sourceOld":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * <p>\n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final CoreContainer container,\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      String leaderCoreNodeName, boolean forcePublishState)\n      throws KeeperException, InterruptedException {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica) replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \" + replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {\n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // we only really need to try to start the LIR process if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n\n        LeaderInitiatedRecoveryThread lirThread =\n            new LeaderInitiatedRecoveryThread(this,\n                container,\n                collection,\n                shardId,\n                replicaCoreProps,\n                120,\n                leaderCoreNodeName); // core node name of current leader\n        ExecutorService executor = container.getUpdateShardHandler().getUpdateExecutor();\n        try {\n          MDC.put(\"DistributedUpdateProcessor.replicaUrlToRecover\", replicaCoreProps.getCoreUrl());\n          executor.execute(lirThread);\n        } finally {\n          MDC.remove(\"DistributedUpdateProcessor.replicaUrlToRecover\");\n        }\n\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }\n    }\n\n    return nodeIsLive;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"84f20f331d8001864545c7021812d8c6509c7593","date":1517216128,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(CoreContainer,String,String,ZkCoreNodeProps,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(CoreContainer,String,String,ZkCoreNodeProps,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * <p>\n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  @Deprecated\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final CoreContainer container,\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      CoreDescriptor leaderCd, boolean forcePublishState)\n      throws KeeperException, InterruptedException {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica) replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \" + replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {\n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // we only really need to try to start the LIR process if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n\n        LeaderInitiatedRecoveryThread lirThread =\n            new LeaderInitiatedRecoveryThread(this,\n                container,\n                collection,\n                shardId,\n                replicaCoreProps,\n                120,\n                leaderCd);\n        ExecutorService executor = container.getUpdateShardHandler().getUpdateExecutor();\n        try {\n          MDC.put(\"DistributedUpdateProcessor.replicaUrlToRecover\", replicaCoreProps.getCoreUrl());\n          executor.execute(lirThread);\n        } finally {\n          MDC.remove(\"DistributedUpdateProcessor.replicaUrlToRecover\");\n        }\n\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)\n      }\n    }\n\n    return nodeIsLive;\n  }\n\n","sourceOld":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * <p>\n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final CoreContainer container,\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      CoreDescriptor leaderCd, boolean forcePublishState)\n      throws KeeperException, InterruptedException {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica) replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \" + replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {\n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // we only really need to try to start the LIR process if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n\n        LeaderInitiatedRecoveryThread lirThread =\n            new LeaderInitiatedRecoveryThread(this,\n                container,\n                collection,\n                shardId,\n                replicaCoreProps,\n                120,\n                leaderCd);\n        ExecutorService executor = container.getUpdateShardHandler().getUpdateExecutor();\n        try {\n          MDC.put(\"DistributedUpdateProcessor.replicaUrlToRecover\", replicaCoreProps.getCoreUrl());\n          executor.execute(lirThread);\n        } finally {\n          MDC.remove(\"DistributedUpdateProcessor.replicaUrlToRecover\");\n        }\n\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }\n    }\n\n    return nodeIsLive;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5e7e88705ca044d3418ed47acf916da63554a2e9","date":1522140943,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(CoreContainer,String,String,ZkCoreNodeProps,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(CoreContainer,String,String,ZkCoreNodeProps,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * <p>\n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  @Deprecated\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final CoreContainer container,\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      CoreDescriptor leaderCd, boolean forcePublishState)\n      throws KeeperException, InterruptedException {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica) replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \" + replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {\n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // we only really need to try to start the LIR process if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)\n          && CloudUtil.replicaExists(getZkStateReader().getClusterState(), collection, shardId, replicaCoreNodeName)) {\n\n        LeaderInitiatedRecoveryThread lirThread =\n            new LeaderInitiatedRecoveryThread(this,\n                container,\n                collection,\n                shardId,\n                replicaCoreProps,\n                120,\n                leaderCd);\n        ExecutorService executor = container.getUpdateShardHandler().getUpdateExecutor();\n        try {\n          MDC.put(\"DistributedUpdateProcessor.replicaUrlToRecover\", replicaCoreProps.getCoreUrl());\n          executor.execute(lirThread);\n        } finally {\n          MDC.remove(\"DistributedUpdateProcessor.replicaUrlToRecover\");\n        }\n\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node {} is not live or replica {} is deleted, so skipping leader-initiated recovery for replica: core={}\",\n            replicaNodeName, replicaCoreNodeName, replicaCoreProps.getCoreName());\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)\n      }\n    }\n\n    return nodeIsLive;\n  }\n\n","sourceOld":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * <p>\n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  @Deprecated\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final CoreContainer container,\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      CoreDescriptor leaderCd, boolean forcePublishState)\n      throws KeeperException, InterruptedException {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica) replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \" + replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {\n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // we only really need to try to start the LIR process if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n\n        LeaderInitiatedRecoveryThread lirThread =\n            new LeaderInitiatedRecoveryThread(this,\n                container,\n                collection,\n                shardId,\n                replicaCoreProps,\n                120,\n                leaderCd);\n        ExecutorService executor = container.getUpdateShardHandler().getUpdateExecutor();\n        try {\n          MDC.put(\"DistributedUpdateProcessor.replicaUrlToRecover\", replicaCoreProps.getCoreUrl());\n          executor.execute(lirThread);\n        } finally {\n          MDC.remove(\"DistributedUpdateProcessor.replicaUrlToRecover\");\n        }\n\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)\n      }\n    }\n\n    return nodeIsLive;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7","date":1522191940,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(CoreContainer,String,String,ZkCoreNodeProps,CoreDescriptor,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(CoreContainer,String,String,ZkCoreNodeProps,CoreDescriptor,boolean).mjava","sourceNew":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * <p>\n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  @Deprecated\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final CoreContainer container,\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      CoreDescriptor leaderCd, boolean forcePublishState)\n      throws KeeperException, InterruptedException {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica) replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \" + replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {\n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // we only really need to try to start the LIR process if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)\n          && CloudUtil.replicaExists(getZkStateReader().getClusterState(), collection, shardId, replicaCoreNodeName)) {\n\n        LeaderInitiatedRecoveryThread lirThread =\n            new LeaderInitiatedRecoveryThread(this,\n                container,\n                collection,\n                shardId,\n                replicaCoreProps,\n                120,\n                leaderCd);\n        ExecutorService executor = container.getUpdateShardHandler().getUpdateExecutor();\n        try {\n          MDC.put(\"DistributedUpdateProcessor.replicaUrlToRecover\", replicaCoreProps.getCoreUrl());\n          executor.execute(lirThread);\n        } finally {\n          MDC.remove(\"DistributedUpdateProcessor.replicaUrlToRecover\");\n        }\n\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node {} is not live or replica {} is deleted, so skipping leader-initiated recovery for replica: core={}\",\n            replicaNodeName, replicaCoreNodeName, replicaCoreProps.getCoreName());\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)\n      }\n    }\n\n    return nodeIsLive;\n  }\n\n","sourceOld":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * <p>\n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  @Deprecated\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final CoreContainer container,\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      CoreDescriptor leaderCd, boolean forcePublishState)\n      throws KeeperException, InterruptedException {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica) replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \" + replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {\n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // we only really need to try to start the LIR process if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n\n        LeaderInitiatedRecoveryThread lirThread =\n            new LeaderInitiatedRecoveryThread(this,\n                container,\n                collection,\n                shardId,\n                replicaCoreProps,\n                120,\n                leaderCd);\n        ExecutorService executor = container.getUpdateShardHandler().getUpdateExecutor();\n        try {\n          MDC.put(\"DistributedUpdateProcessor.replicaUrlToRecover\", replicaCoreProps.getCoreUrl());\n          executor.execute(lirThread);\n        } finally {\n          MDC.remove(\"DistributedUpdateProcessor.replicaUrlToRecover\");\n        }\n\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)\n      }\n    }\n\n    return nodeIsLive;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180","date":1539076849,"type":4,"author":"Cao Manh Dat","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(CoreContainer,String,String,ZkCoreNodeProps,CoreDescriptor,boolean).mjava","sourceNew":null,"sourceOld":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * <p>\n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  @Deprecated\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final CoreContainer container,\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      CoreDescriptor leaderCd, boolean forcePublishState)\n      throws KeeperException, InterruptedException {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \" + replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica) replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \" + replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {\n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // we only really need to try to start the LIR process if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)\n          && CloudUtil.replicaExists(getZkStateReader().getClusterState(), collection, shardId, replicaCoreNodeName)) {\n\n        LeaderInitiatedRecoveryThread lirThread =\n            new LeaderInitiatedRecoveryThread(this,\n                container,\n                collection,\n                shardId,\n                replicaCoreProps,\n                120,\n                leaderCd);\n        ExecutorService executor = container.getUpdateShardHandler().getUpdateExecutor();\n        try {\n          MDC.put(\"DistributedUpdateProcessor.replicaUrlToRecover\", replicaCoreProps.getCoreUrl());\n          executor.execute(lirThread);\n        } finally {\n          MDC.remove(\"DistributedUpdateProcessor.replicaUrlToRecover\");\n        }\n\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node {} is not live or replica {} is deleted, so skipping leader-initiated recovery for replica: core={}\",\n            replicaNodeName, replicaCoreNodeName, replicaCoreProps.getCoreName());\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)\n      }\n    }\n\n    return nodeIsLive;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"5e7e88705ca044d3418ed47acf916da63554a2e9":["84f20f331d8001864545c7021812d8c6509c7593"],"3e0300d1df37d7e9662d491269e91b6f66dca8bd":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7":["84f20f331d8001864545c7021812d8c6509c7593","5e7e88705ca044d3418ed47acf916da63554a2e9"],"84f20f331d8001864545c7021812d8c6509c7593":["3e0300d1df37d7e9662d491269e91b6f66dca8bd"],"b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180":["d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180"]},"commit2Childs":{"5e7e88705ca044d3418ed47acf916da63554a2e9":["d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7"],"3e0300d1df37d7e9662d491269e91b6f66dca8bd":["84f20f331d8001864545c7021812d8c6509c7593"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3e0300d1df37d7e9662d491269e91b6f66dca8bd"],"d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7":["b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180"],"84f20f331d8001864545c7021812d8c6509c7593":["5e7e88705ca044d3418ed47acf916da63554a2e9","d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7"],"b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}