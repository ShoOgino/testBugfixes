{"path":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition().mjava","commits":[{"id":"4350b17bd363cd13a95171b8df1ca62ea4c3e71c","date":1183562198,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition().mjava","pathOld":"/dev/null","sourceNew":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition() {\n\n        final Payload payload = token.getPayload();\n\n        final String tokenString;\n        final int tokenTextLen;\n        final int tokenTextOffset;\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n\n        int code = 0;\n        int code2 = 0;\n\n        if (tokenText == null) {\n\n          // Fallback to String token\n          tokenString = token.termText();\n          tokenTextLen = tokenString.length();\n          tokenTextOffset = 0;\n\n          // Compute hashcode.\n          int downto = tokenTextLen;\n          while (downto > 0)\n            code = (code*31) + tokenString.charAt(--downto);\n          \n          // System.out.println(\"  addPosition: field=\" + fieldInfo.name + \" string=\" + tokenString + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset+token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        } else {\n          tokenString = null;\n          tokenTextLen = token.termBufferLength();\n          tokenTextOffset = token.termBufferOffset();\n\n          // Compute hashcode\n          int downto = tokenTextLen+tokenTextOffset;\n          while (downto > tokenTextOffset)\n            code = (code*31) + tokenText[--downto];\n\n          // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, tokenTextOffset, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n        }\n\n        int hashPos = code & postingsHashMask;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenString, tokenText, tokenTextLen, tokenTextOffset)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = code*1347|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenString, tokenText, tokenTextLen, tokenTextOffset));\n        }\n        \n        final int proxCode;\n\n        if (p != null) {       // term seen since last flush\n\n          if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n            // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n            assert p.docFreq > 0;\n\n            // Now that we know doc freq for previous doc,\n            // write it & lastDocCode\n            freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n            freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n            if (1 == p.docFreq)\n              writeFreqVInt(p.lastDocCode|1);\n            else {\n              writeFreqVInt(p.lastDocCode);\n              writeFreqVInt(p.docFreq);\n            }\n            p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStartCode = offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n\n            p.docFreq = 1;\n\n            // Store code so we can write this after we're\n            // done with this new doc\n            p.lastDocCode = (docID-p.lastDocID) << 1;\n            p.lastDocID = docID;\n\n          } else {                                // term already seen in this doc\n            // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n            p.docFreq++;\n\n            proxCode = position-p.lastPosition;\n\n            if (doVectors) {\n              vector = p.vector;\n              if (vector == null)\n                vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n                offsetStartCode = offsetStart-vector.lastOffset;\n              }\n            }\n          }\n        } else {\t\t\t\t\t  // term not seen before\n          // System.out.println(\"    never seen docID=\" + docID);\n\n          // Refill?\n          if (0 == postingsFreeCount) {\n            postingsFreeCount = postingsFreeList.length;\n            getPostings(postingsFreeList);\n          }\n\n          // Pull next free Posting from free list\n          p = postingsFreeList[--postingsFreeCount];\n\n          final int textLen1 = 1+tokenTextLen;\n          if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE)\n            charPool.nextBuffer();\n          final char[] text = charPool.buffer;\n          final int textUpto = charPool.byteUpto;\n          p.textStart = textUpto + charPool.byteOffset;\n          charPool.byteUpto += textLen1;\n\n          if (tokenString == null)\n            System.arraycopy(tokenText, tokenTextOffset, text, textUpto, tokenTextLen);\n          else\n            tokenString.getChars(0, tokenTextLen, text, textUpto);\n\n          text[textUpto+tokenTextLen] = 0xffff;\n          \n          assert postingsHash[hashPos] == null;\n\n          postingsHash[hashPos] = p;\n          numPostings++;\n\n          if (numPostings == postingsHashHalfSize)\n            rehashPostings(2*postingsHashSize);\n\n          // Init first slice for freq & prox streams\n          final int firstSize = levelSizeArray[0];\n\n          final int upto1 = postingsPool.newSlice(firstSize);\n          p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n          final int upto2 = postingsPool.newSlice(firstSize);\n          p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n          p.lastDocCode = docID << 1;\n          p.lastDocID = docID;\n          p.docFreq = 1;\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offsetStartCode = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n        }\n\n        proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n        prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n        assert prox != null;\n\n        if (payload != null && payload.length > 0) {\n          writeProxVInt((proxCode<<1)|1);\n          writeProxVInt(payload.length);\n          writeProxBytes(payload.data, payload.offset, payload.length);\n          fieldInfo.storePayloads = true;\n        } else\n          writeProxVInt(proxCode<<1);\n\n        p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n        p.lastPosition = position++;\n\n        if (doVectorPositions) {\n          posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n          pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n          writePosVInt(proxCode);\n          vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        if (doVectorOffsets) {\n          offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n          offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n          writeOffsetVInt(offsetStartCode);\n          writeOffsetVInt(offsetEnd-offsetStart);\n          vector.lastOffset = offsetEnd;\n          vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n        }\n      }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["83bbb041887bbef07b8a98d08a0e1713ce137039","da249b441376287ae32d1604bc7b50b35b351d09"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6864413dbc0c12104c978c05456f3da1d45adb03","date":1186770873,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition().mjava","sourceNew":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition(Token token) {\n\n        final Payload payload = token.getPayload();\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n        final int tokenTextLen = token.termLength();\n\n        int code = 0;\n        int code2 = 0;\n\n        // Compute hashcode\n        int downto = tokenTextLen;\n        while (downto > 0)\n          code = (code*31) + tokenText[--downto];\n\n        // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        int hashPos = code & postingsHashMask;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = code*1347|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenText, tokenTextLen));\n        }\n        \n        final int proxCode;\n\n        if (p != null) {       // term seen since last flush\n\n          if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n            // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n            assert p.docFreq > 0;\n\n            // Now that we know doc freq for previous doc,\n            // write it & lastDocCode\n            freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n            freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n            if (1 == p.docFreq)\n              writeFreqVInt(p.lastDocCode|1);\n            else {\n              writeFreqVInt(p.lastDocCode);\n              writeFreqVInt(p.docFreq);\n            }\n            p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStartCode = offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n\n            p.docFreq = 1;\n\n            // Store code so we can write this after we're\n            // done with this new doc\n            p.lastDocCode = (docID-p.lastDocID) << 1;\n            p.lastDocID = docID;\n\n          } else {                                // term already seen in this doc\n            // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n            p.docFreq++;\n\n            proxCode = position-p.lastPosition;\n\n            if (doVectors) {\n              vector = p.vector;\n              if (vector == null)\n                vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n                offsetStartCode = offsetStart-vector.lastOffset;\n              }\n            }\n          }\n        } else {\t\t\t\t\t  // term not seen before\n          // System.out.println(\"    never seen docID=\" + docID);\n\n          // Refill?\n          if (0 == postingsFreeCount) {\n            postingsFreeCount = postingsFreeList.length;\n            getPostings(postingsFreeList);\n          }\n\n          // Pull next free Posting from free list\n          p = postingsFreeList[--postingsFreeCount];\n\n          final int textLen1 = 1+tokenTextLen;\n          if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE)\n            charPool.nextBuffer();\n          final char[] text = charPool.buffer;\n          final int textUpto = charPool.byteUpto;\n          p.textStart = textUpto + charPool.byteOffset;\n          charPool.byteUpto += textLen1;\n\n          System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n          text[textUpto+tokenTextLen] = 0xffff;\n          \n          assert postingsHash[hashPos] == null;\n\n          postingsHash[hashPos] = p;\n          numPostings++;\n\n          if (numPostings == postingsHashHalfSize)\n            rehashPostings(2*postingsHashSize);\n\n          // Init first slice for freq & prox streams\n          final int firstSize = levelSizeArray[0];\n\n          final int upto1 = postingsPool.newSlice(firstSize);\n          p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n          final int upto2 = postingsPool.newSlice(firstSize);\n          p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n          p.lastDocCode = docID << 1;\n          p.lastDocID = docID;\n          p.docFreq = 1;\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offsetStartCode = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n        }\n\n        proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n        prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n        assert prox != null;\n\n        if (payload != null && payload.length > 0) {\n          writeProxVInt((proxCode<<1)|1);\n          writeProxVInt(payload.length);\n          writeProxBytes(payload.data, payload.offset, payload.length);\n          fieldInfo.storePayloads = true;\n        } else\n          writeProxVInt(proxCode<<1);\n\n        p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n        p.lastPosition = position++;\n\n        if (doVectorPositions) {\n          posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n          pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n          writePosVInt(proxCode);\n          vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        if (doVectorOffsets) {\n          offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n          offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n          writeOffsetVInt(offsetStartCode);\n          writeOffsetVInt(offsetEnd-offsetStart);\n          vector.lastOffset = offsetEnd;\n          vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n        }\n      }\n\n","sourceOld":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition() {\n\n        final Payload payload = token.getPayload();\n\n        final String tokenString;\n        final int tokenTextLen;\n        final int tokenTextOffset;\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n\n        int code = 0;\n        int code2 = 0;\n\n        if (tokenText == null) {\n\n          // Fallback to String token\n          tokenString = token.termText();\n          tokenTextLen = tokenString.length();\n          tokenTextOffset = 0;\n\n          // Compute hashcode.\n          int downto = tokenTextLen;\n          while (downto > 0)\n            code = (code*31) + tokenString.charAt(--downto);\n          \n          // System.out.println(\"  addPosition: field=\" + fieldInfo.name + \" string=\" + tokenString + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset+token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        } else {\n          tokenString = null;\n          tokenTextLen = token.termBufferLength();\n          tokenTextOffset = token.termBufferOffset();\n\n          // Compute hashcode\n          int downto = tokenTextLen+tokenTextOffset;\n          while (downto > tokenTextOffset)\n            code = (code*31) + tokenText[--downto];\n\n          // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, tokenTextOffset, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n        }\n\n        int hashPos = code & postingsHashMask;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenString, tokenText, tokenTextLen, tokenTextOffset)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = code*1347|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenString, tokenText, tokenTextLen, tokenTextOffset));\n        }\n        \n        final int proxCode;\n\n        if (p != null) {       // term seen since last flush\n\n          if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n            // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n            assert p.docFreq > 0;\n\n            // Now that we know doc freq for previous doc,\n            // write it & lastDocCode\n            freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n            freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n            if (1 == p.docFreq)\n              writeFreqVInt(p.lastDocCode|1);\n            else {\n              writeFreqVInt(p.lastDocCode);\n              writeFreqVInt(p.docFreq);\n            }\n            p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStartCode = offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n\n            p.docFreq = 1;\n\n            // Store code so we can write this after we're\n            // done with this new doc\n            p.lastDocCode = (docID-p.lastDocID) << 1;\n            p.lastDocID = docID;\n\n          } else {                                // term already seen in this doc\n            // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n            p.docFreq++;\n\n            proxCode = position-p.lastPosition;\n\n            if (doVectors) {\n              vector = p.vector;\n              if (vector == null)\n                vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n                offsetStartCode = offsetStart-vector.lastOffset;\n              }\n            }\n          }\n        } else {\t\t\t\t\t  // term not seen before\n          // System.out.println(\"    never seen docID=\" + docID);\n\n          // Refill?\n          if (0 == postingsFreeCount) {\n            postingsFreeCount = postingsFreeList.length;\n            getPostings(postingsFreeList);\n          }\n\n          // Pull next free Posting from free list\n          p = postingsFreeList[--postingsFreeCount];\n\n          final int textLen1 = 1+tokenTextLen;\n          if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE)\n            charPool.nextBuffer();\n          final char[] text = charPool.buffer;\n          final int textUpto = charPool.byteUpto;\n          p.textStart = textUpto + charPool.byteOffset;\n          charPool.byteUpto += textLen1;\n\n          if (tokenString == null)\n            System.arraycopy(tokenText, tokenTextOffset, text, textUpto, tokenTextLen);\n          else\n            tokenString.getChars(0, tokenTextLen, text, textUpto);\n\n          text[textUpto+tokenTextLen] = 0xffff;\n          \n          assert postingsHash[hashPos] == null;\n\n          postingsHash[hashPos] = p;\n          numPostings++;\n\n          if (numPostings == postingsHashHalfSize)\n            rehashPostings(2*postingsHashSize);\n\n          // Init first slice for freq & prox streams\n          final int firstSize = levelSizeArray[0];\n\n          final int upto1 = postingsPool.newSlice(firstSize);\n          p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n          final int upto2 = postingsPool.newSlice(firstSize);\n          p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n          p.lastDocCode = docID << 1;\n          p.lastDocID = docID;\n          p.docFreq = 1;\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offsetStartCode = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n        }\n\n        proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n        prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n        assert prox != null;\n\n        if (payload != null && payload.length > 0) {\n          writeProxVInt((proxCode<<1)|1);\n          writeProxVInt(payload.length);\n          writeProxBytes(payload.data, payload.offset, payload.length);\n          fieldInfo.storePayloads = true;\n        } else\n          writeProxVInt(proxCode<<1);\n\n        p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n        p.lastPosition = position++;\n\n        if (doVectorPositions) {\n          posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n          pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n          writePosVInt(proxCode);\n          vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        if (doVectorOffsets) {\n          offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n          offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n          writeOffsetVInt(offsetStartCode);\n          writeOffsetVInt(offsetEnd-offsetStart);\n          vector.lastOffset = offsetEnd;\n          vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n        }\n      }\n\n","bugFix":null,"bugIntro":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"6864413dbc0c12104c978c05456f3da1d45adb03":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["6864413dbc0c12104c978c05456f3da1d45adb03"]},"commit2Childs":{"6864413dbc0c12104c978c05456f3da1d45adb03":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["6864413dbc0c12104c978c05456f3da1d45adb03"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}