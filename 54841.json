{"path":"lucene/backwards/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/backwards/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"backwards/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            int offsetEnd = fieldState.offset-1;\n            \n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + offsetAttribute.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            int offsetEnd = fieldState.offset-1;\n            \n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + offsetAttribute.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6","date":1272983566,"type":4,"author":"Uwe Schindler","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/backwards/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":null,"sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            int offsetEnd = fieldState.offset-1;\n            \n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + offsetAttribute.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}