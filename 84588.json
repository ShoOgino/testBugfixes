{"path":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,ZkCoreNodeProps,boolean,String).mjava","commits":[{"id":"dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc","date":1426444850,"type":1,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,ZkCoreNodeProps,boolean,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,String,ZkCoreNodeProps,boolean).mjava","sourceNew":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * \n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(final String collection, \n      final String shardId, final ZkCoreNodeProps replicaCoreProps, boolean forcePublishState, String leaderCoreNodeName)\n          throws KeeperException, InterruptedException \n  {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n    \n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n    \n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica)replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \"+replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {     \n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // if the replica's state is not DOWN right now, make it so ...\n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName, ZkStateReader.DOWN, leaderCoreNodeName);\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \"+\n          replicaNodeName+\" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        publishDownState = true;        \n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }      \n    }    \n    \n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();    \n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\", \n          ZkStateReader.STATE_PROP, ZkStateReader.DOWN, \n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(), \n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}; forcePublishState? \"+forcePublishState,\n          replicaCoreName, replicaCoreNodeName, ZkStateReader.DOWN, replicaUrl);\n      overseerJobQueue.offer(ZkStateReader.toJSON(m));      \n    }\n    \n    return nodeIsLive;\n  }\n\n","sourceOld":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * \n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(final String collection, \n      final String shardId, final String replicaUrl, final ZkCoreNodeProps replicaCoreProps, boolean forcePublishState) \n          throws KeeperException, InterruptedException \n  {    \n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n    \n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n    \n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica)replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \"+replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {     \n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // if the replica's state is not DOWN right now, make it so ...\n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName, ZkStateReader.DOWN);\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \"+\n          replicaNodeName+\" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        publishDownState = true;        \n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }      \n    }    \n    \n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();    \n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\", \n          ZkStateReader.STATE_PROP, ZkStateReader.DOWN, \n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(), \n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}; forcePublishState? \"+forcePublishState,\n          replicaCoreName, replicaCoreNodeName, ZkStateReader.DOWN, replicaUrl);\n      overseerJobQueue.offer(ZkStateReader.toJSON(m));      \n    }\n    \n    return nodeIsLive;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0932eb10135843758b2ca508d5aa2b4798aa07f9","date":1426947197,"type":5,"author":"Ramkumar Aiyengar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,ZkCoreNodeProps,String,boolean,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ensureReplicaInLeaderInitiatedRecovery(String,String,ZkCoreNodeProps,boolean,String).mjava","sourceNew":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * \n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(\n      final String collection, final String shardId, final ZkCoreNodeProps replicaCoreProps,\n      String leaderCoreNodeName, boolean forcePublishState, boolean retryOnConnLoss)\n          throws KeeperException, InterruptedException \n  {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n\n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n\n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica)replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \"+replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {     \n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // if the replica's state is not DOWN right now, make it so ...\n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName, ZkStateReader.DOWN, leaderCoreNodeName, retryOnConnLoss);\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \"+\n          replicaNodeName+\" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        publishDownState = true;        \n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }      \n    }    \n    \n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();    \n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\", \n          ZkStateReader.STATE_PROP, ZkStateReader.DOWN, \n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(), \n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}; forcePublishState? \"+forcePublishState,\n          replicaCoreName, replicaCoreNodeName, ZkStateReader.DOWN, replicaUrl);\n      overseerJobQueue.offer(ZkStateReader.toJSON(m));      \n    }\n    \n    return nodeIsLive;\n  }\n\n","sourceOld":"  /**\n   * When a leader receives a communication error when trying to send a request to a replica,\n   * it calls this method to ensure the replica enters recovery when connectivity is restored.\n   * \n   * returns true if the node hosting the replica is still considered \"live\" by ZooKeeper;\n   * false means the node is not live either, so no point in trying to send recovery commands\n   * to it.\n   */\n  public boolean ensureReplicaInLeaderInitiatedRecovery(final String collection, \n      final String shardId, final ZkCoreNodeProps replicaCoreProps, boolean forcePublishState, String leaderCoreNodeName)\n          throws KeeperException, InterruptedException \n  {\n    final String replicaUrl = replicaCoreProps.getCoreUrl();\n\n    if (collection == null)\n      throw new IllegalArgumentException(\"collection parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n\n    if (shardId == null)\n      throw new IllegalArgumentException(\"shard parameter cannot be null for starting leader-initiated recovery for replica: \"+replicaUrl);\n    \n    if (replicaUrl == null)\n      throw new IllegalArgumentException(\"replicaUrl parameter cannot be null for starting leader-initiated recovery\");\n    \n    // First, determine if this replica is already in recovery handling\n    // which is needed because there can be many concurrent errors flooding in\n    // about the same replica having trouble and we only need to send the \"needs\"\n    // recovery signal once\n    boolean nodeIsLive = true;\n    boolean publishDownState = false;\n    String replicaNodeName = replicaCoreProps.getNodeName();\n    String replicaCoreNodeName = ((Replica)replicaCoreProps.getNodeProps()).getName();\n    assert replicaCoreNodeName != null : \"No core name for replica \"+replicaNodeName;\n    synchronized (replicasInLeaderInitiatedRecovery) {\n      if (replicasInLeaderInitiatedRecovery.containsKey(replicaUrl)) {     \n        if (!forcePublishState) {\n          log.debug(\"Replica {} already in leader-initiated recovery handling.\", replicaUrl);\n          return false; // already in this recovery process\n        }\n      }\n\n      // if the replica's state is not DOWN right now, make it so ...\n      // we only really need to try to send the recovery command if the node itself is \"live\"\n      if (getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLeaderInitiatedRecoveryState(collection, shardId, replicaCoreNodeName, ZkStateReader.DOWN, leaderCoreNodeName);\n        replicasInLeaderInitiatedRecovery.put(replicaUrl,\n            getLeaderInitiatedRecoveryZnodePath(collection, shardId, replicaCoreNodeName));\n        log.info(\"Put replica core={} coreNodeName={} on \"+\n          replicaNodeName+\" into leader-initiated recovery.\", replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        publishDownState = true;        \n      } else {\n        nodeIsLive = false; // we really don't need to send the recovery request if the node is NOT live\n        log.info(\"Node \" + replicaNodeName +\n                \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n            replicaCoreProps.getCoreName(), replicaCoreNodeName);\n        // publishDownState will be false to avoid publishing the \"down\" state too many times\n        // as many errors can occur together and will each call into this method (SOLR-6189)        \n      }      \n    }    \n    \n    if (publishDownState || forcePublishState) {\n      String replicaCoreName = replicaCoreProps.getCoreName();    \n      ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\", \n          ZkStateReader.STATE_PROP, ZkStateReader.DOWN, \n          ZkStateReader.BASE_URL_PROP, replicaCoreProps.getBaseUrl(), \n          ZkStateReader.CORE_NAME_PROP, replicaCoreProps.getCoreName(),\n          ZkStateReader.NODE_NAME_PROP, replicaCoreProps.getNodeName(),\n          ZkStateReader.SHARD_ID_PROP, shardId,\n          ZkStateReader.COLLECTION_PROP, collection);\n      log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}; forcePublishState? \"+forcePublishState,\n          replicaCoreName, replicaCoreNodeName, ZkStateReader.DOWN, replicaUrl);\n      overseerJobQueue.offer(ZkStateReader.toJSON(m));      \n    }\n    \n    return nodeIsLive;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0932eb10135843758b2ca508d5aa2b4798aa07f9":["dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0932eb10135843758b2ca508d5aa2b4798aa07f9"]},"commit2Childs":{"dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc":["0932eb10135843758b2ca508d5aa2b4798aa07f9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["dd042b1ba3cc0f8bc92f7896c2c51438ba60e4fc"],"0932eb10135843758b2ca508d5aa2b4798aa07f9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}