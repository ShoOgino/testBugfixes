{"path":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            if (i == -1) {\n              throw new IOException(\"read past EOF\");\n            }\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a a value smaller than the current chunks size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        }\n      }\n    }\n\n","sourceOld":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            if (i == -1) {\n              throw new IOException(\"read past EOF\");\n            }\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a a value smaller than the current chunks size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"462dfb1d8690f192817503773f5b8b94a702246a","date":1280128992,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            if (i == -1) {\n              throw new IOException(\"read past EOF\");\n            }\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunks size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        }\n      }\n    }\n\n","sourceOld":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            if (i == -1) {\n              throw new IOException(\"read past EOF\");\n            }\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a a value smaller than the current chunks size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        }\n      }\n    }\n\n","bugFix":["37cdff042fc21a4f3d9c437c1022deac5d3bab72"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3242a09f703274d3b9283f2064a1a33064b53a1b","date":1280263474,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            if (i == -1) {\n              throw new IOException(\"read past EOF\");\n            }\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunks size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        }\n      }\n    }\n\n","sourceOld":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            if (i == -1) {\n              throw new IOException(\"read past EOF\");\n            }\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a a value smaller than the current chunks size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bd95727c62a66eec2a2cfb7baac54d20dd738908","date":1305713156,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            if (i == -1) {\n              throw new IOException(\"read past EOF\");\n            }\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        }\n      }\n    }\n\n","sourceOld":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            if (i == -1) {\n              throw new IOException(\"read past EOF\");\n            }\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunks size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c3a8a449466c1ff7ce2274fe73dab487256964b4","date":1305735867,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            if (i == -1) {\n              throw new IOException(\"read past EOF\");\n            }\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        }\n      }\n    }\n\n","sourceOld":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            if (i == -1) {\n              throw new IOException(\"read past EOF\");\n            }\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunks size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            if (i == -1) {\n              throw new IOException(\"read past EOF\");\n            }\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        }\n      }\n    }\n\n","sourceOld":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            if (i == -1) {\n              throw new IOException(\"read past EOF\");\n            }\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunks size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0aab6e810b4b0d3743d6a048be0602801f4b3920","date":1308671625,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = off + getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        if (position + len > end) {\n          throw new IOException(\"read past EOF\");\n        }\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        }\n      }\n    }\n\n","sourceOld":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            if (i == -1) {\n              throw new IOException(\"read past EOF\");\n            }\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = off + getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        if (position + len > end) {\n          throw new IOException(\"read past EOF\");\n        }\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        }\n      }\n    }\n\n","sourceOld":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            if (i == -1) {\n              throw new IOException(\"read past EOF\");\n            }\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = off + getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        if (position + len > end) {\n          throw new IOException(\"read past EOF\");\n        }\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        }\n      }\n    }\n\n","sourceOld":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            if (i == -1) {\n              throw new IOException(\"read past EOF\");\n            }\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f1efe3edca215dd9891cb42af283fed96f792ca0","date":1320428891,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = off + getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        if (position + len > end) {\n          throw new IOException(\"read past EOF: \" + this);\n        }\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        } catch (IOException ioe) {\n          throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n        }\n      }\n    }\n\n","sourceOld":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = off + getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        if (position + len > end) {\n          throw new IOException(\"read past EOF\");\n        }\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e5090f41e198d9dd9374e99981f940b111973af2","date":1325969785,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = off + getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        if (position + len > end) {\n          throw new EOFException(\"read past EOF: \" + this);\n        }\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        } catch (IOException ioe) {\n          throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n        }\n      }\n    }\n\n","sourceOld":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = off + getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        if (position + len > end) {\n          throw new IOException(\"read past EOF: \" + this);\n        }\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        } catch (IOException ioe) {\n          throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/SimpleFSDirectory.SimpleFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = off + getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        if (position + len > end) {\n          throw new EOFException(\"read past EOF: \" + this);\n        }\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        } catch (IOException ioe) {\n          throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n        }\n      }\n    }\n\n","sourceOld":"    /** IndexInput methods */\n    @Override\n    protected void readInternal(byte[] b, int offset, int len)\n         throws IOException {\n      synchronized (file) {\n        long position = off + getFilePointer();\n        if (position != file.position) {\n          file.seek(position);\n          file.position = position;\n        }\n        int total = 0;\n\n        if (position + len > end) {\n          throw new EOFException(\"read past EOF: \" + this);\n        }\n\n        try {\n          do {\n            final int readLength;\n            if (total + chunkSize > len) {\n              readLength = len - total;\n            } else {\n              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks\n              readLength = chunkSize;\n            }\n            final int i = file.read(b, offset + total, readLength);\n            file.position += i;\n            total += i;\n          } while (total < len);\n        } catch (OutOfMemoryError e) {\n          // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n          // with a large chunk size in the fast path.\n          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n          outOfMemoryError.initCause(e);\n          throw outOfMemoryError;\n        } catch (IOException ioe) {\n          throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3242a09f703274d3b9283f2064a1a33064b53a1b":["9454a6510e2db155fb01faa5c049b06ece95fab9","462dfb1d8690f192817503773f5b8b94a702246a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["e5090f41e198d9dd9374e99981f940b111973af2"],"f1efe3edca215dd9891cb42af283fed96f792ca0":["0aab6e810b4b0d3743d6a048be0602801f4b3920"],"0aab6e810b4b0d3743d6a048be0602801f4b3920":["bd95727c62a66eec2a2cfb7baac54d20dd738908"],"2553b00f699380c64959ccb27991289aae87be2e":["a3776dccca01c11e7046323cfad46a3b4a471233","0aab6e810b4b0d3743d6a048be0602801f4b3920"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["bd95727c62a66eec2a2cfb7baac54d20dd738908","0aab6e810b4b0d3743d6a048be0602801f4b3920"],"462dfb1d8690f192817503773f5b8b94a702246a":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"c3a8a449466c1ff7ce2274fe73dab487256964b4":["462dfb1d8690f192817503773f5b8b94a702246a","bd95727c62a66eec2a2cfb7baac54d20dd738908"],"a3776dccca01c11e7046323cfad46a3b4a471233":["462dfb1d8690f192817503773f5b8b94a702246a","bd95727c62a66eec2a2cfb7baac54d20dd738908"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e5090f41e198d9dd9374e99981f940b111973af2":["f1efe3edca215dd9891cb42af283fed96f792ca0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"bd95727c62a66eec2a2cfb7baac54d20dd738908":["462dfb1d8690f192817503773f5b8b94a702246a"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"3242a09f703274d3b9283f2064a1a33064b53a1b":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"f1efe3edca215dd9891cb42af283fed96f792ca0":["e5090f41e198d9dd9374e99981f940b111973af2"],"0aab6e810b4b0d3743d6a048be0602801f4b3920":["f1efe3edca215dd9891cb42af283fed96f792ca0","2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"2553b00f699380c64959ccb27991289aae87be2e":[],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"462dfb1d8690f192817503773f5b8b94a702246a":["3242a09f703274d3b9283f2064a1a33064b53a1b","c3a8a449466c1ff7ce2274fe73dab487256964b4","a3776dccca01c11e7046323cfad46a3b4a471233","bd95727c62a66eec2a2cfb7baac54d20dd738908"],"c3a8a449466c1ff7ce2274fe73dab487256964b4":[],"a3776dccca01c11e7046323cfad46a3b4a471233":["2553b00f699380c64959ccb27991289aae87be2e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"e5090f41e198d9dd9374e99981f940b111973af2":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["3242a09f703274d3b9283f2064a1a33064b53a1b","462dfb1d8690f192817503773f5b8b94a702246a"],"bd95727c62a66eec2a2cfb7baac54d20dd738908":["0aab6e810b4b0d3743d6a048be0602801f4b3920","d083e83f225b11e5fdd900e83d26ddb385b6955c","c3a8a449466c1ff7ce2274fe73dab487256964b4","a3776dccca01c11e7046323cfad46a3b4a471233"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3242a09f703274d3b9283f2064a1a33064b53a1b","2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c","c3a8a449466c1ff7ce2274fe73dab487256964b4","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}