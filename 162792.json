{"path":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","commits":[{"id":"a194d3ac4639a7909ea614667b9a7632a6aa14d1","date":1355475351,"type":0,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    SlowCompositeReaderWrapper wrapper = new SlowCompositeReaderWrapper(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = new IndexSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      QueryUtils.check(random(), cq, searcher);\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<Integer>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b752e241723380b5fdabe8364f28fd5639ebf08c","date":1355481283,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    SlowCompositeReaderWrapper wrapper = new SlowCompositeReaderWrapper(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = new IndexSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<Integer>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","sourceOld":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    SlowCompositeReaderWrapper wrapper = new SlowCompositeReaderWrapper(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = new IndexSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      QueryUtils.check(random(), cq, searcher);\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<Integer>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"88428cf23f55c2838f471aaeaa18fa26805315c9","date":1355481774,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    SlowCompositeReaderWrapper wrapper = new SlowCompositeReaderWrapper(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = new IndexSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      QueryUtils.check(random(), cq, searcher);\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<Integer>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","sourceOld":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    SlowCompositeReaderWrapper wrapper = new SlowCompositeReaderWrapper(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = new IndexSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<Integer>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6e68e0401df142bf4a7d8d3f25edcb4e555764cd","date":1355481902,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    SlowCompositeReaderWrapper wrapper = new SlowCompositeReaderWrapper(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = new IndexSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<Integer>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","sourceOld":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    SlowCompositeReaderWrapper wrapper = new SlowCompositeReaderWrapper(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = new IndexSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      QueryUtils.check(random(), cq, searcher);\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<Integer>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    SlowCompositeReaderWrapper wrapper = new SlowCompositeReaderWrapper(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = new IndexSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<Integer>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad1f7eb1a6bbf19f3d32b3baf2dee3db844eccdc","date":1366056945,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    SlowCompositeReaderWrapper wrapper = new SlowCompositeReaderWrapper(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<Integer>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","sourceOld":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    SlowCompositeReaderWrapper wrapper = new SlowCompositeReaderWrapper(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = new IndexSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<Integer>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"df9bf66ed405ee5c7d32b47bdb36c2e36d2c1392","date":1377503666,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    AtomicReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<Integer>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","sourceOld":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    SlowCompositeReaderWrapper wrapper = new SlowCompositeReaderWrapper(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<Integer>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    AtomicReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<Integer>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","sourceOld":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    SlowCompositeReaderWrapper wrapper = new SlowCompositeReaderWrapper(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<Integer>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e1151ecb4798f5c31137aec032c241638018ed20","date":1394284367,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    AtomicReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<Integer>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","sourceOld":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    AtomicReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<Integer>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a58bbbe1c866963764d3f15d3a26a6a85f6c6af4","date":1394564625,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    AtomicReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<Integer>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","sourceOld":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    AtomicReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<Integer>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    AtomicReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","sourceOld":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    AtomicReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<Integer>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    AtomicReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.shutdown();\n      dir.close();\n    }\n    \n  }\n\n","sourceOld":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    AtomicReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    AtomicReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","sourceOld":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    AtomicReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.shutdown();\n      dir.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    LeafReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","sourceOld":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    AtomicReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a56958d7f71a28824f20031ffbb2e13502a0274e","date":1425573902,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    LeafReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      IOUtils.close(reader, wrapper, w, dir, analyzer);\n    }\n    \n  }\n\n","sourceOld":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    LeafReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    LeafReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      IOUtils.close(reader, wrapper, w, dir, analyzer);\n    }\n    \n  }\n\n","sourceOld":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    LeafReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      reader.close();\n      wrapper.close();\n      w.close();\n      dir.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    LeafReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator();\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      IOUtils.close(reader, wrapper, w, dir, analyzer);\n    }\n    \n  }\n\n","sourceOld":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    LeafReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator(null);\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      IOUtils.close(reader, wrapper, w, dir, analyzer);\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3f68d01cf19df971dcdcb05e30247f4ad7ec9747","date":1434611645,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    LeafReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator();\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery.Builder verifyQuery = new BooleanQuery.Builder();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery.build(), reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      IOUtils.close(reader, wrapper, w, dir, analyzer);\n    }\n    \n  }\n\n","sourceOld":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    LeafReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator();\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery verifyQuery = new BooleanQuery();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery, reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      IOUtils.close(reader, wrapper, w, dir, analyzer);\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1","date":1457644139,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    w.forceMerge(1);\n    DirectoryReader reader = w.getReader();\n    LeafReader wrapper = getOnlyLeafReader(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator();\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery.Builder verifyQuery = new BooleanQuery.Builder();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery.build(), reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      IOUtils.close(wrapper, w, dir, analyzer);\n    }\n    \n  }\n\n","sourceOld":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    DirectoryReader reader = w.getReader();\n    LeafReader wrapper = SlowCompositeReaderWrapper.wrap(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator();\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery.Builder verifyQuery = new BooleanQuery.Builder();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery.build(), reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      IOUtils.close(reader, wrapper, w, dir, analyzer);\n    }\n    \n  }\n\n","bugFix":["c9fb5f46e264daf5ba3860defe623a89d202dd87","a56958d7f71a28824f20031ffbb2e13502a0274e"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"50040d6639d21e7c0f8abca4b21c0d4b9a64f413","date":1457690023,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    w.forceMerge(1);\n    DirectoryReader reader = w.getReader();\n    LeafReader wrapper = getOnlyLeafReader(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator();\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery.Builder verifyQuery = new BooleanQuery.Builder();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery.build(), reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      IOUtils.close(reader, w, dir, analyzer);\n    }\n    \n  }\n\n","sourceOld":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    w.forceMerge(1);\n    DirectoryReader reader = w.getReader();\n    LeafReader wrapper = getOnlyLeafReader(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator();\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery.Builder verifyQuery = new BooleanQuery.Builder();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery.build(), reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      IOUtils.close(wrapper, w, dir, analyzer);\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c8a0e442f7b61f811680273b25da95994a724466","date":1467878549,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    w.forceMerge(1);\n    DirectoryReader reader = w.getReader();\n    LeafReader wrapper = getOnlyLeafReader(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator();\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery.Builder verifyQuery = new BooleanQuery.Builder();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1);\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery.build(), reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      IOUtils.close(reader, w, dir, analyzer);\n    }\n    \n  }\n\n","sourceOld":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    w.forceMerge(1);\n    DirectoryReader reader = w.getReader();\n    LeafReader wrapper = getOnlyLeafReader(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator();\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery.Builder verifyQuery = new BooleanQuery.Builder();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery.build(), reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      IOUtils.close(reader, w, dir, analyzer);\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    w.forceMerge(1);\n    DirectoryReader reader = w.getReader();\n    LeafReader wrapper = getOnlyLeafReader(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator();\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery.Builder verifyQuery = new BooleanQuery.Builder();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1);\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery.build(), reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      IOUtils.close(reader, w, dir, analyzer);\n    }\n    \n  }\n\n","sourceOld":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    w.forceMerge(1);\n    DirectoryReader reader = w.getReader();\n    LeafReader wrapper = getOnlyLeafReader(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator();\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery.Builder verifyQuery = new BooleanQuery.Builder();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1, random().nextBoolean());\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery.build(), reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      IOUtils.close(reader, w, dir, analyzer);\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83788ad129a5154d5c6562c4e8ce3db48793aada","date":1532961485,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","pathOld":"lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest#testRandomIndex().mjava","sourceNew":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    w.forceMerge(1);\n    DirectoryReader reader = w.getReader();\n    LeafReader wrapper = getOnlyLeafReader(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator();\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery.Builder verifyQuery = new BooleanQuery.Builder();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1);\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery.build(), reader.maxDoc());\n      assertEquals(verifySearch.totalHits.value, cqSearch.totalHits.value);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      IOUtils.close(reader, w, dir, analyzer);\n    }\n    \n  }\n\n","sourceOld":"  public void testRandomIndex() throws IOException {\n    Directory dir = newDirectory();\n    MockAnalyzer analyzer = new MockAnalyzer(random());\n    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir, analyzer);\n    createRandomIndex(atLeast(50), w, random().nextLong());\n    w.forceMerge(1);\n    DirectoryReader reader = w.getReader();\n    LeafReader wrapper = getOnlyLeafReader(reader);\n    String field = \"body\";\n    Terms terms = wrapper.terms(field);\n    PriorityQueue<TermAndFreq> lowFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq > b.freq;\n      }\n      \n    };\n    PriorityQueue<TermAndFreq> highFreqQueue = new PriorityQueue<CommonTermsQueryTest.TermAndFreq>(\n        5) {\n      \n      @Override\n      protected boolean lessThan(TermAndFreq a, TermAndFreq b) {\n        return a.freq < b.freq;\n      }\n      \n    };\n    try {\n      TermsEnum iterator = terms.iterator();\n      while (iterator.next() != null) {\n        if (highFreqQueue.size() < 5) {\n          highFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n          lowFreqQueue.add(new TermAndFreq(\n              BytesRef.deepCopyOf(iterator.term()), iterator.docFreq()));\n        } else {\n          if (highFreqQueue.top().freq < iterator.docFreq()) {\n            highFreqQueue.top().freq = iterator.docFreq();\n            highFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            highFreqQueue.updateTop();\n          }\n          \n          if (lowFreqQueue.top().freq > iterator.docFreq()) {\n            lowFreqQueue.top().freq = iterator.docFreq();\n            lowFreqQueue.top().term = BytesRef.deepCopyOf(iterator.term());\n            lowFreqQueue.updateTop();\n          }\n        }\n      }\n      int lowFreq = lowFreqQueue.top().freq;\n      int highFreq = highFreqQueue.top().freq;\n      assumeTrue(\"unlucky index\", highFreq - 1 > lowFreq);\n      List<TermAndFreq> highTerms = queueToList(highFreqQueue);\n      List<TermAndFreq> lowTerms = queueToList(lowFreqQueue);\n      \n      IndexSearcher searcher = newSearcher(reader);\n      Occur lowFreqOccur = randomOccur(random());\n      BooleanQuery.Builder verifyQuery = new BooleanQuery.Builder();\n      CommonTermsQuery cq = new CommonTermsQuery(randomOccur(random()),\n          lowFreqOccur, highFreq - 1);\n      for (TermAndFreq termAndFreq : lowTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n        verifyQuery.add(new BooleanClause(new TermQuery(new Term(field,\n            termAndFreq.term)), lowFreqOccur));\n      }\n      for (TermAndFreq termAndFreq : highTerms) {\n        cq.add(new Term(field, termAndFreq.term));\n      }\n      \n      TopDocs cqSearch = searcher.search(cq, reader.maxDoc());\n      \n      TopDocs verifySearch = searcher.search(verifyQuery.build(), reader.maxDoc());\n      assertEquals(verifySearch.totalHits, cqSearch.totalHits);\n      Set<Integer> hits = new HashSet<>();\n      for (ScoreDoc doc : verifySearch.scoreDocs) {\n        hits.add(doc.doc);\n      }\n      \n      for (ScoreDoc doc : cqSearch.scoreDocs) {\n        assertTrue(hits.remove(doc.doc));\n      }\n      \n      assertTrue(hits.isEmpty());\n      \n      /*\n       *  need to force merge here since QueryUtils adds checks based\n       *  on leave readers which have different statistics than the top\n       *  level reader if we have more than one segment. This could \n       *  result in a different query / results.\n       */\n      w.forceMerge(1); \n      DirectoryReader reader2 = w.getReader();\n      QueryUtils.check(random(), cq, newSearcher(reader2));\n      reader2.close();\n    } finally {\n      IOUtils.close(reader, w, dir, analyzer);\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["a56958d7f71a28824f20031ffbb2e13502a0274e"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","6e68e0401df142bf4a7d8d3f25edcb4e555764cd"],"a58bbbe1c866963764d3f15d3a26a6a85f6c6af4":["df9bf66ed405ee5c7d32b47bdb36c2e36d2c1392","e1151ecb4798f5c31137aec032c241638018ed20"],"a194d3ac4639a7909ea614667b9a7632a6aa14d1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"b752e241723380b5fdabe8364f28fd5639ebf08c":["a194d3ac4639a7909ea614667b9a7632a6aa14d1"],"0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1":["3f68d01cf19df971dcdcb05e30247f4ad7ec9747"],"e1151ecb4798f5c31137aec032c241638018ed20":["df9bf66ed405ee5c7d32b47bdb36c2e36d2c1392"],"3f68d01cf19df971dcdcb05e30247f4ad7ec9747":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["c9fb5f46e264daf5ba3860defe623a89d202dd87","a56958d7f71a28824f20031ffbb2e13502a0274e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"83788ad129a5154d5c6562c4e8ce3db48793aada":["c8a0e442f7b61f811680273b25da95994a724466"],"88428cf23f55c2838f471aaeaa18fa26805315c9":["b752e241723380b5fdabe8364f28fd5639ebf08c"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["e1151ecb4798f5c31137aec032c241638018ed20"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["ad1f7eb1a6bbf19f3d32b3baf2dee3db844eccdc","df9bf66ed405ee5c7d32b47bdb36c2e36d2c1392"],"50040d6639d21e7c0f8abca4b21c0d4b9a64f413":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["50040d6639d21e7c0f8abca4b21c0d4b9a64f413","c8a0e442f7b61f811680273b25da95994a724466"],"6e68e0401df142bf4a7d8d3f25edcb4e555764cd":["88428cf23f55c2838f471aaeaa18fa26805315c9"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"df9bf66ed405ee5c7d32b47bdb36c2e36d2c1392":["ad1f7eb1a6bbf19f3d32b3baf2dee3db844eccdc"],"c8a0e442f7b61f811680273b25da95994a724466":["50040d6639d21e7c0f8abca4b21c0d4b9a64f413"],"a56958d7f71a28824f20031ffbb2e13502a0274e":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"ad1f7eb1a6bbf19f3d32b3baf2dee3db844eccdc":["6e68e0401df142bf4a7d8d3f25edcb4e555764cd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["83788ad129a5154d5c6562c4e8ce3db48793aada"]},"commit2Childs":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["3f68d01cf19df971dcdcb05e30247f4ad7ec9747"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"a58bbbe1c866963764d3f15d3a26a6a85f6c6af4":[],"a194d3ac4639a7909ea614667b9a7632a6aa14d1":["b752e241723380b5fdabe8364f28fd5639ebf08c"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","a56958d7f71a28824f20031ffbb2e13502a0274e"],"b752e241723380b5fdabe8364f28fd5639ebf08c":["88428cf23f55c2838f471aaeaa18fa26805315c9"],"e1151ecb4798f5c31137aec032c241638018ed20":["a58bbbe1c866963764d3f15d3a26a6a85f6c6af4","634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1":["50040d6639d21e7c0f8abca4b21c0d4b9a64f413"],"3f68d01cf19df971dcdcb05e30247f4ad7ec9747":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","a194d3ac4639a7909ea614667b9a7632a6aa14d1"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"83788ad129a5154d5c6562c4e8ce3db48793aada":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"88428cf23f55c2838f471aaeaa18fa26805315c9":["6e68e0401df142bf4a7d8d3f25edcb4e555764cd"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"50040d6639d21e7c0f8abca4b21c0d4b9a64f413":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","c8a0e442f7b61f811680273b25da95994a724466"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"6e68e0401df142bf4a7d8d3f25edcb4e555764cd":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","ad1f7eb1a6bbf19f3d32b3baf2dee3db844eccdc"],"df9bf66ed405ee5c7d32b47bdb36c2e36d2c1392":["a58bbbe1c866963764d3f15d3a26a6a85f6c6af4","e1151ecb4798f5c31137aec032c241638018ed20","3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"c8a0e442f7b61f811680273b25da95994a724466":["83788ad129a5154d5c6562c4e8ce3db48793aada","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"a56958d7f71a28824f20031ffbb2e13502a0274e":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"ad1f7eb1a6bbf19f3d32b3baf2dee3db844eccdc":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","df9bf66ed405ee5c7d32b47bdb36c2e36d2c1392"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","a58bbbe1c866963764d3f15d3a26a6a85f6c6af4","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}