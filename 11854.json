{"path":"lucene/core/src/java/org/apache/lucene/codecs/SortedDocValuesConsumer.Merger#merge(MergeState).mjava","commits":[{"id":"f45b94f31bcc0de4497b99f7b51993765f64c601","date":1352745246,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/SortedDocValuesConsumer.Merger#merge(MergeState).mjava","pathOld":"/dev/null","sourceNew":"    public void merge(MergeState mergeState) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (AtomicReader reader : mergeState.readers) {\n        DocValues docvalues = reader.docValues(mergeState.fieldInfo.name);\n        final SortedSource source;\n        int maxDoc = reader.maxDoc();\n        if (docvalues == null) {\n          source = DocValues.getDefaultSortedSource(mergeState.fieldInfo.getDocValuesType(), maxDoc);\n        } else {\n          source = (SortedSource) docvalues.getDirectSource();\n        }\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.source = source;\n        segStates.add(state);\n        assert source.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(source.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(source.ord(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.source.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      boolean first = true;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          if (lastTerm == null) {\n            fixedLength = lastTerm.length;\n          } else {\n            ord++;\n            if (lastTerm.length != fixedLength) {\n              fixedLength = -1;\n            }\n          }\n          maxLength = Math.max(maxLength, lastTerm.length);\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e3538d7872902c19ad619052fb3130f652f35e35","date":1353005395,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/SortedDocValuesConsumer.Merger#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/SortedDocValuesConsumer.Merger#merge(MergeState).mjava","sourceNew":"    public void merge(MergeState mergeState) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (AtomicReader reader : mergeState.readers) {\n        DocValues docvalues = reader.docValues(mergeState.fieldInfo.name);\n        final SortedSource source;\n        int maxDoc = reader.maxDoc();\n        if (docvalues == null) {\n          source = DocValues.getDefaultSortedSource(mergeState.fieldInfo.getDocValuesType(), maxDoc);\n        } else {\n          source = (SortedSource) docvalues.getDirectSource();\n        }\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.source = source;\n        segStates.add(state);\n        assert source.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(source.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(source.ord(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.source.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      boolean first = true;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          if (lastTerm == null) {\n            fixedLength = lastTerm.length;\n          } else {\n            ord++;\n            if (lastTerm.length != fixedLength) {\n              fixedLength = -1;\n            }\n          }\n          maxLength = Math.max(maxLength, lastTerm.length);\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","sourceOld":"    public void merge(MergeState mergeState) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (AtomicReader reader : mergeState.readers) {\n        DocValues docvalues = reader.docValues(mergeState.fieldInfo.name);\n        final SortedSource source;\n        int maxDoc = reader.maxDoc();\n        if (docvalues == null) {\n          source = DocValues.getDefaultSortedSource(mergeState.fieldInfo.getDocValuesType(), maxDoc);\n        } else {\n          source = (SortedSource) docvalues.getDirectSource();\n        }\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.source = source;\n        segStates.add(state);\n        assert source.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(source.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(source.ord(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.source.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      boolean first = true;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          if (lastTerm == null) {\n            fixedLength = lastTerm.length;\n          } else {\n            ord++;\n            if (lastTerm.length != fixedLength) {\n              fixedLength = -1;\n            }\n          }\n          maxLength = Math.max(maxLength, lastTerm.length);\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a4d374b2bebd0d52acaa61038fbf23068620fba7","date":1353240004,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/SortedDocValuesConsumer.Merger#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/SortedDocValuesConsumer.Merger#merge(MergeState).mjava","sourceNew":"    public void merge(MergeState mergeState) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (AtomicReader reader : mergeState.readers) {\n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.values = reader.getSortedDocValues(mergeState.fieldInfo.name);\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      boolean first = true;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          if (lastTerm == null) {\n            fixedLength = lastTerm.length;\n          } else {\n            ord++;\n            if (lastTerm.length != fixedLength) {\n              fixedLength = -1;\n            }\n          }\n          maxLength = Math.max(maxLength, lastTerm.length);\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","sourceOld":"    public void merge(MergeState mergeState) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (AtomicReader reader : mergeState.readers) {\n        DocValues docvalues = reader.docValues(mergeState.fieldInfo.name);\n        final SortedSource source;\n        int maxDoc = reader.maxDoc();\n        if (docvalues == null) {\n          source = DocValues.getDefaultSortedSource(mergeState.fieldInfo.getDocValuesType(), maxDoc);\n        } else {\n          source = (SortedSource) docvalues.getDirectSource();\n        }\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.source = source;\n        segStates.add(state);\n        assert source.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(source.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(source.ord(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.source.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      boolean first = true;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          if (lastTerm == null) {\n            fixedLength = lastTerm.length;\n          } else {\n            ord++;\n            if (lastTerm.length != fixedLength) {\n              fixedLength = -1;\n            }\n          }\n          maxLength = Math.max(maxLength, lastTerm.length);\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6295f14d43685811599f8a8f02a63d75ec6bd8fe","date":1353248103,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/SortedDocValuesConsumer.Merger#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/SortedDocValuesConsumer.Merger#merge(MergeState).mjava","sourceNew":"    public void merge(MergeState mergeState) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (AtomicReader reader : mergeState.readers) {\n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.values = reader.getSortedDocValues(mergeState.fieldInfo.name);\n        if (state.values == null) {\n          state.values = SortedDocValues.DEFAULT;\n        }\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      boolean first = true;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          if (lastTerm == null) {\n            fixedLength = lastTerm.length;\n          } else {\n            ord++;\n            if (lastTerm.length != fixedLength) {\n              fixedLength = -1;\n            }\n          }\n          maxLength = Math.max(maxLength, lastTerm.length);\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","sourceOld":"    public void merge(MergeState mergeState) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (AtomicReader reader : mergeState.readers) {\n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.values = reader.getSortedDocValues(mergeState.fieldInfo.name);\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      boolean first = true;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          if (lastTerm == null) {\n            fixedLength = lastTerm.length;\n          } else {\n            ord++;\n            if (lastTerm.length != fixedLength) {\n              fixedLength = -1;\n            }\n          }\n          maxLength = Math.max(maxLength, lastTerm.length);\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"767bfba15cdbe84dd2e3b841e0429a1b4ef8feee","date":1353299109,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/SortedDocValuesConsumer.Merger#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/SortedDocValuesConsumer.Merger#merge(MergeState).mjava","sourceNew":"    public void merge(MergeState mergeState) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (AtomicReader reader : mergeState.readers) {\n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.values = reader.getSortedDocValues(mergeState.fieldInfo.name);\n        if (state.values == null) {\n          state.values = new SortedDocValues.EMPTY(maxDoc);\n        }\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      boolean first = true;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          if (lastTerm == null) {\n            fixedLength = lastTerm.length;\n          } else {\n            ord++;\n            if (lastTerm.length != fixedLength) {\n              fixedLength = -1;\n            }\n          }\n          maxLength = Math.max(maxLength, lastTerm.length);\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","sourceOld":"    public void merge(MergeState mergeState) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (AtomicReader reader : mergeState.readers) {\n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.values = reader.getSortedDocValues(mergeState.fieldInfo.name);\n        if (state.values == null) {\n          state.values = SortedDocValues.DEFAULT;\n        }\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      boolean first = true;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          if (lastTerm == null) {\n            fixedLength = lastTerm.length;\n          } else {\n            ord++;\n            if (lastTerm.length != fixedLength) {\n              fixedLength = -1;\n            }\n          }\n          maxLength = Math.max(maxLength, lastTerm.length);\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8f460d9b6c2b8ff9b9a85b282a9622256b182229","date":1353359101,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/SortedDocValuesConsumer.Merger#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/SortedDocValuesConsumer.Merger#merge(MergeState).mjava","sourceNew":"    public void merge(MergeState mergeState) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (AtomicReader reader : mergeState.readers) {\n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.values = reader.getSortedDocValues(mergeState.fieldInfo.name);\n        if (state.values == null) {\n          state.values = new SortedDocValues.EMPTY(maxDoc);\n        }\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          ord++;\n          if (fixedLength == -2) {\n            fixedLength = lastTerm.length;\n          } else {\n            if (lastTerm.length != fixedLength) {\n              fixedLength = -1;\n            }\n          }\n          maxLength = Math.max(maxLength, lastTerm.length);\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","sourceOld":"    public void merge(MergeState mergeState) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (AtomicReader reader : mergeState.readers) {\n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.values = reader.getSortedDocValues(mergeState.fieldInfo.name);\n        if (state.values == null) {\n          state.values = new SortedDocValues.EMPTY(maxDoc);\n        }\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      boolean first = true;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          if (lastTerm == null) {\n            fixedLength = lastTerm.length;\n          } else {\n            ord++;\n            if (lastTerm.length != fixedLength) {\n              fixedLength = -1;\n            }\n          }\n          maxLength = Math.max(maxLength, lastTerm.length);\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4b53a9a930ee01857178a1b512fbab24642f3fa8","date":1354471097,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/SortedDocValuesConsumer.Merger#merge(MergeState,List[SortedDocValues]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/SortedDocValuesConsumer.Merger#merge(MergeState).mjava","sourceNew":"    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {\n        AtomicReader reader = mergeState.readers.get(readerIDX);      \n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.values = toMerge.get(readerIDX);\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          ord++;\n          if (fixedLength == -2) {\n            fixedLength = lastTerm.length;\n          } else {\n            if (lastTerm.length != fixedLength) {\n              fixedLength = -1;\n            }\n          }\n          maxLength = Math.max(maxLength, lastTerm.length);\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","sourceOld":"    public void merge(MergeState mergeState) throws IOException {\n\n      // First pass: mark \"live\" terms\n      for (AtomicReader reader : mergeState.readers) {\n        // nocommit what if this is null...?  need default source?\n        int maxDoc = reader.maxDoc();\n\n        SegmentState state = new SegmentState();\n        state.reader = reader;\n        state.values = reader.getSortedDocValues(mergeState.fieldInfo.name);\n        if (state.values == null) {\n          state.values = new SortedDocValues.EMPTY(maxDoc);\n        }\n\n        segStates.add(state);\n        assert state.values.getValueCount() < Integer.MAX_VALUE;\n        if (reader.hasDeletions()) {\n          state.liveTerms = new FixedBitSet(state.values.getValueCount());\n          Bits liveDocs = reader.getLiveDocs();\n          for(int docID=0;docID<maxDoc;docID++) {\n            if (liveDocs.get(docID)) {\n              state.liveTerms.set(state.values.getOrd(docID));\n            }\n          }\n        }\n\n        // nocommit we can unload the bits to disk to reduce\n        // transient ram spike...\n      }\n\n      // Second pass: merge only the live terms\n\n      TermMergeQueue q = new TermMergeQueue(segStates.size());\n      for(SegmentState segState : segStates) {\n        if (segState.nextTerm() != null) {\n\n          // nocommit we could defer this to 3rd pass (and\n          // reduce transient RAM spike) but then\n          // we'd spend more effort computing the mapping...:\n          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];\n          q.add(segState);\n        }\n      }\n\n      BytesRef lastTerm = null;\n      int ord = 0;\n      while (q.size() != 0) {\n        SegmentState top = q.top();\n        if (lastTerm == null || !lastTerm.equals(top.scratch)) {\n          lastTerm = BytesRef.deepCopyOf(top.scratch);\n          // nocommit we could spill this to disk instead of\n          // RAM, and replay on finish...\n          mergedTerms.add(lastTerm);\n          ord++;\n          if (fixedLength == -2) {\n            fixedLength = lastTerm.length;\n          } else {\n            if (lastTerm.length != fixedLength) {\n              fixedLength = -1;\n            }\n          }\n          maxLength = Math.max(maxLength, lastTerm.length);\n        }\n\n        top.segOrdToMergedOrd[top.ord] = ord-1;\n        if (top.nextTerm() == null) {\n          q.pop();\n        } else {\n          q.updateTop();\n        }\n      }\n\n      numMergedTerms = ord;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"8f460d9b6c2b8ff9b9a85b282a9622256b182229":["767bfba15cdbe84dd2e3b841e0429a1b4ef8feee"],"767bfba15cdbe84dd2e3b841e0429a1b4ef8feee":["6295f14d43685811599f8a8f02a63d75ec6bd8fe"],"e3538d7872902c19ad619052fb3130f652f35e35":["f45b94f31bcc0de4497b99f7b51993765f64c601"],"6295f14d43685811599f8a8f02a63d75ec6bd8fe":["a4d374b2bebd0d52acaa61038fbf23068620fba7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4b53a9a930ee01857178a1b512fbab24642f3fa8":["8f460d9b6c2b8ff9b9a85b282a9622256b182229"],"a4d374b2bebd0d52acaa61038fbf23068620fba7":["e3538d7872902c19ad619052fb3130f652f35e35"],"f45b94f31bcc0de4497b99f7b51993765f64c601":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"8f460d9b6c2b8ff9b9a85b282a9622256b182229":["4b53a9a930ee01857178a1b512fbab24642f3fa8"],"767bfba15cdbe84dd2e3b841e0429a1b4ef8feee":["8f460d9b6c2b8ff9b9a85b282a9622256b182229"],"e3538d7872902c19ad619052fb3130f652f35e35":["a4d374b2bebd0d52acaa61038fbf23068620fba7"],"6295f14d43685811599f8a8f02a63d75ec6bd8fe":["767bfba15cdbe84dd2e3b841e0429a1b4ef8feee"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f45b94f31bcc0de4497b99f7b51993765f64c601","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4b53a9a930ee01857178a1b512fbab24642f3fa8":[],"a4d374b2bebd0d52acaa61038fbf23068620fba7":["6295f14d43685811599f8a8f02a63d75ec6bd8fe"],"f45b94f31bcc0de4497b99f7b51993765f64c601":["e3538d7872902c19ad619052fb3130f652f35e35"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4b53a9a930ee01857178a1b512fbab24642f3fa8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}