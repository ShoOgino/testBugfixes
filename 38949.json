{"path":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","commits":[{"id":"ea9d1f915bc156f5ebbc6a9cdcf631c173a69a91","date":1275174858,"type":0,"author":"Uwe Schindler","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","pathOld":"/dev/null","sourceNew":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    Analyzer a = new LimitTokenCountAnalyzer(new WhitespaceAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2     3  4  5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, 4);\n    assertTokenStreamContents(a.reusableTokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n    \n    a = new LimitTokenCountAnalyzer(new StandardAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n    assertTokenStreamContents(a.reusableTokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cf7efd82433f3f64684711c16edfd149db6af111","date":1317013128,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","sourceNew":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    Analyzer a = new LimitTokenCountAnalyzer(new WhitespaceAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.reusableTokenStream(\"dummy\", new StringReader(\"1  2     3  4  5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, 4);\n    assertTokenStreamContents(a.reusableTokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n    \n    a = new LimitTokenCountAnalyzer(new StandardAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.reusableTokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n  }\n\n","sourceOld":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    Analyzer a = new LimitTokenCountAnalyzer(new WhitespaceAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2     3  4  5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, 4);\n    assertTokenStreamContents(a.reusableTokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n    \n    a = new LimitTokenCountAnalyzer(new StandardAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n    assertTokenStreamContents(a.reusableTokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"69e043c521d4e8db770cc140c63f5ef51f03426a","date":1317187614,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","sourceNew":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    Analyzer a = new LimitTokenCountAnalyzer(new WhitespaceAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2     3  4  5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, 4);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n    \n    a = new LimitTokenCountAnalyzer(new StandardAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n  }\n\n","sourceOld":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    Analyzer a = new LimitTokenCountAnalyzer(new WhitespaceAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.reusableTokenStream(\"dummy\", new StringReader(\"1  2     3  4  5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, 4);\n    assertTokenStreamContents(a.reusableTokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n    \n    a = new LimitTokenCountAnalyzer(new StandardAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.reusableTokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer#testLimitTokenCountAnalyzer().mjava","sourceNew":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    Analyzer a = new LimitTokenCountAnalyzer(new WhitespaceAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2     3  4  5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, 4);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n    \n    a = new LimitTokenCountAnalyzer(new StandardAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n  }\n\n","sourceOld":"  public void testLimitTokenCountAnalyzer() throws IOException {\n    Analyzer a = new LimitTokenCountAnalyzer(new WhitespaceAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1  2     3  4  5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 3 }, new int[] { 1, 4 }, 4);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n    \n    a = new LimitTokenCountAnalyzer(new StandardAnalyzer(TEST_VERSION_CURRENT), 2);\n    // dont use assertAnalyzesTo here, as the end offset is not the end of the string!\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(\"1 2 3 4 5\")), new String[] { \"1\", \"2\" }, new int[] { 0, 2 }, new int[] { 1, 3 }, 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cf7efd82433f3f64684711c16edfd149db6af111":["ea9d1f915bc156f5ebbc6a9cdcf631c173a69a91"],"69e043c521d4e8db770cc140c63f5ef51f03426a":["cf7efd82433f3f64684711c16edfd149db6af111"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"ea9d1f915bc156f5ebbc6a9cdcf631c173a69a91":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ea9d1f915bc156f5ebbc6a9cdcf631c173a69a91"],"cf7efd82433f3f64684711c16edfd149db6af111":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"69e043c521d4e8db770cc140c63f5ef51f03426a":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"ea9d1f915bc156f5ebbc6a9cdcf631c173a69a91":["cf7efd82433f3f64684711c16edfd149db6af111"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}