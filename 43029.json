{"path":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","pathOld":"src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","sourceNew":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n\n    int code = textStart;\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate RawPostingList in hash\n    p = postingsHash[hashPos];\n\n    if (p != null && p.textStart != textStart) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        p = postingsHash[hashPos];\n      } while (p != null && p.textStart != textStart);\n    }\n\n    if (p == null) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n\n      // Refill?\n      if (0 == perThread.freePostingsCount)\n        perThread.morePostings();\n\n      // Pull next free RawPostingList from free list\n      p = perThread.freePostings[--perThread.freePostingsCount];\n      assert p != null;\n\n      p.textStart = textStart;\n          \n      assert postingsHash[hashPos] == null;\n      postingsHash[hashPos] = p;\n      numPostings++;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      p.intStart = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      p.byteStart = intUptos[intUptoStart];\n\n      consumer.newTerm(p);\n\n    } else {\n      intUptos = intPool.buffers[p.intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = p.intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(p);\n    }\n  }\n\n","sourceOld":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n\n    int code = textStart;\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate RawPostingList in hash\n    p = postingsHash[hashPos];\n\n    if (p != null && p.textStart != textStart) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        p = postingsHash[hashPos];\n      } while (p != null && p.textStart != textStart);\n    }\n\n    if (p == null) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n\n      // Refill?\n      if (0 == perThread.freePostingsCount)\n        perThread.morePostings();\n\n      // Pull next free RawPostingList from free list\n      p = perThread.freePostings[--perThread.freePostingsCount];\n      assert p != null;\n\n      p.textStart = textStart;\n          \n      assert postingsHash[hashPos] == null;\n      postingsHash[hashPos] = p;\n      numPostings++;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      p.intStart = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      p.byteStart = intUptos[intUptoStart];\n\n      consumer.newTerm(p);\n\n    } else {\n      intUptos = intPool.buffers[p.intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = p.intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(p);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"741ed634ca00f7fcf06280bd2bf3f7eb9b605cc9","date":1269379515,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","sourceNew":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int code = textStart;\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate RawPostingList in hash\n    int termID = postingsHash[hashPos];\n\n    if (termID != -1 && postingsArray.textStarts[termID] != textStart) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        termID = postingsHash[hashPos];\n      } while (termID != -1 && postingsArray.textStarts[termID] != textStart);\n    }\n\n    if (termID == -1) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n\n      // New posting\n      termID = numPostings++;\n      if (termID >= postingsArray.textStarts.length) {\n        growParallelPostingsArray();\n      }\n      if (perThread.termsHash.trackAllocations) {\n        perThread.termsHash.docWriter.bytesUsed(bytesPerPosting);\n      }\n\n      assert termID >= 0;\n\n      postingsArray.textStarts[termID] = textStart;\n          \n      assert postingsHash[hashPos] == -1;\n      postingsHash[hashPos] = termID;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","sourceOld":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n\n    int code = textStart;\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate RawPostingList in hash\n    p = postingsHash[hashPos];\n\n    if (p != null && p.textStart != textStart) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        p = postingsHash[hashPos];\n      } while (p != null && p.textStart != textStart);\n    }\n\n    if (p == null) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n\n      // Refill?\n      if (0 == perThread.freePostingsCount)\n        perThread.morePostings();\n\n      // Pull next free RawPostingList from free list\n      p = perThread.freePostings[--perThread.freePostingsCount];\n      assert p != null;\n\n      p.textStart = textStart;\n          \n      assert postingsHash[hashPos] == null;\n      postingsHash[hashPos] = p;\n      numPostings++;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      p.intStart = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      p.byteStart = intUptos[intUptoStart];\n\n      consumer.newTerm(p);\n\n    } else {\n      intUptos = intPool.buffers[p.intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = p.intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(p);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","sourceNew":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int code = textStart;\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate RawPostingList in hash\n    int termID = postingsHash[hashPos];\n\n    if (termID != -1 && postingsArray.textStarts[termID] != textStart) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        termID = postingsHash[hashPos];\n      } while (termID != -1 && postingsArray.textStarts[termID] != textStart);\n    }\n\n    if (termID == -1) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n\n      // New posting\n      termID = numPostings++;\n      if (termID >= postingsArray.size) {\n        growParallelPostingsArray();\n      }\n\n      assert termID >= 0;\n\n      postingsArray.textStarts[termID] = textStart;\n          \n      assert postingsHash[hashPos] == -1;\n      postingsHash[hashPos] = termID;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","sourceOld":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int code = textStart;\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate RawPostingList in hash\n    int termID = postingsHash[hashPos];\n\n    if (termID != -1 && postingsArray.textStarts[termID] != textStart) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        termID = postingsHash[hashPos];\n      } while (termID != -1 && postingsArray.textStarts[termID] != textStart);\n    }\n\n    if (termID == -1) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n\n      // New posting\n      termID = numPostings++;\n      if (termID >= postingsArray.textStarts.length) {\n        growParallelPostingsArray();\n      }\n      if (perThread.termsHash.trackAllocations) {\n        perThread.termsHash.docWriter.bytesUsed(bytesPerPosting);\n      }\n\n      assert termID >= 0;\n\n      postingsArray.textStarts[termID] = textStart;\n          \n      assert postingsHash[hashPos] == -1;\n      postingsHash[hashPos] = termID;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6c18273ea5b3974d2f30117f46f1ae416c28f727","date":1279708040,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","sourceNew":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int code = textStart;\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate RawPostingList in hash\n    int termID = postingsHash[hashPos];\n\n    if (termID != -1 && postingsArray.textStarts[termID] != textStart) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        termID = postingsHash[hashPos];\n      } while (termID != -1 && postingsArray.textStarts[termID] != textStart);\n    }\n\n    if (termID == -1) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n\n      // New posting\n      termID = numPostings++;\n      if (termID >= postingsArray.size) {\n        growParallelPostingsArray();\n      }\n\n      assert termID >= 0;\n\n      postingsArray.textStarts[termID] = textStart;\n          \n      assert postingsHash[hashPos] == -1;\n      postingsHash[hashPos] = termID;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterRAMAllocator.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriterRAMAllocator.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterRAMAllocator.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterRAMAllocator.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","sourceOld":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int code = textStart;\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate RawPostingList in hash\n    int termID = postingsHash[hashPos];\n\n    if (termID != -1 && postingsArray.textStarts[termID] != textStart) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        termID = postingsHash[hashPos];\n      } while (termID != -1 && postingsArray.textStarts[termID] != textStart);\n    }\n\n    if (termID == -1) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n\n      // New posting\n      termID = numPostings++;\n      if (termID >= postingsArray.size) {\n        growParallelPostingsArray();\n      }\n\n      assert termID >= 0;\n\n      postingsArray.textStarts[termID] = textStart;\n          \n      assert postingsHash[hashPos] == -1;\n      postingsHash[hashPos] = termID;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5d6c52f55ea3ba9a5b1d5a6dd17f79bc7d308392","date":1286023472,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","sourceNew":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","sourceOld":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int code = textStart;\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate RawPostingList in hash\n    int termID = postingsHash[hashPos];\n\n    if (termID != -1 && postingsArray.textStarts[termID] != textStart) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        termID = postingsHash[hashPos];\n      } while (termID != -1 && postingsArray.textStarts[termID] != textStart);\n    }\n\n    if (termID == -1) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n\n      // New posting\n      termID = numPostings++;\n      if (termID >= postingsArray.size) {\n        growParallelPostingsArray();\n      }\n\n      assert termID >= 0;\n\n      postingsArray.textStarts[termID] = textStart;\n          \n      assert postingsHash[hashPos] == -1;\n      postingsHash[hashPos] = termID;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","sourceNew":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","sourceOld":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int code = textStart;\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate RawPostingList in hash\n    int termID = postingsHash[hashPos];\n\n    if (termID != -1 && postingsArray.textStarts[termID] != textStart) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        termID = postingsHash[hashPos];\n      } while (termID != -1 && postingsArray.textStarts[termID] != textStart);\n    }\n\n    if (termID == -1) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n\n      // New posting\n      termID = numPostings++;\n      if (termID >= postingsArray.size) {\n        growParallelPostingsArray();\n      }\n\n      assert termID >= 0;\n\n      postingsArray.textStarts[termID] = textStart;\n          \n      assert postingsHash[hashPos] == -1;\n      postingsHash[hashPos] = termID;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterRAMAllocator.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriterRAMAllocator.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterRAMAllocator.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterRAMAllocator.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1089766b983c692d45e705f83536e912ab8025fa","date":1304002785,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","sourceNew":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","sourceOld":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","sourceNew":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","sourceOld":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","sourceNew":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","sourceOld":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","sourceNew":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","sourceOld":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","sourceNew":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","sourceOld":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n    int termID = bytesHash.addByPoolOffset(textStart);\n    if (termID >= 0) {      // New posting\n      // First time we are seeing this token since we last\n      // flushed the hash.\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriterPerThread.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) {\n        bytePool.nextBuffer();\n      }\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      postingsArray.byteStarts[termID] = intUptos[intUptoStart];\n\n      consumer.newTerm(termID);\n\n    } else {\n      termID = (-termID)-1;\n      int intStart = postingsArray.intStarts[termID];\n      intUptos = intPool.buffers[intStart >> DocumentsWriterPerThread.INT_BLOCK_SHIFT];\n      intUptoStart = intStart & DocumentsWriterPerThread.INT_BLOCK_MASK;\n      consumer.addTerm(termID);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"5d6c52f55ea3ba9a5b1d5a6dd17f79bc7d308392":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"6c18273ea5b3974d2f30117f46f1ae416c28f727":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["5d6c52f55ea3ba9a5b1d5a6dd17f79bc7d308392","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"741ed634ca00f7fcf06280bd2bf3f7eb9b605cc9":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["741ed634ca00f7fcf06280bd2bf3f7eb9b605cc9"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["5d6c52f55ea3ba9a5b1d5a6dd17f79bc7d308392","1089766b983c692d45e705f83536e912ab8025fa"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["6c18273ea5b3974d2f30117f46f1ae416c28f727","5d6c52f55ea3ba9a5b1d5a6dd17f79bc7d308392"],"a3776dccca01c11e7046323cfad46a3b4a471233":["5d6c52f55ea3ba9a5b1d5a6dd17f79bc7d308392","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1089766b983c692d45e705f83536e912ab8025fa":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"5d6c52f55ea3ba9a5b1d5a6dd17f79bc7d308392":["135621f3a0670a9394eb563224a3b76cc4dddc0f","b3e06be49006ecac364d39d12b9c9f74882f9b9f","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","a3776dccca01c11e7046323cfad46a3b4a471233"],"6c18273ea5b3974d2f30117f46f1ae416c28f727":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"741ed634ca00f7fcf06280bd2bf3f7eb9b605cc9":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["5d6c52f55ea3ba9a5b1d5a6dd17f79bc7d308392","6c18273ea5b3974d2f30117f46f1ae416c28f727"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["1089766b983c692d45e705f83536e912ab8025fa"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"1089766b983c692d45e705f83536e912ab8025fa":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["741ed634ca00f7fcf06280bd2bf3f7eb9b605cc9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}