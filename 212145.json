{"path":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","commits":[{"id":"6c18273ea5b3974d2f30117f46f1ae416c28f727","date":1279708040,"type":1,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    if (tvx != null) {\n\n      if (state.numDocsInStore > 0)\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocsInStore - docWriter.getDocStoreOffset());\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","sourceOld":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n\n    if (tvx != null) {\n\n      if (state.numDocsInStore > 0)\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocsInStore - docWriter.getDocStoreOffset());\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"833a7987bc1c94455fde83e3311f72bddedcfb93","date":1279951470,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    if (tvx != null) {\n\n      if (state.numDocs > 0) {\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocs);\n      }\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n      \n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName))\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n\n      String fldName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_FIELDS_EXTENSION);\n      String docName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n      state.flushedFiles.add(idxName);\n      state.flushedFiles.add(fldName);\n      state.flushedFiles.add(docName);\n\n      docWriter.removeOpenFile(idxName);\n      docWriter.removeOpenFile(fldName);\n      docWriter.removeOpenFile(docName);\n\n      lastDocID = 0;\n\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","sourceOld":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    if (tvx != null) {\n\n      if (state.numDocsInStore > 0)\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocsInStore - docWriter.getDocStoreOffset());\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","bugFix":null,"bugIntro":["5ef87af8c7bd0f8429622b83aa74202383f2e757"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5ef87af8c7bd0f8429622b83aa74202383f2e757","date":1280262785,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    if (tvx != null) {\n\n      if (state.numDocs > 0) {\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocs);\n      }\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n      \n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName))\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n\n      String fldName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_FIELDS_EXTENSION);\n      String docName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n      state.flushedFiles.add(idxName);\n      state.flushedFiles.add(fldName);\n      state.flushedFiles.add(docName);\n\n      lastDocID = 0;\n\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","sourceOld":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    if (tvx != null) {\n\n      if (state.numDocs > 0) {\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocs);\n      }\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n      \n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName))\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n\n      String fldName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_FIELDS_EXTENSION);\n      String docName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n      state.flushedFiles.add(idxName);\n      state.flushedFiles.add(fldName);\n      state.flushedFiles.add(docName);\n\n      docWriter.removeOpenFile(idxName);\n      docWriter.removeOpenFile(fldName);\n      docWriter.removeOpenFile(docName);\n\n      lastDocID = 0;\n\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","bugFix":["833a7987bc1c94455fde83e3311f72bddedcfb93"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      String fldName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_FIELDS_EXTENSION);\n      String docName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName))\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n\n      state.flushedFiles.add(idxName);\n      state.flushedFiles.add(fldName);\n      state.flushedFiles.add(docName);\n\n      lastDocID = 0;\n      state.hasVectors = hasVectors;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","sourceOld":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n\n    if (tvx != null) {\n\n      if (state.numDocs > 0) {\n        // In case there are some final documents that we\n        // didn't see (because they hit a non-aborting exception):\n        fill(state.numDocs);\n      }\n\n      tvx.flush();\n      tvd.flush();\n      tvf.flush();\n      \n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName))\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n\n      String fldName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_FIELDS_EXTENSION);\n      String docName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n      state.flushedFiles.add(idxName);\n      state.flushedFiles.add(fldName);\n      state.flushedFiles.add(docName);\n\n      lastDocID = 0;\n\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName))\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n\n      lastDocID = 0;\n      state.hasVectors = hasVectors;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","sourceOld":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      String fldName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_FIELDS_EXTENSION);\n      String docName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);\n\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName))\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n\n      state.flushedFiles.add(idxName);\n      state.flushedFiles.add(fldName);\n      state.flushedFiles.add(docName);\n\n      lastDocID = 0;\n      state.hasVectors = hasVectors;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"566fc65e31d76e8716a33f9b9add2370a3a62d3b","date":1304006590,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      state.hasVectors = hasVectors;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","sourceOld":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName))\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n\n      lastDocID = 0;\n      state.hasVectors = hasVectors;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":1,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[TermsHashConsumerPerThread,Collection[TermsHashConsumerPerField]],SegmentWriteState).mjava","sourceNew":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      state.hasVectors = hasVectors;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","sourceOld":"  @Override\n  synchronized void flush(Map<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> threadsAndFields, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      if (4 + ((long) state.numDocs) * 16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      state.hasVectors = hasVectors;\n      hasVectors = false;\n    }\n\n    for (Map.Entry<TermsHashConsumerPerThread,Collection<TermsHashConsumerPerField>> entry : threadsAndFields.entrySet()) {\n      for (final TermsHashConsumerPerField field : entry.getValue() ) {\n        TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n        perField.termsHashPerField.reset();\n        perField.shrinkHash();\n      }\n\n      TermVectorsTermsWriterPerThread perThread = (TermVectorsTermsWriterPerThread) entry.getKey();\n      perThread.termsHashPerThread.reset(true);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      state.hasVectors = hasVectors;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3fe2fc74577855eadfb5eae3153c2fffdaaf791","date":1305237079,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","sourceOld":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      state.hasVectors = hasVectors;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","sourceOld":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      state.hasVectors = hasVectors;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"efb7a19703a037c29e30440260d393500febc1f4","date":1306648116,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      IOUtils.closeSafely(false, tvx, tvf, tvd);\n      tvx = tvd = tvf = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","sourceOld":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","date":1306767085,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      IOUtils.closeSafely(false, tvx, tvf, tvd);\n      tvx = tvd = tvf = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","sourceOld":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e10cb22a8bdb44339e282925a29182bb2f3174d","date":1306841137,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      IOUtils.closeSafely(false, tvx, tvf, tvd);\n      tvx = tvd = tvf = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","sourceOld":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      tvx.close();\n      tvf.close();\n      tvd.close();\n      tvx = tvd = tvf = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"24230fe54121f9be9d85f2c2067536296785e421","date":1314462346,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      IOUtils.close(tvx, tvf, tvd);\n      tvx = tvd = tvf = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","sourceOld":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      IOUtils.closeSafely(false, tvx, tvf, tvd);\n      tvx = tvd = tvf = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/TermVectorsConsumer#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter#flush(Map[FieldInfo,TermsHashConsumerPerField],SegmentWriteState).mjava","sourceNew":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (writer != null) {\n      // At least one doc in this run had term vectors enabled\n      try {\n        fill(state.numDocs);\n        assert state.segmentName != null;\n        writer.finish(state.numDocs);\n      } finally {\n        IOUtils.close(writer);\n        writer = null;\n\n        lastDocID = 0;\n        hasVectors = false;\n      }\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsConsumerPerField perField = (TermVectorsConsumerPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","sourceOld":"  @Override\n  void flush(Map<FieldInfo, TermsHashConsumerPerField> fieldsToFlush, final SegmentWriteState state) throws IOException {\n    if (tvx != null) {\n      // At least one doc in this run had term vectors enabled\n      fill(state.numDocs);\n      assert state.segmentName != null;\n      String idxName = IndexFileNames.segmentFileName(state.segmentName, \"\", IndexFileNames.VECTORS_INDEX_EXTENSION);\n      IOUtils.close(tvx, tvf, tvd);\n      tvx = tvd = tvf = null;\n      if (4+((long) state.numDocs)*16 != state.directory.fileLength(idxName)) {\n        throw new RuntimeException(\"after flush: tvx size mismatch: \" + state.numDocs + \" docs vs \" + state.directory.fileLength(idxName) + \" length in bytes of \" + idxName + \" file exists?=\" + state.directory.fileExists(idxName));\n      }\n\n      lastDocID = 0;\n      hasVectors = false;\n    }\n\n    for (final TermsHashConsumerPerField field : fieldsToFlush.values() ) {\n      TermVectorsTermsWriterPerField perField = (TermVectorsTermsWriterPerField) field;\n      perField.termsHashPerField.reset();\n      perField.shrinkHash();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"24230fe54121f9be9d85f2c2067536296785e421":["efb7a19703a037c29e30440260d393500febc1f4"],"566fc65e31d76e8716a33f9b9add2370a3a62d3b":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"d3fe2fc74577855eadfb5eae3153c2fffdaaf791":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"6c18273ea5b3974d2f30117f46f1ae416c28f727":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"efb7a19703a037c29e30440260d393500febc1f4":["d3fe2fc74577855eadfb5eae3153c2fffdaaf791"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"3cc749c053615f5871f3b95715fe292f34e70a53":["24230fe54121f9be9d85f2c2067536296785e421"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","566fc65e31d76e8716a33f9b9add2370a3a62d3b"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["5ef87af8c7bd0f8429622b83aa74202383f2e757","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["a3776dccca01c11e7046323cfad46a3b4a471233","efb7a19703a037c29e30440260d393500febc1f4"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","d3fe2fc74577855eadfb5eae3153c2fffdaaf791"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5ef87af8c7bd0f8429622b83aa74202383f2e757":["833a7987bc1c94455fde83e3311f72bddedcfb93"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["135621f3a0670a9394eb563224a3b76cc4dddc0f","d3fe2fc74577855eadfb5eae3153c2fffdaaf791"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3cc749c053615f5871f3b95715fe292f34e70a53"],"833a7987bc1c94455fde83e3311f72bddedcfb93":["6c18273ea5b3974d2f30117f46f1ae416c28f727"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb","efb7a19703a037c29e30440260d393500febc1f4"]},"commit2Childs":{"24230fe54121f9be9d85f2c2067536296785e421":["3cc749c053615f5871f3b95715fe292f34e70a53"],"566fc65e31d76e8716a33f9b9add2370a3a62d3b":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"d3fe2fc74577855eadfb5eae3153c2fffdaaf791":["efb7a19703a037c29e30440260d393500febc1f4","a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"6c18273ea5b3974d2f30117f46f1ae416c28f727":["833a7987bc1c94455fde83e3311f72bddedcfb93"],"efb7a19703a037c29e30440260d393500febc1f4":["24230fe54121f9be9d85f2c2067536296785e421","5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["d3fe2fc74577855eadfb5eae3153c2fffdaaf791","135621f3a0670a9394eb563224a3b76cc4dddc0f"],"3cc749c053615f5871f3b95715fe292f34e70a53":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":[],"a3776dccca01c11e7046323cfad46a3b4a471233":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["6c18273ea5b3974d2f30117f46f1ae416c28f727","135621f3a0670a9394eb563224a3b76cc4dddc0f","b3e06be49006ecac364d39d12b9c9f74882f9b9f","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","a3776dccca01c11e7046323cfad46a3b4a471233","868da859b43505d9d2a023bfeae6dd0c795f5295"],"5ef87af8c7bd0f8429622b83aa74202383f2e757":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["2e10cb22a8bdb44339e282925a29182bb2f3174d"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["566fc65e31d76e8716a33f9b9add2370a3a62d3b"],"833a7987bc1c94455fde83e3311f72bddedcfb93":["5ef87af8c7bd0f8429622b83aa74202383f2e757"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"2e10cb22a8bdb44339e282925a29182bb2f3174d":[]},"heads":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","cd5edd1f2b162a5cfa08efd17851a07373a96817","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}