{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#assertNeedsIndexSortMerge(SortField,Consumer[Document],Consumer[Document]).mjava","commits":[{"id":"653128722fb3b4713ac331c621491a93f34a4a22","date":1479841816,"type":0,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#assertNeedsIndexSortMerge(SortField,Consumer[Document],Consumer[Document]).mjava","pathOld":"/dev/null","sourceNew":"  private static void assertNeedsIndexSortMerge(SortField sortField, Consumer<Document> defaultValueConsumer, Consumer<Document> randomValueConsumer) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    AssertingNeedsIndexSortCodec codec = new AssertingNeedsIndexSortCodec();\n    iwc.setCodec(codec);\n    Sort indexSort = new Sort(sortField,\n        new SortField(\"id\", SortField.Type.INT));\n    iwc.setIndexSort(indexSort);\n    iwc.setMergePolicy(newLogMergePolicy());\n\n    // add already sorted documents\n    codec.numCalls = 0;\n    codec.needsIndexSort = false;\n    IndexWriter w = new IndexWriter(dir, iwc);\n    boolean withValues = random().nextBoolean();\n    for (int i = 100; i < 200; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      if (withValues) {\n        defaultValueConsumer.accept(doc);\n      }\n      w.addDocument(doc);\n      if (i % 10 == 0) {\n        w.commit();\n      }\n    }\n    Set<Integer> deletedDocs = new HashSet<> ();\n    int num = random().nextInt(20);\n    for (int i = 0; i < num; i++) {\n      int nextDoc = random().nextInt(100);\n      w.deleteDocuments(new Term(\"id\", Integer.toString(nextDoc)));\n      deletedDocs.add(nextDoc);\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n\n    // merge sort is needed\n    codec.numCalls = 0;\n    codec.needsIndexSort = true;\n    for (int i = 10; i >= 0; i--) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      if (withValues) {\n        defaultValueConsumer.accept(doc);\n      }\n      w.addDocument(doc);\n      w.commit();\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n    // segment sort is needed\n    codec.needsIndexSort = true;\n    codec.numCalls = 0;\n    for (int i = 200; i < 300; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      randomValueConsumer.accept(doc);\n      w.addDocument(doc);\n      if (i % 10 == 0) {\n        w.commit();\n      }\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["4556b0efd847b88322058ba45656d418e7dce68a"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4556b0efd847b88322058ba45656d418e7dce68a","date":1479993019,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#assertNeedsIndexSortMerge(SortField,Consumer[Document],Consumer[Document]).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#assertNeedsIndexSortMerge(SortField,Consumer[Document],Consumer[Document]).mjava","sourceNew":"  private static void assertNeedsIndexSortMerge(SortField sortField, Consumer<Document> defaultValueConsumer, Consumer<Document> randomValueConsumer) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    AssertingNeedsIndexSortCodec codec = new AssertingNeedsIndexSortCodec();\n    iwc.setCodec(codec);\n    Sort indexSort = new Sort(sortField,\n        new SortField(\"id\", SortField.Type.INT));\n    iwc.setIndexSort(indexSort);\n    LogMergePolicy policy = newLogMergePolicy();\n    // make sure that merge factor is always > 2\n    if (policy.getMergeFactor() <= 2) {\n      policy.setMergeFactor(3);\n    }\n    iwc.setMergePolicy(policy);\n\n    // add already sorted documents\n    codec.numCalls = 0;\n    codec.needsIndexSort = false;\n    IndexWriter w = new IndexWriter(dir, iwc);\n    boolean withValues = random().nextBoolean();\n    for (int i = 100; i < 200; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      if (withValues) {\n        defaultValueConsumer.accept(doc);\n      }\n      w.addDocument(doc);\n      if (i % 10 == 0) {\n        w.commit();\n      }\n    }\n    Set<Integer> deletedDocs = new HashSet<> ();\n    int num = random().nextInt(20);\n    for (int i = 0; i < num; i++) {\n      int nextDoc = random().nextInt(100);\n      w.deleteDocuments(new Term(\"id\", Integer.toString(nextDoc)));\n      deletedDocs.add(nextDoc);\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n\n    // merge sort is needed\n    codec.numCalls = 0;\n    codec.needsIndexSort = true;\n    for (int i = 10; i >= 0; i--) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      if (withValues) {\n        defaultValueConsumer.accept(doc);\n      }\n      w.addDocument(doc);\n      w.commit();\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n    // segment sort is needed\n    codec.needsIndexSort = true;\n    codec.numCalls = 0;\n    for (int i = 200; i < 300; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      randomValueConsumer.accept(doc);\n      w.addDocument(doc);\n      if (i % 10 == 0) {\n        w.commit();\n      }\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  private static void assertNeedsIndexSortMerge(SortField sortField, Consumer<Document> defaultValueConsumer, Consumer<Document> randomValueConsumer) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    AssertingNeedsIndexSortCodec codec = new AssertingNeedsIndexSortCodec();\n    iwc.setCodec(codec);\n    Sort indexSort = new Sort(sortField,\n        new SortField(\"id\", SortField.Type.INT));\n    iwc.setIndexSort(indexSort);\n    iwc.setMergePolicy(newLogMergePolicy());\n\n    // add already sorted documents\n    codec.numCalls = 0;\n    codec.needsIndexSort = false;\n    IndexWriter w = new IndexWriter(dir, iwc);\n    boolean withValues = random().nextBoolean();\n    for (int i = 100; i < 200; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      if (withValues) {\n        defaultValueConsumer.accept(doc);\n      }\n      w.addDocument(doc);\n      if (i % 10 == 0) {\n        w.commit();\n      }\n    }\n    Set<Integer> deletedDocs = new HashSet<> ();\n    int num = random().nextInt(20);\n    for (int i = 0; i < num; i++) {\n      int nextDoc = random().nextInt(100);\n      w.deleteDocuments(new Term(\"id\", Integer.toString(nextDoc)));\n      deletedDocs.add(nextDoc);\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n\n    // merge sort is needed\n    codec.numCalls = 0;\n    codec.needsIndexSort = true;\n    for (int i = 10; i >= 0; i--) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      if (withValues) {\n        defaultValueConsumer.accept(doc);\n      }\n      w.addDocument(doc);\n      w.commit();\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n    // segment sort is needed\n    codec.needsIndexSort = true;\n    codec.numCalls = 0;\n    for (int i = 200; i < 300; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      randomValueConsumer.accept(doc);\n      w.addDocument(doc);\n      if (i % 10 == 0) {\n        w.commit();\n      }\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":["653128722fb3b4713ac331c621491a93f34a4a22"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"927b161eb8a968101cf45f48d11ebe24beed054d","date":1480015309,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#assertNeedsIndexSortMerge(SortField,Consumer[Document],Consumer[Document]).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#assertNeedsIndexSortMerge(SortField,Consumer[Document],Consumer[Document]).mjava","sourceNew":"  private static void assertNeedsIndexSortMerge(SortField sortField, Consumer<Document> defaultValueConsumer, Consumer<Document> randomValueConsumer) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    AssertingNeedsIndexSortCodec codec = new AssertingNeedsIndexSortCodec();\n    iwc.setCodec(codec);\n    Sort indexSort = new Sort(sortField,\n        new SortField(\"id\", SortField.Type.INT));\n    iwc.setIndexSort(indexSort);\n    LogMergePolicy policy = newLogMergePolicy();\n    // make sure that merge factor is always > 2\n    if (policy.getMergeFactor() <= 2) {\n      policy.setMergeFactor(3);\n    }\n    iwc.setMergePolicy(policy);\n\n    // add already sorted documents\n    codec.numCalls = 0;\n    codec.needsIndexSort = false;\n    IndexWriter w = new IndexWriter(dir, iwc);\n    boolean withValues = random().nextBoolean();\n    for (int i = 100; i < 200; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      if (withValues) {\n        defaultValueConsumer.accept(doc);\n      }\n      w.addDocument(doc);\n      if (i % 10 == 0) {\n        w.commit();\n      }\n    }\n    Set<Integer> deletedDocs = new HashSet<> ();\n    int num = random().nextInt(20);\n    for (int i = 0; i < num; i++) {\n      int nextDoc = random().nextInt(100);\n      w.deleteDocuments(new Term(\"id\", Integer.toString(nextDoc)));\n      deletedDocs.add(nextDoc);\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n\n    // merge sort is needed\n    codec.numCalls = 0;\n    codec.needsIndexSort = true;\n    for (int i = 10; i >= 0; i--) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      if (withValues) {\n        defaultValueConsumer.accept(doc);\n      }\n      w.addDocument(doc);\n      w.commit();\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n    // segment sort is needed\n    codec.needsIndexSort = true;\n    codec.numCalls = 0;\n    for (int i = 200; i < 300; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      randomValueConsumer.accept(doc);\n      w.addDocument(doc);\n      if (i % 10 == 0) {\n        w.commit();\n      }\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  private static void assertNeedsIndexSortMerge(SortField sortField, Consumer<Document> defaultValueConsumer, Consumer<Document> randomValueConsumer) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    AssertingNeedsIndexSortCodec codec = new AssertingNeedsIndexSortCodec();\n    iwc.setCodec(codec);\n    Sort indexSort = new Sort(sortField,\n        new SortField(\"id\", SortField.Type.INT));\n    iwc.setIndexSort(indexSort);\n    iwc.setMergePolicy(newLogMergePolicy());\n\n    // add already sorted documents\n    codec.numCalls = 0;\n    codec.needsIndexSort = false;\n    IndexWriter w = new IndexWriter(dir, iwc);\n    boolean withValues = random().nextBoolean();\n    for (int i = 100; i < 200; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      if (withValues) {\n        defaultValueConsumer.accept(doc);\n      }\n      w.addDocument(doc);\n      if (i % 10 == 0) {\n        w.commit();\n      }\n    }\n    Set<Integer> deletedDocs = new HashSet<> ();\n    int num = random().nextInt(20);\n    for (int i = 0; i < num; i++) {\n      int nextDoc = random().nextInt(100);\n      w.deleteDocuments(new Term(\"id\", Integer.toString(nextDoc)));\n      deletedDocs.add(nextDoc);\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n\n    // merge sort is needed\n    codec.numCalls = 0;\n    codec.needsIndexSort = true;\n    for (int i = 10; i >= 0; i--) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      if (withValues) {\n        defaultValueConsumer.accept(doc);\n      }\n      w.addDocument(doc);\n      w.commit();\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n    // segment sort is needed\n    codec.needsIndexSort = true;\n    codec.numCalls = 0;\n    for (int i = 200; i < 300; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      randomValueConsumer.accept(doc);\n      w.addDocument(doc);\n      if (i % 10 == 0) {\n        w.commit();\n      }\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"727bb765ff2542275f6d31f67be18d7104bae148","date":1480353976,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#assertNeedsIndexSortMerge(SortField,Consumer[Document],Consumer[Document]).mjava","pathOld":"/dev/null","sourceNew":"  private static void assertNeedsIndexSortMerge(SortField sortField, Consumer<Document> defaultValueConsumer, Consumer<Document> randomValueConsumer) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    AssertingNeedsIndexSortCodec codec = new AssertingNeedsIndexSortCodec();\n    iwc.setCodec(codec);\n    Sort indexSort = new Sort(sortField,\n        new SortField(\"id\", SortField.Type.INT));\n    iwc.setIndexSort(indexSort);\n    LogMergePolicy policy = newLogMergePolicy();\n    // make sure that merge factor is always > 2\n    if (policy.getMergeFactor() <= 2) {\n      policy.setMergeFactor(3);\n    }\n    iwc.setMergePolicy(policy);\n\n    // add already sorted documents\n    codec.numCalls = 0;\n    codec.needsIndexSort = false;\n    IndexWriter w = new IndexWriter(dir, iwc);\n    boolean withValues = random().nextBoolean();\n    for (int i = 100; i < 200; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      if (withValues) {\n        defaultValueConsumer.accept(doc);\n      }\n      w.addDocument(doc);\n      if (i % 10 == 0) {\n        w.commit();\n      }\n    }\n    Set<Integer> deletedDocs = new HashSet<> ();\n    int num = random().nextInt(20);\n    for (int i = 0; i < num; i++) {\n      int nextDoc = random().nextInt(100);\n      w.deleteDocuments(new Term(\"id\", Integer.toString(nextDoc)));\n      deletedDocs.add(nextDoc);\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n\n    // merge sort is needed\n    codec.numCalls = 0;\n    codec.needsIndexSort = true;\n    for (int i = 10; i >= 0; i--) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      if (withValues) {\n        defaultValueConsumer.accept(doc);\n      }\n      w.addDocument(doc);\n      w.commit();\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n    // segment sort is needed\n    codec.needsIndexSort = true;\n    codec.numCalls = 0;\n    for (int i = 200; i < 300; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      randomValueConsumer.accept(doc);\n      w.addDocument(doc);\n      if (i % 10 == 0) {\n        w.commit();\n      }\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7de356f47e528530d3a127a900a35e166550ac3","date":1486472651,"type":3,"author":"Jim Ferenczi","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#assertNeedsIndexSortMerge(SortField,Consumer[Document],Consumer[Document]).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#assertNeedsIndexSortMerge(SortField,Consumer[Document],Consumer[Document]).mjava","sourceNew":"  private static void assertNeedsIndexSortMerge(SortField sortField, Consumer<Document> defaultValueConsumer, Consumer<Document> randomValueConsumer) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    AssertingNeedsIndexSortCodec codec = new AssertingNeedsIndexSortCodec();\n    iwc.setCodec(codec);\n    Sort indexSort = new Sort(sortField,\n        new SortField(\"id\", SortField.Type.INT));\n    iwc.setIndexSort(indexSort);\n    LogMergePolicy policy = newLogMergePolicy();\n    // make sure that merge factor is always > 2\n    if (policy.getMergeFactor() <= 2) {\n      policy.setMergeFactor(3);\n    }\n    iwc.setMergePolicy(policy);\n\n    // add already sorted documents\n    codec.numCalls = 0;\n    codec.needsIndexSort = false;\n    IndexWriter w = new IndexWriter(dir, iwc);\n    boolean withValues = random().nextBoolean();\n    for (int i = 100; i < 200; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      if (withValues) {\n        defaultValueConsumer.accept(doc);\n      }\n      w.addDocument(doc);\n      if (i % 10 == 0) {\n        w.commit();\n      }\n    }\n    Set<Integer> deletedDocs = new HashSet<> ();\n    int num = random().nextInt(20);\n    for (int i = 0; i < num; i++) {\n      int nextDoc = random().nextInt(100);\n      w.deleteDocuments(new Term(\"id\", Integer.toString(nextDoc)));\n      deletedDocs.add(nextDoc);\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n\n    // merge sort is needed\n    codec.numCalls = 0;\n    codec.needsIndexSort = true;\n    for (int i = 10; i >= 0; i--) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      if (withValues) {\n        defaultValueConsumer.accept(doc);\n      }\n      w.addDocument(doc);\n      w.commit();\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n    // segment sort is needed\n    codec.needsIndexSort = true;\n    codec.numCalls = 0;\n    for (int i = 201; i < 300; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      randomValueConsumer.accept(doc);\n      w.addDocument(doc);\n      if (i % 10 == 0) {\n        w.commit();\n      }\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  private static void assertNeedsIndexSortMerge(SortField sortField, Consumer<Document> defaultValueConsumer, Consumer<Document> randomValueConsumer) throws Exception {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    AssertingNeedsIndexSortCodec codec = new AssertingNeedsIndexSortCodec();\n    iwc.setCodec(codec);\n    Sort indexSort = new Sort(sortField,\n        new SortField(\"id\", SortField.Type.INT));\n    iwc.setIndexSort(indexSort);\n    LogMergePolicy policy = newLogMergePolicy();\n    // make sure that merge factor is always > 2\n    if (policy.getMergeFactor() <= 2) {\n      policy.setMergeFactor(3);\n    }\n    iwc.setMergePolicy(policy);\n\n    // add already sorted documents\n    codec.numCalls = 0;\n    codec.needsIndexSort = false;\n    IndexWriter w = new IndexWriter(dir, iwc);\n    boolean withValues = random().nextBoolean();\n    for (int i = 100; i < 200; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      if (withValues) {\n        defaultValueConsumer.accept(doc);\n      }\n      w.addDocument(doc);\n      if (i % 10 == 0) {\n        w.commit();\n      }\n    }\n    Set<Integer> deletedDocs = new HashSet<> ();\n    int num = random().nextInt(20);\n    for (int i = 0; i < num; i++) {\n      int nextDoc = random().nextInt(100);\n      w.deleteDocuments(new Term(\"id\", Integer.toString(nextDoc)));\n      deletedDocs.add(nextDoc);\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n\n    // merge sort is needed\n    codec.numCalls = 0;\n    codec.needsIndexSort = true;\n    for (int i = 10; i >= 0; i--) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      if (withValues) {\n        defaultValueConsumer.accept(doc);\n      }\n      w.addDocument(doc);\n      w.commit();\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n    // segment sort is needed\n    codec.needsIndexSort = true;\n    codec.numCalls = 0;\n    for (int i = 200; i < 300; i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(i), Store.YES));\n      doc.add(new NumericDocValuesField(\"id\", i));\n      doc.add(new IntPoint(\"point\", random().nextInt()));\n      randomValueConsumer.accept(doc);\n      w.addDocument(doc);\n      if (i % 10 == 0) {\n        w.commit();\n      }\n    }\n    w.commit();\n    w.waitForMerges();\n    w.forceMerge(1);\n    assertTrue(codec.numCalls > 0);\n\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"727bb765ff2542275f6d31f67be18d7104bae148":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","927b161eb8a968101cf45f48d11ebe24beed054d"],"927b161eb8a968101cf45f48d11ebe24beed054d":["653128722fb3b4713ac331c621491a93f34a4a22","4556b0efd847b88322058ba45656d418e7dce68a"],"4556b0efd847b88322058ba45656d418e7dce68a":["653128722fb3b4713ac331c621491a93f34a4a22"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e7de356f47e528530d3a127a900a35e166550ac3":["927b161eb8a968101cf45f48d11ebe24beed054d"],"653128722fb3b4713ac331c621491a93f34a4a22":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e7de356f47e528530d3a127a900a35e166550ac3"]},"commit2Childs":{"727bb765ff2542275f6d31f67be18d7104bae148":[],"927b161eb8a968101cf45f48d11ebe24beed054d":["727bb765ff2542275f6d31f67be18d7104bae148","e7de356f47e528530d3a127a900a35e166550ac3"],"4556b0efd847b88322058ba45656d418e7dce68a":["927b161eb8a968101cf45f48d11ebe24beed054d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["727bb765ff2542275f6d31f67be18d7104bae148","653128722fb3b4713ac331c621491a93f34a4a22"],"653128722fb3b4713ac331c621491a93f34a4a22":["927b161eb8a968101cf45f48d11ebe24beed054d","4556b0efd847b88322058ba45656d418e7dce68a"],"e7de356f47e528530d3a127a900a35e166550ac3":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["727bb765ff2542275f6d31f67be18d7104bae148","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}