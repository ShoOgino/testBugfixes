{"path":"lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/MappingCharFilter#read().mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/MappingCharFilter#read().mjava","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/charfilter/MappingCharFilter#read().mjava","sourceNew":"  @Override\n  public int read() throws IOException {\n    while(true) {\n      if (replacement != null && charPointer < replacement.length()) {\n        return replacement.charAt(charPointer++);\n      }\n\n      int firstChar = nextChar();\n      if (firstChar == -1) return -1;\n      NormalizeCharMap nm = normMap.submap != null ?\n        normMap.submap.get(Character.valueOf((char) firstChar)) : null;\n      if (nm == null) return firstChar;\n      NormalizeCharMap result = match(nm);\n      if (result == null) return firstChar;\n      replacement = result.normStr;\n      charPointer = 0;\n      if (result.diff != 0) {\n        int prevCumulativeDiff = getLastCumulativeDiff();\n        if (result.diff < 0) {\n          for(int i = 0; i < -result.diff ; i++)\n            addOffCorrectMap(nextCharCounter + i - prevCumulativeDiff, prevCumulativeDiff - 1 - i);\n        } else {\n          addOffCorrectMap(nextCharCounter - result.diff - prevCumulativeDiff, prevCumulativeDiff + result.diff);\n        }\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public int read() throws IOException {\n    while(true) {\n      if (replacement != null && charPointer < replacement.length()) {\n        return replacement.charAt(charPointer++);\n      }\n\n      int firstChar = nextChar();\n      if (firstChar == -1) return -1;\n      NormalizeCharMap nm = normMap.submap != null ?\n        normMap.submap.get(Character.valueOf((char) firstChar)) : null;\n      if (nm == null) return firstChar;\n      NormalizeCharMap result = match(nm);\n      if (result == null) return firstChar;\n      replacement = result.normStr;\n      charPointer = 0;\n      if (result.diff != 0) {\n        int prevCumulativeDiff = getLastCumulativeDiff();\n        if (result.diff < 0) {\n          for(int i = 0; i < -result.diff ; i++)\n            addOffCorrectMap(nextCharCounter + i - prevCumulativeDiff, prevCumulativeDiff - 1 - i);\n        } else {\n          addOffCorrectMap(nextCharCounter - result.diff - prevCumulativeDiff, prevCumulativeDiff + result.diff);\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b4e5bbc7f726dbcc466cb9b3c029d539a06f6545","date":1336310014,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/MappingCharFilter#read().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/MappingCharFilter#read().mjava","sourceNew":"  @Override\n  public int read() throws IOException {\n\n    //System.out.println(\"\\nread\");\n    while(true) {\n\n      if (replacement != null && replacementPointer < replacement.length) {\n        //System.out.println(\"  return repl[\" + replacementPointer + \"]=\" + replacement.chars[replacement.offset + replacementPointer]);\n        return replacement.chars[replacement.offset + replacementPointer++];\n      }\n\n      // TODO: a more efficient approach would be Aho/Corasick's\n      // algorithm\n      // (http://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_string_matching_algorithm)\n      // or this generalizatio: www.cis.uni-muenchen.de/people/Schulz/Pub/dictle5.ps\n      //\n      // I think this would be (almost?) equivalent to 1) adding\n      // epsilon arcs from all final nodes back to the init\n      // node in the FST, 2) adding a .* (skip any char)\n      // loop on the initial node, and 3) determinizing\n      // that.  Then we would not have to restart matching\n      // at each position.\n\n      int lastMatchLen = -1;\n      CharsRef lastMatch = null;\n\n      final int firstCH = buffer.get(inputOff);\n      if (firstCH != -1) {\n        FST.Arc<CharsRef> arc = cachedRootArcs.get(Character.valueOf((char) firstCH));\n        if (arc != null) {\n          if (!FST.targetHasArcs(arc)) {\n            // Fast pass for single character match:\n            assert arc.isFinal();\n            lastMatchLen = 1;\n            lastMatch = arc.output;\n          } else {\n            int lookahead = 0;\n            CharsRef output = arc.output;\n            while (true) {\n              lookahead++;\n\n              if (arc.isFinal()) {\n                // Match! (to node is final)\n                lastMatchLen = lookahead;\n                lastMatch = outputs.add(output, arc.nextFinalOutput);\n                // Greedy: keep searching to see if there's a\n                // longer match...\n              }\n\n              if (!FST.targetHasArcs(arc)) {\n                break;\n              }\n\n              int ch = buffer.get(inputOff + lookahead);\n              if (ch == -1) {\n                break;\n              }\n              if ((arc = map.findTargetArc(ch, arc, scratchArc, fstReader)) == null) {\n                // Dead end\n                break;\n              }\n              output = outputs.add(output, arc.output);\n            }\n          }\n        }\n      }\n\n      if (lastMatch != null) {\n        inputOff += lastMatchLen;\n        //System.out.println(\"  match!  len=\" + lastMatchLen + \" repl=\" + lastMatch);\n\n        final int diff = lastMatchLen - lastMatch.length;\n\n        if (diff != 0) {\n          final int prevCumulativeDiff = getLastCumulativeDiff();\n          if (diff > 0) {\n            // Replacement is shorter than matched input:\n            addOffCorrectMap(inputOff - diff - prevCumulativeDiff, prevCumulativeDiff + diff);\n          } else {\n            // Replacement is longer than matched input: remap\n            // the \"extra\" chars all back to the same input\n            // offset:\n            final int outputStart = inputOff - prevCumulativeDiff;\n            for(int extraIDX=0;extraIDX<-diff;extraIDX++) {\n              addOffCorrectMap(outputStart + extraIDX, prevCumulativeDiff - extraIDX - 1);\n            }\n          }\n        }\n\n        replacement = lastMatch;\n        replacementPointer = 0;\n\n      } else {\n        final int ret = buffer.get(inputOff);\n        if (ret != -1) {\n          inputOff++;\n          buffer.freeBefore(inputOff);\n        }\n        return ret;\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public int read() throws IOException {\n    while(true) {\n      if (replacement != null && charPointer < replacement.length()) {\n        return replacement.charAt(charPointer++);\n      }\n\n      int firstChar = nextChar();\n      if (firstChar == -1) return -1;\n      NormalizeCharMap nm = normMap.submap != null ?\n        normMap.submap.get(Character.valueOf((char) firstChar)) : null;\n      if (nm == null) return firstChar;\n      NormalizeCharMap result = match(nm);\n      if (result == null) return firstChar;\n      replacement = result.normStr;\n      charPointer = 0;\n      if (result.diff != 0) {\n        int prevCumulativeDiff = getLastCumulativeDiff();\n        if (result.diff < 0) {\n          for(int i = 0; i < -result.diff ; i++)\n            addOffCorrectMap(nextCharCounter + i - prevCumulativeDiff, prevCumulativeDiff - 1 - i);\n        } else {\n          addOffCorrectMap(nextCharCounter - result.diff - prevCumulativeDiff, prevCumulativeDiff + result.diff);\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54204c8a3ca26aeafd273139fc29baf70d0f6786","date":1564170395,"type":3,"author":"Michael Sokolov","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/MappingCharFilter#read().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/MappingCharFilter#read().mjava","sourceNew":"  @Override\n  public int read() throws IOException {\n\n    //System.out.println(\"\\nread\");\n    while(true) {\n\n      if (replacement != null && replacementPointer < replacement.length) {\n        //System.out.println(\"  return repl[\" + replacementPointer + \"]=\" + replacement.chars[replacement.offset + replacementPointer]);\n        return replacement.chars[replacement.offset + replacementPointer++];\n      }\n\n      // TODO: a more efficient approach would be Aho/Corasick's\n      // algorithm\n      // (http://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_string_matching_algorithm)\n      // or this generalizatio: www.cis.uni-muenchen.de/people/Schulz/Pub/dictle5.ps\n      //\n      // I think this would be (almost?) equivalent to 1) adding\n      // epsilon arcs from all final nodes back to the init\n      // node in the FST, 2) adding a .* (skip any char)\n      // loop on the initial node, and 3) determinizing\n      // that.  Then we would not have to restart matching\n      // at each position.\n\n      int lastMatchLen = -1;\n      CharsRef lastMatch = null;\n\n      final int firstCH = buffer.get(inputOff);\n      if (firstCH != -1) {\n        FST.Arc<CharsRef> arc = cachedRootArcs.get(Character.valueOf((char) firstCH));\n        if (arc != null) {\n          if (!FST.targetHasArcs(arc)) {\n            // Fast pass for single character match:\n            assert arc.isFinal();\n            lastMatchLen = 1;\n            lastMatch = arc.output();\n          } else {\n            int lookahead = 0;\n            CharsRef output = arc.output();\n            while (true) {\n              lookahead++;\n\n              if (arc.isFinal()) {\n                // Match! (to node is final)\n                lastMatchLen = lookahead;\n                lastMatch = outputs.add(output, arc.nextFinalOutput());\n                // Greedy: keep searching to see if there's a\n                // longer match...\n              }\n\n              if (!FST.targetHasArcs(arc)) {\n                break;\n              }\n\n              int ch = buffer.get(inputOff + lookahead);\n              if (ch == -1) {\n                break;\n              }\n              if ((arc = map.findTargetArc(ch, arc, scratchArc, fstReader)) == null) {\n                // Dead end\n                break;\n              }\n              output = outputs.add(output, arc.output());\n            }\n          }\n        }\n      }\n\n      if (lastMatch != null) {\n        inputOff += lastMatchLen;\n        //System.out.println(\"  match!  len=\" + lastMatchLen + \" repl=\" + lastMatch);\n\n        final int diff = lastMatchLen - lastMatch.length;\n\n        if (diff != 0) {\n          final int prevCumulativeDiff = getLastCumulativeDiff();\n          if (diff > 0) {\n            // Replacement is shorter than matched input:\n            addOffCorrectMap(inputOff - diff - prevCumulativeDiff, prevCumulativeDiff + diff);\n          } else {\n            // Replacement is longer than matched input: remap\n            // the \"extra\" chars all back to the same input\n            // offset:\n            final int outputStart = inputOff - prevCumulativeDiff;\n            for(int extraIDX=0;extraIDX<-diff;extraIDX++) {\n              addOffCorrectMap(outputStart + extraIDX, prevCumulativeDiff - extraIDX - 1);\n            }\n          }\n        }\n\n        replacement = lastMatch;\n        replacementPointer = 0;\n\n      } else {\n        final int ret = buffer.get(inputOff);\n        if (ret != -1) {\n          inputOff++;\n          buffer.freeBefore(inputOff);\n        }\n        return ret;\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public int read() throws IOException {\n\n    //System.out.println(\"\\nread\");\n    while(true) {\n\n      if (replacement != null && replacementPointer < replacement.length) {\n        //System.out.println(\"  return repl[\" + replacementPointer + \"]=\" + replacement.chars[replacement.offset + replacementPointer]);\n        return replacement.chars[replacement.offset + replacementPointer++];\n      }\n\n      // TODO: a more efficient approach would be Aho/Corasick's\n      // algorithm\n      // (http://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_string_matching_algorithm)\n      // or this generalizatio: www.cis.uni-muenchen.de/people/Schulz/Pub/dictle5.ps\n      //\n      // I think this would be (almost?) equivalent to 1) adding\n      // epsilon arcs from all final nodes back to the init\n      // node in the FST, 2) adding a .* (skip any char)\n      // loop on the initial node, and 3) determinizing\n      // that.  Then we would not have to restart matching\n      // at each position.\n\n      int lastMatchLen = -1;\n      CharsRef lastMatch = null;\n\n      final int firstCH = buffer.get(inputOff);\n      if (firstCH != -1) {\n        FST.Arc<CharsRef> arc = cachedRootArcs.get(Character.valueOf((char) firstCH));\n        if (arc != null) {\n          if (!FST.targetHasArcs(arc)) {\n            // Fast pass for single character match:\n            assert arc.isFinal();\n            lastMatchLen = 1;\n            lastMatch = arc.output;\n          } else {\n            int lookahead = 0;\n            CharsRef output = arc.output;\n            while (true) {\n              lookahead++;\n\n              if (arc.isFinal()) {\n                // Match! (to node is final)\n                lastMatchLen = lookahead;\n                lastMatch = outputs.add(output, arc.nextFinalOutput);\n                // Greedy: keep searching to see if there's a\n                // longer match...\n              }\n\n              if (!FST.targetHasArcs(arc)) {\n                break;\n              }\n\n              int ch = buffer.get(inputOff + lookahead);\n              if (ch == -1) {\n                break;\n              }\n              if ((arc = map.findTargetArc(ch, arc, scratchArc, fstReader)) == null) {\n                // Dead end\n                break;\n              }\n              output = outputs.add(output, arc.output);\n            }\n          }\n        }\n      }\n\n      if (lastMatch != null) {\n        inputOff += lastMatchLen;\n        //System.out.println(\"  match!  len=\" + lastMatchLen + \" repl=\" + lastMatch);\n\n        final int diff = lastMatchLen - lastMatch.length;\n\n        if (diff != 0) {\n          final int prevCumulativeDiff = getLastCumulativeDiff();\n          if (diff > 0) {\n            // Replacement is shorter than matched input:\n            addOffCorrectMap(inputOff - diff - prevCumulativeDiff, prevCumulativeDiff + diff);\n          } else {\n            // Replacement is longer than matched input: remap\n            // the \"extra\" chars all back to the same input\n            // offset:\n            final int outputStart = inputOff - prevCumulativeDiff;\n            for(int extraIDX=0;extraIDX<-diff;extraIDX++) {\n              addOffCorrectMap(outputStart + extraIDX, prevCumulativeDiff - extraIDX - 1);\n            }\n          }\n        }\n\n        replacement = lastMatch;\n        replacementPointer = 0;\n\n      } else {\n        final int ret = buffer.get(inputOff);\n        if (ret != -1) {\n          inputOff++;\n          buffer.freeBefore(inputOff);\n        }\n        return ret;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f8061ddd97f3352007d927dae445884a6f3d857b","date":1564988276,"type":3,"author":"Atri Sharma","isMerge":true,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/MappingCharFilter#read().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/MappingCharFilter#read().mjava","sourceNew":"  @Override\n  public int read() throws IOException {\n\n    //System.out.println(\"\\nread\");\n    while(true) {\n\n      if (replacement != null && replacementPointer < replacement.length) {\n        //System.out.println(\"  return repl[\" + replacementPointer + \"]=\" + replacement.chars[replacement.offset + replacementPointer]);\n        return replacement.chars[replacement.offset + replacementPointer++];\n      }\n\n      // TODO: a more efficient approach would be Aho/Corasick's\n      // algorithm\n      // (http://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_string_matching_algorithm)\n      // or this generalizatio: www.cis.uni-muenchen.de/people/Schulz/Pub/dictle5.ps\n      //\n      // I think this would be (almost?) equivalent to 1) adding\n      // epsilon arcs from all final nodes back to the init\n      // node in the FST, 2) adding a .* (skip any char)\n      // loop on the initial node, and 3) determinizing\n      // that.  Then we would not have to restart matching\n      // at each position.\n\n      int lastMatchLen = -1;\n      CharsRef lastMatch = null;\n\n      final int firstCH = buffer.get(inputOff);\n      if (firstCH != -1) {\n        FST.Arc<CharsRef> arc = cachedRootArcs.get(Character.valueOf((char) firstCH));\n        if (arc != null) {\n          if (!FST.targetHasArcs(arc)) {\n            // Fast pass for single character match:\n            assert arc.isFinal();\n            lastMatchLen = 1;\n            lastMatch = arc.output();\n          } else {\n            int lookahead = 0;\n            CharsRef output = arc.output();\n            while (true) {\n              lookahead++;\n\n              if (arc.isFinal()) {\n                // Match! (to node is final)\n                lastMatchLen = lookahead;\n                lastMatch = outputs.add(output, arc.nextFinalOutput());\n                // Greedy: keep searching to see if there's a\n                // longer match...\n              }\n\n              if (!FST.targetHasArcs(arc)) {\n                break;\n              }\n\n              int ch = buffer.get(inputOff + lookahead);\n              if (ch == -1) {\n                break;\n              }\n              if ((arc = map.findTargetArc(ch, arc, scratchArc, fstReader)) == null) {\n                // Dead end\n                break;\n              }\n              output = outputs.add(output, arc.output());\n            }\n          }\n        }\n      }\n\n      if (lastMatch != null) {\n        inputOff += lastMatchLen;\n        //System.out.println(\"  match!  len=\" + lastMatchLen + \" repl=\" + lastMatch);\n\n        final int diff = lastMatchLen - lastMatch.length;\n\n        if (diff != 0) {\n          final int prevCumulativeDiff = getLastCumulativeDiff();\n          if (diff > 0) {\n            // Replacement is shorter than matched input:\n            addOffCorrectMap(inputOff - diff - prevCumulativeDiff, prevCumulativeDiff + diff);\n          } else {\n            // Replacement is longer than matched input: remap\n            // the \"extra\" chars all back to the same input\n            // offset:\n            final int outputStart = inputOff - prevCumulativeDiff;\n            for(int extraIDX=0;extraIDX<-diff;extraIDX++) {\n              addOffCorrectMap(outputStart + extraIDX, prevCumulativeDiff - extraIDX - 1);\n            }\n          }\n        }\n\n        replacement = lastMatch;\n        replacementPointer = 0;\n\n      } else {\n        final int ret = buffer.get(inputOff);\n        if (ret != -1) {\n          inputOff++;\n          buffer.freeBefore(inputOff);\n        }\n        return ret;\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public int read() throws IOException {\n\n    //System.out.println(\"\\nread\");\n    while(true) {\n\n      if (replacement != null && replacementPointer < replacement.length) {\n        //System.out.println(\"  return repl[\" + replacementPointer + \"]=\" + replacement.chars[replacement.offset + replacementPointer]);\n        return replacement.chars[replacement.offset + replacementPointer++];\n      }\n\n      // TODO: a more efficient approach would be Aho/Corasick's\n      // algorithm\n      // (http://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_string_matching_algorithm)\n      // or this generalizatio: www.cis.uni-muenchen.de/people/Schulz/Pub/dictle5.ps\n      //\n      // I think this would be (almost?) equivalent to 1) adding\n      // epsilon arcs from all final nodes back to the init\n      // node in the FST, 2) adding a .* (skip any char)\n      // loop on the initial node, and 3) determinizing\n      // that.  Then we would not have to restart matching\n      // at each position.\n\n      int lastMatchLen = -1;\n      CharsRef lastMatch = null;\n\n      final int firstCH = buffer.get(inputOff);\n      if (firstCH != -1) {\n        FST.Arc<CharsRef> arc = cachedRootArcs.get(Character.valueOf((char) firstCH));\n        if (arc != null) {\n          if (!FST.targetHasArcs(arc)) {\n            // Fast pass for single character match:\n            assert arc.isFinal();\n            lastMatchLen = 1;\n            lastMatch = arc.output;\n          } else {\n            int lookahead = 0;\n            CharsRef output = arc.output;\n            while (true) {\n              lookahead++;\n\n              if (arc.isFinal()) {\n                // Match! (to node is final)\n                lastMatchLen = lookahead;\n                lastMatch = outputs.add(output, arc.nextFinalOutput);\n                // Greedy: keep searching to see if there's a\n                // longer match...\n              }\n\n              if (!FST.targetHasArcs(arc)) {\n                break;\n              }\n\n              int ch = buffer.get(inputOff + lookahead);\n              if (ch == -1) {\n                break;\n              }\n              if ((arc = map.findTargetArc(ch, arc, scratchArc, fstReader)) == null) {\n                // Dead end\n                break;\n              }\n              output = outputs.add(output, arc.output);\n            }\n          }\n        }\n      }\n\n      if (lastMatch != null) {\n        inputOff += lastMatchLen;\n        //System.out.println(\"  match!  len=\" + lastMatchLen + \" repl=\" + lastMatch);\n\n        final int diff = lastMatchLen - lastMatch.length;\n\n        if (diff != 0) {\n          final int prevCumulativeDiff = getLastCumulativeDiff();\n          if (diff > 0) {\n            // Replacement is shorter than matched input:\n            addOffCorrectMap(inputOff - diff - prevCumulativeDiff, prevCumulativeDiff + diff);\n          } else {\n            // Replacement is longer than matched input: remap\n            // the \"extra\" chars all back to the same input\n            // offset:\n            final int outputStart = inputOff - prevCumulativeDiff;\n            for(int extraIDX=0;extraIDX<-diff;extraIDX++) {\n              addOffCorrectMap(outputStart + extraIDX, prevCumulativeDiff - extraIDX - 1);\n            }\n          }\n        }\n\n        replacement = lastMatch;\n        replacementPointer = 0;\n\n      } else {\n        final int ret = buffer.get(inputOff);\n        if (ret != -1) {\n          inputOff++;\n          buffer.freeBefore(inputOff);\n        }\n        return ret;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"54204c8a3ca26aeafd273139fc29baf70d0f6786":["b4e5bbc7f726dbcc466cb9b3c029d539a06f6545"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"b4e5bbc7f726dbcc466cb9b3c029d539a06f6545":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f8061ddd97f3352007d927dae445884a6f3d857b":["b4e5bbc7f726dbcc466cb9b3c029d539a06f6545","54204c8a3ca26aeafd273139fc29baf70d0f6786"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["54204c8a3ca26aeafd273139fc29baf70d0f6786"]},"commit2Childs":{"54204c8a3ca26aeafd273139fc29baf70d0f6786":["f8061ddd97f3352007d927dae445884a6f3d857b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["b4e5bbc7f726dbcc466cb9b3c029d539a06f6545"],"b4e5bbc7f726dbcc466cb9b3c029d539a06f6545":["54204c8a3ca26aeafd273139fc29baf70d0f6786","f8061ddd97f3352007d927dae445884a6f3d857b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"f8061ddd97f3352007d927dae445884a6f3d857b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["f8061ddd97f3352007d927dae445884a6f3d857b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}