{"path":"lucene/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","commits":[{"id":"a0ae5e3ed1232483b7b8a014f175a5fe43595982","date":1324062192,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/values/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer, new DocValuesField(\"promote\"),\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2, new DocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer, new DocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    ReaderContext topReaderContext = reader.getTopReaderContext();\n    ReaderContext[] children = topReaderContext.children();\n    DocValues docValues = children[0].reader.docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer, new DocValuesField(\"promote\"),\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2, new DocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer, new DocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    ReaderContext topReaderContext = reader.getTopReaderContext();\n    ReaderContext[] children = topReaderContext.children();\n    DocValues docValues = children[0].reader.docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fa0f44f887719e97183771e977cfc4bfb485b766","date":1326668713,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    ReaderContext topReaderContext = reader.getTopReaderContext();\n    ReaderContext[] children = topReaderContext.children();\n    DocValues docValues = children[0].reader.docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer, new DocValuesField(\"promote\"),\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2, new DocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer, new DocValuesField(\"promote\"),\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    ReaderContext topReaderContext = reader.getTopReaderContext();\n    ReaderContext[] children = topReaderContext.children();\n    DocValues docValues = children[0].reader.docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["ff266254aa2c0b84006f8f3088ee25337661554d","1e59c344a45b9502f40ec44f5fe4e20ed2291dbe"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c99bc7b4d9f186ec629b191eeef99bc1ac4e1612","date":1327859675,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    ReaderContext topReaderContext = reader.getTopReaderContext();\n    AtomicReaderContext[] children = topReaderContext.leaves();\n    DocValues docValues = children[0].reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    ReaderContext topReaderContext = reader.getTopReaderContext();\n    ReaderContext[] children = topReaderContext.children();\n    DocValues docValues = children[0].reader.docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8493985e6883b3fa8231d172694d2aa3a85cb182","date":1327920390,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    AtomicReaderContext[] children = topReaderContext.leaves();\n    DocValues docValues = children[0].reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    ReaderContext topReaderContext = reader.getTopReaderContext();\n    AtomicReaderContext[] children = topReaderContext.leaves();\n    DocValues docValues = children[0].reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    AtomicReaderContext[] children = topReaderContext.leaves();\n    DocValues docValues = children[0].reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    ReaderContext topReaderContext = reader.getTopReaderContext();\n    ReaderContext[] children = topReaderContext.children();\n    DocValues docValues = children[0].reader.docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestTypePromotion#testMergeIncompatibleTypes().mjava","sourceNew":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    AtomicReaderContext[] children = topReaderContext.leaves();\n    DocValues docValues = children[0].reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testMergeIncompatibleTypes() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values\n    IndexWriter writer = new IndexWriter(dir, writerConfig);\n    int num_1 = atLeast(200);\n    int num_2 = atLeast(200);\n    long[] values = new long[num_1 + num_2];\n    index(writer,\n        randomValueType(INTEGERS, random), values, 0, num_1);\n    writer.commit();\n    \n    if (random.nextInt(4) == 0) {\n      // once in a while use addIndexes\n      Directory dir_2 = newDirectory() ;\n      IndexWriter writer_2 = new IndexWriter(dir_2,\n                       newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));\n      index(writer_2,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer_2.commit();\n      writer_2.close();\n      if (random.nextBoolean()) {\n        writer.addIndexes(dir_2);\n      } else {\n        // do a real merge here\n        IndexReader open = IndexReader.open(dir_2);\n        writer.addIndexes(open);\n        open.close();\n      }\n      dir_2.close();\n    } else {\n      index(writer,\n          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);\n      writer.commit();\n    }\n    writer.close();\n    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {\n      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)\n    }\n    writer = new IndexWriter(dir, writerConfig);\n    // now merge\n    writer.forceMerge(1);\n    writer.close();\n    DirectoryReader reader = DirectoryReader.open(dir);\n    assertEquals(1, reader.getSequentialSubReaders().length);\n    IndexReaderContext topReaderContext = reader.getTopReaderContext();\n    AtomicReaderContext[] children = topReaderContext.leaves();\n    DocValues docValues = children[0].reader().docValues(\"promote\");\n    assertNotNull(docValues);\n    assertValues(TestType.Byte, dir, values);\n    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"8493985e6883b3fa8231d172694d2aa3a85cb182":["c99bc7b4d9f186ec629b191eeef99bc1ac4e1612"],"c99bc7b4d9f186ec629b191eeef99bc1ac4e1612":["fa0f44f887719e97183771e977cfc4bfb485b766"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fa0f44f887719e97183771e977cfc4bfb485b766":["a0ae5e3ed1232483b7b8a014f175a5fe43595982"],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["fa0f44f887719e97183771e977cfc4bfb485b766","8493985e6883b3fa8231d172694d2aa3a85cb182"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"8493985e6883b3fa8231d172694d2aa3a85cb182":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"c99bc7b4d9f186ec629b191eeef99bc1ac4e1612":["8493985e6883b3fa8231d172694d2aa3a85cb182"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a0ae5e3ed1232483b7b8a014f175a5fe43595982"],"fa0f44f887719e97183771e977cfc4bfb485b766":["c99bc7b4d9f186ec629b191eeef99bc1ac4e1612","5cab9a86bd67202d20b6adc463008c8e982b070a"],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["fa0f44f887719e97183771e977cfc4bfb485b766"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}