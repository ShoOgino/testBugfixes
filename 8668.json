{"path":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorNumeric#calcFacets().mjava","commits":[{"id":"dd0759e8803a09424422a329163d5900f6b10c42","date":1431227616,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorNumeric#calcFacets().mjava","pathOld":"/dev/null","sourceNew":"  public SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n    if (freq.missing) {\n      missingSlot = numSlots++;\n    }\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n\n    // we set the countAcc first so it won't be created here\n    createAccs(fcontext.base.size(), numSlots);\n    setSortAcc(numSlots);\n    prepareForCollection();\n\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReader(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val == 0 && !docsWithField.get(segDoc)) {\n        // missing\n        if (missingSlot >= 0) {\n          numMissing++;\n          collect(segDoc, missingSlot);\n        }\n      } else {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        collect(segDoc, slot);\n\n        if (allBucketsSlot >= 0) {\n          collect(segDoc, allBucketsSlot);\n        }\n      }\n    }\n\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit > 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n\n    SimpleOrderedMap res = new SimpleOrderedMap();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap map = new SimpleOrderedMap(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      for (SlotAcc acc : accs) {\n        acc.setValues(allBuckets, allBucketsSlot);\n      }\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      // countAcc.setValues(missingBucket, missingSlot);\n      missingBucket.add(\"count\", numMissing);\n      for (SlotAcc acc : accs) {\n        acc.setValues(missingBucket, missingSlot);\n      }\n\n      if (freq.getSubFacets().size() > 0) {\n        // TODO: we can do better than this!\n        DocSet missingDocSet = null;\n        if (missingDocSet == null) {\n          missingDocSet = getFieldMissing(fcontext.searcher, fcontext.base, freq.field);\n        }\n        processSubs(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), missingDocSet);\n      }\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList bucketList = new ArrayList(collectCount);\n    res.add(\"buckets\", bucketList);\n\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      // add stats for this bucket\n      // TODO: this gets count from countAcc\n      // addStats(bucket, slotNum);\n      bucket.add(\"count\", table.counts[slotNum]);\n\n      for (SlotAcc acc : accs) {\n        acc.setValues(bucket, slotNum);\n      }\n\n      // handle sub-facets for this bucket\n      if (freq.getSubFacets().size() > 0) {\n        Query filter = sf.getType().getFieldQuery(null, sf, calc.formatValue(val));\n        processSubs(bucket, filter, fcontext.searcher.getDocSet(filter, fcontext.base) );\n      }\n\n      bucketList.add(bucket);\n    }\n\n\n\n    return res;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["b834e175d4d6b99680745b76417f082cfad6b76f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9e13d0d4d8b6dc352cb304974502b9a36c153f78","date":1436492687,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorNumeric#calcFacets().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorNumeric#calcFacets().mjava","sourceNew":"  public SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n    if (freq.missing) {\n      missingSlot = numSlots++;\n    }\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    if (freq.missing) {\n      // TODO: optimize case when missingSlot can be contiguous with other slots\n      missingAcc = new SpecialSlotAcc(fcontext, collectAcc, missingSlot, otherAccs, 1);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val == 0 && !docsWithField.get(segDoc)) {\n        if (missingAcc != null) {\n          missingAcc.collect(segDoc, -1);\n        }\n      } else {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit > 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n\n    SimpleOrderedMap res = new SimpleOrderedMap();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap map = new SimpleOrderedMap(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field));\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList bucketList = new ArrayList(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n\n\n    return res;\n  }\n\n","sourceOld":"  public SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n    if (freq.missing) {\n      missingSlot = numSlots++;\n    }\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n\n    // we set the countAcc first so it won't be created here\n    createAccs(fcontext.base.size(), numSlots);\n    setSortAcc(numSlots);\n    prepareForCollection();\n\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReader(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val == 0 && !docsWithField.get(segDoc)) {\n        // missing\n        if (missingSlot >= 0) {\n          numMissing++;\n          collect(segDoc, missingSlot);\n        }\n      } else {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        collect(segDoc, slot);\n\n        if (allBucketsSlot >= 0) {\n          collect(segDoc, allBucketsSlot);\n        }\n      }\n    }\n\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit > 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n\n    SimpleOrderedMap res = new SimpleOrderedMap();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap map = new SimpleOrderedMap(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      for (SlotAcc acc : accs) {\n        acc.setValues(allBuckets, allBucketsSlot);\n      }\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      // countAcc.setValues(missingBucket, missingSlot);\n      missingBucket.add(\"count\", numMissing);\n      for (SlotAcc acc : accs) {\n        acc.setValues(missingBucket, missingSlot);\n      }\n\n      if (freq.getSubFacets().size() > 0) {\n        // TODO: we can do better than this!\n        DocSet missingDocSet = null;\n        if (missingDocSet == null) {\n          missingDocSet = getFieldMissing(fcontext.searcher, fcontext.base, freq.field);\n        }\n        processSubs(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), missingDocSet);\n      }\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList bucketList = new ArrayList(collectCount);\n    res.add(\"buckets\", bucketList);\n\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      // add stats for this bucket\n      // TODO: this gets count from countAcc\n      // addStats(bucket, slotNum);\n      bucket.add(\"count\", table.counts[slotNum]);\n\n      for (SlotAcc acc : accs) {\n        acc.setValues(bucket, slotNum);\n      }\n\n      // handle sub-facets for this bucket\n      if (freq.getSubFacets().size() > 0) {\n        Query filter = sf.getType().getFieldQuery(null, sf, calc.formatValue(val));\n        processSubs(bucket, filter, fcontext.searcher.getDocSet(filter, fcontext.base) );\n      }\n\n      bucketList.add(bucket);\n    }\n\n\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"168f749bbf9022a1ba5fea29c54baa1c00883d1d","date":1437587676,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorNumeric#calcFacets().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorNumeric#calcFacets().mjava","sourceNew":"  public SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 && docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit > 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n\n    SimpleOrderedMap res = new SimpleOrderedMap();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap map = new SimpleOrderedMap(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList bucketList = new ArrayList(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n\n\n    return res;\n  }\n\n","sourceOld":"  public SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n    if (freq.missing) {\n      missingSlot = numSlots++;\n    }\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    if (freq.missing) {\n      // TODO: optimize case when missingSlot can be contiguous with other slots\n      missingAcc = new SpecialSlotAcc(fcontext, collectAcc, missingSlot, otherAccs, 1);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val == 0 && !docsWithField.get(segDoc)) {\n        if (missingAcc != null) {\n          missingAcc.collect(segDoc, -1);\n        }\n      } else {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit > 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n\n    SimpleOrderedMap res = new SimpleOrderedMap();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap map = new SimpleOrderedMap(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field));\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList bucketList = new ArrayList(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":["bc29c1aa000e45b2ce3f5f9b737bba360313e90a"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bc29c1aa000e45b2ce3f5f9b737bba360313e90a","date":1439476266,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorNumeric#calcFacets().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorNumeric#calcFacets().mjava","sourceNew":"  public SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 || docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit > 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n\n    SimpleOrderedMap res = new SimpleOrderedMap();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap map = new SimpleOrderedMap(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList bucketList = new ArrayList(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n\n\n    return res;\n  }\n\n","sourceOld":"  public SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 && docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit > 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n\n    SimpleOrderedMap res = new SimpleOrderedMap();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap map = new SimpleOrderedMap(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList bucketList = new ArrayList(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n\n\n    return res;\n  }\n\n","bugFix":["168f749bbf9022a1ba5fea29c54baa1c00883d1d"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b834e175d4d6b99680745b76417f082cfad6b76f","date":1445799013,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorNumeric#calcFacets().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorNumeric#calcFacets().mjava","sourceNew":"  public SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 || docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit >= 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n\n    SimpleOrderedMap res = new SimpleOrderedMap();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap map = new SimpleOrderedMap(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList bucketList = new ArrayList(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n\n\n    return res;\n  }\n\n","sourceOld":"  public SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 || docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit > 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n\n    SimpleOrderedMap res = new SimpleOrderedMap();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap map = new SimpleOrderedMap(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList bucketList = new ArrayList(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n\n\n    return res;\n  }\n\n","bugFix":["dd0759e8803a09424422a329163d5900f6b10c42"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1480a9d175b147e15628e77342c4175ad3fc4611","date":1452646257,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorNumeric#calcFacets().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorNumeric#calcFacets().mjava","sourceNew":"  public SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 || docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit >= 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n\n    SimpleOrderedMap res = new SimpleOrderedMap();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap map = new SimpleOrderedMap(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    FacetDebugInfo fdebug = fcontext.getDebugInfo();\n    if (fdebug != null) fdebug.putInfoItem(\"numBuckets\", new Long(numBuckets));\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList bucketList = new ArrayList(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n\n\n    return res;\n  }\n\n","sourceOld":"  public SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 || docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit >= 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n\n    SimpleOrderedMap res = new SimpleOrderedMap();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap map = new SimpleOrderedMap(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList bucketList = new ArrayList(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"79759974460bc59933cd169acc94f5c6b16368d5","date":1471318443,"type":5,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByHashNumeric#calcFacets().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorNumeric#calcFacets().mjava","sourceNew":"  private SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 || docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList<>(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit >= 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n    SimpleOrderedMap<Object> res = new SimpleOrderedMap<>();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap<Object> map = new SimpleOrderedMap<>(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    FacetDebugInfo fdebug = fcontext.getDebugInfo();\n    if (fdebug != null) fdebug.putInfoItem(\"numBuckets\", (long) numBuckets);\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList<SimpleOrderedMap> bucketList = new ArrayList<>(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 || docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit >= 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n\n    SimpleOrderedMap res = new SimpleOrderedMap();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap map = new SimpleOrderedMap(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    FacetDebugInfo fdebug = fcontext.getDebugInfo();\n    if (fdebug != null) fdebug.putInfoItem(\"numBuckets\", new Long(numBuckets));\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList bucketList = new ArrayList(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","date":1471496851,"type":5,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByHashNumeric#calcFacets().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorNumeric#calcFacets().mjava","sourceNew":"  private SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 || docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList<>(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit >= 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n    SimpleOrderedMap<Object> res = new SimpleOrderedMap<>();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap<Object> map = new SimpleOrderedMap<>(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    FacetDebugInfo fdebug = fcontext.getDebugInfo();\n    if (fdebug != null) fdebug.putInfoItem(\"numBuckets\", (long) numBuckets);\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList<SimpleOrderedMap> bucketList = new ArrayList<>(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 || docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit >= 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n\n    SimpleOrderedMap res = new SimpleOrderedMap();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap map = new SimpleOrderedMap(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    FacetDebugInfo fdebug = fcontext.getDebugInfo();\n    if (fdebug != null) fdebug.putInfoItem(\"numBuckets\", new Long(numBuckets));\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList bucketList = new ArrayList(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"403d05f7f8d69b65659157eff1bc1d2717f04c66","date":1471692961,"type":5,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByHashNumeric#calcFacets().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorNumeric#calcFacets().mjava","sourceNew":"  private SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 || docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList<>(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit >= 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n    SimpleOrderedMap<Object> res = new SimpleOrderedMap<>();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap<Object> map = new SimpleOrderedMap<>(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    FacetDebugInfo fdebug = fcontext.getDebugInfo();\n    if (fdebug != null) fdebug.putInfoItem(\"numBuckets\", (long) numBuckets);\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList<SimpleOrderedMap> bucketList = new ArrayList<>(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 || docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit >= 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n\n    SimpleOrderedMap res = new SimpleOrderedMap();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap map = new SimpleOrderedMap(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    FacetDebugInfo fdebug = fcontext.getDebugInfo();\n    if (fdebug != null) fdebug.putInfoItem(\"numBuckets\", new Long(numBuckets));\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList bucketList = new ArrayList(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorNumeric#calcFacets().mjava","sourceNew":null,"sourceOld":"  public SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 || docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit >= 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n\n    SimpleOrderedMap res = new SimpleOrderedMap();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap map = new SimpleOrderedMap(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    FacetDebugInfo fdebug = fcontext.getDebugInfo();\n    if (fdebug != null) fdebug.putInfoItem(\"numBuckets\", new Long(numBuckets));\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList bucketList = new ArrayList(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b834e175d4d6b99680745b76417f082cfad6b76f":["bc29c1aa000e45b2ce3f5f9b737bba360313e90a"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["1480a9d175b147e15628e77342c4175ad3fc4611","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"1480a9d175b147e15628e77342c4175ad3fc4611":["b834e175d4d6b99680745b76417f082cfad6b76f"],"9e13d0d4d8b6dc352cb304974502b9a36c153f78":["dd0759e8803a09424422a329163d5900f6b10c42"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["1480a9d175b147e15628e77342c4175ad3fc4611","79759974460bc59933cd169acc94f5c6b16368d5"],"dd0759e8803a09424422a329163d5900f6b10c42":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"79759974460bc59933cd169acc94f5c6b16368d5":["1480a9d175b147e15628e77342c4175ad3fc4611"],"bc29c1aa000e45b2ce3f5f9b737bba360313e90a":["168f749bbf9022a1ba5fea29c54baa1c00883d1d"],"168f749bbf9022a1ba5fea29c54baa1c00883d1d":["9e13d0d4d8b6dc352cb304974502b9a36c153f78"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["1480a9d175b147e15628e77342c4175ad3fc4611","403d05f7f8d69b65659157eff1bc1d2717f04c66"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["403d05f7f8d69b65659157eff1bc1d2717f04c66"]},"commit2Childs":{"b834e175d4d6b99680745b76417f082cfad6b76f":["1480a9d175b147e15628e77342c4175ad3fc4611"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"1480a9d175b147e15628e77342c4175ad3fc4611":["403d05f7f8d69b65659157eff1bc1d2717f04c66","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","79759974460bc59933cd169acc94f5c6b16368d5","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"9e13d0d4d8b6dc352cb304974502b9a36c153f78":["168f749bbf9022a1ba5fea29c54baa1c00883d1d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["dd0759e8803a09424422a329163d5900f6b10c42"],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"dd0759e8803a09424422a329163d5900f6b10c42":["9e13d0d4d8b6dc352cb304974502b9a36c153f78"],"79759974460bc59933cd169acc94f5c6b16368d5":["2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"bc29c1aa000e45b2ce3f5f9b737bba360313e90a":["b834e175d4d6b99680745b76417f082cfad6b76f"],"168f749bbf9022a1ba5fea29c54baa1c00883d1d":["bc29c1aa000e45b2ce3f5f9b737bba360313e90a"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}