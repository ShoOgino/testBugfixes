{"path":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#doTest().mjava","commits":[{"id":"92751ba9273251eab6a2e379ec42a1697a32ff96","date":1407954233,"type":0,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#doTest().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it garuntees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism for buildRandomPivot calls\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      ModifiableSolrParams pivotP = params(FACET,\"true\",\n                                           FACET_PIVOT, buildRandomPivot(fieldNames));\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_PIVOT, buildRandomPivot(fieldNames));\n      }\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3b122d93eefe1a868fa637f336c6a741dfc7ec07","date":1408050132,"type":3,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(30000); // TODO: why whould we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it garuntees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism for buildRandomPivot calls\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      ModifiableSolrParams pivotP = params(FACET,\"true\",\n                                           FACET_PIVOT, buildRandomPivot(fieldNames));\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_PIVOT, buildRandomPivot(fieldNames));\n      }\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it garuntees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism for buildRandomPivot calls\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      ModifiableSolrParams pivotP = params(FACET,\"true\",\n                                           FACET_PIVOT, buildRandomPivot(fieldNames));\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_PIVOT, buildRandomPivot(fieldNames));\n      }\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4c18a95e9168a30be855f5d5e7d61a863186fdc5","date":1415142581,"type":3,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n\n    sanityCheckAssertDoubles();\n\n    waitForThingsToLevelOut(30000); // TODO: why whould we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n      pivotP.add(FACET_PIVOT, buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotP.add(FACET_PIVOT, buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    waitForThingsToLevelOut(30000); // TODO: why whould we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it garuntees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism for buildRandomPivot calls\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      ModifiableSolrParams pivotP = params(FACET,\"true\",\n                                           FACET_PIVOT, buildRandomPivot(fieldNames));\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_PIVOT, buildRandomPivot(fieldNames));\n      }\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","bugFix":null,"bugIntro":["dbdfda95da145c32af2267b537c92481acc7a522"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"abb23fcc2461782ab204e61213240feb77d355aa","date":1422029612,"type":5,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#doTest().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n\n    sanityCheckAssertDoubles();\n\n    waitForThingsToLevelOut(30000); // TODO: why whould we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n      pivotP.add(FACET_PIVOT, buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotP.add(FACET_PIVOT, buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n\n    sanityCheckAssertDoubles();\n\n    waitForThingsToLevelOut(30000); // TODO: why whould we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n      pivotP.add(FACET_PIVOT, buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotP.add(FACET_PIVOT, buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"92751ba9273251eab6a2e379ec42a1697a32ff96":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"abb23fcc2461782ab204e61213240feb77d355aa":["4c18a95e9168a30be855f5d5e7d61a863186fdc5"],"4c18a95e9168a30be855f5d5e7d61a863186fdc5":["3b122d93eefe1a868fa637f336c6a741dfc7ec07"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3b122d93eefe1a868fa637f336c6a741dfc7ec07":["92751ba9273251eab6a2e379ec42a1697a32ff96"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["abb23fcc2461782ab204e61213240feb77d355aa"]},"commit2Childs":{"92751ba9273251eab6a2e379ec42a1697a32ff96":["3b122d93eefe1a868fa637f336c6a741dfc7ec07"],"abb23fcc2461782ab204e61213240feb77d355aa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4c18a95e9168a30be855f5d5e7d61a863186fdc5":["abb23fcc2461782ab204e61213240feb77d355aa"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["92751ba9273251eab6a2e379ec42a1697a32ff96"],"3b122d93eefe1a868fa637f336c6a741dfc7ec07":["4c18a95e9168a30be855f5d5e7d61a863186fdc5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}