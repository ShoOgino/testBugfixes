{"path":"backwards/src/test/org/apache/lucene/search/TestMultiSearcher#testEmptyIndex().mjava","commits":[{"id":"480d01e5b0ef8efb136d51670fec297ae5ae2c9c","date":1268821447,"type":0,"author":"Uwe Schindler","isMerge":false,"pathNew":"backwards/src/test/org/apache/lucene/search/TestMultiSearcher#testEmptyIndex().mjava","pathOld":"/dev/null","sourceNew":"    public void testEmptyIndex()\n        throws Exception\n    {\n        // creating two directories for indices\n        Directory indexStoreA = new MockRAMDirectory();\n        Directory indexStoreB = new MockRAMDirectory();\n\n        // creating a document to store\n        Document lDoc = new Document();\n        lDoc.add(new Field(\"fulltext\", \"Once upon a time.....\", Field.Store.YES, Field.Index.ANALYZED));\n        lDoc.add(new Field(\"id\", \"doc1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n        lDoc.add(new Field(\"handle\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n\n        // creating a document to store\n        Document lDoc2 = new Document();\n        lDoc2.add(new Field(\"fulltext\", \"in a galaxy far far away.....\",\n            Field.Store.YES, Field.Index.ANALYZED));\n        lDoc2.add(new Field(\"id\", \"doc2\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n        lDoc2.add(new Field(\"handle\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n\n        // creating a document to store\n        Document lDoc3 = new Document();\n        lDoc3.add(new Field(\"fulltext\", \"a bizarre bug manifested itself....\",\n            Field.Store.YES, Field.Index.ANALYZED));\n        lDoc3.add(new Field(\"id\", \"doc3\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n        lDoc3.add(new Field(\"handle\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n\n        // creating an index writer for the first index\n        IndexWriter writerA = new IndexWriter(indexStoreA, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n        // creating an index writer for the second index, but writing nothing\n        IndexWriter writerB = new IndexWriter(indexStoreB, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n\n        //--------------------------------------------------------------------\n        // scenario 1\n        //--------------------------------------------------------------------\n\n        // writing the documents to the first index\n        writerA.addDocument(lDoc);\n        writerA.addDocument(lDoc2);\n        writerA.addDocument(lDoc3);\n        writerA.optimize();\n        writerA.close();\n\n        // closing the second index\n        writerB.close();\n\n        // creating the query\n        QueryParser parser = new QueryParser(Version.LUCENE_CURRENT, \"fulltext\", new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));\n        Query query = parser.parse(\"handle:1\");\n\n        // building the searchables\n        Searcher[] searchers = new Searcher[2];\n        // VITAL STEP:adding the searcher for the empty index first, before the searcher for the populated index\n        searchers[0] = new IndexSearcher(indexStoreB, true);\n        searchers[1] = new IndexSearcher(indexStoreA, true);\n        // creating the multiSearcher\n        Searcher mSearcher = getMultiSearcherInstance(searchers);\n        // performing the search\n        ScoreDoc[] hits = mSearcher.search(query, null, 1000).scoreDocs;\n\n        assertEquals(3, hits.length);\n\n        // iterating over the hit documents\n        for (int i = 0; i < hits.length; i++) {\n          mSearcher.doc(hits[i].doc);\n        }\n        mSearcher.close();\n\n\n        //--------------------------------------------------------------------\n        // scenario 2\n        //--------------------------------------------------------------------\n\n        // adding one document to the empty index\n        writerB = new IndexWriter(indexStoreB, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);\n        writerB.addDocument(lDoc);\n        writerB.optimize();\n        writerB.close();\n\n        // building the searchables\n        Searcher[] searchers2 = new Searcher[2];\n        // VITAL STEP:adding the searcher for the empty index first, before the searcher for the populated index\n        searchers2[0] = new IndexSearcher(indexStoreB, true);\n        searchers2[1] = new IndexSearcher(indexStoreA, true);\n        // creating the mulitSearcher\n        MultiSearcher mSearcher2 = getMultiSearcherInstance(searchers2);\n        // performing the same search\n        ScoreDoc[] hits2 = mSearcher2.search(query, null, 1000).scoreDocs;\n\n        assertEquals(4, hits2.length);\n\n        // iterating over the hit documents\n        for (int i = 0; i < hits2.length; i++) {\n          // no exception should happen at this point\n          mSearcher2.doc(hits2[i].doc);\n        }\n\n        // test the subSearcher() method:\n        Query subSearcherQuery = parser.parse(\"id:doc1\");\n        hits2 = mSearcher2.search(subSearcherQuery, null, 1000).scoreDocs;\n        assertEquals(2, hits2.length);\n        assertEquals(0, mSearcher2.subSearcher(hits2[0].doc));   // hit from searchers2[0]\n        assertEquals(1, mSearcher2.subSearcher(hits2[1].doc));   // hit from searchers2[1]\n        subSearcherQuery = parser.parse(\"id:doc2\");\n        hits2 = mSearcher2.search(subSearcherQuery, null, 1000).scoreDocs;\n        assertEquals(1, hits2.length);\n        assertEquals(1, mSearcher2.subSearcher(hits2[0].doc));   // hit from searchers2[1]\n        mSearcher2.close();\n\n        //--------------------------------------------------------------------\n        // scenario 3\n        //--------------------------------------------------------------------\n\n        // deleting the document just added, this will cause a different exception to take place\n        Term term = new Term(\"id\", \"doc1\");\n        IndexReader readerB = IndexReader.open(indexStoreB, false);\n        readerB.deleteDocuments(term);\n        readerB.close();\n\n        // optimizing the index with the writer\n        writerB = new IndexWriter(indexStoreB, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);\n        writerB.optimize();\n        writerB.close();\n\n        // building the searchables\n        Searcher[] searchers3 = new Searcher[2];\n\n        searchers3[0] = new IndexSearcher(indexStoreB, true);\n        searchers3[1] = new IndexSearcher(indexStoreA, true);\n        // creating the mulitSearcher\n        Searcher mSearcher3 = getMultiSearcherInstance(searchers3);\n        // performing the same search\n        ScoreDoc[] hits3 = mSearcher3.search(query, null, 1000).scoreDocs;\n\n        assertEquals(3, hits3.length);\n\n        // iterating over the hit documents\n        for (int i = 0; i < hits3.length; i++) {\n          mSearcher3.doc(hits3[i].doc);\n        }\n        mSearcher3.close();\n        indexStoreA.close();\n        indexStoreB.close();\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/backwards/src/test/org/apache/lucene/search/TestMultiSearcher#testEmptyIndex().mjava","pathOld":"backwards/src/test/org/apache/lucene/search/TestMultiSearcher#testEmptyIndex().mjava","sourceNew":"    public void testEmptyIndex()\n        throws Exception\n    {\n        // creating two directories for indices\n        Directory indexStoreA = new MockRAMDirectory();\n        Directory indexStoreB = new MockRAMDirectory();\n\n        // creating a document to store\n        Document lDoc = new Document();\n        lDoc.add(new Field(\"fulltext\", \"Once upon a time.....\", Field.Store.YES, Field.Index.ANALYZED));\n        lDoc.add(new Field(\"id\", \"doc1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n        lDoc.add(new Field(\"handle\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n\n        // creating a document to store\n        Document lDoc2 = new Document();\n        lDoc2.add(new Field(\"fulltext\", \"in a galaxy far far away.....\",\n            Field.Store.YES, Field.Index.ANALYZED));\n        lDoc2.add(new Field(\"id\", \"doc2\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n        lDoc2.add(new Field(\"handle\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n\n        // creating a document to store\n        Document lDoc3 = new Document();\n        lDoc3.add(new Field(\"fulltext\", \"a bizarre bug manifested itself....\",\n            Field.Store.YES, Field.Index.ANALYZED));\n        lDoc3.add(new Field(\"id\", \"doc3\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n        lDoc3.add(new Field(\"handle\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n\n        // creating an index writer for the first index\n        IndexWriter writerA = new IndexWriter(indexStoreA, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n        // creating an index writer for the second index, but writing nothing\n        IndexWriter writerB = new IndexWriter(indexStoreB, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n\n        //--------------------------------------------------------------------\n        // scenario 1\n        //--------------------------------------------------------------------\n\n        // writing the documents to the first index\n        writerA.addDocument(lDoc);\n        writerA.addDocument(lDoc2);\n        writerA.addDocument(lDoc3);\n        writerA.optimize();\n        writerA.close();\n\n        // closing the second index\n        writerB.close();\n\n        // creating the query\n        QueryParser parser = new QueryParser(Version.LUCENE_CURRENT, \"fulltext\", new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));\n        Query query = parser.parse(\"handle:1\");\n\n        // building the searchables\n        Searcher[] searchers = new Searcher[2];\n        // VITAL STEP:adding the searcher for the empty index first, before the searcher for the populated index\n        searchers[0] = new IndexSearcher(indexStoreB, true);\n        searchers[1] = new IndexSearcher(indexStoreA, true);\n        // creating the multiSearcher\n        Searcher mSearcher = getMultiSearcherInstance(searchers);\n        // performing the search\n        ScoreDoc[] hits = mSearcher.search(query, null, 1000).scoreDocs;\n\n        assertEquals(3, hits.length);\n\n        // iterating over the hit documents\n        for (int i = 0; i < hits.length; i++) {\n          mSearcher.doc(hits[i].doc);\n        }\n        mSearcher.close();\n\n\n        //--------------------------------------------------------------------\n        // scenario 2\n        //--------------------------------------------------------------------\n\n        // adding one document to the empty index\n        writerB = new IndexWriter(indexStoreB, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);\n        writerB.addDocument(lDoc);\n        writerB.optimize();\n        writerB.close();\n\n        // building the searchables\n        Searcher[] searchers2 = new Searcher[2];\n        // VITAL STEP:adding the searcher for the empty index first, before the searcher for the populated index\n        searchers2[0] = new IndexSearcher(indexStoreB, true);\n        searchers2[1] = new IndexSearcher(indexStoreA, true);\n        // creating the mulitSearcher\n        MultiSearcher mSearcher2 = getMultiSearcherInstance(searchers2);\n        // performing the same search\n        ScoreDoc[] hits2 = mSearcher2.search(query, null, 1000).scoreDocs;\n\n        assertEquals(4, hits2.length);\n\n        // iterating over the hit documents\n        for (int i = 0; i < hits2.length; i++) {\n          // no exception should happen at this point\n          mSearcher2.doc(hits2[i].doc);\n        }\n\n        // test the subSearcher() method:\n        Query subSearcherQuery = parser.parse(\"id:doc1\");\n        hits2 = mSearcher2.search(subSearcherQuery, null, 1000).scoreDocs;\n        assertEquals(2, hits2.length);\n        assertEquals(0, mSearcher2.subSearcher(hits2[0].doc));   // hit from searchers2[0]\n        assertEquals(1, mSearcher2.subSearcher(hits2[1].doc));   // hit from searchers2[1]\n        subSearcherQuery = parser.parse(\"id:doc2\");\n        hits2 = mSearcher2.search(subSearcherQuery, null, 1000).scoreDocs;\n        assertEquals(1, hits2.length);\n        assertEquals(1, mSearcher2.subSearcher(hits2[0].doc));   // hit from searchers2[1]\n        mSearcher2.close();\n\n        //--------------------------------------------------------------------\n        // scenario 3\n        //--------------------------------------------------------------------\n\n        // deleting the document just added, this will cause a different exception to take place\n        Term term = new Term(\"id\", \"doc1\");\n        IndexReader readerB = IndexReader.open(indexStoreB, false);\n        readerB.deleteDocuments(term);\n        readerB.close();\n\n        // optimizing the index with the writer\n        writerB = new IndexWriter(indexStoreB, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);\n        writerB.optimize();\n        writerB.close();\n\n        // building the searchables\n        Searcher[] searchers3 = new Searcher[2];\n\n        searchers3[0] = new IndexSearcher(indexStoreB, true);\n        searchers3[1] = new IndexSearcher(indexStoreA, true);\n        // creating the mulitSearcher\n        Searcher mSearcher3 = getMultiSearcherInstance(searchers3);\n        // performing the same search\n        ScoreDoc[] hits3 = mSearcher3.search(query, null, 1000).scoreDocs;\n\n        assertEquals(3, hits3.length);\n\n        // iterating over the hit documents\n        for (int i = 0; i < hits3.length; i++) {\n          mSearcher3.doc(hits3[i].doc);\n        }\n        mSearcher3.close();\n        indexStoreA.close();\n        indexStoreB.close();\n    }\n\n","sourceOld":"    public void testEmptyIndex()\n        throws Exception\n    {\n        // creating two directories for indices\n        Directory indexStoreA = new MockRAMDirectory();\n        Directory indexStoreB = new MockRAMDirectory();\n\n        // creating a document to store\n        Document lDoc = new Document();\n        lDoc.add(new Field(\"fulltext\", \"Once upon a time.....\", Field.Store.YES, Field.Index.ANALYZED));\n        lDoc.add(new Field(\"id\", \"doc1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n        lDoc.add(new Field(\"handle\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n\n        // creating a document to store\n        Document lDoc2 = new Document();\n        lDoc2.add(new Field(\"fulltext\", \"in a galaxy far far away.....\",\n            Field.Store.YES, Field.Index.ANALYZED));\n        lDoc2.add(new Field(\"id\", \"doc2\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n        lDoc2.add(new Field(\"handle\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n\n        // creating a document to store\n        Document lDoc3 = new Document();\n        lDoc3.add(new Field(\"fulltext\", \"a bizarre bug manifested itself....\",\n            Field.Store.YES, Field.Index.ANALYZED));\n        lDoc3.add(new Field(\"id\", \"doc3\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n        lDoc3.add(new Field(\"handle\", \"1\", Field.Store.YES, Field.Index.NOT_ANALYZED));\n\n        // creating an index writer for the first index\n        IndexWriter writerA = new IndexWriter(indexStoreA, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n        // creating an index writer for the second index, but writing nothing\n        IndexWriter writerB = new IndexWriter(indexStoreB, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n\n        //--------------------------------------------------------------------\n        // scenario 1\n        //--------------------------------------------------------------------\n\n        // writing the documents to the first index\n        writerA.addDocument(lDoc);\n        writerA.addDocument(lDoc2);\n        writerA.addDocument(lDoc3);\n        writerA.optimize();\n        writerA.close();\n\n        // closing the second index\n        writerB.close();\n\n        // creating the query\n        QueryParser parser = new QueryParser(Version.LUCENE_CURRENT, \"fulltext\", new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT));\n        Query query = parser.parse(\"handle:1\");\n\n        // building the searchables\n        Searcher[] searchers = new Searcher[2];\n        // VITAL STEP:adding the searcher for the empty index first, before the searcher for the populated index\n        searchers[0] = new IndexSearcher(indexStoreB, true);\n        searchers[1] = new IndexSearcher(indexStoreA, true);\n        // creating the multiSearcher\n        Searcher mSearcher = getMultiSearcherInstance(searchers);\n        // performing the search\n        ScoreDoc[] hits = mSearcher.search(query, null, 1000).scoreDocs;\n\n        assertEquals(3, hits.length);\n\n        // iterating over the hit documents\n        for (int i = 0; i < hits.length; i++) {\n          mSearcher.doc(hits[i].doc);\n        }\n        mSearcher.close();\n\n\n        //--------------------------------------------------------------------\n        // scenario 2\n        //--------------------------------------------------------------------\n\n        // adding one document to the empty index\n        writerB = new IndexWriter(indexStoreB, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);\n        writerB.addDocument(lDoc);\n        writerB.optimize();\n        writerB.close();\n\n        // building the searchables\n        Searcher[] searchers2 = new Searcher[2];\n        // VITAL STEP:adding the searcher for the empty index first, before the searcher for the populated index\n        searchers2[0] = new IndexSearcher(indexStoreB, true);\n        searchers2[1] = new IndexSearcher(indexStoreA, true);\n        // creating the mulitSearcher\n        MultiSearcher mSearcher2 = getMultiSearcherInstance(searchers2);\n        // performing the same search\n        ScoreDoc[] hits2 = mSearcher2.search(query, null, 1000).scoreDocs;\n\n        assertEquals(4, hits2.length);\n\n        // iterating over the hit documents\n        for (int i = 0; i < hits2.length; i++) {\n          // no exception should happen at this point\n          mSearcher2.doc(hits2[i].doc);\n        }\n\n        // test the subSearcher() method:\n        Query subSearcherQuery = parser.parse(\"id:doc1\");\n        hits2 = mSearcher2.search(subSearcherQuery, null, 1000).scoreDocs;\n        assertEquals(2, hits2.length);\n        assertEquals(0, mSearcher2.subSearcher(hits2[0].doc));   // hit from searchers2[0]\n        assertEquals(1, mSearcher2.subSearcher(hits2[1].doc));   // hit from searchers2[1]\n        subSearcherQuery = parser.parse(\"id:doc2\");\n        hits2 = mSearcher2.search(subSearcherQuery, null, 1000).scoreDocs;\n        assertEquals(1, hits2.length);\n        assertEquals(1, mSearcher2.subSearcher(hits2[0].doc));   // hit from searchers2[1]\n        mSearcher2.close();\n\n        //--------------------------------------------------------------------\n        // scenario 3\n        //--------------------------------------------------------------------\n\n        // deleting the document just added, this will cause a different exception to take place\n        Term term = new Term(\"id\", \"doc1\");\n        IndexReader readerB = IndexReader.open(indexStoreB, false);\n        readerB.deleteDocuments(term);\n        readerB.close();\n\n        // optimizing the index with the writer\n        writerB = new IndexWriter(indexStoreB, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), false, IndexWriter.MaxFieldLength.LIMITED);\n        writerB.optimize();\n        writerB.close();\n\n        // building the searchables\n        Searcher[] searchers3 = new Searcher[2];\n\n        searchers3[0] = new IndexSearcher(indexStoreB, true);\n        searchers3[1] = new IndexSearcher(indexStoreA, true);\n        // creating the mulitSearcher\n        Searcher mSearcher3 = getMultiSearcherInstance(searchers3);\n        // performing the same search\n        ScoreDoc[] hits3 = mSearcher3.search(query, null, 1000).scoreDocs;\n\n        assertEquals(3, hits3.length);\n\n        // iterating over the hit documents\n        for (int i = 0; i < hits3.length; i++) {\n          mSearcher3.doc(hits3[i].doc);\n        }\n        mSearcher3.close();\n        indexStoreA.close();\n        indexStoreB.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"480d01e5b0ef8efb136d51670fec297ae5ae2c9c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["480d01e5b0ef8efb136d51670fec297ae5ae2c9c"]},"commit2Childs":{"480d01e5b0ef8efb136d51670fec297ae5ae2c9c":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["480d01e5b0ef8efb136d51670fec297ae5ae2c9c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}