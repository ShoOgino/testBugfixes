{"path":"src/java/org/apache/lucene/index/IndexWriter#flush(boolean,boolean).mjava","commits":[{"id":"4350b17bd363cd13a95171b8df1ca62ea4c3e71c","date":1183562198,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#flush(boolean,boolean).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Flush all in-memory buffered updates (adds and deletes)\n   * to the Directory. \n   * <p>Note: if <code>autoCommit=false</code>, flushed data would still \n   * not be visible to readers, until {@link #close} is called.\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public final synchronized void flush(boolean triggerMerge, boolean flushDocStores) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    // Make sure no threads are actively adding a document\n    docWriter.pauseAllThreads();\n\n    try {\n\n      SegmentInfo newSegment = null;\n\n      final int numDocs = docWriter.getNumDocsInRAM();\n\n      // Always flush docs if there are any\n      boolean flushDocs = numDocs > 0;\n\n      // With autoCommit=true we always must flush the doc\n      // stores when we flush\n      flushDocStores |= autoCommit;\n      String docStoreSegment = docWriter.getDocStoreSegment();\n      if (docStoreSegment == null)\n        flushDocStores = false;\n\n      // Always flush deletes if there are any delete terms.\n      // TODO: when autoCommit=false we don't have to flush\n      // deletes with every flushed segment; we can save\n      // CPU/IO by buffering longer & flushing deletes only\n      // when they are full or writer is being closed.  We\n      // have to fix the \"applyDeletesSelectively\" logic to\n      // apply to more than just the last flushed segment\n      boolean flushDeletes = bufferedDeleteTerms.size() > 0;\n\n      if (infoStream != null)\n        infoStream.println(\"  flush: flushDocs=\" + flushDocs +\n                           \" flushDeletes=\" + flushDeletes +\n                           \" flushDocStores=\" + flushDocStores +\n                           \" numDocs=\" + numDocs);\n\n      int docStoreOffset = docWriter.getDocStoreOffset();\n      boolean docStoreIsCompoundFile = false;\n\n      // Check if the doc stores must be separately flushed\n      // because other segments, besides the one we are about\n      // to flush, reference it\n      if (flushDocStores && (!flushDocs || !docWriter.getSegment().equals(docWriter.getDocStoreSegment()))) {\n        // We must separately flush the doc store\n        if (infoStream != null)\n          infoStream.println(\"  flush shared docStore segment \" + docStoreSegment);\n      \n        flushDocStores();\n        flushDocStores = false;\n        docStoreIsCompoundFile = useCompoundFile;\n      }\n\n      String segment = docWriter.getSegment();\n\n      if (flushDocs || flushDeletes) {\n\n        SegmentInfos rollback = null;\n\n        if (flushDeletes)\n          rollback = (SegmentInfos) segmentInfos.clone();\n\n        boolean success = false;\n\n        try {\n          if (flushDocs) {\n\n            if (0 == docStoreOffset && flushDocStores) {\n              // This means we are flushing private doc stores\n              // with this segment, so it will not be shared\n              // with other segments\n              assert docStoreSegment != null;\n              assert docStoreSegment.equals(segment);\n              docStoreOffset = -1;\n              docStoreIsCompoundFile = false;\n              docStoreSegment = null;\n            }\n\n            int flushedDocCount = docWriter.flush(flushDocStores);\n          \n            newSegment = new SegmentInfo(segment,\n                                         flushedDocCount,\n                                         directory, false, true,\n                                         docStoreOffset, docStoreSegment,\n                                         docStoreIsCompoundFile);\n            segmentInfos.addElement(newSegment);\n          }\n\n          if (flushDeletes) {\n            // we should be able to change this so we can\n            // buffer deletes longer and then flush them to\n            // multiple flushed segments, when\n            // autoCommit=false\n            applyDeletes(flushDocs);\n            doAfterFlush();\n          }\n\n          checkpoint();\n          success = true;\n        } finally {\n          if (!success) {\n            if (flushDeletes) {\n              // Fully replace the segmentInfos since flushed\n              // deletes could have changed any of the\n              // SegmentInfo instances:\n              segmentInfos.clear();\n              segmentInfos.addAll(rollback);\n            } else {\n              // Remove segment we added, if any:\n              if (newSegment != null && \n                  segmentInfos.size() > 0 && \n                  segmentInfos.info(segmentInfos.size()-1) == newSegment)\n                segmentInfos.remove(segmentInfos.size()-1);\n            }\n            if (flushDocs)\n              docWriter.abort();\n            deleter.checkpoint(segmentInfos, false);\n            deleter.refresh();\n          }\n        }\n\n        deleter.checkpoint(segmentInfos, autoCommit);\n\n        if (flushDocs && useCompoundFile) {\n          success = false;\n          try {\n            docWriter.createCompoundFile(segment);\n            newSegment.setUseCompoundFile(true);\n            checkpoint();\n            success = true;\n          } finally {\n            if (!success) {\n              newSegment.setUseCompoundFile(false);\n              deleter.refresh();\n            }\n          }\n\n          deleter.checkpoint(segmentInfos, autoCommit);\n        }\n\n        /* new merge policy\n        if (0 == docWriter.getMaxBufferedDocs())\n          maybeMergeSegments(mergeFactor * numDocs / 2);\n        else\n          maybeMergeSegments(docWriter.getMaxBufferedDocs());\n        */\n        maybeMergeSegments(docWriter.getMaxBufferedDocs());\n      }\n    } finally {\n      docWriter.clearFlushPending();\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["fde68de507dbf344495d7b5e8052866fe5f254ab"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a30c6ea5c2c1da6d2956e42721e02ccb29910be5","date":1184278520,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#flush(boolean,boolean).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#flush(boolean,boolean).mjava","sourceNew":"  /**\n   * Flush all in-memory buffered udpates (adds and deletes)\n   * to the Directory.\n   * @param triggerMerge if true, we may merge segments (if\n   *  deletes or docs were flushed) if necessary\n   * @param flushDocStores if false we are allowed to keep\n   *  doc stores open to share with the next segment\n   */\n  protected final synchronized void flush(boolean triggerMerge, boolean flushDocStores) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    // Make sure no threads are actively adding a document\n    docWriter.pauseAllThreads();\n\n    try {\n\n      SegmentInfo newSegment = null;\n\n      final int numDocs = docWriter.getNumDocsInRAM();\n\n      // Always flush docs if there are any\n      boolean flushDocs = numDocs > 0;\n\n      // With autoCommit=true we always must flush the doc\n      // stores when we flush\n      flushDocStores |= autoCommit;\n      String docStoreSegment = docWriter.getDocStoreSegment();\n      if (docStoreSegment == null)\n        flushDocStores = false;\n\n      // Always flush deletes if there are any delete terms.\n      // TODO: when autoCommit=false we don't have to flush\n      // deletes with every flushed segment; we can save\n      // CPU/IO by buffering longer & flushing deletes only\n      // when they are full or writer is being closed.  We\n      // have to fix the \"applyDeletesSelectively\" logic to\n      // apply to more than just the last flushed segment\n      boolean flushDeletes = bufferedDeleteTerms.size() > 0;\n\n      if (infoStream != null)\n        infoStream.println(\"  flush: flushDocs=\" + flushDocs +\n                           \" flushDeletes=\" + flushDeletes +\n                           \" flushDocStores=\" + flushDocStores +\n                           \" numDocs=\" + numDocs);\n\n      int docStoreOffset = docWriter.getDocStoreOffset();\n      boolean docStoreIsCompoundFile = false;\n\n      // Check if the doc stores must be separately flushed\n      // because other segments, besides the one we are about\n      // to flush, reference it\n      if (flushDocStores && (!flushDocs || !docWriter.getSegment().equals(docWriter.getDocStoreSegment()))) {\n        // We must separately flush the doc store\n        if (infoStream != null)\n          infoStream.println(\"  flush shared docStore segment \" + docStoreSegment);\n      \n        flushDocStores();\n        flushDocStores = false;\n        docStoreIsCompoundFile = useCompoundFile;\n      }\n\n      String segment = docWriter.getSegment();\n\n      if (flushDocs || flushDeletes) {\n\n        SegmentInfos rollback = null;\n\n        if (flushDeletes)\n          rollback = (SegmentInfos) segmentInfos.clone();\n\n        boolean success = false;\n\n        try {\n          if (flushDocs) {\n\n            if (0 == docStoreOffset && flushDocStores) {\n              // This means we are flushing private doc stores\n              // with this segment, so it will not be shared\n              // with other segments\n              assert docStoreSegment != null;\n              assert docStoreSegment.equals(segment);\n              docStoreOffset = -1;\n              docStoreIsCompoundFile = false;\n              docStoreSegment = null;\n            }\n\n            int flushedDocCount = docWriter.flush(flushDocStores);\n          \n            newSegment = new SegmentInfo(segment,\n                                         flushedDocCount,\n                                         directory, false, true,\n                                         docStoreOffset, docStoreSegment,\n                                         docStoreIsCompoundFile);\n            segmentInfos.addElement(newSegment);\n          }\n\n          if (flushDeletes) {\n            // we should be able to change this so we can\n            // buffer deletes longer and then flush them to\n            // multiple flushed segments, when\n            // autoCommit=false\n            applyDeletes(flushDocs);\n            doAfterFlush();\n          }\n\n          checkpoint();\n          success = true;\n        } finally {\n          if (!success) {\n            if (flushDeletes) {\n              // Fully replace the segmentInfos since flushed\n              // deletes could have changed any of the\n              // SegmentInfo instances:\n              segmentInfos.clear();\n              segmentInfos.addAll(rollback);\n            } else {\n              // Remove segment we added, if any:\n              if (newSegment != null && \n                  segmentInfos.size() > 0 && \n                  segmentInfos.info(segmentInfos.size()-1) == newSegment)\n                segmentInfos.remove(segmentInfos.size()-1);\n            }\n            if (flushDocs)\n              docWriter.abort();\n            deleter.checkpoint(segmentInfos, false);\n            deleter.refresh();\n          }\n        }\n\n        deleter.checkpoint(segmentInfos, autoCommit);\n\n        if (flushDocs && useCompoundFile) {\n          success = false;\n          try {\n            docWriter.createCompoundFile(segment);\n            newSegment.setUseCompoundFile(true);\n            checkpoint();\n            success = true;\n          } finally {\n            if (!success) {\n              newSegment.setUseCompoundFile(false);\n              deleter.refresh();\n            }\n          }\n\n          deleter.checkpoint(segmentInfos, autoCommit);\n        }\n\n        /* new merge policy\n        if (0 == docWriter.getMaxBufferedDocs())\n          maybeMergeSegments(mergeFactor * numDocs / 2);\n        else\n          maybeMergeSegments(docWriter.getMaxBufferedDocs());\n        */\n        if (triggerMerge)\n          maybeMergeSegments(docWriter.getMaxBufferedDocs());\n      }\n    } finally {\n      docWriter.clearFlushPending();\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /**\n   * Flush all in-memory buffered updates (adds and deletes)\n   * to the Directory. \n   * <p>Note: if <code>autoCommit=false</code>, flushed data would still \n   * not be visible to readers, until {@link #close} is called.\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public final synchronized void flush(boolean triggerMerge, boolean flushDocStores) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    // Make sure no threads are actively adding a document\n    docWriter.pauseAllThreads();\n\n    try {\n\n      SegmentInfo newSegment = null;\n\n      final int numDocs = docWriter.getNumDocsInRAM();\n\n      // Always flush docs if there are any\n      boolean flushDocs = numDocs > 0;\n\n      // With autoCommit=true we always must flush the doc\n      // stores when we flush\n      flushDocStores |= autoCommit;\n      String docStoreSegment = docWriter.getDocStoreSegment();\n      if (docStoreSegment == null)\n        flushDocStores = false;\n\n      // Always flush deletes if there are any delete terms.\n      // TODO: when autoCommit=false we don't have to flush\n      // deletes with every flushed segment; we can save\n      // CPU/IO by buffering longer & flushing deletes only\n      // when they are full or writer is being closed.  We\n      // have to fix the \"applyDeletesSelectively\" logic to\n      // apply to more than just the last flushed segment\n      boolean flushDeletes = bufferedDeleteTerms.size() > 0;\n\n      if (infoStream != null)\n        infoStream.println(\"  flush: flushDocs=\" + flushDocs +\n                           \" flushDeletes=\" + flushDeletes +\n                           \" flushDocStores=\" + flushDocStores +\n                           \" numDocs=\" + numDocs);\n\n      int docStoreOffset = docWriter.getDocStoreOffset();\n      boolean docStoreIsCompoundFile = false;\n\n      // Check if the doc stores must be separately flushed\n      // because other segments, besides the one we are about\n      // to flush, reference it\n      if (flushDocStores && (!flushDocs || !docWriter.getSegment().equals(docWriter.getDocStoreSegment()))) {\n        // We must separately flush the doc store\n        if (infoStream != null)\n          infoStream.println(\"  flush shared docStore segment \" + docStoreSegment);\n      \n        flushDocStores();\n        flushDocStores = false;\n        docStoreIsCompoundFile = useCompoundFile;\n      }\n\n      String segment = docWriter.getSegment();\n\n      if (flushDocs || flushDeletes) {\n\n        SegmentInfos rollback = null;\n\n        if (flushDeletes)\n          rollback = (SegmentInfos) segmentInfos.clone();\n\n        boolean success = false;\n\n        try {\n          if (flushDocs) {\n\n            if (0 == docStoreOffset && flushDocStores) {\n              // This means we are flushing private doc stores\n              // with this segment, so it will not be shared\n              // with other segments\n              assert docStoreSegment != null;\n              assert docStoreSegment.equals(segment);\n              docStoreOffset = -1;\n              docStoreIsCompoundFile = false;\n              docStoreSegment = null;\n            }\n\n            int flushedDocCount = docWriter.flush(flushDocStores);\n          \n            newSegment = new SegmentInfo(segment,\n                                         flushedDocCount,\n                                         directory, false, true,\n                                         docStoreOffset, docStoreSegment,\n                                         docStoreIsCompoundFile);\n            segmentInfos.addElement(newSegment);\n          }\n\n          if (flushDeletes) {\n            // we should be able to change this so we can\n            // buffer deletes longer and then flush them to\n            // multiple flushed segments, when\n            // autoCommit=false\n            applyDeletes(flushDocs);\n            doAfterFlush();\n          }\n\n          checkpoint();\n          success = true;\n        } finally {\n          if (!success) {\n            if (flushDeletes) {\n              // Fully replace the segmentInfos since flushed\n              // deletes could have changed any of the\n              // SegmentInfo instances:\n              segmentInfos.clear();\n              segmentInfos.addAll(rollback);\n            } else {\n              // Remove segment we added, if any:\n              if (newSegment != null && \n                  segmentInfos.size() > 0 && \n                  segmentInfos.info(segmentInfos.size()-1) == newSegment)\n                segmentInfos.remove(segmentInfos.size()-1);\n            }\n            if (flushDocs)\n              docWriter.abort();\n            deleter.checkpoint(segmentInfos, false);\n            deleter.refresh();\n          }\n        }\n\n        deleter.checkpoint(segmentInfos, autoCommit);\n\n        if (flushDocs && useCompoundFile) {\n          success = false;\n          try {\n            docWriter.createCompoundFile(segment);\n            newSegment.setUseCompoundFile(true);\n            checkpoint();\n            success = true;\n          } finally {\n            if (!success) {\n              newSegment.setUseCompoundFile(false);\n              deleter.refresh();\n            }\n          }\n\n          deleter.checkpoint(segmentInfos, autoCommit);\n        }\n\n        /* new merge policy\n        if (0 == docWriter.getMaxBufferedDocs())\n          maybeMergeSegments(mergeFactor * numDocs / 2);\n        else\n          maybeMergeSegments(docWriter.getMaxBufferedDocs());\n        */\n        maybeMergeSegments(docWriter.getMaxBufferedDocs());\n      }\n    } finally {\n      docWriter.clearFlushPending();\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"abd5d0ae26eed0e7cbbbbed19f6480fe16055e9b","date":1184336627,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#flush(boolean,boolean).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#flush(boolean,boolean).mjava","sourceNew":"  /**\n   * Flush all in-memory buffered udpates (adds and deletes)\n   * to the Directory.\n   * @param triggerMerge if true, we may merge segments (if\n   *  deletes or docs were flushed) if necessary\n   * @param flushDocStores if false we are allowed to keep\n   *  doc stores open to share with the next segment\n   */\n  protected final synchronized void flush(boolean triggerMerge, boolean flushDocStores) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    // Make sure no threads are actively adding a document\n    docWriter.pauseAllThreads();\n\n    try {\n\n      SegmentInfo newSegment = null;\n\n      final int numDocs = docWriter.getNumDocsInRAM();\n\n      // Always flush docs if there are any\n      boolean flushDocs = numDocs > 0;\n\n      // With autoCommit=true we always must flush the doc\n      // stores when we flush\n      flushDocStores |= autoCommit;\n      String docStoreSegment = docWriter.getDocStoreSegment();\n      if (docStoreSegment == null)\n        flushDocStores = false;\n\n      // Always flush deletes if there are any delete terms.\n      // TODO: when autoCommit=false we don't have to flush\n      // deletes with every flushed segment; we can save\n      // CPU/IO by buffering longer & flushing deletes only\n      // when they are full or writer is being closed.  We\n      // have to fix the \"applyDeletesSelectively\" logic to\n      // apply to more than just the last flushed segment\n      boolean flushDeletes = bufferedDeleteTerms.size() > 0;\n\n      if (infoStream != null)\n        infoStream.println(\"  flush: flushDocs=\" + flushDocs +\n                           \" flushDeletes=\" + flushDeletes +\n                           \" flushDocStores=\" + flushDocStores +\n                           \" numDocs=\" + numDocs);\n\n      int docStoreOffset = docWriter.getDocStoreOffset();\n      boolean docStoreIsCompoundFile = false;\n\n      // Check if the doc stores must be separately flushed\n      // because other segments, besides the one we are about\n      // to flush, reference it\n      if (flushDocStores && (!flushDocs || !docWriter.getSegment().equals(docWriter.getDocStoreSegment()))) {\n        // We must separately flush the doc store\n        if (infoStream != null)\n          infoStream.println(\"  flush shared docStore segment \" + docStoreSegment);\n      \n        flushDocStores();\n        flushDocStores = false;\n        docStoreIsCompoundFile = useCompoundFile;\n      }\n\n      String segment = docWriter.getSegment();\n\n      if (flushDocs || flushDeletes) {\n\n        SegmentInfos rollback = null;\n\n        HashMap saveBufferedDeleteTerms = null;\n        int saveNumBufferedDeleteTerms = 0;\n\n        if (flushDeletes)\n          rollback = (SegmentInfos) segmentInfos.clone();\n\n        boolean success = false;\n\n        try {\n          if (flushDocs) {\n\n            if (0 == docStoreOffset && flushDocStores) {\n              // This means we are flushing private doc stores\n              // with this segment, so it will not be shared\n              // with other segments\n              assert docStoreSegment != null;\n              assert docStoreSegment.equals(segment);\n              docStoreOffset = -1;\n              docStoreIsCompoundFile = false;\n              docStoreSegment = null;\n            }\n\n            int flushedDocCount = docWriter.flush(flushDocStores);\n          \n            newSegment = new SegmentInfo(segment,\n                                         flushedDocCount,\n                                         directory, false, true,\n                                         docStoreOffset, docStoreSegment,\n                                         docStoreIsCompoundFile);\n            segmentInfos.addElement(newSegment);\n          }\n\n          if (flushDeletes) {\n            // we should be able to change this so we can\n            // buffer deletes longer and then flush them to\n            // multiple flushed segments, when\n            // autoCommit=false\n            saveBufferedDeleteTerms = bufferedDeleteTerms;\n            saveNumBufferedDeleteTerms = numBufferedDeleteTerms;\n            applyDeletes(flushDocs);\n            doAfterFlush();\n          }\n\n          checkpoint();\n          success = true;\n        } finally {\n          if (!success) {\n            if (flushDeletes) {\n              // Fully replace the segmentInfos since flushed\n              // deletes could have changed any of the\n              // SegmentInfo instances:\n              segmentInfos.clear();\n              segmentInfos.addAll(rollback);\n\n              if (saveBufferedDeleteTerms != null) {\n                numBufferedDeleteTerms = saveNumBufferedDeleteTerms;\n                bufferedDeleteTerms = saveBufferedDeleteTerms;\n              }\n              \n            } else {\n              // Remove segment we added, if any:\n              if (newSegment != null && \n                  segmentInfos.size() > 0 && \n                  segmentInfos.info(segmentInfos.size()-1) == newSegment)\n                segmentInfos.remove(segmentInfos.size()-1);\n            }\n            if (flushDocs)\n              docWriter.abort();\n            deleter.checkpoint(segmentInfos, false);\n            deleter.refresh();\n          }\n        }\n\n        deleter.checkpoint(segmentInfos, autoCommit);\n\n        if (flushDocs && useCompoundFile) {\n          success = false;\n          try {\n            docWriter.createCompoundFile(segment);\n            newSegment.setUseCompoundFile(true);\n            checkpoint();\n            success = true;\n          } finally {\n            if (!success) {\n              newSegment.setUseCompoundFile(false);\n              deleter.refresh();\n            }\n          }\n\n          deleter.checkpoint(segmentInfos, autoCommit);\n        }\n\n        /* new merge policy\n        if (0 == docWriter.getMaxBufferedDocs())\n          maybeMergeSegments(mergeFactor * numDocs / 2);\n        else\n          maybeMergeSegments(docWriter.getMaxBufferedDocs());\n        */\n        if (triggerMerge)\n          maybeMergeSegments(docWriter.getMaxBufferedDocs());\n      }\n    } finally {\n      docWriter.clearFlushPending();\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /**\n   * Flush all in-memory buffered udpates (adds and deletes)\n   * to the Directory.\n   * @param triggerMerge if true, we may merge segments (if\n   *  deletes or docs were flushed) if necessary\n   * @param flushDocStores if false we are allowed to keep\n   *  doc stores open to share with the next segment\n   */\n  protected final synchronized void flush(boolean triggerMerge, boolean flushDocStores) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    // Make sure no threads are actively adding a document\n    docWriter.pauseAllThreads();\n\n    try {\n\n      SegmentInfo newSegment = null;\n\n      final int numDocs = docWriter.getNumDocsInRAM();\n\n      // Always flush docs if there are any\n      boolean flushDocs = numDocs > 0;\n\n      // With autoCommit=true we always must flush the doc\n      // stores when we flush\n      flushDocStores |= autoCommit;\n      String docStoreSegment = docWriter.getDocStoreSegment();\n      if (docStoreSegment == null)\n        flushDocStores = false;\n\n      // Always flush deletes if there are any delete terms.\n      // TODO: when autoCommit=false we don't have to flush\n      // deletes with every flushed segment; we can save\n      // CPU/IO by buffering longer & flushing deletes only\n      // when they are full or writer is being closed.  We\n      // have to fix the \"applyDeletesSelectively\" logic to\n      // apply to more than just the last flushed segment\n      boolean flushDeletes = bufferedDeleteTerms.size() > 0;\n\n      if (infoStream != null)\n        infoStream.println(\"  flush: flushDocs=\" + flushDocs +\n                           \" flushDeletes=\" + flushDeletes +\n                           \" flushDocStores=\" + flushDocStores +\n                           \" numDocs=\" + numDocs);\n\n      int docStoreOffset = docWriter.getDocStoreOffset();\n      boolean docStoreIsCompoundFile = false;\n\n      // Check if the doc stores must be separately flushed\n      // because other segments, besides the one we are about\n      // to flush, reference it\n      if (flushDocStores && (!flushDocs || !docWriter.getSegment().equals(docWriter.getDocStoreSegment()))) {\n        // We must separately flush the doc store\n        if (infoStream != null)\n          infoStream.println(\"  flush shared docStore segment \" + docStoreSegment);\n      \n        flushDocStores();\n        flushDocStores = false;\n        docStoreIsCompoundFile = useCompoundFile;\n      }\n\n      String segment = docWriter.getSegment();\n\n      if (flushDocs || flushDeletes) {\n\n        SegmentInfos rollback = null;\n\n        if (flushDeletes)\n          rollback = (SegmentInfos) segmentInfos.clone();\n\n        boolean success = false;\n\n        try {\n          if (flushDocs) {\n\n            if (0 == docStoreOffset && flushDocStores) {\n              // This means we are flushing private doc stores\n              // with this segment, so it will not be shared\n              // with other segments\n              assert docStoreSegment != null;\n              assert docStoreSegment.equals(segment);\n              docStoreOffset = -1;\n              docStoreIsCompoundFile = false;\n              docStoreSegment = null;\n            }\n\n            int flushedDocCount = docWriter.flush(flushDocStores);\n          \n            newSegment = new SegmentInfo(segment,\n                                         flushedDocCount,\n                                         directory, false, true,\n                                         docStoreOffset, docStoreSegment,\n                                         docStoreIsCompoundFile);\n            segmentInfos.addElement(newSegment);\n          }\n\n          if (flushDeletes) {\n            // we should be able to change this so we can\n            // buffer deletes longer and then flush them to\n            // multiple flushed segments, when\n            // autoCommit=false\n            applyDeletes(flushDocs);\n            doAfterFlush();\n          }\n\n          checkpoint();\n          success = true;\n        } finally {\n          if (!success) {\n            if (flushDeletes) {\n              // Fully replace the segmentInfos since flushed\n              // deletes could have changed any of the\n              // SegmentInfo instances:\n              segmentInfos.clear();\n              segmentInfos.addAll(rollback);\n            } else {\n              // Remove segment we added, if any:\n              if (newSegment != null && \n                  segmentInfos.size() > 0 && \n                  segmentInfos.info(segmentInfos.size()-1) == newSegment)\n                segmentInfos.remove(segmentInfos.size()-1);\n            }\n            if (flushDocs)\n              docWriter.abort();\n            deleter.checkpoint(segmentInfos, false);\n            deleter.refresh();\n          }\n        }\n\n        deleter.checkpoint(segmentInfos, autoCommit);\n\n        if (flushDocs && useCompoundFile) {\n          success = false;\n          try {\n            docWriter.createCompoundFile(segment);\n            newSegment.setUseCompoundFile(true);\n            checkpoint();\n            success = true;\n          } finally {\n            if (!success) {\n              newSegment.setUseCompoundFile(false);\n              deleter.refresh();\n            }\n          }\n\n          deleter.checkpoint(segmentInfos, autoCommit);\n        }\n\n        /* new merge policy\n        if (0 == docWriter.getMaxBufferedDocs())\n          maybeMergeSegments(mergeFactor * numDocs / 2);\n        else\n          maybeMergeSegments(docWriter.getMaxBufferedDocs());\n        */\n        if (triggerMerge)\n          maybeMergeSegments(docWriter.getMaxBufferedDocs());\n      }\n    } finally {\n      docWriter.clearFlushPending();\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":["fde68de507dbf344495d7b5e8052866fe5f254ab"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fde68de507dbf344495d7b5e8052866fe5f254ab","date":1189434831,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#flush(boolean,boolean).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#flush(boolean,boolean).mjava","sourceNew":"  /**\n   * Flush all in-memory buffered udpates (adds and deletes)\n   * to the Directory.\n   * @param triggerMerge if true, we may merge segments (if\n   *  deletes or docs were flushed) if necessary\n   * @param flushDocStores if false we are allowed to keep\n   *  doc stores open to share with the next segment\n   */\n  protected final synchronized void flush(boolean triggerMerge, boolean flushDocStores) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    // Make sure no threads are actively adding a document\n    docWriter.pauseAllThreads();\n\n    try {\n\n      SegmentInfo newSegment = null;\n\n      final int numDocs = docWriter.getNumDocsInRAM();\n\n      // Always flush docs if there are any\n      boolean flushDocs = numDocs > 0;\n\n      // With autoCommit=true we always must flush the doc\n      // stores when we flush\n      flushDocStores |= autoCommit;\n      String docStoreSegment = docWriter.getDocStoreSegment();\n      if (docStoreSegment == null)\n        flushDocStores = false;\n\n      // Always flush deletes if there are any delete terms.\n      // TODO: when autoCommit=false we don't have to flush\n      // deletes with every flushed segment; we can save\n      // CPU/IO by buffering longer & flushing deletes only\n      // when they are full or writer is being closed.  We\n      // have to fix the \"applyDeletesSelectively\" logic to\n      // apply to more than just the last flushed segment\n      boolean flushDeletes = docWriter.hasDeletes();\n\n      if (infoStream != null)\n        infoStream.println(\"  flush: flushDocs=\" + flushDocs +\n                           \" flushDeletes=\" + flushDeletes +\n                           \" flushDocStores=\" + flushDocStores +\n                           \" numDocs=\" + numDocs);\n\n      int docStoreOffset = docWriter.getDocStoreOffset();\n      boolean docStoreIsCompoundFile = false;\n\n      // Check if the doc stores must be separately flushed\n      // because other segments, besides the one we are about\n      // to flush, reference it\n      if (flushDocStores && (!flushDocs || !docWriter.getSegment().equals(docWriter.getDocStoreSegment()))) {\n        // We must separately flush the doc store\n        if (infoStream != null)\n          infoStream.println(\"  flush shared docStore segment \" + docStoreSegment);\n      \n        flushDocStores();\n        flushDocStores = false;\n        docStoreIsCompoundFile = useCompoundFile;\n      }\n\n      String segment = docWriter.getSegment();\n\n      if (flushDocs || flushDeletes) {\n\n        SegmentInfos rollback = null;\n\n        if (flushDeletes)\n          rollback = (SegmentInfos) segmentInfos.clone();\n\n        boolean success = false;\n\n        try {\n          if (flushDocs) {\n\n            if (0 == docStoreOffset && flushDocStores) {\n              // This means we are flushing private doc stores\n              // with this segment, so it will not be shared\n              // with other segments\n              assert docStoreSegment != null;\n              assert docStoreSegment.equals(segment);\n              docStoreOffset = -1;\n              docStoreIsCompoundFile = false;\n              docStoreSegment = null;\n            }\n\n            int flushedDocCount = docWriter.flush(flushDocStores);\n          \n            newSegment = new SegmentInfo(segment,\n                                         flushedDocCount,\n                                         directory, false, true,\n                                         docStoreOffset, docStoreSegment,\n                                         docStoreIsCompoundFile);\n            segmentInfos.addElement(newSegment);\n          }\n\n          if (flushDeletes) {\n            // we should be able to change this so we can\n            // buffer deletes longer and then flush them to\n            // multiple flushed segments, when\n            // autoCommit=false\n            int delCount = applyDeletes(flushDocs);\n            if (infoStream != null)\n              infoStream.println(\"flushed \" + delCount + \" deleted documents\");\n            doAfterFlush();\n          }\n\n          checkpoint();\n          success = true;\n        } finally {\n          if (!success) {\n            if (flushDeletes) {\n              // Fully replace the segmentInfos since flushed\n              // deletes could have changed any of the\n              // SegmentInfo instances:\n              segmentInfos.clear();\n              segmentInfos.addAll(rollback);\n              \n            } else {\n              // Remove segment we added, if any:\n              if (newSegment != null && \n                  segmentInfos.size() > 0 && \n                  segmentInfos.info(segmentInfos.size()-1) == newSegment)\n                segmentInfos.remove(segmentInfos.size()-1);\n            }\n            if (flushDocs)\n              docWriter.abort();\n            deleter.checkpoint(segmentInfos, false);\n            deleter.refresh();\n          }\n        }\n\n        deleter.checkpoint(segmentInfos, autoCommit);\n\n        if (flushDocs && useCompoundFile) {\n          success = false;\n          try {\n            docWriter.createCompoundFile(segment);\n            newSegment.setUseCompoundFile(true);\n            checkpoint();\n            success = true;\n          } finally {\n            if (!success) {\n              newSegment.setUseCompoundFile(false);\n              deleter.refresh();\n            }\n          }\n\n          deleter.checkpoint(segmentInfos, autoCommit);\n        }\n\n        /* new merge policy\n        if (0 == docWriter.getMaxBufferedDocs())\n          maybeMergeSegments(mergeFactor * numDocs / 2);\n        else\n          maybeMergeSegments(docWriter.getMaxBufferedDocs());\n        */\n        if (triggerMerge)\n          maybeMergeSegments(docWriter.getMaxBufferedDocs());\n      }\n    } finally {\n      docWriter.clearFlushPending();\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /**\n   * Flush all in-memory buffered udpates (adds and deletes)\n   * to the Directory.\n   * @param triggerMerge if true, we may merge segments (if\n   *  deletes or docs were flushed) if necessary\n   * @param flushDocStores if false we are allowed to keep\n   *  doc stores open to share with the next segment\n   */\n  protected final synchronized void flush(boolean triggerMerge, boolean flushDocStores) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    // Make sure no threads are actively adding a document\n    docWriter.pauseAllThreads();\n\n    try {\n\n      SegmentInfo newSegment = null;\n\n      final int numDocs = docWriter.getNumDocsInRAM();\n\n      // Always flush docs if there are any\n      boolean flushDocs = numDocs > 0;\n\n      // With autoCommit=true we always must flush the doc\n      // stores when we flush\n      flushDocStores |= autoCommit;\n      String docStoreSegment = docWriter.getDocStoreSegment();\n      if (docStoreSegment == null)\n        flushDocStores = false;\n\n      // Always flush deletes if there are any delete terms.\n      // TODO: when autoCommit=false we don't have to flush\n      // deletes with every flushed segment; we can save\n      // CPU/IO by buffering longer & flushing deletes only\n      // when they are full or writer is being closed.  We\n      // have to fix the \"applyDeletesSelectively\" logic to\n      // apply to more than just the last flushed segment\n      boolean flushDeletes = bufferedDeleteTerms.size() > 0;\n\n      if (infoStream != null)\n        infoStream.println(\"  flush: flushDocs=\" + flushDocs +\n                           \" flushDeletes=\" + flushDeletes +\n                           \" flushDocStores=\" + flushDocStores +\n                           \" numDocs=\" + numDocs);\n\n      int docStoreOffset = docWriter.getDocStoreOffset();\n      boolean docStoreIsCompoundFile = false;\n\n      // Check if the doc stores must be separately flushed\n      // because other segments, besides the one we are about\n      // to flush, reference it\n      if (flushDocStores && (!flushDocs || !docWriter.getSegment().equals(docWriter.getDocStoreSegment()))) {\n        // We must separately flush the doc store\n        if (infoStream != null)\n          infoStream.println(\"  flush shared docStore segment \" + docStoreSegment);\n      \n        flushDocStores();\n        flushDocStores = false;\n        docStoreIsCompoundFile = useCompoundFile;\n      }\n\n      String segment = docWriter.getSegment();\n\n      if (flushDocs || flushDeletes) {\n\n        SegmentInfos rollback = null;\n\n        HashMap saveBufferedDeleteTerms = null;\n        int saveNumBufferedDeleteTerms = 0;\n\n        if (flushDeletes)\n          rollback = (SegmentInfos) segmentInfos.clone();\n\n        boolean success = false;\n\n        try {\n          if (flushDocs) {\n\n            if (0 == docStoreOffset && flushDocStores) {\n              // This means we are flushing private doc stores\n              // with this segment, so it will not be shared\n              // with other segments\n              assert docStoreSegment != null;\n              assert docStoreSegment.equals(segment);\n              docStoreOffset = -1;\n              docStoreIsCompoundFile = false;\n              docStoreSegment = null;\n            }\n\n            int flushedDocCount = docWriter.flush(flushDocStores);\n          \n            newSegment = new SegmentInfo(segment,\n                                         flushedDocCount,\n                                         directory, false, true,\n                                         docStoreOffset, docStoreSegment,\n                                         docStoreIsCompoundFile);\n            segmentInfos.addElement(newSegment);\n          }\n\n          if (flushDeletes) {\n            // we should be able to change this so we can\n            // buffer deletes longer and then flush them to\n            // multiple flushed segments, when\n            // autoCommit=false\n            saveBufferedDeleteTerms = bufferedDeleteTerms;\n            saveNumBufferedDeleteTerms = numBufferedDeleteTerms;\n            applyDeletes(flushDocs);\n            doAfterFlush();\n          }\n\n          checkpoint();\n          success = true;\n        } finally {\n          if (!success) {\n            if (flushDeletes) {\n              // Fully replace the segmentInfos since flushed\n              // deletes could have changed any of the\n              // SegmentInfo instances:\n              segmentInfos.clear();\n              segmentInfos.addAll(rollback);\n\n              if (saveBufferedDeleteTerms != null) {\n                numBufferedDeleteTerms = saveNumBufferedDeleteTerms;\n                bufferedDeleteTerms = saveBufferedDeleteTerms;\n              }\n              \n            } else {\n              // Remove segment we added, if any:\n              if (newSegment != null && \n                  segmentInfos.size() > 0 && \n                  segmentInfos.info(segmentInfos.size()-1) == newSegment)\n                segmentInfos.remove(segmentInfos.size()-1);\n            }\n            if (flushDocs)\n              docWriter.abort();\n            deleter.checkpoint(segmentInfos, false);\n            deleter.refresh();\n          }\n        }\n\n        deleter.checkpoint(segmentInfos, autoCommit);\n\n        if (flushDocs && useCompoundFile) {\n          success = false;\n          try {\n            docWriter.createCompoundFile(segment);\n            newSegment.setUseCompoundFile(true);\n            checkpoint();\n            success = true;\n          } finally {\n            if (!success) {\n              newSegment.setUseCompoundFile(false);\n              deleter.refresh();\n            }\n          }\n\n          deleter.checkpoint(segmentInfos, autoCommit);\n        }\n\n        /* new merge policy\n        if (0 == docWriter.getMaxBufferedDocs())\n          maybeMergeSegments(mergeFactor * numDocs / 2);\n        else\n          maybeMergeSegments(docWriter.getMaxBufferedDocs());\n        */\n        if (triggerMerge)\n          maybeMergeSegments(docWriter.getMaxBufferedDocs());\n      }\n    } finally {\n      docWriter.clearFlushPending();\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c","abd5d0ae26eed0e7cbbbbed19f6480fe16055e9b"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b1405362241b561f5590ff4a87d5d6e173bcd9cf","date":1190107634,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#flush(boolean,boolean).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#flush(boolean,boolean).mjava","sourceNew":"  /**\n   * Flush all in-memory buffered udpates (adds and deletes)\n   * to the Directory.\n   * @param triggerMerge if true, we may merge segments (if\n   *  deletes or docs were flushed) if necessary\n   * @param flushDocStores if false we are allowed to keep\n   *  doc stores open to share with the next segment\n   */\n  protected final void flush(boolean triggerMerge, boolean flushDocStores) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    if (doFlush(flushDocStores) && triggerMerge)\n      maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Flush all in-memory buffered udpates (adds and deletes)\n   * to the Directory.\n   * @param triggerMerge if true, we may merge segments (if\n   *  deletes or docs were flushed) if necessary\n   * @param flushDocStores if false we are allowed to keep\n   *  doc stores open to share with the next segment\n   */\n  protected final synchronized void flush(boolean triggerMerge, boolean flushDocStores) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    // Make sure no threads are actively adding a document\n    docWriter.pauseAllThreads();\n\n    try {\n\n      SegmentInfo newSegment = null;\n\n      final int numDocs = docWriter.getNumDocsInRAM();\n\n      // Always flush docs if there are any\n      boolean flushDocs = numDocs > 0;\n\n      // With autoCommit=true we always must flush the doc\n      // stores when we flush\n      flushDocStores |= autoCommit;\n      String docStoreSegment = docWriter.getDocStoreSegment();\n      if (docStoreSegment == null)\n        flushDocStores = false;\n\n      // Always flush deletes if there are any delete terms.\n      // TODO: when autoCommit=false we don't have to flush\n      // deletes with every flushed segment; we can save\n      // CPU/IO by buffering longer & flushing deletes only\n      // when they are full or writer is being closed.  We\n      // have to fix the \"applyDeletesSelectively\" logic to\n      // apply to more than just the last flushed segment\n      boolean flushDeletes = docWriter.hasDeletes();\n\n      if (infoStream != null)\n        infoStream.println(\"  flush: flushDocs=\" + flushDocs +\n                           \" flushDeletes=\" + flushDeletes +\n                           \" flushDocStores=\" + flushDocStores +\n                           \" numDocs=\" + numDocs);\n\n      int docStoreOffset = docWriter.getDocStoreOffset();\n      boolean docStoreIsCompoundFile = false;\n\n      // Check if the doc stores must be separately flushed\n      // because other segments, besides the one we are about\n      // to flush, reference it\n      if (flushDocStores && (!flushDocs || !docWriter.getSegment().equals(docWriter.getDocStoreSegment()))) {\n        // We must separately flush the doc store\n        if (infoStream != null)\n          infoStream.println(\"  flush shared docStore segment \" + docStoreSegment);\n      \n        flushDocStores();\n        flushDocStores = false;\n        docStoreIsCompoundFile = useCompoundFile;\n      }\n\n      String segment = docWriter.getSegment();\n\n      if (flushDocs || flushDeletes) {\n\n        SegmentInfos rollback = null;\n\n        if (flushDeletes)\n          rollback = (SegmentInfos) segmentInfos.clone();\n\n        boolean success = false;\n\n        try {\n          if (flushDocs) {\n\n            if (0 == docStoreOffset && flushDocStores) {\n              // This means we are flushing private doc stores\n              // with this segment, so it will not be shared\n              // with other segments\n              assert docStoreSegment != null;\n              assert docStoreSegment.equals(segment);\n              docStoreOffset = -1;\n              docStoreIsCompoundFile = false;\n              docStoreSegment = null;\n            }\n\n            int flushedDocCount = docWriter.flush(flushDocStores);\n          \n            newSegment = new SegmentInfo(segment,\n                                         flushedDocCount,\n                                         directory, false, true,\n                                         docStoreOffset, docStoreSegment,\n                                         docStoreIsCompoundFile);\n            segmentInfos.addElement(newSegment);\n          }\n\n          if (flushDeletes) {\n            // we should be able to change this so we can\n            // buffer deletes longer and then flush them to\n            // multiple flushed segments, when\n            // autoCommit=false\n            int delCount = applyDeletes(flushDocs);\n            if (infoStream != null)\n              infoStream.println(\"flushed \" + delCount + \" deleted documents\");\n            doAfterFlush();\n          }\n\n          checkpoint();\n          success = true;\n        } finally {\n          if (!success) {\n            if (flushDeletes) {\n              // Fully replace the segmentInfos since flushed\n              // deletes could have changed any of the\n              // SegmentInfo instances:\n              segmentInfos.clear();\n              segmentInfos.addAll(rollback);\n              \n            } else {\n              // Remove segment we added, if any:\n              if (newSegment != null && \n                  segmentInfos.size() > 0 && \n                  segmentInfos.info(segmentInfos.size()-1) == newSegment)\n                segmentInfos.remove(segmentInfos.size()-1);\n            }\n            if (flushDocs)\n              docWriter.abort();\n            deleter.checkpoint(segmentInfos, false);\n            deleter.refresh();\n          }\n        }\n\n        deleter.checkpoint(segmentInfos, autoCommit);\n\n        if (flushDocs && useCompoundFile) {\n          success = false;\n          try {\n            docWriter.createCompoundFile(segment);\n            newSegment.setUseCompoundFile(true);\n            checkpoint();\n            success = true;\n          } finally {\n            if (!success) {\n              newSegment.setUseCompoundFile(false);\n              deleter.refresh();\n            }\n          }\n\n          deleter.checkpoint(segmentInfos, autoCommit);\n        }\n\n        /* new merge policy\n        if (0 == docWriter.getMaxBufferedDocs())\n          maybeMergeSegments(mergeFactor * numDocs / 2);\n        else\n          maybeMergeSegments(docWriter.getMaxBufferedDocs());\n        */\n        if (triggerMerge)\n          maybeMergeSegments(docWriter.getMaxBufferedDocs());\n      }\n    } finally {\n      docWriter.clearFlushPending();\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be","date":1204801324,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#flush(boolean,boolean,boolean).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#flush(boolean,boolean).mjava","sourceNew":"  /**\n   * Flush all in-memory buffered udpates (adds and deletes)\n   * to the Directory.\n   * @param triggerMerge if true, we may merge segments (if\n   *  deletes or docs were flushed) if necessary\n   * @param flushDocStores if false we are allowed to keep\n   *  doc stores open to share with the next segment\n   * @param flushDeletes whether pending deletes should also\n   *  be flushed\n   */\n  protected final void flush(boolean triggerMerge, boolean flushDocStores, boolean flushDeletes) throws CorruptIndexException, IOException {\n    ensureOpen();\n    if (doFlush(flushDocStores, flushDeletes) && triggerMerge)\n      maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Flush all in-memory buffered udpates (adds and deletes)\n   * to the Directory.\n   * @param triggerMerge if true, we may merge segments (if\n   *  deletes or docs were flushed) if necessary\n   * @param flushDocStores if false we are allowed to keep\n   *  doc stores open to share with the next segment\n   */\n  protected final void flush(boolean triggerMerge, boolean flushDocStores) throws CorruptIndexException, IOException {\n    ensureOpen();\n\n    if (doFlush(flushDocStores) && triggerMerge)\n      maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["fde68de507dbf344495d7b5e8052866fe5f254ab"],"a30c6ea5c2c1da6d2956e42721e02ccb29910be5":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"fde68de507dbf344495d7b5e8052866fe5f254ab":["abd5d0ae26eed0e7cbbbbed19f6480fe16055e9b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"abd5d0ae26eed0e7cbbbbed19f6480fe16055e9b":["a30c6ea5c2c1da6d2956e42721e02ccb29910be5"],"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be"]},"commit2Childs":{"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be"],"a30c6ea5c2c1da6d2956e42721e02ccb29910be5":["abd5d0ae26eed0e7cbbbbed19f6480fe16055e9b"],"fde68de507dbf344495d7b5e8052866fe5f254ab":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"abd5d0ae26eed0e7cbbbbed19f6480fe16055e9b":["fde68de507dbf344495d7b5e8052866fe5f254ab"],"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["a30c6ea5c2c1da6d2956e42721e02ccb29910be5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}