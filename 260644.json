{"path":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer();\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          if (i == -1) {\n            throw new IOException(\"read past EOF\");\n          }\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer();\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          if (i == -1) {\n            throw new IOException(\"read past EOF\");\n          }\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bd95727c62a66eec2a2cfb7baac54d20dd738908","date":1305713156,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer();\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          if (i == -1) {\n            throw new IOException(\"read past EOF\");\n          }\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer();\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          if (i == -1) {\n            throw new IOException(\"read past EOF\");\n          }\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c3a8a449466c1ff7ce2274fe73dab487256964b4","date":1305735867,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer();\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          if (i == -1) {\n            throw new IOException(\"read past EOF\");\n          }\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer();\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          if (i == -1) {\n            throw new IOException(\"read past EOF\");\n          }\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer();\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          if (i == -1) {\n            throw new IOException(\"read past EOF\");\n          }\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer();\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          if (i == -1) {\n            throw new IOException(\"read past EOF\");\n          }\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0aab6e810b4b0d3743d6a048be0602801f4b3920","date":1308671625,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new IOException(\"read past EOF\");\n      }\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer();\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          if (i == -1) {\n            throw new IOException(\"read past EOF\");\n          }\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new IOException(\"read past EOF\");\n      }\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer();\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          if (i == -1) {\n            throw new IOException(\"read past EOF\");\n          }\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new IOException(\"read past EOF\");\n      }\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer();\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          if (i == -1) {\n            throw new IOException(\"read past EOF\");\n          }\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f1efe3edca215dd9891cb42af283fed96f792ca0","date":1320428891,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new IOException(\"read past EOF: \" + this);\n      }\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      } catch (IOException ioe) {\n        throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new IOException(\"read past EOF\");\n      }\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"88822d307a7b7c2a5f97d80b7779ece9a1f82518","date":1320883044,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new EOFException(\"read past EOF (resource: \" + this + \")\");\n      }\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      } catch (IOException ioe) {\n        throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new IOException(\"read past EOF: \" + this);\n      }\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      } catch (IOException ioe) {\n        throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e5090f41e198d9dd9374e99981f940b111973af2","date":1325969785,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new EOFException(\"read past EOF: \" + this);\n      }\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      } catch (IOException ioe) {\n        throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new EOFException(\"read past EOF (resource: \" + this + \")\");\n      }\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      } catch (IOException ioe) {\n        throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f4bcdfcc72cce0bce60985b59b79fd2a1e348e7","date":1326737128,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        bb = ByteBuffer.wrap(b, offset, len);\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new EOFException(\"read past EOF: \" + this);\n      }\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      } catch (IOException ioe) {\n        throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        if (offset == 0) {\n          if (otherBuffer != b) {\n            // Now wrap this other buffer; with compound\n            // file, we are repeatedly called with its\n            // buffer, so we wrap it once and then re-use it\n            // on subsequent calls\n            otherBuffer = b;\n            otherByteBuf = ByteBuffer.wrap(b);\n          } else\n            otherByteBuf.clear();\n          otherByteBuf.limit(len);\n          bb = otherByteBuf;\n        } else {\n          // Always wrap when offset != 0\n          bb = ByteBuffer.wrap(b, offset, len);\n        }\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new EOFException(\"read past EOF: \" + this);\n      }\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      } catch (IOException ioe) {\n        throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/store/NIOFSDirectory.NIOFSIndexInput#readInternal(byte[],int,int).mjava","sourceNew":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        bb = ByteBuffer.wrap(b, offset, len);\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new EOFException(\"read past EOF: \" + this);\n      }\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      } catch (IOException ioe) {\n        throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected void readInternal(byte[] b, int offset, int len) throws IOException {\n\n      final ByteBuffer bb;\n\n      // Determine the ByteBuffer we should use\n      if (b == buffer && 0 == offset) {\n        // Use our own pre-wrapped byteBuf:\n        assert byteBuf != null;\n        byteBuf.clear();\n        byteBuf.limit(len);\n        bb = byteBuf;\n      } else {\n        bb = ByteBuffer.wrap(b, offset, len);\n      }\n\n      int readOffset = bb.position();\n      int readLength = bb.limit() - readOffset;\n      assert readLength == len;\n\n      long pos = getFilePointer() + off;\n      \n      if (pos + len > end) {\n        throw new EOFException(\"read past EOF: \" + this);\n      }\n\n      try {\n        while (readLength > 0) {\n          final int limit;\n          if (readLength > chunkSize) {\n            // LUCENE-1566 - work around JVM Bug by breaking\n            // very large reads into chunks\n            limit = readOffset + chunkSize;\n          } else {\n            limit = readOffset + readLength;\n          }\n          bb.limit(limit);\n          int i = channel.read(bb, pos);\n          pos += i;\n          readOffset += i;\n          readLength -= i;\n        }\n      } catch (OutOfMemoryError e) {\n        // propagate OOM up and add a hint for 32bit VM Users hitting the bug\n        // with a large chunk size in the fast path.\n        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(\n              \"OutOfMemoryError likely caused by the Sun VM Bug described in \"\n              + \"https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize \"\n              + \"with a value smaller than the current chunk size (\" + chunkSize + \")\");\n        outOfMemoryError.initCause(e);\n        throw outOfMemoryError;\n      } catch (IOException ioe) {\n        throw new IOException(ioe.getMessage() + \": \" + this, ioe);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["5f4bcdfcc72cce0bce60985b59b79fd2a1e348e7"],"f1efe3edca215dd9891cb42af283fed96f792ca0":["0aab6e810b4b0d3743d6a048be0602801f4b3920"],"88822d307a7b7c2a5f97d80b7779ece9a1f82518":["f1efe3edca215dd9891cb42af283fed96f792ca0"],"0aab6e810b4b0d3743d6a048be0602801f4b3920":["bd95727c62a66eec2a2cfb7baac54d20dd738908"],"2553b00f699380c64959ccb27991289aae87be2e":["a3776dccca01c11e7046323cfad46a3b4a471233","0aab6e810b4b0d3743d6a048be0602801f4b3920"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["bd95727c62a66eec2a2cfb7baac54d20dd738908","0aab6e810b4b0d3743d6a048be0602801f4b3920"],"c3a8a449466c1ff7ce2274fe73dab487256964b4":["9454a6510e2db155fb01faa5c049b06ece95fab9","bd95727c62a66eec2a2cfb7baac54d20dd738908"],"a3776dccca01c11e7046323cfad46a3b4a471233":["9454a6510e2db155fb01faa5c049b06ece95fab9","bd95727c62a66eec2a2cfb7baac54d20dd738908"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5f4bcdfcc72cce0bce60985b59b79fd2a1e348e7":["e5090f41e198d9dd9374e99981f940b111973af2"],"e5090f41e198d9dd9374e99981f940b111973af2":["88822d307a7b7c2a5f97d80b7779ece9a1f82518"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"bd95727c62a66eec2a2cfb7baac54d20dd738908":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"f1efe3edca215dd9891cb42af283fed96f792ca0":["88822d307a7b7c2a5f97d80b7779ece9a1f82518"],"88822d307a7b7c2a5f97d80b7779ece9a1f82518":["e5090f41e198d9dd9374e99981f940b111973af2"],"0aab6e810b4b0d3743d6a048be0602801f4b3920":["f1efe3edca215dd9891cb42af283fed96f792ca0","2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"2553b00f699380c64959ccb27991289aae87be2e":[],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"c3a8a449466c1ff7ce2274fe73dab487256964b4":[],"a3776dccca01c11e7046323cfad46a3b4a471233":["2553b00f699380c64959ccb27991289aae87be2e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"5f4bcdfcc72cce0bce60985b59b79fd2a1e348e7":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"e5090f41e198d9dd9374e99981f940b111973af2":["5f4bcdfcc72cce0bce60985b59b79fd2a1e348e7"],"bd95727c62a66eec2a2cfb7baac54d20dd738908":["0aab6e810b4b0d3743d6a048be0602801f4b3920","d083e83f225b11e5fdd900e83d26ddb385b6955c","c3a8a449466c1ff7ce2274fe73dab487256964b4","a3776dccca01c11e7046323cfad46a3b4a471233"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["c3a8a449466c1ff7ce2274fe73dab487256964b4","a3776dccca01c11e7046323cfad46a3b4a471233","bd95727c62a66eec2a2cfb7baac54d20dd738908"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c","c3a8a449466c1ff7ce2274fe73dab487256964b4","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}