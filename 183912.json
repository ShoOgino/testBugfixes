{"path":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","commits":[{"id":"6864413dbc0c12104c978c05456f3da1d45adb03","date":1186770873,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition().mjava","sourceNew":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition(Token token) {\n\n        final Payload payload = token.getPayload();\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n        final int tokenTextLen = token.termLength();\n\n        int code = 0;\n        int code2 = 0;\n\n        // Compute hashcode\n        int downto = tokenTextLen;\n        while (downto > 0)\n          code = (code*31) + tokenText[--downto];\n\n        // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        int hashPos = code & postingsHashMask;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = code*1347|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenText, tokenTextLen));\n        }\n        \n        final int proxCode;\n\n        if (p != null) {       // term seen since last flush\n\n          if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n            // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n            assert p.docFreq > 0;\n\n            // Now that we know doc freq for previous doc,\n            // write it & lastDocCode\n            freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n            freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n            if (1 == p.docFreq)\n              writeFreqVInt(p.lastDocCode|1);\n            else {\n              writeFreqVInt(p.lastDocCode);\n              writeFreqVInt(p.docFreq);\n            }\n            p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStartCode = offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n\n            p.docFreq = 1;\n\n            // Store code so we can write this after we're\n            // done with this new doc\n            p.lastDocCode = (docID-p.lastDocID) << 1;\n            p.lastDocID = docID;\n\n          } else {                                // term already seen in this doc\n            // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n            p.docFreq++;\n\n            proxCode = position-p.lastPosition;\n\n            if (doVectors) {\n              vector = p.vector;\n              if (vector == null)\n                vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n                offsetStartCode = offsetStart-vector.lastOffset;\n              }\n            }\n          }\n        } else {\t\t\t\t\t  // term not seen before\n          // System.out.println(\"    never seen docID=\" + docID);\n\n          // Refill?\n          if (0 == postingsFreeCount) {\n            postingsFreeCount = postingsFreeList.length;\n            getPostings(postingsFreeList);\n          }\n\n          // Pull next free Posting from free list\n          p = postingsFreeList[--postingsFreeCount];\n\n          final int textLen1 = 1+tokenTextLen;\n          if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE)\n            charPool.nextBuffer();\n          final char[] text = charPool.buffer;\n          final int textUpto = charPool.byteUpto;\n          p.textStart = textUpto + charPool.byteOffset;\n          charPool.byteUpto += textLen1;\n\n          System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n          text[textUpto+tokenTextLen] = 0xffff;\n          \n          assert postingsHash[hashPos] == null;\n\n          postingsHash[hashPos] = p;\n          numPostings++;\n\n          if (numPostings == postingsHashHalfSize)\n            rehashPostings(2*postingsHashSize);\n\n          // Init first slice for freq & prox streams\n          final int firstSize = levelSizeArray[0];\n\n          final int upto1 = postingsPool.newSlice(firstSize);\n          p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n          final int upto2 = postingsPool.newSlice(firstSize);\n          p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n          p.lastDocCode = docID << 1;\n          p.lastDocID = docID;\n          p.docFreq = 1;\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offsetStartCode = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n        }\n\n        proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n        prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n        assert prox != null;\n\n        if (payload != null && payload.length > 0) {\n          writeProxVInt((proxCode<<1)|1);\n          writeProxVInt(payload.length);\n          writeProxBytes(payload.data, payload.offset, payload.length);\n          fieldInfo.storePayloads = true;\n        } else\n          writeProxVInt(proxCode<<1);\n\n        p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n        p.lastPosition = position++;\n\n        if (doVectorPositions) {\n          posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n          pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n          writePosVInt(proxCode);\n          vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        if (doVectorOffsets) {\n          offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n          offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n          writeOffsetVInt(offsetStartCode);\n          writeOffsetVInt(offsetEnd-offsetStart);\n          vector.lastOffset = offsetEnd;\n          vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n        }\n      }\n\n","sourceOld":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition() {\n\n        final Payload payload = token.getPayload();\n\n        final String tokenString;\n        final int tokenTextLen;\n        final int tokenTextOffset;\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n\n        int code = 0;\n        int code2 = 0;\n\n        if (tokenText == null) {\n\n          // Fallback to String token\n          tokenString = token.termText();\n          tokenTextLen = tokenString.length();\n          tokenTextOffset = 0;\n\n          // Compute hashcode.\n          int downto = tokenTextLen;\n          while (downto > 0)\n            code = (code*31) + tokenString.charAt(--downto);\n          \n          // System.out.println(\"  addPosition: field=\" + fieldInfo.name + \" string=\" + tokenString + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset+token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        } else {\n          tokenString = null;\n          tokenTextLen = token.termBufferLength();\n          tokenTextOffset = token.termBufferOffset();\n\n          // Compute hashcode\n          int downto = tokenTextLen+tokenTextOffset;\n          while (downto > tokenTextOffset)\n            code = (code*31) + tokenText[--downto];\n\n          // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, tokenTextOffset, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n        }\n\n        int hashPos = code & postingsHashMask;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenString, tokenText, tokenTextLen, tokenTextOffset)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = code*1347|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenString, tokenText, tokenTextLen, tokenTextOffset));\n        }\n        \n        final int proxCode;\n\n        if (p != null) {       // term seen since last flush\n\n          if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n            // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n            assert p.docFreq > 0;\n\n            // Now that we know doc freq for previous doc,\n            // write it & lastDocCode\n            freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n            freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n            if (1 == p.docFreq)\n              writeFreqVInt(p.lastDocCode|1);\n            else {\n              writeFreqVInt(p.lastDocCode);\n              writeFreqVInt(p.docFreq);\n            }\n            p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStartCode = offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n\n            p.docFreq = 1;\n\n            // Store code so we can write this after we're\n            // done with this new doc\n            p.lastDocCode = (docID-p.lastDocID) << 1;\n            p.lastDocID = docID;\n\n          } else {                                // term already seen in this doc\n            // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n            p.docFreq++;\n\n            proxCode = position-p.lastPosition;\n\n            if (doVectors) {\n              vector = p.vector;\n              if (vector == null)\n                vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n                offsetStartCode = offsetStart-vector.lastOffset;\n              }\n            }\n          }\n        } else {\t\t\t\t\t  // term not seen before\n          // System.out.println(\"    never seen docID=\" + docID);\n\n          // Refill?\n          if (0 == postingsFreeCount) {\n            postingsFreeCount = postingsFreeList.length;\n            getPostings(postingsFreeList);\n          }\n\n          // Pull next free Posting from free list\n          p = postingsFreeList[--postingsFreeCount];\n\n          final int textLen1 = 1+tokenTextLen;\n          if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE)\n            charPool.nextBuffer();\n          final char[] text = charPool.buffer;\n          final int textUpto = charPool.byteUpto;\n          p.textStart = textUpto + charPool.byteOffset;\n          charPool.byteUpto += textLen1;\n\n          if (tokenString == null)\n            System.arraycopy(tokenText, tokenTextOffset, text, textUpto, tokenTextLen);\n          else\n            tokenString.getChars(0, tokenTextLen, text, textUpto);\n\n          text[textUpto+tokenTextLen] = 0xffff;\n          \n          assert postingsHash[hashPos] == null;\n\n          postingsHash[hashPos] = p;\n          numPostings++;\n\n          if (numPostings == postingsHashHalfSize)\n            rehashPostings(2*postingsHashSize);\n\n          // Init first slice for freq & prox streams\n          final int firstSize = levelSizeArray[0];\n\n          final int upto1 = postingsPool.newSlice(firstSize);\n          p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n          final int upto2 = postingsPool.newSlice(firstSize);\n          p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n          p.lastDocCode = docID << 1;\n          p.lastDocID = docID;\n          p.docFreq = 1;\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offsetStartCode = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n        }\n\n        proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n        prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n        assert prox != null;\n\n        if (payload != null && payload.length > 0) {\n          writeProxVInt((proxCode<<1)|1);\n          writeProxVInt(payload.length);\n          writeProxBytes(payload.data, payload.offset, payload.length);\n          fieldInfo.storePayloads = true;\n        } else\n          writeProxVInt(proxCode<<1);\n\n        p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n        p.lastPosition = position++;\n\n        if (doVectorPositions) {\n          posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n          pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n          writePosVInt(proxCode);\n          vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        if (doVectorOffsets) {\n          offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n          offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n          writeOffsetVInt(offsetStartCode);\n          writeOffsetVInt(offsetEnd-offsetStart);\n          vector.lastOffset = offsetEnd;\n          vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n        }\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"95fcbec956d64aff326919be88d27ba5b60c046e","date":1187017143,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","sourceNew":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition(Token token) {\n\n        final Payload payload = token.getPayload();\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n        final int tokenTextLen = token.termLength();\n\n        int code = 0;\n        int code2 = 0;\n\n        // Compute hashcode\n        int downto = tokenTextLen;\n        while (downto > 0)\n          code = (code*31) + tokenText[--downto];\n\n        // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        int hashPos = code & postingsHashMask;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = ((code>>8)+code)|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenText, tokenTextLen));\n        }\n        \n        final int proxCode;\n\n        if (p != null) {       // term seen since last flush\n\n          if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n            // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n            assert p.docFreq > 0;\n\n            // Now that we know doc freq for previous doc,\n            // write it & lastDocCode\n            freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n            freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n            if (1 == p.docFreq)\n              writeFreqVInt(p.lastDocCode|1);\n            else {\n              writeFreqVInt(p.lastDocCode);\n              writeFreqVInt(p.docFreq);\n            }\n            p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStartCode = offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n\n            p.docFreq = 1;\n\n            // Store code so we can write this after we're\n            // done with this new doc\n            p.lastDocCode = (docID-p.lastDocID) << 1;\n            p.lastDocID = docID;\n\n          } else {                                // term already seen in this doc\n            // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n            p.docFreq++;\n\n            proxCode = position-p.lastPosition;\n\n            if (doVectors) {\n              vector = p.vector;\n              if (vector == null)\n                vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n                offsetStartCode = offsetStart-vector.lastOffset;\n              }\n            }\n          }\n        } else {\t\t\t\t\t  // term not seen before\n          // System.out.println(\"    never seen docID=\" + docID);\n\n          // Refill?\n          if (0 == postingsFreeCount) {\n            postingsFreeCount = postingsFreeList.length;\n            getPostings(postingsFreeList);\n          }\n\n          // Pull next free Posting from free list\n          p = postingsFreeList[--postingsFreeCount];\n\n          final int textLen1 = 1+tokenTextLen;\n          if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE)\n            charPool.nextBuffer();\n          final char[] text = charPool.buffer;\n          final int textUpto = charPool.byteUpto;\n          p.textStart = textUpto + charPool.byteOffset;\n          charPool.byteUpto += textLen1;\n\n          System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n          text[textUpto+tokenTextLen] = 0xffff;\n          \n          assert postingsHash[hashPos] == null;\n\n          postingsHash[hashPos] = p;\n          numPostings++;\n\n          if (numPostings == postingsHashHalfSize)\n            rehashPostings(2*postingsHashSize);\n\n          // Init first slice for freq & prox streams\n          final int firstSize = levelSizeArray[0];\n\n          final int upto1 = postingsPool.newSlice(firstSize);\n          p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n          final int upto2 = postingsPool.newSlice(firstSize);\n          p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n          p.lastDocCode = docID << 1;\n          p.lastDocID = docID;\n          p.docFreq = 1;\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offsetStartCode = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n        }\n\n        proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n        prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n        assert prox != null;\n\n        if (payload != null && payload.length > 0) {\n          writeProxVInt((proxCode<<1)|1);\n          writeProxVInt(payload.length);\n          writeProxBytes(payload.data, payload.offset, payload.length);\n          fieldInfo.storePayloads = true;\n        } else\n          writeProxVInt(proxCode<<1);\n\n        p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n        p.lastPosition = position++;\n\n        if (doVectorPositions) {\n          posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n          pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n          writePosVInt(proxCode);\n          vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        if (doVectorOffsets) {\n          offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n          offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n          writeOffsetVInt(offsetStartCode);\n          writeOffsetVInt(offsetEnd-offsetStart);\n          vector.lastOffset = offsetEnd;\n          vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n        }\n      }\n\n","sourceOld":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition(Token token) {\n\n        final Payload payload = token.getPayload();\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n        final int tokenTextLen = token.termLength();\n\n        int code = 0;\n        int code2 = 0;\n\n        // Compute hashcode\n        int downto = tokenTextLen;\n        while (downto > 0)\n          code = (code*31) + tokenText[--downto];\n\n        // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        int hashPos = code & postingsHashMask;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = code*1347|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenText, tokenTextLen));\n        }\n        \n        final int proxCode;\n\n        if (p != null) {       // term seen since last flush\n\n          if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n            // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n            assert p.docFreq > 0;\n\n            // Now that we know doc freq for previous doc,\n            // write it & lastDocCode\n            freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n            freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n            if (1 == p.docFreq)\n              writeFreqVInt(p.lastDocCode|1);\n            else {\n              writeFreqVInt(p.lastDocCode);\n              writeFreqVInt(p.docFreq);\n            }\n            p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStartCode = offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n\n            p.docFreq = 1;\n\n            // Store code so we can write this after we're\n            // done with this new doc\n            p.lastDocCode = (docID-p.lastDocID) << 1;\n            p.lastDocID = docID;\n\n          } else {                                // term already seen in this doc\n            // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n            p.docFreq++;\n\n            proxCode = position-p.lastPosition;\n\n            if (doVectors) {\n              vector = p.vector;\n              if (vector == null)\n                vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n                offsetStartCode = offsetStart-vector.lastOffset;\n              }\n            }\n          }\n        } else {\t\t\t\t\t  // term not seen before\n          // System.out.println(\"    never seen docID=\" + docID);\n\n          // Refill?\n          if (0 == postingsFreeCount) {\n            postingsFreeCount = postingsFreeList.length;\n            getPostings(postingsFreeList);\n          }\n\n          // Pull next free Posting from free list\n          p = postingsFreeList[--postingsFreeCount];\n\n          final int textLen1 = 1+tokenTextLen;\n          if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE)\n            charPool.nextBuffer();\n          final char[] text = charPool.buffer;\n          final int textUpto = charPool.byteUpto;\n          p.textStart = textUpto + charPool.byteOffset;\n          charPool.byteUpto += textLen1;\n\n          System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n          text[textUpto+tokenTextLen] = 0xffff;\n          \n          assert postingsHash[hashPos] == null;\n\n          postingsHash[hashPos] = p;\n          numPostings++;\n\n          if (numPostings == postingsHashHalfSize)\n            rehashPostings(2*postingsHashSize);\n\n          // Init first slice for freq & prox streams\n          final int firstSize = levelSizeArray[0];\n\n          final int upto1 = postingsPool.newSlice(firstSize);\n          p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n          final int upto2 = postingsPool.newSlice(firstSize);\n          p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n          p.lastDocCode = docID << 1;\n          p.lastDocID = docID;\n          p.docFreq = 1;\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offsetStartCode = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n        }\n\n        proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n        prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n        assert prox != null;\n\n        if (payload != null && payload.length > 0) {\n          writeProxVInt((proxCode<<1)|1);\n          writeProxVInt(payload.length);\n          writeProxBytes(payload.data, payload.offset, payload.length);\n          fieldInfo.storePayloads = true;\n        } else\n          writeProxVInt(proxCode<<1);\n\n        p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n        p.lastPosition = position++;\n\n        if (doVectorPositions) {\n          posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n          pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n          writePosVInt(proxCode);\n          vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        if (doVectorOffsets) {\n          offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n          offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n          writeOffsetVInt(offsetStartCode);\n          writeOffsetVInt(offsetEnd-offsetStart);\n          vector.lastOffset = offsetEnd;\n          vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n        }\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"da249b441376287ae32d1604bc7b50b35b351d09","date":1187478914,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","sourceNew":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition(Token token) {\n\n        final Payload payload = token.getPayload();\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n        final int tokenTextLen = token.termLength();\n\n        int code = 0;\n        int code2 = 0;\n\n        // Compute hashcode\n        int downto = tokenTextLen;\n        while (downto > 0)\n          code = (code*31) + tokenText[--downto];\n\n        // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        int hashPos = code & postingsHashMask;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = ((code>>8)+code)|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenText, tokenTextLen));\n        }\n        \n        final int proxCode;\n\n        if (p != null) {       // term seen since last flush\n\n          if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n            // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n            assert p.docFreq > 0;\n\n            // Now that we know doc freq for previous doc,\n            // write it & lastDocCode\n            freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n            freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n            if (1 == p.docFreq)\n              writeFreqVInt(p.lastDocCode|1);\n            else {\n              writeFreqVInt(p.lastDocCode);\n              writeFreqVInt(p.docFreq);\n            }\n            p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStartCode = offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n\n            p.docFreq = 1;\n\n            // Store code so we can write this after we're\n            // done with this new doc\n            p.lastDocCode = (docID-p.lastDocID) << 1;\n            p.lastDocID = docID;\n\n          } else {                                // term already seen in this doc\n            // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n            p.docFreq++;\n\n            proxCode = position-p.lastPosition;\n\n            if (doVectors) {\n              vector = p.vector;\n              if (vector == null)\n                vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n                offsetStartCode = offsetStart-vector.lastOffset;\n              }\n            }\n          }\n        } else {\t\t\t\t\t  // term not seen before\n          // System.out.println(\"    never seen docID=\" + docID);\n\n          // Refill?\n          if (0 == postingsFreeCount) {\n            postingsFreeCount = postingsFreeList.length;\n            getPostings(postingsFreeList);\n          }\n\n          // Pull next free Posting from free list\n          p = postingsFreeList[--postingsFreeCount];\n\n          final int textLen1 = 1+tokenTextLen;\n          if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE) {\n            if (textLen1 > CHAR_BLOCK_SIZE)\n              throw new IllegalArgumentException(\"term length \" + tokenTextLen + \" exceeds max term length \" + (CHAR_BLOCK_SIZE-1));\n            charPool.nextBuffer();\n          }\n          final char[] text = charPool.buffer;\n          final int textUpto = charPool.byteUpto;\n          p.textStart = textUpto + charPool.byteOffset;\n          charPool.byteUpto += textLen1;\n\n          System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n          text[textUpto+tokenTextLen] = 0xffff;\n          \n          assert postingsHash[hashPos] == null;\n\n          postingsHash[hashPos] = p;\n          numPostings++;\n\n          if (numPostings == postingsHashHalfSize)\n            rehashPostings(2*postingsHashSize);\n\n          // Init first slice for freq & prox streams\n          final int firstSize = levelSizeArray[0];\n\n          final int upto1 = postingsPool.newSlice(firstSize);\n          p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n          final int upto2 = postingsPool.newSlice(firstSize);\n          p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n          p.lastDocCode = docID << 1;\n          p.lastDocID = docID;\n          p.docFreq = 1;\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offsetStartCode = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n        }\n\n        proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n        prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n        assert prox != null;\n\n        if (payload != null && payload.length > 0) {\n          writeProxVInt((proxCode<<1)|1);\n          writeProxVInt(payload.length);\n          writeProxBytes(payload.data, payload.offset, payload.length);\n          fieldInfo.storePayloads = true;\n        } else\n          writeProxVInt(proxCode<<1);\n\n        p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n        p.lastPosition = position++;\n\n        if (doVectorPositions) {\n          posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n          pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n          writePosVInt(proxCode);\n          vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        if (doVectorOffsets) {\n          offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n          offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n          writeOffsetVInt(offsetStartCode);\n          writeOffsetVInt(offsetEnd-offsetStart);\n          vector.lastOffset = offsetEnd;\n          vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n        }\n      }\n\n","sourceOld":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition(Token token) {\n\n        final Payload payload = token.getPayload();\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n        final int tokenTextLen = token.termLength();\n\n        int code = 0;\n        int code2 = 0;\n\n        // Compute hashcode\n        int downto = tokenTextLen;\n        while (downto > 0)\n          code = (code*31) + tokenText[--downto];\n\n        // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        int hashPos = code & postingsHashMask;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = ((code>>8)+code)|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenText, tokenTextLen));\n        }\n        \n        final int proxCode;\n\n        if (p != null) {       // term seen since last flush\n\n          if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n            // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n            assert p.docFreq > 0;\n\n            // Now that we know doc freq for previous doc,\n            // write it & lastDocCode\n            freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n            freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n            if (1 == p.docFreq)\n              writeFreqVInt(p.lastDocCode|1);\n            else {\n              writeFreqVInt(p.lastDocCode);\n              writeFreqVInt(p.docFreq);\n            }\n            p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStartCode = offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n\n            p.docFreq = 1;\n\n            // Store code so we can write this after we're\n            // done with this new doc\n            p.lastDocCode = (docID-p.lastDocID) << 1;\n            p.lastDocID = docID;\n\n          } else {                                // term already seen in this doc\n            // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n            p.docFreq++;\n\n            proxCode = position-p.lastPosition;\n\n            if (doVectors) {\n              vector = p.vector;\n              if (vector == null)\n                vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n                offsetStartCode = offsetStart-vector.lastOffset;\n              }\n            }\n          }\n        } else {\t\t\t\t\t  // term not seen before\n          // System.out.println(\"    never seen docID=\" + docID);\n\n          // Refill?\n          if (0 == postingsFreeCount) {\n            postingsFreeCount = postingsFreeList.length;\n            getPostings(postingsFreeList);\n          }\n\n          // Pull next free Posting from free list\n          p = postingsFreeList[--postingsFreeCount];\n\n          final int textLen1 = 1+tokenTextLen;\n          if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE)\n            charPool.nextBuffer();\n          final char[] text = charPool.buffer;\n          final int textUpto = charPool.byteUpto;\n          p.textStart = textUpto + charPool.byteOffset;\n          charPool.byteUpto += textLen1;\n\n          System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n          text[textUpto+tokenTextLen] = 0xffff;\n          \n          assert postingsHash[hashPos] == null;\n\n          postingsHash[hashPos] = p;\n          numPostings++;\n\n          if (numPostings == postingsHashHalfSize)\n            rehashPostings(2*postingsHashSize);\n\n          // Init first slice for freq & prox streams\n          final int firstSize = levelSizeArray[0];\n\n          final int upto1 = postingsPool.newSlice(firstSize);\n          p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n          final int upto2 = postingsPool.newSlice(firstSize);\n          p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n          p.lastDocCode = docID << 1;\n          p.lastDocID = docID;\n          p.docFreq = 1;\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offsetStartCode = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n        }\n\n        proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n        prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n        assert prox != null;\n\n        if (payload != null && payload.length > 0) {\n          writeProxVInt((proxCode<<1)|1);\n          writeProxVInt(payload.length);\n          writeProxBytes(payload.data, payload.offset, payload.length);\n          fieldInfo.storePayloads = true;\n        } else\n          writeProxVInt(proxCode<<1);\n\n        p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n        p.lastPosition = position++;\n\n        if (doVectorPositions) {\n          posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n          pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n          writePosVInt(proxCode);\n          vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        if (doVectorOffsets) {\n          offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n          offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n          writeOffsetVInt(offsetStartCode);\n          writeOffsetVInt(offsetEnd-offsetStart);\n          vector.lastOffset = offsetEnd;\n          vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n        }\n      }\n\n","bugFix":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"bugIntro":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b6a1f29c9b1051488fd5fa7d56c98db5f4388408","date":1196281221,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","sourceNew":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition(Token token) {\n\n        final Payload payload = token.getPayload();\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n        final int tokenTextLen = token.termLength();\n\n        int code = 0;\n\n        // Compute hashcode\n        int downto = tokenTextLen;\n        while (downto > 0)\n          code = (code*31) + tokenText[--downto];\n\n        // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        int hashPos = code & postingsHashMask;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = ((code>>8)+code)|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenText, tokenTextLen));\n        }\n        \n        final int proxCode;\n\n        if (p != null) {       // term seen since last flush\n\n          if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n            // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n            assert p.docFreq > 0;\n\n            // Now that we know doc freq for previous doc,\n            // write it & lastDocCode\n            freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n            freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n            if (1 == p.docFreq)\n              writeFreqVInt(p.lastDocCode|1);\n            else {\n              writeFreqVInt(p.lastDocCode);\n              writeFreqVInt(p.docFreq);\n            }\n            p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStartCode = offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n\n            p.docFreq = 1;\n\n            // Store code so we can write this after we're\n            // done with this new doc\n            p.lastDocCode = (docID-p.lastDocID) << 1;\n            p.lastDocID = docID;\n\n          } else {                                // term already seen in this doc\n            // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n            p.docFreq++;\n\n            proxCode = position-p.lastPosition;\n\n            if (doVectors) {\n              vector = p.vector;\n              if (vector == null)\n                vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n                offsetStartCode = offsetStart-vector.lastOffset;\n              }\n            }\n          }\n        } else {\t\t\t\t\t  // term not seen before\n          // System.out.println(\"    never seen docID=\" + docID);\n\n          // Refill?\n          if (0 == postingsFreeCount) {\n            postingsFreeCount = postingsFreeList.length;\n            getPostings(postingsFreeList);\n          }\n\n          // Pull next free Posting from free list\n          p = postingsFreeList[--postingsFreeCount];\n\n          final int textLen1 = 1+tokenTextLen;\n          if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE) {\n            if (textLen1 > CHAR_BLOCK_SIZE)\n              throw new IllegalArgumentException(\"term length \" + tokenTextLen + \" exceeds max term length \" + (CHAR_BLOCK_SIZE-1));\n            charPool.nextBuffer();\n          }\n          final char[] text = charPool.buffer;\n          final int textUpto = charPool.byteUpto;\n          p.textStart = textUpto + charPool.byteOffset;\n          charPool.byteUpto += textLen1;\n\n          System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n          text[textUpto+tokenTextLen] = 0xffff;\n          \n          assert postingsHash[hashPos] == null;\n\n          postingsHash[hashPos] = p;\n          numPostings++;\n\n          if (numPostings == postingsHashHalfSize)\n            rehashPostings(2*postingsHashSize);\n\n          // Init first slice for freq & prox streams\n          final int firstSize = levelSizeArray[0];\n\n          final int upto1 = postingsPool.newSlice(firstSize);\n          p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n          final int upto2 = postingsPool.newSlice(firstSize);\n          p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n          p.lastDocCode = docID << 1;\n          p.lastDocID = docID;\n          p.docFreq = 1;\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offsetStartCode = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n        }\n\n        proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n        prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n        assert prox != null;\n\n        if (payload != null && payload.length > 0) {\n          writeProxVInt((proxCode<<1)|1);\n          writeProxVInt(payload.length);\n          writeProxBytes(payload.data, payload.offset, payload.length);\n          fieldInfo.storePayloads = true;\n        } else\n          writeProxVInt(proxCode<<1);\n\n        p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n        p.lastPosition = position++;\n\n        if (doVectorPositions) {\n          posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n          pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n          writePosVInt(proxCode);\n          vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        if (doVectorOffsets) {\n          offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n          offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n          writeOffsetVInt(offsetStartCode);\n          writeOffsetVInt(offsetEnd-offsetStart);\n          vector.lastOffset = offsetEnd;\n          vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n        }\n      }\n\n","sourceOld":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition(Token token) {\n\n        final Payload payload = token.getPayload();\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n        final int tokenTextLen = token.termLength();\n\n        int code = 0;\n        int code2 = 0;\n\n        // Compute hashcode\n        int downto = tokenTextLen;\n        while (downto > 0)\n          code = (code*31) + tokenText[--downto];\n\n        // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        int hashPos = code & postingsHashMask;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = ((code>>8)+code)|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenText, tokenTextLen));\n        }\n        \n        final int proxCode;\n\n        if (p != null) {       // term seen since last flush\n\n          if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n            // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n            assert p.docFreq > 0;\n\n            // Now that we know doc freq for previous doc,\n            // write it & lastDocCode\n            freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n            freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n            if (1 == p.docFreq)\n              writeFreqVInt(p.lastDocCode|1);\n            else {\n              writeFreqVInt(p.lastDocCode);\n              writeFreqVInt(p.docFreq);\n            }\n            p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStartCode = offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n\n            p.docFreq = 1;\n\n            // Store code so we can write this after we're\n            // done with this new doc\n            p.lastDocCode = (docID-p.lastDocID) << 1;\n            p.lastDocID = docID;\n\n          } else {                                // term already seen in this doc\n            // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n            p.docFreq++;\n\n            proxCode = position-p.lastPosition;\n\n            if (doVectors) {\n              vector = p.vector;\n              if (vector == null)\n                vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n                offsetStartCode = offsetStart-vector.lastOffset;\n              }\n            }\n          }\n        } else {\t\t\t\t\t  // term not seen before\n          // System.out.println(\"    never seen docID=\" + docID);\n\n          // Refill?\n          if (0 == postingsFreeCount) {\n            postingsFreeCount = postingsFreeList.length;\n            getPostings(postingsFreeList);\n          }\n\n          // Pull next free Posting from free list\n          p = postingsFreeList[--postingsFreeCount];\n\n          final int textLen1 = 1+tokenTextLen;\n          if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE) {\n            if (textLen1 > CHAR_BLOCK_SIZE)\n              throw new IllegalArgumentException(\"term length \" + tokenTextLen + \" exceeds max term length \" + (CHAR_BLOCK_SIZE-1));\n            charPool.nextBuffer();\n          }\n          final char[] text = charPool.buffer;\n          final int textUpto = charPool.byteUpto;\n          p.textStart = textUpto + charPool.byteOffset;\n          charPool.byteUpto += textLen1;\n\n          System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n          text[textUpto+tokenTextLen] = 0xffff;\n          \n          assert postingsHash[hashPos] == null;\n\n          postingsHash[hashPos] = p;\n          numPostings++;\n\n          if (numPostings == postingsHashHalfSize)\n            rehashPostings(2*postingsHashSize);\n\n          // Init first slice for freq & prox streams\n          final int firstSize = levelSizeArray[0];\n\n          final int upto1 = postingsPool.newSlice(firstSize);\n          p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n          final int upto2 = postingsPool.newSlice(firstSize);\n          p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n          p.lastDocCode = docID << 1;\n          p.lastDocID = docID;\n          p.docFreq = 1;\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offsetStartCode = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n        }\n\n        proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n        prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n        assert prox != null;\n\n        if (payload != null && payload.length > 0) {\n          writeProxVInt((proxCode<<1)|1);\n          writeProxVInt(payload.length);\n          writeProxBytes(payload.data, payload.offset, payload.length);\n          fieldInfo.storePayloads = true;\n        } else\n          writeProxVInt(proxCode<<1);\n\n        p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n        p.lastPosition = position++;\n\n        if (doVectorPositions) {\n          posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n          pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n          writePosVInt(proxCode);\n          vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        if (doVectorOffsets) {\n          offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n          offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n          writeOffsetVInt(offsetStartCode);\n          writeOffsetVInt(offsetEnd-offsetStart);\n          vector.lastOffset = offsetEnd;\n          vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n        }\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"741a5cca05cabe1e7482410a29e563a08379251a","date":1196676550,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","sourceNew":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition(Token token) {\n\n        final Payload payload = token.getPayload();\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n        final int tokenTextLen = token.termLength();\n\n        int code = 0;\n\n        // Compute hashcode\n        int downto = tokenTextLen;\n        while (downto > 0)\n          code = (code*31) + tokenText[--downto];\n\n        // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        int hashPos = code & postingsHashMask;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = ((code>>8)+code)|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenText, tokenTextLen));\n        }\n        \n        final int proxCode;\n\n        if (p != null) {       // term seen since last flush\n\n          if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n            // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n            assert p.docFreq > 0;\n\n            // Now that we know doc freq for previous doc,\n            // write it & lastDocCode\n            freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n            freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n            if (1 == p.docFreq)\n              writeFreqVInt(p.lastDocCode|1);\n            else {\n              writeFreqVInt(p.lastDocCode);\n              writeFreqVInt(p.docFreq);\n            }\n            p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStartCode = offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n\n            p.docFreq = 1;\n\n            // Store code so we can write this after we're\n            // done with this new doc\n            p.lastDocCode = (docID-p.lastDocID) << 1;\n            p.lastDocID = docID;\n\n          } else {                                // term already seen in this doc\n            // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n            p.docFreq++;\n\n            proxCode = position-p.lastPosition;\n\n            if (doVectors) {\n              vector = p.vector;\n              if (vector == null)\n                vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n                offsetStartCode = offsetStart-vector.lastOffset;\n              }\n            }\n          }\n        } else {\t\t\t\t\t  // term not seen before\n          // System.out.println(\"    never seen docID=\" + docID);\n\n          // Refill?\n          if (0 == postingsFreeCount) {\n            postingsFreeCount = postingsFreeList.length;\n            getPostings(postingsFreeList);\n          }\n\n          final int textLen1 = 1+tokenTextLen;\n          if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE) {\n            if (textLen1 > CHAR_BLOCK_SIZE) {\n              maxTermHit = tokenTextLen;\n              // Just skip this term; we will throw an\n              // exception after processing all accepted\n              // terms in the doc\n              return;\n            }\n            charPool.nextBuffer();\n          }\n          final char[] text = charPool.buffer;\n          final int textUpto = charPool.byteUpto;\n\n          // Pull next free Posting from free list\n          p = postingsFreeList[--postingsFreeCount];\n\n          p.textStart = textUpto + charPool.byteOffset;\n          charPool.byteUpto += textLen1;\n\n          System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n          text[textUpto+tokenTextLen] = 0xffff;\n          \n          assert postingsHash[hashPos] == null;\n\n          postingsHash[hashPos] = p;\n          numPostings++;\n\n          if (numPostings == postingsHashHalfSize)\n            rehashPostings(2*postingsHashSize);\n\n          // Init first slice for freq & prox streams\n          final int firstSize = levelSizeArray[0];\n\n          final int upto1 = postingsPool.newSlice(firstSize);\n          p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n          final int upto2 = postingsPool.newSlice(firstSize);\n          p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n          p.lastDocCode = docID << 1;\n          p.lastDocID = docID;\n          p.docFreq = 1;\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offsetStartCode = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n        }\n\n        proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n        prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n        assert prox != null;\n\n        if (payload != null && payload.length > 0) {\n          writeProxVInt((proxCode<<1)|1);\n          writeProxVInt(payload.length);\n          writeProxBytes(payload.data, payload.offset, payload.length);\n          fieldInfo.storePayloads = true;\n        } else\n          writeProxVInt(proxCode<<1);\n\n        p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n        p.lastPosition = position++;\n\n        if (doVectorPositions) {\n          posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n          pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n          writePosVInt(proxCode);\n          vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        if (doVectorOffsets) {\n          offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n          offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n          writeOffsetVInt(offsetStartCode);\n          writeOffsetVInt(offsetEnd-offsetStart);\n          vector.lastOffset = offsetEnd;\n          vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n        }\n      }\n\n","sourceOld":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition(Token token) {\n\n        final Payload payload = token.getPayload();\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n        final int tokenTextLen = token.termLength();\n\n        int code = 0;\n\n        // Compute hashcode\n        int downto = tokenTextLen;\n        while (downto > 0)\n          code = (code*31) + tokenText[--downto];\n\n        // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        int hashPos = code & postingsHashMask;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = ((code>>8)+code)|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenText, tokenTextLen));\n        }\n        \n        final int proxCode;\n\n        if (p != null) {       // term seen since last flush\n\n          if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n            // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n            assert p.docFreq > 0;\n\n            // Now that we know doc freq for previous doc,\n            // write it & lastDocCode\n            freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n            freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n            if (1 == p.docFreq)\n              writeFreqVInt(p.lastDocCode|1);\n            else {\n              writeFreqVInt(p.lastDocCode);\n              writeFreqVInt(p.docFreq);\n            }\n            p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStartCode = offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n\n            p.docFreq = 1;\n\n            // Store code so we can write this after we're\n            // done with this new doc\n            p.lastDocCode = (docID-p.lastDocID) << 1;\n            p.lastDocID = docID;\n\n          } else {                                // term already seen in this doc\n            // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n            p.docFreq++;\n\n            proxCode = position-p.lastPosition;\n\n            if (doVectors) {\n              vector = p.vector;\n              if (vector == null)\n                vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n                offsetStartCode = offsetStart-vector.lastOffset;\n              }\n            }\n          }\n        } else {\t\t\t\t\t  // term not seen before\n          // System.out.println(\"    never seen docID=\" + docID);\n\n          // Refill?\n          if (0 == postingsFreeCount) {\n            postingsFreeCount = postingsFreeList.length;\n            getPostings(postingsFreeList);\n          }\n\n          // Pull next free Posting from free list\n          p = postingsFreeList[--postingsFreeCount];\n\n          final int textLen1 = 1+tokenTextLen;\n          if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE) {\n            if (textLen1 > CHAR_BLOCK_SIZE)\n              throw new IllegalArgumentException(\"term length \" + tokenTextLen + \" exceeds max term length \" + (CHAR_BLOCK_SIZE-1));\n            charPool.nextBuffer();\n          }\n          final char[] text = charPool.buffer;\n          final int textUpto = charPool.byteUpto;\n          p.textStart = textUpto + charPool.byteOffset;\n          charPool.byteUpto += textLen1;\n\n          System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n          text[textUpto+tokenTextLen] = 0xffff;\n          \n          assert postingsHash[hashPos] == null;\n\n          postingsHash[hashPos] = p;\n          numPostings++;\n\n          if (numPostings == postingsHashHalfSize)\n            rehashPostings(2*postingsHashSize);\n\n          // Init first slice for freq & prox streams\n          final int firstSize = levelSizeArray[0];\n\n          final int upto1 = postingsPool.newSlice(firstSize);\n          p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n          final int upto2 = postingsPool.newSlice(firstSize);\n          p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n          p.lastDocCode = docID << 1;\n          p.lastDocID = docID;\n          p.docFreq = 1;\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offsetStartCode = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n        }\n\n        proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n        prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n        assert prox != null;\n\n        if (payload != null && payload.length > 0) {\n          writeProxVInt((proxCode<<1)|1);\n          writeProxVInt(payload.length);\n          writeProxBytes(payload.data, payload.offset, payload.length);\n          fieldInfo.storePayloads = true;\n        } else\n          writeProxVInt(proxCode<<1);\n\n        p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n        p.lastPosition = position++;\n\n        if (doVectorPositions) {\n          posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n          pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n          writePosVInt(proxCode);\n          vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        if (doVectorOffsets) {\n          offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n          offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n          writeOffsetVInt(offsetStartCode);\n          writeOffsetVInt(offsetEnd-offsetStart);\n          vector.lastOffset = offsetEnd;\n          vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n        }\n      }\n\n","bugFix":null,"bugIntro":["83bbb041887bbef07b8a98d08a0e1713ce137039","5a251aa47d1808cbae42c0e172d698c377430e60"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8560794cda5bcd510c60e38ed553e9c5a6204983","date":1196807382,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","sourceNew":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition(Token token) {\n\n        final Payload payload = token.getPayload();\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n        final int tokenTextLen = token.termLength();\n\n        int code = 0;\n\n        // Compute hashcode\n        int downto = tokenTextLen;\n        while (downto > 0)\n          code = (code*31) + tokenText[--downto];\n\n        // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        int hashPos = code & postingsHashMask;\n\n        assert !postingsCompacted;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = ((code>>8)+code)|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenText, tokenTextLen));\n        }\n        \n        final int proxCode;\n\n        // If we hit an exception below, it's possible the\n        // posting list or term vectors data will be\n        // partially written and thus inconsistent if\n        // flushed, so we have to abort:\n        abortOnExc = true;\n\n        if (p != null) {       // term seen since last flush\n\n          if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n            // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n            assert p.docFreq > 0;\n\n            // Now that we know doc freq for previous doc,\n            // write it & lastDocCode\n            freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n            freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n            if (1 == p.docFreq)\n              writeFreqVInt(p.lastDocCode|1);\n            else {\n              writeFreqVInt(p.lastDocCode);\n              writeFreqVInt(p.docFreq);\n            }\n            p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStartCode = offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n\n            p.docFreq = 1;\n\n            // Store code so we can write this after we're\n            // done with this new doc\n            p.lastDocCode = (docID-p.lastDocID) << 1;\n            p.lastDocID = docID;\n\n          } else {                                // term already seen in this doc\n            // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n            p.docFreq++;\n\n            proxCode = position-p.lastPosition;\n\n            if (doVectors) {\n              vector = p.vector;\n              if (vector == null)\n                vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n                offsetStartCode = offsetStart-vector.lastOffset;\n              }\n            }\n          }\n        } else {\t\t\t\t\t  // term not seen before\n          // System.out.println(\"    never seen docID=\" + docID);\n\n          // Refill?\n          if (0 == postingsFreeCount) {\n            postingsFreeCount = postingsFreeList.length;\n            getPostings(postingsFreeList);\n          }\n\n          final int textLen1 = 1+tokenTextLen;\n          if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE) {\n            if (textLen1 > CHAR_BLOCK_SIZE) {\n              maxTermHit = tokenTextLen;\n              // Just skip this term; we will throw an\n              // exception after processing all accepted\n              // terms in the doc\n              abortOnExc = false;\n              return;\n            }\n            charPool.nextBuffer();\n          }\n          final char[] text = charPool.buffer;\n          final int textUpto = charPool.byteUpto;\n\n          // Pull next free Posting from free list\n          p = postingsFreeList[--postingsFreeCount];\n\n          p.textStart = textUpto + charPool.byteOffset;\n          charPool.byteUpto += textLen1;\n\n          System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n          text[textUpto+tokenTextLen] = 0xffff;\n          \n          assert postingsHash[hashPos] == null;\n\n          postingsHash[hashPos] = p;\n          numPostings++;\n\n          if (numPostings == postingsHashHalfSize)\n            rehashPostings(2*postingsHashSize);\n\n          // Init first slice for freq & prox streams\n          final int firstSize = levelSizeArray[0];\n\n          final int upto1 = postingsPool.newSlice(firstSize);\n          p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n          final int upto2 = postingsPool.newSlice(firstSize);\n          p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n          p.lastDocCode = docID << 1;\n          p.lastDocID = docID;\n          p.docFreq = 1;\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offsetStartCode = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n        }\n\n        proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n        prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n        assert prox != null;\n\n        if (payload != null && payload.length > 0) {\n          writeProxVInt((proxCode<<1)|1);\n          writeProxVInt(payload.length);\n          writeProxBytes(payload.data, payload.offset, payload.length);\n          fieldInfo.storePayloads = true;\n        } else\n          writeProxVInt(proxCode<<1);\n\n        p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n        p.lastPosition = position++;\n\n        if (doVectorPositions) {\n          posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n          pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n          writePosVInt(proxCode);\n          vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        if (doVectorOffsets) {\n          offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n          offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n          writeOffsetVInt(offsetStartCode);\n          writeOffsetVInt(offsetEnd-offsetStart);\n          vector.lastOffset = offsetEnd;\n          vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        abortOnExc = false;\n      }\n\n","sourceOld":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition(Token token) {\n\n        final Payload payload = token.getPayload();\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n        final int tokenTextLen = token.termLength();\n\n        int code = 0;\n\n        // Compute hashcode\n        int downto = tokenTextLen;\n        while (downto > 0)\n          code = (code*31) + tokenText[--downto];\n\n        // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        int hashPos = code & postingsHashMask;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = ((code>>8)+code)|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenText, tokenTextLen));\n        }\n        \n        final int proxCode;\n\n        if (p != null) {       // term seen since last flush\n\n          if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n            // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n            assert p.docFreq > 0;\n\n            // Now that we know doc freq for previous doc,\n            // write it & lastDocCode\n            freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n            freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n            if (1 == p.docFreq)\n              writeFreqVInt(p.lastDocCode|1);\n            else {\n              writeFreqVInt(p.lastDocCode);\n              writeFreqVInt(p.docFreq);\n            }\n            p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStartCode = offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n\n            p.docFreq = 1;\n\n            // Store code so we can write this after we're\n            // done with this new doc\n            p.lastDocCode = (docID-p.lastDocID) << 1;\n            p.lastDocID = docID;\n\n          } else {                                // term already seen in this doc\n            // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n            p.docFreq++;\n\n            proxCode = position-p.lastPosition;\n\n            if (doVectors) {\n              vector = p.vector;\n              if (vector == null)\n                vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n                offsetStartCode = offsetStart-vector.lastOffset;\n              }\n            }\n          }\n        } else {\t\t\t\t\t  // term not seen before\n          // System.out.println(\"    never seen docID=\" + docID);\n\n          // Refill?\n          if (0 == postingsFreeCount) {\n            postingsFreeCount = postingsFreeList.length;\n            getPostings(postingsFreeList);\n          }\n\n          final int textLen1 = 1+tokenTextLen;\n          if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE) {\n            if (textLen1 > CHAR_BLOCK_SIZE) {\n              maxTermHit = tokenTextLen;\n              // Just skip this term; we will throw an\n              // exception after processing all accepted\n              // terms in the doc\n              return;\n            }\n            charPool.nextBuffer();\n          }\n          final char[] text = charPool.buffer;\n          final int textUpto = charPool.byteUpto;\n\n          // Pull next free Posting from free list\n          p = postingsFreeList[--postingsFreeCount];\n\n          p.textStart = textUpto + charPool.byteOffset;\n          charPool.byteUpto += textLen1;\n\n          System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n          text[textUpto+tokenTextLen] = 0xffff;\n          \n          assert postingsHash[hashPos] == null;\n\n          postingsHash[hashPos] = p;\n          numPostings++;\n\n          if (numPostings == postingsHashHalfSize)\n            rehashPostings(2*postingsHashSize);\n\n          // Init first slice for freq & prox streams\n          final int firstSize = levelSizeArray[0];\n\n          final int upto1 = postingsPool.newSlice(firstSize);\n          p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n          final int upto2 = postingsPool.newSlice(firstSize);\n          p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n          p.lastDocCode = docID << 1;\n          p.lastDocID = docID;\n          p.docFreq = 1;\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offsetStartCode = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n        }\n\n        proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n        prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n        assert prox != null;\n\n        if (payload != null && payload.length > 0) {\n          writeProxVInt((proxCode<<1)|1);\n          writeProxVInt(payload.length);\n          writeProxBytes(payload.data, payload.offset, payload.length);\n          fieldInfo.storePayloads = true;\n        } else\n          writeProxVInt(proxCode<<1);\n\n        p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n        p.lastPosition = position++;\n\n        if (doVectorPositions) {\n          posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n          pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n          writePosVInt(proxCode);\n          vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        if (doVectorOffsets) {\n          offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n          offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n          writeOffsetVInt(offsetStartCode);\n          writeOffsetVInt(offsetEnd-offsetStart);\n          vector.lastOffset = offsetEnd;\n          vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n        }\n      }\n\n","bugFix":null,"bugIntro":["c9a0deca56efc5191d6b3c41047fd538f3fae1d8","83bbb041887bbef07b8a98d08a0e1713ce137039"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9a0deca56efc5191d6b3c41047fd538f3fae1d8","date":1198156049,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","sourceNew":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition(Token token) {\n\n        final Payload payload = token.getPayload();\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n        final int tokenTextLen = token.termLength();\n\n        int code = 0;\n\n        // Compute hashcode\n        int downto = tokenTextLen;\n        while (downto > 0)\n          code = (code*31) + tokenText[--downto];\n\n        // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        int hashPos = code & postingsHashMask;\n\n        assert !postingsCompacted;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = ((code>>8)+code)|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenText, tokenTextLen));\n        }\n        \n        final int proxCode;\n\n        // If we hit an exception below, it's possible the\n        // posting list or term vectors data will be\n        // partially written and thus inconsistent if\n        // flushed, so we have to abort all documents\n        // since the last flush:\n        abortOnExc = true;\n\n        if (p != null) {       // term seen since last flush\n\n          if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n            // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n            assert p.docFreq > 0;\n\n            // Now that we know doc freq for previous doc,\n            // write it & lastDocCode\n            freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n            freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n            if (1 == p.docFreq)\n              writeFreqVInt(p.lastDocCode|1);\n            else {\n              writeFreqVInt(p.lastDocCode);\n              writeFreqVInt(p.docFreq);\n            }\n            p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStartCode = offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n\n            p.docFreq = 1;\n\n            // Store code so we can write this after we're\n            // done with this new doc\n            p.lastDocCode = (docID-p.lastDocID) << 1;\n            p.lastDocID = docID;\n\n          } else {                                // term already seen in this doc\n            // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n            p.docFreq++;\n\n            proxCode = position-p.lastPosition;\n\n            if (doVectors) {\n              vector = p.vector;\n              if (vector == null)\n                vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n                offsetStartCode = offsetStart-vector.lastOffset;\n              }\n            }\n          }\n        } else {\t\t\t\t\t  // term not seen before\n          // System.out.println(\"    never seen docID=\" + docID);\n\n          // Refill?\n          if (0 == postingsFreeCount) {\n            postingsFreeCount = postingsFreeList.length;\n            getPostings(postingsFreeList);\n          }\n\n          final int textLen1 = 1+tokenTextLen;\n          if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE) {\n            if (textLen1 > CHAR_BLOCK_SIZE) {\n              maxTermHit = tokenTextLen;\n              // Just skip this term; we will throw an\n              // exception after processing all accepted\n              // terms in the doc\n              abortOnExc = false;\n              return;\n            }\n            charPool.nextBuffer();\n          }\n          final char[] text = charPool.buffer;\n          final int textUpto = charPool.byteUpto;\n\n          // Pull next free Posting from free list\n          p = postingsFreeList[--postingsFreeCount];\n\n          p.textStart = textUpto + charPool.byteOffset;\n          charPool.byteUpto += textLen1;\n\n          System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n          text[textUpto+tokenTextLen] = 0xffff;\n          \n          assert postingsHash[hashPos] == null;\n\n          postingsHash[hashPos] = p;\n          numPostings++;\n\n          if (numPostings == postingsHashHalfSize)\n            rehashPostings(2*postingsHashSize);\n\n          // Init first slice for freq & prox streams\n          final int firstSize = levelSizeArray[0];\n\n          final int upto1 = postingsPool.newSlice(firstSize);\n          p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n          final int upto2 = postingsPool.newSlice(firstSize);\n          p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n          p.lastDocCode = docID << 1;\n          p.lastDocID = docID;\n          p.docFreq = 1;\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offsetStartCode = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n        }\n\n        proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n        prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n        assert prox != null;\n\n        if (payload != null && payload.length > 0) {\n          writeProxVInt((proxCode<<1)|1);\n          writeProxVInt(payload.length);\n          writeProxBytes(payload.data, payload.offset, payload.length);\n          fieldInfo.storePayloads = true;\n        } else\n          writeProxVInt(proxCode<<1);\n\n        p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n        p.lastPosition = position++;\n\n        if (doVectorPositions) {\n          posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n          pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n          writePosVInt(proxCode);\n          vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        if (doVectorOffsets) {\n          offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n          offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n          writeOffsetVInt(offsetStartCode);\n          writeOffsetVInt(offsetEnd-offsetStart);\n          vector.lastOffset = offsetEnd;\n          vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        abortOnExc = false;\n      }\n\n","sourceOld":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition(Token token) {\n\n        final Payload payload = token.getPayload();\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n        final int tokenTextLen = token.termLength();\n\n        int code = 0;\n\n        // Compute hashcode\n        int downto = tokenTextLen;\n        while (downto > 0)\n          code = (code*31) + tokenText[--downto];\n\n        // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        int hashPos = code & postingsHashMask;\n\n        assert !postingsCompacted;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = ((code>>8)+code)|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenText, tokenTextLen));\n        }\n        \n        final int proxCode;\n\n        // If we hit an exception below, it's possible the\n        // posting list or term vectors data will be\n        // partially written and thus inconsistent if\n        // flushed, so we have to abort:\n        abortOnExc = true;\n\n        if (p != null) {       // term seen since last flush\n\n          if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n            // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n            assert p.docFreq > 0;\n\n            // Now that we know doc freq for previous doc,\n            // write it & lastDocCode\n            freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n            freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n            if (1 == p.docFreq)\n              writeFreqVInt(p.lastDocCode|1);\n            else {\n              writeFreqVInt(p.lastDocCode);\n              writeFreqVInt(p.docFreq);\n            }\n            p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStartCode = offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n\n            p.docFreq = 1;\n\n            // Store code so we can write this after we're\n            // done with this new doc\n            p.lastDocCode = (docID-p.lastDocID) << 1;\n            p.lastDocID = docID;\n\n          } else {                                // term already seen in this doc\n            // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n            p.docFreq++;\n\n            proxCode = position-p.lastPosition;\n\n            if (doVectors) {\n              vector = p.vector;\n              if (vector == null)\n                vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n                offsetStartCode = offsetStart-vector.lastOffset;\n              }\n            }\n          }\n        } else {\t\t\t\t\t  // term not seen before\n          // System.out.println(\"    never seen docID=\" + docID);\n\n          // Refill?\n          if (0 == postingsFreeCount) {\n            postingsFreeCount = postingsFreeList.length;\n            getPostings(postingsFreeList);\n          }\n\n          final int textLen1 = 1+tokenTextLen;\n          if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE) {\n            if (textLen1 > CHAR_BLOCK_SIZE) {\n              maxTermHit = tokenTextLen;\n              // Just skip this term; we will throw an\n              // exception after processing all accepted\n              // terms in the doc\n              abortOnExc = false;\n              return;\n            }\n            charPool.nextBuffer();\n          }\n          final char[] text = charPool.buffer;\n          final int textUpto = charPool.byteUpto;\n\n          // Pull next free Posting from free list\n          p = postingsFreeList[--postingsFreeCount];\n\n          p.textStart = textUpto + charPool.byteOffset;\n          charPool.byteUpto += textLen1;\n\n          System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n          text[textUpto+tokenTextLen] = 0xffff;\n          \n          assert postingsHash[hashPos] == null;\n\n          postingsHash[hashPos] = p;\n          numPostings++;\n\n          if (numPostings == postingsHashHalfSize)\n            rehashPostings(2*postingsHashSize);\n\n          // Init first slice for freq & prox streams\n          final int firstSize = levelSizeArray[0];\n\n          final int upto1 = postingsPool.newSlice(firstSize);\n          p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n          final int upto2 = postingsPool.newSlice(firstSize);\n          p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n          p.lastDocCode = docID << 1;\n          p.lastDocID = docID;\n          p.docFreq = 1;\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offsetStartCode = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n        }\n\n        proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n        prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n        assert prox != null;\n\n        if (payload != null && payload.length > 0) {\n          writeProxVInt((proxCode<<1)|1);\n          writeProxVInt(payload.length);\n          writeProxBytes(payload.data, payload.offset, payload.length);\n          fieldInfo.storePayloads = true;\n        } else\n          writeProxVInt(proxCode<<1);\n\n        p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n        p.lastPosition = position++;\n\n        if (doVectorPositions) {\n          posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n          pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n          writePosVInt(proxCode);\n          vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        if (doVectorOffsets) {\n          offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n          offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n          writeOffsetVInt(offsetStartCode);\n          writeOffsetVInt(offsetEnd-offsetStart);\n          vector.lastOffset = offsetEnd;\n          vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        abortOnExc = false;\n      }\n\n","bugFix":["8560794cda5bcd510c60e38ed553e9c5a6204983"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5a251aa47d1808cbae42c0e172d698c377430e60","date":1199375390,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","sourceNew":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition(Token token) {\n\n        final Payload payload = token.getPayload();\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n        final int tokenTextLen = token.termLength();\n\n        int code = 0;\n\n        // Compute hashcode\n        int downto = tokenTextLen;\n        while (downto > 0)\n          code = (code*31) + tokenText[--downto];\n\n        // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        int hashPos = code & postingsHashMask;\n\n        assert !postingsCompacted;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = ((code>>8)+code)|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenText, tokenTextLen));\n        }\n        \n        final int proxCode;\n\n        // If we hit an exception below, it's possible the\n        // posting list or term vectors data will be\n        // partially written and thus inconsistent if\n        // flushed, so we have to abort all documents\n        // since the last flush:\n        abortOnExc = true;\n\n        if (p != null) {       // term seen since last flush\n\n          if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n            // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n            assert p.docFreq > 0;\n\n            // Now that we know doc freq for previous doc,\n            // write it & lastDocCode\n            freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n            freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n            if (1 == p.docFreq)\n              writeFreqVInt(p.lastDocCode|1);\n            else {\n              writeFreqVInt(p.lastDocCode);\n              writeFreqVInt(p.docFreq);\n            }\n            p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStartCode = offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n\n            p.docFreq = 1;\n\n            // Store code so we can write this after we're\n            // done with this new doc\n            p.lastDocCode = (docID-p.lastDocID) << 1;\n            p.lastDocID = docID;\n\n          } else {                                // term already seen in this doc\n            // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n            p.docFreq++;\n\n            proxCode = position-p.lastPosition;\n\n            if (doVectors) {\n              vector = p.vector;\n              if (vector == null)\n                vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n                offsetStartCode = offsetStart-vector.lastOffset;\n              }\n            }\n          }\n        } else {\t\t\t\t\t  // term not seen before\n          // System.out.println(\"    never seen docID=\" + docID);\n\n          // Refill?\n          if (0 == postingsFreeCount) {\n            postingsFreeCount = postingsFreeList.length;\n            getPostings(postingsFreeList);\n          }\n\n          final int textLen1 = 1+tokenTextLen;\n          if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE) {\n            if (textLen1 > CHAR_BLOCK_SIZE) {\n              // Just skip this term, to remain as robust as\n              // possible during indexing.  A TokenFilter\n              // can be inserted into the analyzer chain if\n              // other behavior is wanted (pruning the term\n              // to a prefix, throwing an exception, etc).\n              abortOnExc = false;\n              if (maxTermPrefix == null)\n                maxTermPrefix = new String(tokenText, 0, 30);\n\n              // Still increment position:\n              position++;\n              return;\n            }\n            charPool.nextBuffer();\n          }\n          final char[] text = charPool.buffer;\n          final int textUpto = charPool.byteUpto;\n\n          // Pull next free Posting from free list\n          p = postingsFreeList[--postingsFreeCount];\n\n          p.textStart = textUpto + charPool.byteOffset;\n          charPool.byteUpto += textLen1;\n\n          System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n          text[textUpto+tokenTextLen] = 0xffff;\n          \n          assert postingsHash[hashPos] == null;\n\n          postingsHash[hashPos] = p;\n          numPostings++;\n\n          if (numPostings == postingsHashHalfSize)\n            rehashPostings(2*postingsHashSize);\n\n          // Init first slice for freq & prox streams\n          final int firstSize = levelSizeArray[0];\n\n          final int upto1 = postingsPool.newSlice(firstSize);\n          p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n          final int upto2 = postingsPool.newSlice(firstSize);\n          p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n          p.lastDocCode = docID << 1;\n          p.lastDocID = docID;\n          p.docFreq = 1;\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offsetStartCode = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n        }\n\n        proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n        prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n        assert prox != null;\n\n        if (payload != null && payload.length > 0) {\n          writeProxVInt((proxCode<<1)|1);\n          writeProxVInt(payload.length);\n          writeProxBytes(payload.data, payload.offset, payload.length);\n          fieldInfo.storePayloads = true;\n        } else\n          writeProxVInt(proxCode<<1);\n\n        p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n        p.lastPosition = position++;\n\n        if (doVectorPositions) {\n          posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n          pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n          writePosVInt(proxCode);\n          vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        if (doVectorOffsets) {\n          offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n          offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n          writeOffsetVInt(offsetStartCode);\n          writeOffsetVInt(offsetEnd-offsetStart);\n          vector.lastOffset = offsetEnd;\n          vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        abortOnExc = false;\n      }\n\n","sourceOld":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition(Token token) {\n\n        final Payload payload = token.getPayload();\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n        final int tokenTextLen = token.termLength();\n\n        int code = 0;\n\n        // Compute hashcode\n        int downto = tokenTextLen;\n        while (downto > 0)\n          code = (code*31) + tokenText[--downto];\n\n        // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        int hashPos = code & postingsHashMask;\n\n        assert !postingsCompacted;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = ((code>>8)+code)|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenText, tokenTextLen));\n        }\n        \n        final int proxCode;\n\n        // If we hit an exception below, it's possible the\n        // posting list or term vectors data will be\n        // partially written and thus inconsistent if\n        // flushed, so we have to abort all documents\n        // since the last flush:\n        abortOnExc = true;\n\n        if (p != null) {       // term seen since last flush\n\n          if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n            // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n            assert p.docFreq > 0;\n\n            // Now that we know doc freq for previous doc,\n            // write it & lastDocCode\n            freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n            freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n            if (1 == p.docFreq)\n              writeFreqVInt(p.lastDocCode|1);\n            else {\n              writeFreqVInt(p.lastDocCode);\n              writeFreqVInt(p.docFreq);\n            }\n            p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStartCode = offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n\n            p.docFreq = 1;\n\n            // Store code so we can write this after we're\n            // done with this new doc\n            p.lastDocCode = (docID-p.lastDocID) << 1;\n            p.lastDocID = docID;\n\n          } else {                                // term already seen in this doc\n            // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n            p.docFreq++;\n\n            proxCode = position-p.lastPosition;\n\n            if (doVectors) {\n              vector = p.vector;\n              if (vector == null)\n                vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n                offsetStartCode = offsetStart-vector.lastOffset;\n              }\n            }\n          }\n        } else {\t\t\t\t\t  // term not seen before\n          // System.out.println(\"    never seen docID=\" + docID);\n\n          // Refill?\n          if (0 == postingsFreeCount) {\n            postingsFreeCount = postingsFreeList.length;\n            getPostings(postingsFreeList);\n          }\n\n          final int textLen1 = 1+tokenTextLen;\n          if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE) {\n            if (textLen1 > CHAR_BLOCK_SIZE) {\n              maxTermHit = tokenTextLen;\n              // Just skip this term; we will throw an\n              // exception after processing all accepted\n              // terms in the doc\n              abortOnExc = false;\n              return;\n            }\n            charPool.nextBuffer();\n          }\n          final char[] text = charPool.buffer;\n          final int textUpto = charPool.byteUpto;\n\n          // Pull next free Posting from free list\n          p = postingsFreeList[--postingsFreeCount];\n\n          p.textStart = textUpto + charPool.byteOffset;\n          charPool.byteUpto += textLen1;\n\n          System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n          text[textUpto+tokenTextLen] = 0xffff;\n          \n          assert postingsHash[hashPos] == null;\n\n          postingsHash[hashPos] = p;\n          numPostings++;\n\n          if (numPostings == postingsHashHalfSize)\n            rehashPostings(2*postingsHashSize);\n\n          // Init first slice for freq & prox streams\n          final int firstSize = levelSizeArray[0];\n\n          final int upto1 = postingsPool.newSlice(firstSize);\n          p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n          final int upto2 = postingsPool.newSlice(firstSize);\n          p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n          p.lastDocCode = docID << 1;\n          p.lastDocID = docID;\n          p.docFreq = 1;\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offsetStartCode = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n        }\n\n        proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n        prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n        assert prox != null;\n\n        if (payload != null && payload.length > 0) {\n          writeProxVInt((proxCode<<1)|1);\n          writeProxVInt(payload.length);\n          writeProxBytes(payload.data, payload.offset, payload.length);\n          fieldInfo.storePayloads = true;\n        } else\n          writeProxVInt(proxCode<<1);\n\n        p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n        p.lastPosition = position++;\n\n        if (doVectorPositions) {\n          posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n          pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n          writePosVInt(proxCode);\n          vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        if (doVectorOffsets) {\n          offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n          offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n          writeOffsetVInt(offsetStartCode);\n          writeOffsetVInt(offsetEnd-offsetStart);\n          vector.lastOffset = offsetEnd;\n          vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        abortOnExc = false;\n      }\n\n","bugFix":["741a5cca05cabe1e7482410a29e563a08379251a"],"bugIntro":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"83bbb041887bbef07b8a98d08a0e1713ce137039","date":1200330381,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","sourceNew":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition(Token token) throws AbortException {\n\n        final Payload payload = token.getPayload();\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n        final int tokenTextLen = token.termLength();\n\n        int code = 0;\n\n        // Compute hashcode\n        int downto = tokenTextLen;\n        while (downto > 0)\n          code = (code*31) + tokenText[--downto];\n\n        // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        int hashPos = code & postingsHashMask;\n\n        assert !postingsCompacted;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = ((code>>8)+code)|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenText, tokenTextLen));\n        }\n        \n        final int proxCode;\n\n        // If we hit an exception below, it's possible the\n        // posting list or term vectors data will be\n        // partially written and thus inconsistent if\n        // flushed, so we have to abort all documents\n        // since the last flush:\n\n        try {\n\n          if (p != null) {       // term seen since last flush\n\n            if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n              // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n              assert p.docFreq > 0;\n\n              // Now that we know doc freq for previous doc,\n              // write it & lastDocCode\n              freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n              freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n              if (1 == p.docFreq)\n                writeFreqVInt(p.lastDocCode|1);\n              else {\n                writeFreqVInt(p.lastDocCode);\n                writeFreqVInt(p.docFreq);\n              }\n              p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n              if (doVectors) {\n                vector = addNewVector();\n                if (doVectorOffsets) {\n                  offsetStartCode = offsetStart = offset + token.startOffset();\n                  offsetEnd = offset + token.endOffset();\n                }\n              }\n\n              proxCode = position;\n\n              p.docFreq = 1;\n\n              // Store code so we can write this after we're\n              // done with this new doc\n              p.lastDocCode = (docID-p.lastDocID) << 1;\n              p.lastDocID = docID;\n\n            } else {                                // term already seen in this doc\n              // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n              p.docFreq++;\n\n              proxCode = position-p.lastPosition;\n\n              if (doVectors) {\n                vector = p.vector;\n                if (vector == null)\n                  vector = addNewVector();\n                if (doVectorOffsets) {\n                  offsetStart = offset + token.startOffset();\n                  offsetEnd = offset + token.endOffset();\n                  offsetStartCode = offsetStart-vector.lastOffset;\n                }\n              }\n            }\n          } else {\t\t\t\t\t  // term not seen before\n            // System.out.println(\"    never seen docID=\" + docID);\n\n            // Refill?\n            if (0 == postingsFreeCount) {\n              postingsFreeCount = postingsFreeList.length;\n              getPostings(postingsFreeList);\n            }\n\n            final int textLen1 = 1+tokenTextLen;\n            if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE) {\n              if (textLen1 > CHAR_BLOCK_SIZE) {\n                // Just skip this term, to remain as robust as\n                // possible during indexing.  A TokenFilter\n                // can be inserted into the analyzer chain if\n                // other behavior is wanted (pruning the term\n                // to a prefix, throwing an exception, etc).\n                if (maxTermPrefix == null)\n                  maxTermPrefix = new String(tokenText, 0, 30);\n\n                // Still increment position:\n                position++;\n                return;\n              }\n              charPool.nextBuffer();\n            }\n            final char[] text = charPool.buffer;\n            final int textUpto = charPool.byteUpto;\n\n            // Pull next free Posting from free list\n            p = postingsFreeList[--postingsFreeCount];\n\n            p.textStart = textUpto + charPool.byteOffset;\n            charPool.byteUpto += textLen1;\n\n            System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n            text[textUpto+tokenTextLen] = 0xffff;\n          \n            assert postingsHash[hashPos] == null;\n\n            postingsHash[hashPos] = p;\n            numPostings++;\n\n            if (numPostings == postingsHashHalfSize)\n              rehashPostings(2*postingsHashSize);\n\n            // Init first slice for freq & prox streams\n            final int firstSize = levelSizeArray[0];\n\n            final int upto1 = postingsPool.newSlice(firstSize);\n            p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n            final int upto2 = postingsPool.newSlice(firstSize);\n            p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n            p.lastDocCode = docID << 1;\n            p.lastDocID = docID;\n            p.docFreq = 1;\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offsetStartCode = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n          }\n\n          proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n          prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n          assert prox != null;\n\n          if (payload != null && payload.length > 0) {\n            writeProxVInt((proxCode<<1)|1);\n            writeProxVInt(payload.length);\n            writeProxBytes(payload.data, payload.offset, payload.length);\n            fieldInfo.storePayloads = true;\n          } else\n            writeProxVInt(proxCode<<1);\n\n          p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n          p.lastPosition = position++;\n\n          if (doVectorPositions) {\n            posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n            pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n            writePosVInt(proxCode);\n            vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n          }\n\n          if (doVectorOffsets) {\n            offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n            offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n            writeOffsetVInt(offsetStartCode);\n            writeOffsetVInt(offsetEnd-offsetStart);\n            vector.lastOffset = offsetEnd;\n            vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n          }\n        } catch (Throwable t) {\n          throw new AbortException(t, DocumentsWriter.this);\n        }\n      }\n\n","sourceOld":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition(Token token) {\n\n        final Payload payload = token.getPayload();\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n        final int tokenTextLen = token.termLength();\n\n        int code = 0;\n\n        // Compute hashcode\n        int downto = tokenTextLen;\n        while (downto > 0)\n          code = (code*31) + tokenText[--downto];\n\n        // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        int hashPos = code & postingsHashMask;\n\n        assert !postingsCompacted;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = ((code>>8)+code)|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenText, tokenTextLen));\n        }\n        \n        final int proxCode;\n\n        // If we hit an exception below, it's possible the\n        // posting list or term vectors data will be\n        // partially written and thus inconsistent if\n        // flushed, so we have to abort all documents\n        // since the last flush:\n        abortOnExc = true;\n\n        if (p != null) {       // term seen since last flush\n\n          if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n            // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n            assert p.docFreq > 0;\n\n            // Now that we know doc freq for previous doc,\n            // write it & lastDocCode\n            freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n            freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n            if (1 == p.docFreq)\n              writeFreqVInt(p.lastDocCode|1);\n            else {\n              writeFreqVInt(p.lastDocCode);\n              writeFreqVInt(p.docFreq);\n            }\n            p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStartCode = offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n\n            p.docFreq = 1;\n\n            // Store code so we can write this after we're\n            // done with this new doc\n            p.lastDocCode = (docID-p.lastDocID) << 1;\n            p.lastDocID = docID;\n\n          } else {                                // term already seen in this doc\n            // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n            p.docFreq++;\n\n            proxCode = position-p.lastPosition;\n\n            if (doVectors) {\n              vector = p.vector;\n              if (vector == null)\n                vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n                offsetStartCode = offsetStart-vector.lastOffset;\n              }\n            }\n          }\n        } else {\t\t\t\t\t  // term not seen before\n          // System.out.println(\"    never seen docID=\" + docID);\n\n          // Refill?\n          if (0 == postingsFreeCount) {\n            postingsFreeCount = postingsFreeList.length;\n            getPostings(postingsFreeList);\n          }\n\n          final int textLen1 = 1+tokenTextLen;\n          if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE) {\n            if (textLen1 > CHAR_BLOCK_SIZE) {\n              // Just skip this term, to remain as robust as\n              // possible during indexing.  A TokenFilter\n              // can be inserted into the analyzer chain if\n              // other behavior is wanted (pruning the term\n              // to a prefix, throwing an exception, etc).\n              abortOnExc = false;\n              if (maxTermPrefix == null)\n                maxTermPrefix = new String(tokenText, 0, 30);\n\n              // Still increment position:\n              position++;\n              return;\n            }\n            charPool.nextBuffer();\n          }\n          final char[] text = charPool.buffer;\n          final int textUpto = charPool.byteUpto;\n\n          // Pull next free Posting from free list\n          p = postingsFreeList[--postingsFreeCount];\n\n          p.textStart = textUpto + charPool.byteOffset;\n          charPool.byteUpto += textLen1;\n\n          System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n          text[textUpto+tokenTextLen] = 0xffff;\n          \n          assert postingsHash[hashPos] == null;\n\n          postingsHash[hashPos] = p;\n          numPostings++;\n\n          if (numPostings == postingsHashHalfSize)\n            rehashPostings(2*postingsHashSize);\n\n          // Init first slice for freq & prox streams\n          final int firstSize = levelSizeArray[0];\n\n          final int upto1 = postingsPool.newSlice(firstSize);\n          p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n          final int upto2 = postingsPool.newSlice(firstSize);\n          p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n          p.lastDocCode = docID << 1;\n          p.lastDocID = docID;\n          p.docFreq = 1;\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offsetStartCode = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n        }\n\n        proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n        prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n        assert prox != null;\n\n        if (payload != null && payload.length > 0) {\n          writeProxVInt((proxCode<<1)|1);\n          writeProxVInt(payload.length);\n          writeProxBytes(payload.data, payload.offset, payload.length);\n          fieldInfo.storePayloads = true;\n        } else\n          writeProxVInt(proxCode<<1);\n\n        p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n        p.lastPosition = position++;\n\n        if (doVectorPositions) {\n          posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n          pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n          writePosVInt(proxCode);\n          vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        if (doVectorOffsets) {\n          offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n          offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n          writeOffsetVInt(offsetStartCode);\n          writeOffsetVInt(offsetEnd-offsetStart);\n          vector.lastOffset = offsetEnd;\n          vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n        }\n\n        abortOnExc = false;\n      }\n\n","bugFix":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c","5a251aa47d1808cbae42c0e172d698c377430e60","741a5cca05cabe1e7482410a29e563a08379251a","8560794cda5bcd510c60e38ed553e9c5a6204983","6864413dbc0c12104c978c05456f3da1d45adb03","da249b441376287ae32d1604bc7b50b35b351d09"],"bugIntro":["176324efd1eab6bd44a6d81c27c9b3a1a175ba3d"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"176324efd1eab6bd44a6d81c27c9b3a1a175ba3d","date":1202734547,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","sourceNew":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition(Token token) throws AbortException {\n\n        final Payload payload = token.getPayload();\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n        final int tokenTextLen = token.termLength();\n\n        int code = 0;\n\n        // Compute hashcode\n        int downto = tokenTextLen;\n        while (downto > 0)\n          code = (code*31) + tokenText[--downto];\n\n        // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        int hashPos = code & postingsHashMask;\n\n        assert !postingsCompacted;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = ((code>>8)+code)|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenText, tokenTextLen));\n        }\n        \n        final int proxCode;\n\n        // If we hit an exception below, it's possible the\n        // posting list or term vectors data will be\n        // partially written and thus inconsistent if\n        // flushed, so we have to abort all documents\n        // since the last flush:\n\n        try {\n\n          if (p != null) {       // term seen since last flush\n\n            if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n              // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n              assert p.docFreq > 0;\n\n              // Now that we know doc freq for previous doc,\n              // write it & lastDocCode\n              freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n              freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n              if (1 == p.docFreq)\n                writeFreqVInt(p.lastDocCode|1);\n              else {\n                writeFreqVInt(p.lastDocCode);\n                writeFreqVInt(p.docFreq);\n              }\n              p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n              if (doVectors) {\n                vector = addNewVector();\n                if (doVectorOffsets) {\n                  offsetStartCode = offsetStart = offset + token.startOffset();\n                  offsetEnd = offset + token.endOffset();\n                }\n              }\n\n              proxCode = position;\n\n              p.docFreq = 1;\n\n              // Store code so we can write this after we're\n              // done with this new doc\n              p.lastDocCode = (docID-p.lastDocID) << 1;\n              p.lastDocID = docID;\n\n            } else {                                // term already seen in this doc\n              // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n              p.docFreq++;\n\n              proxCode = position-p.lastPosition;\n\n              if (doVectors) {\n                vector = p.vector;\n                if (vector == null)\n                  vector = addNewVector();\n                if (doVectorOffsets) {\n                  offsetStart = offset + token.startOffset();\n                  offsetEnd = offset + token.endOffset();\n                  offsetStartCode = offsetStart-vector.lastOffset;\n                }\n              }\n            }\n          } else {\t\t\t\t\t  // term not seen before\n            // System.out.println(\"    never seen docID=\" + docID);\n\n            // Refill?\n            if (0 == postingsFreeCount) {\n              getPostings(postingsFreeList);\n              postingsFreeCount = postingsFreeList.length;\n            }\n\n            final int textLen1 = 1+tokenTextLen;\n            if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE) {\n              if (textLen1 > CHAR_BLOCK_SIZE) {\n                // Just skip this term, to remain as robust as\n                // possible during indexing.  A TokenFilter\n                // can be inserted into the analyzer chain if\n                // other behavior is wanted (pruning the term\n                // to a prefix, throwing an exception, etc).\n                if (maxTermPrefix == null)\n                  maxTermPrefix = new String(tokenText, 0, 30);\n\n                // Still increment position:\n                position++;\n                return;\n              }\n              charPool.nextBuffer();\n            }\n            final char[] text = charPool.buffer;\n            final int textUpto = charPool.byteUpto;\n\n            // Pull next free Posting from free list\n            p = postingsFreeList[--postingsFreeCount];\n\n            p.textStart = textUpto + charPool.byteOffset;\n            charPool.byteUpto += textLen1;\n\n            System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n            text[textUpto+tokenTextLen] = 0xffff;\n          \n            assert postingsHash[hashPos] == null;\n\n            postingsHash[hashPos] = p;\n            numPostings++;\n\n            if (numPostings == postingsHashHalfSize)\n              rehashPostings(2*postingsHashSize);\n\n            // Init first slice for freq & prox streams\n            final int firstSize = levelSizeArray[0];\n\n            final int upto1 = postingsPool.newSlice(firstSize);\n            p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n            final int upto2 = postingsPool.newSlice(firstSize);\n            p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n            p.lastDocCode = docID << 1;\n            p.lastDocID = docID;\n            p.docFreq = 1;\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offsetStartCode = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n          }\n\n          proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n          prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n          assert prox != null;\n\n          if (payload != null && payload.length > 0) {\n            writeProxVInt((proxCode<<1)|1);\n            writeProxVInt(payload.length);\n            writeProxBytes(payload.data, payload.offset, payload.length);\n            fieldInfo.storePayloads = true;\n          } else\n            writeProxVInt(proxCode<<1);\n\n          p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n          p.lastPosition = position++;\n\n          if (doVectorPositions) {\n            posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n            pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n            writePosVInt(proxCode);\n            vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n          }\n\n          if (doVectorOffsets) {\n            offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n            offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n            writeOffsetVInt(offsetStartCode);\n            writeOffsetVInt(offsetEnd-offsetStart);\n            vector.lastOffset = offsetEnd;\n            vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n          }\n        } catch (Throwable t) {\n          throw new AbortException(t, DocumentsWriter.this);\n        }\n      }\n\n","sourceOld":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition(Token token) throws AbortException {\n\n        final Payload payload = token.getPayload();\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n        final int tokenTextLen = token.termLength();\n\n        int code = 0;\n\n        // Compute hashcode\n        int downto = tokenTextLen;\n        while (downto > 0)\n          code = (code*31) + tokenText[--downto];\n\n        // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        int hashPos = code & postingsHashMask;\n\n        assert !postingsCompacted;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = ((code>>8)+code)|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenText, tokenTextLen));\n        }\n        \n        final int proxCode;\n\n        // If we hit an exception below, it's possible the\n        // posting list or term vectors data will be\n        // partially written and thus inconsistent if\n        // flushed, so we have to abort all documents\n        // since the last flush:\n\n        try {\n\n          if (p != null) {       // term seen since last flush\n\n            if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n              // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n              assert p.docFreq > 0;\n\n              // Now that we know doc freq for previous doc,\n              // write it & lastDocCode\n              freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n              freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n              if (1 == p.docFreq)\n                writeFreqVInt(p.lastDocCode|1);\n              else {\n                writeFreqVInt(p.lastDocCode);\n                writeFreqVInt(p.docFreq);\n              }\n              p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n              if (doVectors) {\n                vector = addNewVector();\n                if (doVectorOffsets) {\n                  offsetStartCode = offsetStart = offset + token.startOffset();\n                  offsetEnd = offset + token.endOffset();\n                }\n              }\n\n              proxCode = position;\n\n              p.docFreq = 1;\n\n              // Store code so we can write this after we're\n              // done with this new doc\n              p.lastDocCode = (docID-p.lastDocID) << 1;\n              p.lastDocID = docID;\n\n            } else {                                // term already seen in this doc\n              // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n              p.docFreq++;\n\n              proxCode = position-p.lastPosition;\n\n              if (doVectors) {\n                vector = p.vector;\n                if (vector == null)\n                  vector = addNewVector();\n                if (doVectorOffsets) {\n                  offsetStart = offset + token.startOffset();\n                  offsetEnd = offset + token.endOffset();\n                  offsetStartCode = offsetStart-vector.lastOffset;\n                }\n              }\n            }\n          } else {\t\t\t\t\t  // term not seen before\n            // System.out.println(\"    never seen docID=\" + docID);\n\n            // Refill?\n            if (0 == postingsFreeCount) {\n              postingsFreeCount = postingsFreeList.length;\n              getPostings(postingsFreeList);\n            }\n\n            final int textLen1 = 1+tokenTextLen;\n            if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE) {\n              if (textLen1 > CHAR_BLOCK_SIZE) {\n                // Just skip this term, to remain as robust as\n                // possible during indexing.  A TokenFilter\n                // can be inserted into the analyzer chain if\n                // other behavior is wanted (pruning the term\n                // to a prefix, throwing an exception, etc).\n                if (maxTermPrefix == null)\n                  maxTermPrefix = new String(tokenText, 0, 30);\n\n                // Still increment position:\n                position++;\n                return;\n              }\n              charPool.nextBuffer();\n            }\n            final char[] text = charPool.buffer;\n            final int textUpto = charPool.byteUpto;\n\n            // Pull next free Posting from free list\n            p = postingsFreeList[--postingsFreeCount];\n\n            p.textStart = textUpto + charPool.byteOffset;\n            charPool.byteUpto += textLen1;\n\n            System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n            text[textUpto+tokenTextLen] = 0xffff;\n          \n            assert postingsHash[hashPos] == null;\n\n            postingsHash[hashPos] = p;\n            numPostings++;\n\n            if (numPostings == postingsHashHalfSize)\n              rehashPostings(2*postingsHashSize);\n\n            // Init first slice for freq & prox streams\n            final int firstSize = levelSizeArray[0];\n\n            final int upto1 = postingsPool.newSlice(firstSize);\n            p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n            final int upto2 = postingsPool.newSlice(firstSize);\n            p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n            p.lastDocCode = docID << 1;\n            p.lastDocID = docID;\n            p.docFreq = 1;\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offsetStartCode = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n          }\n\n          proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n          prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n          assert prox != null;\n\n          if (payload != null && payload.length > 0) {\n            writeProxVInt((proxCode<<1)|1);\n            writeProxVInt(payload.length);\n            writeProxBytes(payload.data, payload.offset, payload.length);\n            fieldInfo.storePayloads = true;\n          } else\n            writeProxVInt(proxCode<<1);\n\n          p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n          p.lastPosition = position++;\n\n          if (doVectorPositions) {\n            posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n            pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n            writePosVInt(proxCode);\n            vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n          }\n\n          if (doVectorOffsets) {\n            offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n            offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n            writeOffsetVInt(offsetStartCode);\n            writeOffsetVInt(offsetEnd-offsetStart);\n            vector.lastOffset = offsetEnd;\n            vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n          }\n        } catch (Throwable t) {\n          throw new AbortException(t, DocumentsWriter.this);\n        }\n      }\n\n","bugFix":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a0af3a442be522899177e5e11384a45a6784a3f","date":1205348952,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#addPosition(Token).mjava","sourceNew":null,"sourceOld":"      /** This is the hotspot of indexing: it's called once\n       *  for every term of every document.  Its job is to *\n       *  update the postings byte stream (Postings hash) *\n       *  based on the occurence of a single term. */\n      private void addPosition(Token token) throws AbortException {\n\n        final Payload payload = token.getPayload();\n\n        // Get the text of this term.  Term can either\n        // provide a String token or offset into a char[]\n        // array\n        final char[] tokenText = token.termBuffer();\n        final int tokenTextLen = token.termLength();\n\n        int code = 0;\n\n        // Compute hashcode\n        int downto = tokenTextLen;\n        while (downto > 0)\n          code = (code*31) + tokenText[--downto];\n\n        // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n        int hashPos = code & postingsHashMask;\n\n        assert !postingsCompacted;\n\n        // Locate Posting in hash\n        p = postingsHash[hashPos];\n\n        if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n          // Conflict: keep searching different locations in\n          // the hash table.\n          final int inc = ((code>>8)+code)|1;\n          do {\n            code += inc;\n            hashPos = code & postingsHashMask;\n            p = postingsHash[hashPos];\n          } while (p != null && !postingEquals(tokenText, tokenTextLen));\n        }\n        \n        final int proxCode;\n\n        // If we hit an exception below, it's possible the\n        // posting list or term vectors data will be\n        // partially written and thus inconsistent if\n        // flushed, so we have to abort all documents\n        // since the last flush:\n\n        try {\n\n          if (p != null) {       // term seen since last flush\n\n            if (docID != p.lastDocID) { // term not yet seen in this doc\n            \n              // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n              assert p.docFreq > 0;\n\n              // Now that we know doc freq for previous doc,\n              // write it & lastDocCode\n              freqUpto = p.freqUpto & BYTE_BLOCK_MASK;\n              freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];\n              if (1 == p.docFreq)\n                writeFreqVInt(p.lastDocCode|1);\n              else {\n                writeFreqVInt(p.lastDocCode);\n                writeFreqVInt(p.docFreq);\n              }\n              p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);\n\n              if (doVectors) {\n                vector = addNewVector();\n                if (doVectorOffsets) {\n                  offsetStartCode = offsetStart = offset + token.startOffset();\n                  offsetEnd = offset + token.endOffset();\n                }\n              }\n\n              proxCode = position;\n\n              p.docFreq = 1;\n\n              // Store code so we can write this after we're\n              // done with this new doc\n              p.lastDocCode = (docID-p.lastDocID) << 1;\n              p.lastDocID = docID;\n\n            } else {                                // term already seen in this doc\n              // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n              p.docFreq++;\n\n              proxCode = position-p.lastPosition;\n\n              if (doVectors) {\n                vector = p.vector;\n                if (vector == null)\n                  vector = addNewVector();\n                if (doVectorOffsets) {\n                  offsetStart = offset + token.startOffset();\n                  offsetEnd = offset + token.endOffset();\n                  offsetStartCode = offsetStart-vector.lastOffset;\n                }\n              }\n            }\n          } else {\t\t\t\t\t  // term not seen before\n            // System.out.println(\"    never seen docID=\" + docID);\n\n            // Refill?\n            if (0 == postingsFreeCount) {\n              getPostings(postingsFreeList);\n              postingsFreeCount = postingsFreeList.length;\n            }\n\n            final int textLen1 = 1+tokenTextLen;\n            if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE) {\n              if (textLen1 > CHAR_BLOCK_SIZE) {\n                // Just skip this term, to remain as robust as\n                // possible during indexing.  A TokenFilter\n                // can be inserted into the analyzer chain if\n                // other behavior is wanted (pruning the term\n                // to a prefix, throwing an exception, etc).\n                if (maxTermPrefix == null)\n                  maxTermPrefix = new String(tokenText, 0, 30);\n\n                // Still increment position:\n                position++;\n                return;\n              }\n              charPool.nextBuffer();\n            }\n            final char[] text = charPool.buffer;\n            final int textUpto = charPool.byteUpto;\n\n            // Pull next free Posting from free list\n            p = postingsFreeList[--postingsFreeCount];\n\n            p.textStart = textUpto + charPool.byteOffset;\n            charPool.byteUpto += textLen1;\n\n            System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n            text[textUpto+tokenTextLen] = 0xffff;\n          \n            assert postingsHash[hashPos] == null;\n\n            postingsHash[hashPos] = p;\n            numPostings++;\n\n            if (numPostings == postingsHashHalfSize)\n              rehashPostings(2*postingsHashSize);\n\n            // Init first slice for freq & prox streams\n            final int firstSize = levelSizeArray[0];\n\n            final int upto1 = postingsPool.newSlice(firstSize);\n            p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;\n\n            final int upto2 = postingsPool.newSlice(firstSize);\n            p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;\n\n            p.lastDocCode = docID << 1;\n            p.lastDocID = docID;\n            p.docFreq = 1;\n\n            if (doVectors) {\n              vector = addNewVector();\n              if (doVectorOffsets) {\n                offsetStart = offsetStartCode = offset + token.startOffset();\n                offsetEnd = offset + token.endOffset();\n              }\n            }\n\n            proxCode = position;\n          }\n\n          proxUpto = p.proxUpto & BYTE_BLOCK_MASK;\n          prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];\n          assert prox != null;\n\n          if (payload != null && payload.length > 0) {\n            writeProxVInt((proxCode<<1)|1);\n            writeProxVInt(payload.length);\n            writeProxBytes(payload.data, payload.offset, payload.length);\n            fieldInfo.storePayloads = true;\n          } else\n            writeProxVInt(proxCode<<1);\n\n          p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);\n\n          p.lastPosition = position++;\n\n          if (doVectorPositions) {\n            posUpto = vector.posUpto & BYTE_BLOCK_MASK;\n            pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];\n            writePosVInt(proxCode);\n            vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);\n          }\n\n          if (doVectorOffsets) {\n            offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;\n            offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];\n            writeOffsetVInt(offsetStartCode);\n            writeOffsetVInt(offsetEnd-offsetStart);\n            vector.lastOffset = offsetEnd;\n            vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);\n          }\n        } catch (Throwable t) {\n          throw new AbortException(t, DocumentsWriter.this);\n        }\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"6864413dbc0c12104c978c05456f3da1d45adb03":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"83bbb041887bbef07b8a98d08a0e1713ce137039":["5a251aa47d1808cbae42c0e172d698c377430e60"],"176324efd1eab6bd44a6d81c27c9b3a1a175ba3d":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"da249b441376287ae32d1604bc7b50b35b351d09":["95fcbec956d64aff326919be88d27ba5b60c046e"],"741a5cca05cabe1e7482410a29e563a08379251a":["b6a1f29c9b1051488fd5fa7d56c98db5f4388408"],"b6a1f29c9b1051488fd5fa7d56c98db5f4388408":["da249b441376287ae32d1604bc7b50b35b351d09"],"95fcbec956d64aff326919be88d27ba5b60c046e":["6864413dbc0c12104c978c05456f3da1d45adb03"],"c9a0deca56efc5191d6b3c41047fd538f3fae1d8":["8560794cda5bcd510c60e38ed553e9c5a6204983"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5a251aa47d1808cbae42c0e172d698c377430e60":["c9a0deca56efc5191d6b3c41047fd538f3fae1d8"],"8560794cda5bcd510c60e38ed553e9c5a6204983":["741a5cca05cabe1e7482410a29e563a08379251a"],"5a0af3a442be522899177e5e11384a45a6784a3f":["176324efd1eab6bd44a6d81c27c9b3a1a175ba3d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5a0af3a442be522899177e5e11384a45a6784a3f"]},"commit2Childs":{"6864413dbc0c12104c978c05456f3da1d45adb03":["95fcbec956d64aff326919be88d27ba5b60c046e"],"83bbb041887bbef07b8a98d08a0e1713ce137039":["176324efd1eab6bd44a6d81c27c9b3a1a175ba3d"],"176324efd1eab6bd44a6d81c27c9b3a1a175ba3d":["5a0af3a442be522899177e5e11384a45a6784a3f"],"da249b441376287ae32d1604bc7b50b35b351d09":["b6a1f29c9b1051488fd5fa7d56c98db5f4388408"],"741a5cca05cabe1e7482410a29e563a08379251a":["8560794cda5bcd510c60e38ed553e9c5a6204983"],"b6a1f29c9b1051488fd5fa7d56c98db5f4388408":["741a5cca05cabe1e7482410a29e563a08379251a"],"95fcbec956d64aff326919be88d27ba5b60c046e":["da249b441376287ae32d1604bc7b50b35b351d09"],"c9a0deca56efc5191d6b3c41047fd538f3fae1d8":["5a251aa47d1808cbae42c0e172d698c377430e60"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["6864413dbc0c12104c978c05456f3da1d45adb03"],"5a251aa47d1808cbae42c0e172d698c377430e60":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"8560794cda5bcd510c60e38ed553e9c5a6204983":["c9a0deca56efc5191d6b3c41047fd538f3fae1d8"],"5a0af3a442be522899177e5e11384a45a6784a3f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}