{"path":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass().mjava","commits":[{"id":"d0d579490a72f2e6297eaa648940611234c57cf1","date":1395917140,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","sourceNew":"  public static MiniDFSCluster setupClass() throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    File dir = TestUtil.createTempDir(LuceneTestCase.getTestClass().getSimpleName());\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir.getAbsolutePath() + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir.getAbsolutePath() + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    System.setProperty(\"solr.hdfs.home\", \"/solr_hdfs_home\");\n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n    \n    int rnd = LuceneTestCase.random().nextInt(10000);\n    Timer timer = new Timer();\n    timer.schedule(new TimerTask() {\n      \n      @Override\n      public void run() {\n        NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n      }\n    }, rnd);\n    \n    timers.put(dfsCluster, timer);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dataDir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n    File dir = new File(dataDir);\n    new File(dataDir).mkdirs();\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir.getAbsolutePath() + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir.getAbsolutePath() + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    System.setProperty(\"solr.hdfs.home\", \"/solr_hdfs_home\");\n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n    \n    int rnd = LuceneTestCase.random().nextInt(10000);\n    Timer timer = new Timer();\n    timer.schedule(new TimerTask() {\n      \n      @Override\n      public void run() {\n        NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n      }\n    }, rnd);\n    \n    timers.put(dfsCluster, timer);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bb1f22cfa77230b5f05b7784feae5367f6bbb488","date":1395968145,"type":5,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass().mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    System.setProperty(\"solr.hdfs.home\", \"/solr_hdfs_home\");\n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n    \n    int rnd = LuceneTestCase.random().nextInt(10000);\n    Timer timer = new Timer();\n    timer.schedule(new TimerTask() {\n      \n      @Override\n      public void run() {\n        NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n      }\n    }, rnd);\n    \n    timers.put(dfsCluster, timer);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass() throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    File dir = TestUtil.createTempDir(LuceneTestCase.getTestClass().getSimpleName());\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir.getAbsolutePath() + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir.getAbsolutePath() + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir.getAbsolutePath() + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    System.setProperty(\"solr.hdfs.home\", \"/solr_hdfs_home\");\n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n    \n    int rnd = LuceneTestCase.random().nextInt(10000);\n    Timer timer = new Timer();\n    timer.schedule(new TimerTask() {\n      \n      @Override\n      public void run() {\n        NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n      }\n    }, rnd);\n    \n    timers.put(dfsCluster, timer);\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"bb1f22cfa77230b5f05b7784feae5367f6bbb488":["d0d579490a72f2e6297eaa648940611234c57cf1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0d579490a72f2e6297eaa648940611234c57cf1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"bb1f22cfa77230b5f05b7784feae5367f6bbb488":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d0d579490a72f2e6297eaa648940611234c57cf1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d0d579490a72f2e6297eaa648940611234c57cf1":["bb1f22cfa77230b5f05b7784feae5367f6bbb488"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["bb1f22cfa77230b5f05b7784feae5367f6bbb488","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}