{"path":"lucene/facet/src/test/org/apache/lucene/facet/util/TestFacetsPayloadMigrationReader#createIndex(Directory,Directory,FacetIndexingParams).mjava","commits":[{"id":"607428da722dcb3e86bbd11c63de8986e6275c36","date":1360334150,"type":1,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/util/TestFacetsPayloadMigrationReader#createIndex(Directory,Directory,FacetIndexingParams).mjava","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/index/TestFacetsPayloadMigrationReader#createIndex(Directory,Directory,FacetIndexingParams).mjava","sourceNew":"  private HashMap<String,Integer> createIndex(Directory indexDir, Directory taxoDir, FacetIndexingParams fip) \n      throws Exception {\n    Random random = random();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setMaxBufferedDocs(2); // force few segments\n    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // avoid merges so that we're left with few segments\n    IndexWriter indexWriter = new IndexWriter(indexDir, conf);\n    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);\n    \n    FacetFields facetFields = new PayloadFacetFields(taxoWriter, fip);\n    \n    HashMap<String,Integer> expectedCounts = new HashMap<String,Integer>(DIMENSIONS.length);\n    int numDocs = atLeast(10);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numCategories = random.nextInt(3) + 1;\n      ArrayList<CategoryPath> categories = new ArrayList<CategoryPath>(numCategories);\n      HashSet<String> docDimensions = new HashSet<String>();\n      while (numCategories-- > 0) {\n        String dim = DIMENSIONS[random.nextInt(DIMENSIONS.length)];\n        // we should only increment the expected count by 1 per document\n        docDimensions.add(dim);\n        categories.add(new CategoryPath(dim, Integer.toString(i), Integer.toString(numCategories)));\n      }\n      facetFields.addFields(doc, categories);\n      doc.add(new StringField(\"docid\", Integer.toString(i), Store.YES));\n      doc.add(new TextField(\"foo\", \"content\" + i, Store.YES));\n      indexWriter.addDocument(doc);\n\n      // update expected count per dimension\n      for (String dim : docDimensions) {\n        Integer val = expectedCounts.get(dim);\n        if (val == null) {\n          expectedCounts.put(dim, Integer.valueOf(1));\n        } else {\n          expectedCounts.put(dim, Integer.valueOf(val.intValue() + 1));\n        }\n      }\n      \n      if (random.nextDouble() < 0.2) { // add some documents that will be deleted\n        doc = new Document();\n        doc.add(new StringField(\"del\", \"key\", Store.NO));\n        facetFields.addFields(doc, Collections.singletonList(new CategoryPath(\"dummy\")));\n        indexWriter.addDocument(doc);\n      }\n    }\n    \n    indexWriter.commit();\n    taxoWriter.commit();\n\n    // delete the docs that were marked for deletion. note that the 'dummy'\n    // category is not removed from the taxonomy, so must account for it when we\n    // verify the migrated index.\n    indexWriter.deleteDocuments(new Term(\"del\", \"key\"));\n    indexWriter.commit();\n    \n    IOUtils.close(indexWriter, taxoWriter);\n    \n    return expectedCounts;\n  }\n\n","sourceOld":"  private HashMap<String,Integer> createIndex(Directory indexDir, Directory taxoDir, FacetIndexingParams fip) \n      throws Exception {\n    Random random = random();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setMaxBufferedDocs(2); // force few segments\n    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // avoid merges so that we're left with few segments\n    IndexWriter indexWriter = new IndexWriter(indexDir, conf);\n    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);\n    \n    FacetFields facetFields = new PayloadFacetFields(taxoWriter, fip);\n    \n    HashMap<String,Integer> expectedCounts = new HashMap<String,Integer>(DIMENSIONS.length);\n    int numDocs = atLeast(10);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numCategories = random.nextInt(3) + 1;\n      ArrayList<CategoryPath> categories = new ArrayList<CategoryPath>(numCategories);\n      HashSet<String> docDimensions = new HashSet<String>();\n      while (numCategories-- > 0) {\n        String dim = DIMENSIONS[random.nextInt(DIMENSIONS.length)];\n        // we should only increment the expected count by 1 per document\n        docDimensions.add(dim);\n        categories.add(new CategoryPath(dim, Integer.toString(i), Integer.toString(numCategories)));\n      }\n      facetFields.addFields(doc, categories);\n      doc.add(new StringField(\"docid\", Integer.toString(i), Store.YES));\n      doc.add(new TextField(\"foo\", \"content\" + i, Store.YES));\n      indexWriter.addDocument(doc);\n\n      // update expected count per dimension\n      for (String dim : docDimensions) {\n        Integer val = expectedCounts.get(dim);\n        if (val == null) {\n          expectedCounts.put(dim, Integer.valueOf(1));\n        } else {\n          expectedCounts.put(dim, Integer.valueOf(val.intValue() + 1));\n        }\n      }\n      \n      if (random.nextDouble() < 0.2) { // add some documents that will be deleted\n        doc = new Document();\n        doc.add(new StringField(\"del\", \"key\", Store.NO));\n        facetFields.addFields(doc, Collections.singletonList(new CategoryPath(\"dummy\")));\n        indexWriter.addDocument(doc);\n      }\n    }\n    \n    indexWriter.commit();\n    taxoWriter.commit();\n\n    // delete the docs that were marked for deletion. note that the 'dummy'\n    // category is not removed from the taxonomy, so must account for it when we\n    // verify the migrated index.\n    indexWriter.deleteDocuments(new Term(\"del\", \"key\"));\n    indexWriter.commit();\n    \n    IOUtils.close(indexWriter, taxoWriter);\n    \n    return expectedCounts;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c190847801a50f4dd20fd639bdc29b54ea3b288b","date":1384461522,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/util/TestFacetsPayloadMigrationReader#createIndex(Directory,Directory,FacetIndexingParams).mjava","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/util/TestFacetsPayloadMigrationReader#createIndex(Directory,Directory,FacetIndexingParams).mjava","sourceNew":"  private HashMap<String,Integer> createIndex(Directory indexDir, Directory taxoDir, FacetIndexingParams fip) \n      throws Exception {\n    Random random = random();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setMaxBufferedDocs(2); // force few segments\n    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // avoid merges so that we're left with few segments\n    IndexWriter indexWriter = new IndexWriter(indexDir, conf);\n    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);\n    \n    FacetFields facetFields = new PayloadFacetFields(taxoWriter, fip);\n    \n    HashMap<String,Integer> expectedCounts = new HashMap<String,Integer>(DIMENSIONS.length);\n    int numDocs = atLeast(10);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numCategories = random.nextInt(3) + 1;\n      ArrayList<FacetLabel> categories = new ArrayList<FacetLabel>(numCategories);\n      HashSet<String> docDimensions = new HashSet<String>();\n      while (numCategories-- > 0) {\n        String dim = DIMENSIONS[random.nextInt(DIMENSIONS.length)];\n        // we should only increment the expected count by 1 per document\n        docDimensions.add(dim);\n        categories.add(new FacetLabel(dim, Integer.toString(i), Integer.toString(numCategories)));\n      }\n      facetFields.addFields(doc, categories);\n      doc.add(new StringField(\"docid\", Integer.toString(i), Store.YES));\n      doc.add(new TextField(\"foo\", \"content\" + i, Store.YES));\n      indexWriter.addDocument(doc);\n\n      // update expected count per dimension\n      for (String dim : docDimensions) {\n        Integer val = expectedCounts.get(dim);\n        if (val == null) {\n          expectedCounts.put(dim, Integer.valueOf(1));\n        } else {\n          expectedCounts.put(dim, Integer.valueOf(val.intValue() + 1));\n        }\n      }\n      \n      if (random.nextDouble() < 0.2) { // add some documents that will be deleted\n        doc = new Document();\n        doc.add(new StringField(\"del\", \"key\", Store.NO));\n        facetFields.addFields(doc, Collections.singletonList(new FacetLabel(\"dummy\")));\n        indexWriter.addDocument(doc);\n      }\n    }\n    \n    indexWriter.commit();\n    taxoWriter.commit();\n\n    // delete the docs that were marked for deletion. note that the 'dummy'\n    // category is not removed from the taxonomy, so must account for it when we\n    // verify the migrated index.\n    indexWriter.deleteDocuments(new Term(\"del\", \"key\"));\n    indexWriter.commit();\n    \n    IOUtils.close(indexWriter, taxoWriter);\n    \n    return expectedCounts;\n  }\n\n","sourceOld":"  private HashMap<String,Integer> createIndex(Directory indexDir, Directory taxoDir, FacetIndexingParams fip) \n      throws Exception {\n    Random random = random();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setMaxBufferedDocs(2); // force few segments\n    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // avoid merges so that we're left with few segments\n    IndexWriter indexWriter = new IndexWriter(indexDir, conf);\n    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);\n    \n    FacetFields facetFields = new PayloadFacetFields(taxoWriter, fip);\n    \n    HashMap<String,Integer> expectedCounts = new HashMap<String,Integer>(DIMENSIONS.length);\n    int numDocs = atLeast(10);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numCategories = random.nextInt(3) + 1;\n      ArrayList<CategoryPath> categories = new ArrayList<CategoryPath>(numCategories);\n      HashSet<String> docDimensions = new HashSet<String>();\n      while (numCategories-- > 0) {\n        String dim = DIMENSIONS[random.nextInt(DIMENSIONS.length)];\n        // we should only increment the expected count by 1 per document\n        docDimensions.add(dim);\n        categories.add(new CategoryPath(dim, Integer.toString(i), Integer.toString(numCategories)));\n      }\n      facetFields.addFields(doc, categories);\n      doc.add(new StringField(\"docid\", Integer.toString(i), Store.YES));\n      doc.add(new TextField(\"foo\", \"content\" + i, Store.YES));\n      indexWriter.addDocument(doc);\n\n      // update expected count per dimension\n      for (String dim : docDimensions) {\n        Integer val = expectedCounts.get(dim);\n        if (val == null) {\n          expectedCounts.put(dim, Integer.valueOf(1));\n        } else {\n          expectedCounts.put(dim, Integer.valueOf(val.intValue() + 1));\n        }\n      }\n      \n      if (random.nextDouble() < 0.2) { // add some documents that will be deleted\n        doc = new Document();\n        doc.add(new StringField(\"del\", \"key\", Store.NO));\n        facetFields.addFields(doc, Collections.singletonList(new CategoryPath(\"dummy\")));\n        indexWriter.addDocument(doc);\n      }\n    }\n    \n    indexWriter.commit();\n    taxoWriter.commit();\n\n    // delete the docs that were marked for deletion. note that the 'dummy'\n    // category is not removed from the taxonomy, so must account for it when we\n    // verify the migrated index.\n    indexWriter.deleteDocuments(new Term(\"del\", \"key\"));\n    indexWriter.commit();\n    \n    IOUtils.close(indexWriter, taxoWriter);\n    \n    return expectedCounts;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d33e19a97046248623a7591aeaa6547233fd15e2","date":1385424777,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/util/TestFacetsPayloadMigrationReader#createIndex(Directory,Directory,FacetIndexingParams).mjava","sourceNew":null,"sourceOld":"  private HashMap<String,Integer> createIndex(Directory indexDir, Directory taxoDir, FacetIndexingParams fip) \n      throws Exception {\n    Random random = random();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setMaxBufferedDocs(2); // force few segments\n    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // avoid merges so that we're left with few segments\n    IndexWriter indexWriter = new IndexWriter(indexDir, conf);\n    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);\n    \n    FacetFields facetFields = new PayloadFacetFields(taxoWriter, fip);\n    \n    HashMap<String,Integer> expectedCounts = new HashMap<String,Integer>(DIMENSIONS.length);\n    int numDocs = atLeast(10);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numCategories = random.nextInt(3) + 1;\n      ArrayList<FacetLabel> categories = new ArrayList<FacetLabel>(numCategories);\n      HashSet<String> docDimensions = new HashSet<String>();\n      while (numCategories-- > 0) {\n        String dim = DIMENSIONS[random.nextInt(DIMENSIONS.length)];\n        // we should only increment the expected count by 1 per document\n        docDimensions.add(dim);\n        categories.add(new FacetLabel(dim, Integer.toString(i), Integer.toString(numCategories)));\n      }\n      facetFields.addFields(doc, categories);\n      doc.add(new StringField(\"docid\", Integer.toString(i), Store.YES));\n      doc.add(new TextField(\"foo\", \"content\" + i, Store.YES));\n      indexWriter.addDocument(doc);\n\n      // update expected count per dimension\n      for (String dim : docDimensions) {\n        Integer val = expectedCounts.get(dim);\n        if (val == null) {\n          expectedCounts.put(dim, Integer.valueOf(1));\n        } else {\n          expectedCounts.put(dim, Integer.valueOf(val.intValue() + 1));\n        }\n      }\n      \n      if (random.nextDouble() < 0.2) { // add some documents that will be deleted\n        doc = new Document();\n        doc.add(new StringField(\"del\", \"key\", Store.NO));\n        facetFields.addFields(doc, Collections.singletonList(new FacetLabel(\"dummy\")));\n        indexWriter.addDocument(doc);\n      }\n    }\n    \n    indexWriter.commit();\n    taxoWriter.commit();\n\n    // delete the docs that were marked for deletion. note that the 'dummy'\n    // category is not removed from the taxonomy, so must account for it when we\n    // verify the migrated index.\n    indexWriter.deleteDocuments(new Term(\"del\", \"key\"));\n    indexWriter.commit();\n    \n    IOUtils.close(indexWriter, taxoWriter);\n    \n    return expectedCounts;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc728b07df73b197e6d940d27f9b08b63918f13","date":1388834348,"type":4,"author":"Michael McCandless","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/util/TestFacetsPayloadMigrationReader#createIndex(Directory,Directory,FacetIndexingParams).mjava","sourceNew":null,"sourceOld":"  private HashMap<String,Integer> createIndex(Directory indexDir, Directory taxoDir, FacetIndexingParams fip) \n      throws Exception {\n    Random random = random();\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n    conf.setMaxBufferedDocs(2); // force few segments\n    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // avoid merges so that we're left with few segments\n    IndexWriter indexWriter = new IndexWriter(indexDir, conf);\n    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);\n    \n    FacetFields facetFields = new PayloadFacetFields(taxoWriter, fip);\n    \n    HashMap<String,Integer> expectedCounts = new HashMap<String,Integer>(DIMENSIONS.length);\n    int numDocs = atLeast(10);\n    for (int i = 0; i < numDocs; i++) {\n      Document doc = new Document();\n      int numCategories = random.nextInt(3) + 1;\n      ArrayList<CategoryPath> categories = new ArrayList<CategoryPath>(numCategories);\n      HashSet<String> docDimensions = new HashSet<String>();\n      while (numCategories-- > 0) {\n        String dim = DIMENSIONS[random.nextInt(DIMENSIONS.length)];\n        // we should only increment the expected count by 1 per document\n        docDimensions.add(dim);\n        categories.add(new CategoryPath(dim, Integer.toString(i), Integer.toString(numCategories)));\n      }\n      facetFields.addFields(doc, categories);\n      doc.add(new StringField(\"docid\", Integer.toString(i), Store.YES));\n      doc.add(new TextField(\"foo\", \"content\" + i, Store.YES));\n      indexWriter.addDocument(doc);\n\n      // update expected count per dimension\n      for (String dim : docDimensions) {\n        Integer val = expectedCounts.get(dim);\n        if (val == null) {\n          expectedCounts.put(dim, Integer.valueOf(1));\n        } else {\n          expectedCounts.put(dim, Integer.valueOf(val.intValue() + 1));\n        }\n      }\n      \n      if (random.nextDouble() < 0.2) { // add some documents that will be deleted\n        doc = new Document();\n        doc.add(new StringField(\"del\", \"key\", Store.NO));\n        facetFields.addFields(doc, Collections.singletonList(new CategoryPath(\"dummy\")));\n        indexWriter.addDocument(doc);\n      }\n    }\n    \n    indexWriter.commit();\n    taxoWriter.commit();\n\n    // delete the docs that were marked for deletion. note that the 'dummy'\n    // category is not removed from the taxonomy, so must account for it when we\n    // verify the migrated index.\n    indexWriter.deleteDocuments(new Term(\"del\", \"key\"));\n    indexWriter.commit();\n    \n    IOUtils.close(indexWriter, taxoWriter);\n    \n    return expectedCounts;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"607428da722dcb3e86bbd11c63de8986e6275c36":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d33e19a97046248623a7591aeaa6547233fd15e2":["c190847801a50f4dd20fd639bdc29b54ea3b288b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cc728b07df73b197e6d940d27f9b08b63918f13":["607428da722dcb3e86bbd11c63de8986e6275c36","d33e19a97046248623a7591aeaa6547233fd15e2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3cc728b07df73b197e6d940d27f9b08b63918f13"],"c190847801a50f4dd20fd639bdc29b54ea3b288b":["607428da722dcb3e86bbd11c63de8986e6275c36"]},"commit2Childs":{"607428da722dcb3e86bbd11c63de8986e6275c36":["3cc728b07df73b197e6d940d27f9b08b63918f13","c190847801a50f4dd20fd639bdc29b54ea3b288b"],"d33e19a97046248623a7591aeaa6547233fd15e2":["3cc728b07df73b197e6d940d27f9b08b63918f13"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["607428da722dcb3e86bbd11c63de8986e6275c36"],"3cc728b07df73b197e6d940d27f9b08b63918f13":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c190847801a50f4dd20fd639bdc29b54ea3b288b":["d33e19a97046248623a7591aeaa6547233fd15e2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}