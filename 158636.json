{"path":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getSentences(String,int).mjava","commits":[{"id":"ea82415927cafd7c8b8bceca08f31a63db1cbdde","date":1133588579,"type":0,"author":"Wolfgang Hoschek","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getSentences(String,int).mjava","pathOld":"/dev/null","sourceNew":"\t/**\n\t * Returns at most the first N sentences of the given text. Delimiting\n\t * characters are excluded from the results. Each returned sentence is\n\t * whitespace-trimmed via String.trim(), potentially an empty string.\n\t * \n\t * @param text\n\t *            the text to tokenize into sentences\n\t * @param limit\n\t *            the maximum number of sentences to return; zero indicates \"as\n\t *            many as possible\".\n\t * @return the first N sentences\n\t */\n\tpublic static String[] getSentences(String text, int limit) {\n//\t\treturn tokenize(SENTENCES, text, limit); // equivalent but slower\n\t\tint len = text.length();\n\t\tif (len == 0) return new String[] { text };\n\t\tif (limit <= 0) limit = Integer.MAX_VALUE;\n\t\t\n\t\t// average sentence length heuristic\n\t\tString[] tokens = new String[Math.min(limit, 1 + len/40)];\n\t\tint size = 0;\n\t\tint i = 0;\n\t\t\n\t\twhile (i < len && size < limit) {\n\t\t\t\n\t\t\t// scan to end of current sentence\n\t\t\tint start = i;\n\t\t\twhile (i < len && !isSentenceSeparator(text.charAt(i))) i++;\n\t\t\t\n\t\t\tif (size == tokens.length) { // grow array\n\t\t\t\tString[] tmp = new String[tokens.length << 1];\n\t\t\t\tSystem.arraycopy(tokens, 0, tmp, 0, size);\n\t\t\t\ttokens = tmp;\n\t\t\t}\n\t\t\t// add sentence (potentially empty)\n\t\t\ttokens[size++] = text.substring(start, i).trim();\n\n\t\t\t// scan to beginning of next sentence\n\t\t\twhile (i < len && isSentenceSeparator(text.charAt(i))) i++;\n\t\t}\n\t\t\n\t\tif (size == tokens.length) return tokens;\n\t\tString[] results = new String[size];\n\t\tSystem.arraycopy(tokens, 0, results, 0, size);\n\t\treturn results;\n\t}\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7f68e24227d5556d33ee6d586fd9010cd9ff8bec","date":1150091176,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getSentences(String,int).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getSentences(String,int).mjava","sourceNew":"  /**\n   * Returns at most the first N sentences of the given text. Delimiting\n   * characters are excluded from the results. Each returned sentence is\n   * whitespace-trimmed via String.trim(), potentially an empty string.\n   * \n   * @param text\n   *            the text to tokenize into sentences\n   * @param limit\n   *            the maximum number of sentences to return; zero indicates \"as\n   *            many as possible\".\n   * @return the first N sentences\n   */\n  public static String[] getSentences(String text, int limit) {\n//    return tokenize(SENTENCES, text, limit); // equivalent but slower\n    int len = text.length();\n    if (len == 0) return new String[] { text };\n    if (limit <= 0) limit = Integer.MAX_VALUE;\n    \n    // average sentence length heuristic\n    String[] tokens = new String[Math.min(limit, 1 + len/40)];\n    int size = 0;\n    int i = 0;\n    \n    while (i < len && size < limit) {\n      \n      // scan to end of current sentence\n      int start = i;\n      while (i < len && !isSentenceSeparator(text.charAt(i))) i++;\n      \n      if (size == tokens.length) { // grow array\n        String[] tmp = new String[tokens.length << 1];\n        System.arraycopy(tokens, 0, tmp, 0, size);\n        tokens = tmp;\n      }\n      // add sentence (potentially empty)\n      tokens[size++] = text.substring(start, i).trim();\n\n      // scan to beginning of next sentence\n      while (i < len && isSentenceSeparator(text.charAt(i))) i++;\n    }\n    \n    if (size == tokens.length) return tokens;\n    String[] results = new String[size];\n    System.arraycopy(tokens, 0, results, 0, size);\n    return results;\n  }\n\n","sourceOld":"\t/**\n\t * Returns at most the first N sentences of the given text. Delimiting\n\t * characters are excluded from the results. Each returned sentence is\n\t * whitespace-trimmed via String.trim(), potentially an empty string.\n\t * \n\t * @param text\n\t *            the text to tokenize into sentences\n\t * @param limit\n\t *            the maximum number of sentences to return; zero indicates \"as\n\t *            many as possible\".\n\t * @return the first N sentences\n\t */\n\tpublic static String[] getSentences(String text, int limit) {\n//\t\treturn tokenize(SENTENCES, text, limit); // equivalent but slower\n\t\tint len = text.length();\n\t\tif (len == 0) return new String[] { text };\n\t\tif (limit <= 0) limit = Integer.MAX_VALUE;\n\t\t\n\t\t// average sentence length heuristic\n\t\tString[] tokens = new String[Math.min(limit, 1 + len/40)];\n\t\tint size = 0;\n\t\tint i = 0;\n\t\t\n\t\twhile (i < len && size < limit) {\n\t\t\t\n\t\t\t// scan to end of current sentence\n\t\t\tint start = i;\n\t\t\twhile (i < len && !isSentenceSeparator(text.charAt(i))) i++;\n\t\t\t\n\t\t\tif (size == tokens.length) { // grow array\n\t\t\t\tString[] tmp = new String[tokens.length << 1];\n\t\t\t\tSystem.arraycopy(tokens, 0, tmp, 0, size);\n\t\t\t\ttokens = tmp;\n\t\t\t}\n\t\t\t// add sentence (potentially empty)\n\t\t\ttokens[size++] = text.substring(start, i).trim();\n\n\t\t\t// scan to beginning of next sentence\n\t\t\twhile (i < len && isSentenceSeparator(text.charAt(i))) i++;\n\t\t}\n\t\t\n\t\tif (size == tokens.length) return tokens;\n\t\tString[] results = new String[size];\n\t\tSystem.arraycopy(tokens, 0, results, 0, size);\n\t\treturn results;\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d68e5c46e6a5ebdf4dafec4a123344092b915cc0","date":1256752193,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil#getSentences(String,int).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getSentences(String,int).mjava","sourceNew":"  /**\n   * Returns at most the first N sentences of the given text. Delimiting\n   * characters are excluded from the results. Each returned sentence is\n   * whitespace-trimmed via String.trim(), potentially an empty string.\n   * \n   * @param text\n   *            the text to tokenize into sentences\n   * @param limit\n   *            the maximum number of sentences to return; zero indicates \"as\n   *            many as possible\".\n   * @return the first N sentences\n   */\n  public static String[] getSentences(String text, int limit) {\n//    return tokenize(SENTENCES, text, limit); // equivalent but slower\n    int len = text.length();\n    if (len == 0) return new String[] { text };\n    if (limit <= 0) limit = Integer.MAX_VALUE;\n    \n    // average sentence length heuristic\n    String[] tokens = new String[Math.min(limit, 1 + len/40)];\n    int size = 0;\n    int i = 0;\n    \n    while (i < len && size < limit) {\n      \n      // scan to end of current sentence\n      int start = i;\n      while (i < len && !isSentenceSeparator(text.charAt(i))) i++;\n      \n      if (size == tokens.length) { // grow array\n        String[] tmp = new String[tokens.length << 1];\n        System.arraycopy(tokens, 0, tmp, 0, size);\n        tokens = tmp;\n      }\n      // add sentence (potentially empty)\n      tokens[size++] = text.substring(start, i).trim();\n\n      // scan to beginning of next sentence\n      while (i < len && isSentenceSeparator(text.charAt(i))) i++;\n    }\n    \n    if (size == tokens.length) return tokens;\n    String[] results = new String[size];\n    System.arraycopy(tokens, 0, results, 0, size);\n    return results;\n  }\n\n","sourceOld":"  /**\n   * Returns at most the first N sentences of the given text. Delimiting\n   * characters are excluded from the results. Each returned sentence is\n   * whitespace-trimmed via String.trim(), potentially an empty string.\n   * \n   * @param text\n   *            the text to tokenize into sentences\n   * @param limit\n   *            the maximum number of sentences to return; zero indicates \"as\n   *            many as possible\".\n   * @return the first N sentences\n   */\n  public static String[] getSentences(String text, int limit) {\n//    return tokenize(SENTENCES, text, limit); // equivalent but slower\n    int len = text.length();\n    if (len == 0) return new String[] { text };\n    if (limit <= 0) limit = Integer.MAX_VALUE;\n    \n    // average sentence length heuristic\n    String[] tokens = new String[Math.min(limit, 1 + len/40)];\n    int size = 0;\n    int i = 0;\n    \n    while (i < len && size < limit) {\n      \n      // scan to end of current sentence\n      int start = i;\n      while (i < len && !isSentenceSeparator(text.charAt(i))) i++;\n      \n      if (size == tokens.length) { // grow array\n        String[] tmp = new String[tokens.length << 1];\n        System.arraycopy(tokens, 0, tmp, 0, size);\n        tokens = tmp;\n      }\n      // add sentence (potentially empty)\n      tokens[size++] = text.substring(start, i).trim();\n\n      // scan to beginning of next sentence\n      while (i < len && isSentenceSeparator(text.charAt(i))) i++;\n    }\n    \n    if (size == tokens.length) return tokens;\n    String[] results = new String[size];\n    System.arraycopy(tokens, 0, results, 0, size);\n    return results;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ea82415927cafd7c8b8bceca08f31a63db1cbdde":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"7f68e24227d5556d33ee6d586fd9010cd9ff8bec":["ea82415927cafd7c8b8bceca08f31a63db1cbdde"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d68e5c46e6a5ebdf4dafec4a123344092b915cc0"],"d68e5c46e6a5ebdf4dafec4a123344092b915cc0":["7f68e24227d5556d33ee6d586fd9010cd9ff8bec"]},"commit2Childs":{"ea82415927cafd7c8b8bceca08f31a63db1cbdde":["7f68e24227d5556d33ee6d586fd9010cd9ff8bec"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ea82415927cafd7c8b8bceca08f31a63db1cbdde"],"7f68e24227d5556d33ee6d586fd9010cd9ff8bec":["d68e5c46e6a5ebdf4dafec4a123344092b915cc0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"d68e5c46e6a5ebdf4dafec4a123344092b915cc0":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}