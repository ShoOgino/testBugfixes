{"path":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#publishFlushedSegment(SegmentCommitInfo,FieldInfos,FrozenBufferedUpdates,FrozenBufferedUpdates,Sorter.DocMap).mjava","commits":[{"id":"15e716649e2bd79a98b5e68c464154ea4c44677a","date":1523975212,"type":1,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#publishFlushedSegment(SegmentCommitInfo,FieldInfos,FrozenBufferedUpdates,FrozenBufferedUpdates,Sorter.DocMap).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#publishFlushedSegment(SegmentCommitInfo,FrozenBufferedUpdates,FrozenBufferedUpdates,Sorter.DocMap).mjava","sourceNew":"  /**\n   * Atomically adds the segment private delete packet and publishes the flushed\n   * segments SegmentInfo to the index writer.\n   */\n  synchronized void publishFlushedSegment(SegmentCommitInfo newSegment,\n                                          FieldInfos fieldInfos, FrozenBufferedUpdates packet, FrozenBufferedUpdates globalPacket,\n                                          Sorter.DocMap sortMap) throws IOException {\n    boolean published = false;\n    try {\n      // Lock order IW -> BDS\n      ensureOpen(false);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publishFlushedSegment \" + newSegment);\n      }\n\n      if (globalPacket != null && globalPacket.any()) {\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        bufferedUpdatesStream.push(globalPacket);\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(globalPacket));\n      }\n\n      // Publishing the segment must be sync'd on IW -> BDS to make the sure\n      // that no merge prunes away the seg. private delete packet\n      final long nextGen;\n      if (packet != null && packet.any()) {\n        nextGen = bufferedUpdatesStream.push(packet);\n\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(packet));\n\n      } else {\n        // Since we don't have a delete packet to apply we can get a new\n        // generation right away\n        nextGen = bufferedUpdatesStream.getNextGen();\n        // No deletes/updates here, so marked finished immediately:\n        bufferedUpdatesStream.finishedSegment(nextGen);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publish sets newSegment delGen=\" + nextGen + \" seg=\" + segString(newSegment));\n      }\n      newSegment.setBufferedDeletesGen(nextGen);\n      segmentInfos.add(newSegment);\n      published = true;\n      checkpoint();\n      if (packet != null && packet.any() && sortMap != null) {\n        // TODO: not great we do this heavyish op while holding IW's monitor lock,\n        // but it only applies if you are using sorted indices and updating doc values:\n        ReadersAndUpdates rld = readerPool.get(newSegment, true);\n        rld.sortMap = sortMap;\n        // DON't release this ReadersAndUpdates we need to stick with that sortMap\n      }\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(config.softDeletesField); // will return null if no soft deletes are present\n      // this is a corner case where documents delete them-self with soft deletes. This is used to\n      // build delete tombstones etc. in this case we haven't seen any updates to the DV in this fresh flushed segment.\n      // if we have seen updates the update code checks if the segment is fully deleted.\n      boolean hasInitialSoftDeleted = (fieldInfo != null\n          && fieldInfo.getDocValuesGen() == -1\n          && fieldInfo.getDocValuesType() != DocValuesType.NONE);\n      final boolean isFullyHardDeleted = newSegment.getDelCount() == newSegment.info.maxDoc();\n      // we either have a fully hard-deleted segment or one or more docs are soft-deleted. In both cases we need\n      // to go and check if they are fully deleted. This has the nice side-effect that we now have accurate numbers\n      // for the soft delete right after we flushed to disk.\n      if (hasInitialSoftDeleted || isFullyHardDeleted){\n        // this operation is only really executed if needed an if soft-deletes are not configured it only be executed\n        // if we deleted all docs in this newly flushed segment.\n        ReadersAndUpdates rld = readerPool.get(newSegment, true);\n        try {\n          if (isFullyDeleted(rld)) {\n            dropDeletedSegment(newSegment);\n          }\n        } finally {\n          readerPool.release(rld);\n        }\n      }\n\n    } finally {\n      if (published == false) {\n        adjustPendingNumDocs(-newSegment.info.maxDoc());\n      }\n      flushCount.incrementAndGet();\n      doAfterFlush();\n    }\n\n  }\n\n","sourceOld":"  /**\n   * Atomically adds the segment private delete packet and publishes the flushed\n   * segments SegmentInfo to the index writer.\n   */\n  synchronized void publishFlushedSegment(SegmentCommitInfo newSegment,\n                                          FrozenBufferedUpdates packet, FrozenBufferedUpdates globalPacket,\n                                          Sorter.DocMap sortMap) throws IOException {\n    boolean published = false;\n    try {\n      // Lock order IW -> BDS\n      ensureOpen(false);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publishFlushedSegment \" + newSegment);\n      }\n\n      if (globalPacket != null && globalPacket.any()) {\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        bufferedUpdatesStream.push(globalPacket);\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(globalPacket));\n      }\n\n      // Publishing the segment must be sync'd on IW -> BDS to make the sure\n      // that no merge prunes away the seg. private delete packet\n      final long nextGen;\n      if (packet != null && packet.any()) {\n        nextGen = bufferedUpdatesStream.push(packet);\n\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(packet));\n          \n      } else {\n        // Since we don't have a delete packet to apply we can get a new\n        // generation right away\n        nextGen = bufferedUpdatesStream.getNextGen();\n        // No deletes/updates here, so marked finished immediately:\n        bufferedUpdatesStream.finishedSegment(nextGen);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publish sets newSegment delGen=\" + nextGen + \" seg=\" + segString(newSegment));\n      }\n      newSegment.setBufferedDeletesGen(nextGen);\n      segmentInfos.add(newSegment);\n      published = true;\n      checkpoint();\n\n      if (packet != null && packet.any() && sortMap != null) {\n        // TODO: not great we do this heavyish op while holding IW's monitor lock,\n        // but it only applies if you are using sorted indices and updating doc values:\n        ReadersAndUpdates rld = readerPool.get(newSegment, true);\n        rld.sortMap = sortMap;\n      }\n      \n    } finally {\n      if (published == false) {\n        adjustPendingNumDocs(-newSegment.info.maxDoc());\n      }\n      flushCount.incrementAndGet();\n      doAfterFlush();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1926100d9b67becc9701c54266fee3ba7878a5f0","date":1524472150,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#publishFlushedSegment(SegmentCommitInfo,FieldInfos,FrozenBufferedUpdates,FrozenBufferedUpdates,Sorter.DocMap).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#publishFlushedSegment(SegmentCommitInfo,FieldInfos,FrozenBufferedUpdates,FrozenBufferedUpdates,Sorter.DocMap).mjava","sourceNew":"  /**\n   * Atomically adds the segment private delete packet and publishes the flushed\n   * segments SegmentInfo to the index writer.\n   */\n  synchronized void publishFlushedSegment(SegmentCommitInfo newSegment,\n                                          FieldInfos fieldInfos, FrozenBufferedUpdates packet, FrozenBufferedUpdates globalPacket,\n                                          Sorter.DocMap sortMap) throws IOException {\n    boolean published = false;\n    try {\n      // Lock order IW -> BDS\n      ensureOpen(false);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publishFlushedSegment \" + newSegment);\n      }\n\n      if (globalPacket != null && globalPacket.any()) {\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        bufferedUpdatesStream.push(globalPacket);\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(globalPacket));\n      }\n\n      // Publishing the segment must be sync'd on IW -> BDS to make the sure\n      // that no merge prunes away the seg. private delete packet\n      final long nextGen;\n      if (packet != null && packet.any()) {\n        nextGen = bufferedUpdatesStream.push(packet);\n\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(packet));\n\n      } else {\n        // Since we don't have a delete packet to apply we can get a new\n        // generation right away\n        nextGen = bufferedUpdatesStream.getNextGen();\n        // No deletes/updates here, so marked finished immediately:\n        bufferedUpdatesStream.finishedSegment(nextGen);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publish sets newSegment delGen=\" + nextGen + \" seg=\" + segString(newSegment));\n      }\n      newSegment.setBufferedDeletesGen(nextGen);\n      segmentInfos.add(newSegment);\n      published = true;\n      checkpoint();\n      if (packet != null && packet.any() && sortMap != null) {\n        // TODO: not great we do this heavyish op while holding IW's monitor lock,\n        // but it only applies if you are using sorted indices and updating doc values:\n        ReadersAndUpdates rld = getPooledInstance(newSegment, true);\n        rld.sortMap = sortMap;\n        // DON't release this ReadersAndUpdates we need to stick with that sortMap\n      }\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(config.softDeletesField); // will return null if no soft deletes are present\n      // this is a corner case where documents delete them-self with soft deletes. This is used to\n      // build delete tombstones etc. in this case we haven't seen any updates to the DV in this fresh flushed segment.\n      // if we have seen updates the update code checks if the segment is fully deleted.\n      boolean hasInitialSoftDeleted = (fieldInfo != null\n          && fieldInfo.getDocValuesGen() == -1\n          && fieldInfo.getDocValuesType() != DocValuesType.NONE);\n      final boolean isFullyHardDeleted = newSegment.getDelCount() == newSegment.info.maxDoc();\n      // we either have a fully hard-deleted segment or one or more docs are soft-deleted. In both cases we need\n      // to go and check if they are fully deleted. This has the nice side-effect that we now have accurate numbers\n      // for the soft delete right after we flushed to disk.\n      if (hasInitialSoftDeleted || isFullyHardDeleted){\n        // this operation is only really executed if needed an if soft-deletes are not configured it only be executed\n        // if we deleted all docs in this newly flushed segment.\n        ReadersAndUpdates rld = getPooledInstance(newSegment, true);\n        try {\n          if (isFullyDeleted(rld)) {\n            dropDeletedSegment(newSegment);\n          }\n        } finally {\n          release(rld);\n        }\n      }\n\n    } finally {\n      if (published == false) {\n        adjustPendingNumDocs(-newSegment.info.maxDoc());\n      }\n      flushCount.incrementAndGet();\n      doAfterFlush();\n    }\n\n  }\n\n","sourceOld":"  /**\n   * Atomically adds the segment private delete packet and publishes the flushed\n   * segments SegmentInfo to the index writer.\n   */\n  synchronized void publishFlushedSegment(SegmentCommitInfo newSegment,\n                                          FieldInfos fieldInfos, FrozenBufferedUpdates packet, FrozenBufferedUpdates globalPacket,\n                                          Sorter.DocMap sortMap) throws IOException {\n    boolean published = false;\n    try {\n      // Lock order IW -> BDS\n      ensureOpen(false);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publishFlushedSegment \" + newSegment);\n      }\n\n      if (globalPacket != null && globalPacket.any()) {\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        bufferedUpdatesStream.push(globalPacket);\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(globalPacket));\n      }\n\n      // Publishing the segment must be sync'd on IW -> BDS to make the sure\n      // that no merge prunes away the seg. private delete packet\n      final long nextGen;\n      if (packet != null && packet.any()) {\n        nextGen = bufferedUpdatesStream.push(packet);\n\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(packet));\n\n      } else {\n        // Since we don't have a delete packet to apply we can get a new\n        // generation right away\n        nextGen = bufferedUpdatesStream.getNextGen();\n        // No deletes/updates here, so marked finished immediately:\n        bufferedUpdatesStream.finishedSegment(nextGen);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publish sets newSegment delGen=\" + nextGen + \" seg=\" + segString(newSegment));\n      }\n      newSegment.setBufferedDeletesGen(nextGen);\n      segmentInfos.add(newSegment);\n      published = true;\n      checkpoint();\n      if (packet != null && packet.any() && sortMap != null) {\n        // TODO: not great we do this heavyish op while holding IW's monitor lock,\n        // but it only applies if you are using sorted indices and updating doc values:\n        ReadersAndUpdates rld = readerPool.get(newSegment, true);\n        rld.sortMap = sortMap;\n        // DON't release this ReadersAndUpdates we need to stick with that sortMap\n      }\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(config.softDeletesField); // will return null if no soft deletes are present\n      // this is a corner case where documents delete them-self with soft deletes. This is used to\n      // build delete tombstones etc. in this case we haven't seen any updates to the DV in this fresh flushed segment.\n      // if we have seen updates the update code checks if the segment is fully deleted.\n      boolean hasInitialSoftDeleted = (fieldInfo != null\n          && fieldInfo.getDocValuesGen() == -1\n          && fieldInfo.getDocValuesType() != DocValuesType.NONE);\n      final boolean isFullyHardDeleted = newSegment.getDelCount() == newSegment.info.maxDoc();\n      // we either have a fully hard-deleted segment or one or more docs are soft-deleted. In both cases we need\n      // to go and check if they are fully deleted. This has the nice side-effect that we now have accurate numbers\n      // for the soft delete right after we flushed to disk.\n      if (hasInitialSoftDeleted || isFullyHardDeleted){\n        // this operation is only really executed if needed an if soft-deletes are not configured it only be executed\n        // if we deleted all docs in this newly flushed segment.\n        ReadersAndUpdates rld = readerPool.get(newSegment, true);\n        try {\n          if (isFullyDeleted(rld)) {\n            dropDeletedSegment(newSegment);\n          }\n        } finally {\n          readerPool.release(rld);\n        }\n      }\n\n    } finally {\n      if (published == false) {\n        adjustPendingNumDocs(-newSegment.info.maxDoc());\n      }\n      flushCount.incrementAndGet();\n      doAfterFlush();\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7","date":1524496660,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#publishFlushedSegment(SegmentCommitInfo,FieldInfos,FrozenBufferedUpdates,FrozenBufferedUpdates,Sorter.DocMap).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#publishFlushedSegment(SegmentCommitInfo,FieldInfos,FrozenBufferedUpdates,FrozenBufferedUpdates,Sorter.DocMap).mjava","sourceNew":"  /**\n   * Atomically adds the segment private delete packet and publishes the flushed\n   * segments SegmentInfo to the index writer.\n   */\n  synchronized void publishFlushedSegment(SegmentCommitInfo newSegment,\n                                          FieldInfos fieldInfos, FrozenBufferedUpdates packet, FrozenBufferedUpdates globalPacket,\n                                          Sorter.DocMap sortMap) throws IOException {\n    boolean published = false;\n    try {\n      // Lock order IW -> BDS\n      ensureOpen(false);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publishFlushedSegment \" + newSegment);\n      }\n\n      if (globalPacket != null && globalPacket.any()) {\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        bufferedUpdatesStream.push(globalPacket);\n        eventQueue.add(new ResolveUpdatesEvent(globalPacket));\n      }\n\n      // Publishing the segment must be sync'd on IW -> BDS to make the sure\n      // that no merge prunes away the seg. private delete packet\n      final long nextGen;\n      if (packet != null && packet.any()) {\n        nextGen = bufferedUpdatesStream.push(packet);\n\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        eventQueue.add(new ResolveUpdatesEvent(packet));\n\n      } else {\n        // Since we don't have a delete packet to apply we can get a new\n        // generation right away\n        nextGen = bufferedUpdatesStream.getNextGen();\n        // No deletes/updates here, so marked finished immediately:\n        bufferedUpdatesStream.finishedSegment(nextGen);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publish sets newSegment delGen=\" + nextGen + \" seg=\" + segString(newSegment));\n      }\n      newSegment.setBufferedDeletesGen(nextGen);\n      segmentInfos.add(newSegment);\n      published = true;\n      checkpoint();\n      if (packet != null && packet.any() && sortMap != null) {\n        // TODO: not great we do this heavyish op while holding IW's monitor lock,\n        // but it only applies if you are using sorted indices and updating doc values:\n        ReadersAndUpdates rld = getPooledInstance(newSegment, true);\n        rld.sortMap = sortMap;\n        // DON't release this ReadersAndUpdates we need to stick with that sortMap\n      }\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(config.softDeletesField); // will return null if no soft deletes are present\n      // this is a corner case where documents delete them-self with soft deletes. This is used to\n      // build delete tombstones etc. in this case we haven't seen any updates to the DV in this fresh flushed segment.\n      // if we have seen updates the update code checks if the segment is fully deleted.\n      boolean hasInitialSoftDeleted = (fieldInfo != null\n          && fieldInfo.getDocValuesGen() == -1\n          && fieldInfo.getDocValuesType() != DocValuesType.NONE);\n      final boolean isFullyHardDeleted = newSegment.getDelCount() == newSegment.info.maxDoc();\n      // we either have a fully hard-deleted segment or one or more docs are soft-deleted. In both cases we need\n      // to go and check if they are fully deleted. This has the nice side-effect that we now have accurate numbers\n      // for the soft delete right after we flushed to disk.\n      if (hasInitialSoftDeleted || isFullyHardDeleted){\n        // this operation is only really executed if needed an if soft-deletes are not configured it only be executed\n        // if we deleted all docs in this newly flushed segment.\n        ReadersAndUpdates rld = getPooledInstance(newSegment, true);\n        try {\n          if (isFullyDeleted(rld)) {\n            dropDeletedSegment(newSegment);\n          }\n        } finally {\n          release(rld);\n        }\n      }\n\n    } finally {\n      if (published == false) {\n        adjustPendingNumDocs(-newSegment.info.maxDoc());\n      }\n      flushCount.incrementAndGet();\n      doAfterFlush();\n    }\n\n  }\n\n","sourceOld":"  /**\n   * Atomically adds the segment private delete packet and publishes the flushed\n   * segments SegmentInfo to the index writer.\n   */\n  synchronized void publishFlushedSegment(SegmentCommitInfo newSegment,\n                                          FieldInfos fieldInfos, FrozenBufferedUpdates packet, FrozenBufferedUpdates globalPacket,\n                                          Sorter.DocMap sortMap) throws IOException {\n    boolean published = false;\n    try {\n      // Lock order IW -> BDS\n      ensureOpen(false);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publishFlushedSegment \" + newSegment);\n      }\n\n      if (globalPacket != null && globalPacket.any()) {\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        bufferedUpdatesStream.push(globalPacket);\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(globalPacket));\n      }\n\n      // Publishing the segment must be sync'd on IW -> BDS to make the sure\n      // that no merge prunes away the seg. private delete packet\n      final long nextGen;\n      if (packet != null && packet.any()) {\n        nextGen = bufferedUpdatesStream.push(packet);\n\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        docWriter.putEvent(new DocumentsWriter.ResolveUpdatesEvent(packet));\n\n      } else {\n        // Since we don't have a delete packet to apply we can get a new\n        // generation right away\n        nextGen = bufferedUpdatesStream.getNextGen();\n        // No deletes/updates here, so marked finished immediately:\n        bufferedUpdatesStream.finishedSegment(nextGen);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publish sets newSegment delGen=\" + nextGen + \" seg=\" + segString(newSegment));\n      }\n      newSegment.setBufferedDeletesGen(nextGen);\n      segmentInfos.add(newSegment);\n      published = true;\n      checkpoint();\n      if (packet != null && packet.any() && sortMap != null) {\n        // TODO: not great we do this heavyish op while holding IW's monitor lock,\n        // but it only applies if you are using sorted indices and updating doc values:\n        ReadersAndUpdates rld = getPooledInstance(newSegment, true);\n        rld.sortMap = sortMap;\n        // DON't release this ReadersAndUpdates we need to stick with that sortMap\n      }\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(config.softDeletesField); // will return null if no soft deletes are present\n      // this is a corner case where documents delete them-self with soft deletes. This is used to\n      // build delete tombstones etc. in this case we haven't seen any updates to the DV in this fresh flushed segment.\n      // if we have seen updates the update code checks if the segment is fully deleted.\n      boolean hasInitialSoftDeleted = (fieldInfo != null\n          && fieldInfo.getDocValuesGen() == -1\n          && fieldInfo.getDocValuesType() != DocValuesType.NONE);\n      final boolean isFullyHardDeleted = newSegment.getDelCount() == newSegment.info.maxDoc();\n      // we either have a fully hard-deleted segment or one or more docs are soft-deleted. In both cases we need\n      // to go and check if they are fully deleted. This has the nice side-effect that we now have accurate numbers\n      // for the soft delete right after we flushed to disk.\n      if (hasInitialSoftDeleted || isFullyHardDeleted){\n        // this operation is only really executed if needed an if soft-deletes are not configured it only be executed\n        // if we deleted all docs in this newly flushed segment.\n        ReadersAndUpdates rld = getPooledInstance(newSegment, true);\n        try {\n          if (isFullyDeleted(rld)) {\n            dropDeletedSegment(newSegment);\n          }\n        } finally {\n          release(rld);\n        }\n      }\n\n    } finally {\n      if (published == false) {\n        adjustPendingNumDocs(-newSegment.info.maxDoc());\n      }\n      flushCount.incrementAndGet();\n      doAfterFlush();\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6b8498afacfc8322268ca0d659d274fcce08d557","date":1524577248,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#publishFlushedSegment(SegmentCommitInfo,FieldInfos,FrozenBufferedUpdates,FrozenBufferedUpdates,Sorter.DocMap).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#publishFlushedSegment(SegmentCommitInfo,FieldInfos,FrozenBufferedUpdates,FrozenBufferedUpdates,Sorter.DocMap).mjava","sourceNew":"  /**\n   * Atomically adds the segment private delete packet and publishes the flushed\n   * segments SegmentInfo to the index writer.\n   */\n  private synchronized void publishFlushedSegment(SegmentCommitInfo newSegment, FieldInfos fieldInfos,\n                                                  FrozenBufferedUpdates packet, FrozenBufferedUpdates globalPacket,\n                                                  Sorter.DocMap sortMap) throws IOException {\n    boolean published = false;\n    try {\n      // Lock order IW -> BDS\n      ensureOpen(false);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publishFlushedSegment \" + newSegment);\n      }\n\n      if (globalPacket != null && globalPacket.any()) {\n        publishFrozenUpdates(globalPacket);\n      }\n\n      // Publishing the segment must be sync'd on IW -> BDS to make the sure\n      // that no merge prunes away the seg. private delete packet\n      final long nextGen;\n      if (packet != null && packet.any()) {\n        nextGen = publishFrozenUpdates(packet);\n      } else {\n        // Since we don't have a delete packet to apply we can get a new\n        // generation right away\n        nextGen = bufferedUpdatesStream.getNextGen();\n        // No deletes/updates here, so marked finished immediately:\n        bufferedUpdatesStream.finishedSegment(nextGen);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publish sets newSegment delGen=\" + nextGen + \" seg=\" + segString(newSegment));\n      }\n      newSegment.setBufferedDeletesGen(nextGen);\n      segmentInfos.add(newSegment);\n      published = true;\n      checkpoint();\n      if (packet != null && packet.any() && sortMap != null) {\n        // TODO: not great we do this heavyish op while holding IW's monitor lock,\n        // but it only applies if you are using sorted indices and updating doc values:\n        ReadersAndUpdates rld = getPooledInstance(newSegment, true);\n        rld.sortMap = sortMap;\n        // DON't release this ReadersAndUpdates we need to stick with that sortMap\n      }\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(config.softDeletesField); // will return null if no soft deletes are present\n      // this is a corner case where documents delete them-self with soft deletes. This is used to\n      // build delete tombstones etc. in this case we haven't seen any updates to the DV in this fresh flushed segment.\n      // if we have seen updates the update code checks if the segment is fully deleted.\n      boolean hasInitialSoftDeleted = (fieldInfo != null\n          && fieldInfo.getDocValuesGen() == -1\n          && fieldInfo.getDocValuesType() != DocValuesType.NONE);\n      final boolean isFullyHardDeleted = newSegment.getDelCount() == newSegment.info.maxDoc();\n      // we either have a fully hard-deleted segment or one or more docs are soft-deleted. In both cases we need\n      // to go and check if they are fully deleted. This has the nice side-effect that we now have accurate numbers\n      // for the soft delete right after we flushed to disk.\n      if (hasInitialSoftDeleted || isFullyHardDeleted){\n        // this operation is only really executed if needed an if soft-deletes are not configured it only be executed\n        // if we deleted all docs in this newly flushed segment.\n        ReadersAndUpdates rld = getPooledInstance(newSegment, true);\n        try {\n          if (isFullyDeleted(rld)) {\n            dropDeletedSegment(newSegment);\n          }\n        } finally {\n          release(rld);\n        }\n      }\n\n    } finally {\n      if (published == false) {\n        adjustPendingNumDocs(-newSegment.info.maxDoc());\n      }\n      flushCount.incrementAndGet();\n      doAfterFlush();\n    }\n\n  }\n\n","sourceOld":"  /**\n   * Atomically adds the segment private delete packet and publishes the flushed\n   * segments SegmentInfo to the index writer.\n   */\n  synchronized void publishFlushedSegment(SegmentCommitInfo newSegment,\n                                          FieldInfos fieldInfos, FrozenBufferedUpdates packet, FrozenBufferedUpdates globalPacket,\n                                          Sorter.DocMap sortMap) throws IOException {\n    boolean published = false;\n    try {\n      // Lock order IW -> BDS\n      ensureOpen(false);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publishFlushedSegment \" + newSegment);\n      }\n\n      if (globalPacket != null && globalPacket.any()) {\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        bufferedUpdatesStream.push(globalPacket);\n        eventQueue.add(new ResolveUpdatesEvent(globalPacket));\n      }\n\n      // Publishing the segment must be sync'd on IW -> BDS to make the sure\n      // that no merge prunes away the seg. private delete packet\n      final long nextGen;\n      if (packet != null && packet.any()) {\n        nextGen = bufferedUpdatesStream.push(packet);\n\n        // Do this as an event so it applies higher in the stack when we are not holding DocumentsWriterFlushQueue.purgeLock:\n        eventQueue.add(new ResolveUpdatesEvent(packet));\n\n      } else {\n        // Since we don't have a delete packet to apply we can get a new\n        // generation right away\n        nextGen = bufferedUpdatesStream.getNextGen();\n        // No deletes/updates here, so marked finished immediately:\n        bufferedUpdatesStream.finishedSegment(nextGen);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publish sets newSegment delGen=\" + nextGen + \" seg=\" + segString(newSegment));\n      }\n      newSegment.setBufferedDeletesGen(nextGen);\n      segmentInfos.add(newSegment);\n      published = true;\n      checkpoint();\n      if (packet != null && packet.any() && sortMap != null) {\n        // TODO: not great we do this heavyish op while holding IW's monitor lock,\n        // but it only applies if you are using sorted indices and updating doc values:\n        ReadersAndUpdates rld = getPooledInstance(newSegment, true);\n        rld.sortMap = sortMap;\n        // DON't release this ReadersAndUpdates we need to stick with that sortMap\n      }\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(config.softDeletesField); // will return null if no soft deletes are present\n      // this is a corner case where documents delete them-self with soft deletes. This is used to\n      // build delete tombstones etc. in this case we haven't seen any updates to the DV in this fresh flushed segment.\n      // if we have seen updates the update code checks if the segment is fully deleted.\n      boolean hasInitialSoftDeleted = (fieldInfo != null\n          && fieldInfo.getDocValuesGen() == -1\n          && fieldInfo.getDocValuesType() != DocValuesType.NONE);\n      final boolean isFullyHardDeleted = newSegment.getDelCount() == newSegment.info.maxDoc();\n      // we either have a fully hard-deleted segment or one or more docs are soft-deleted. In both cases we need\n      // to go and check if they are fully deleted. This has the nice side-effect that we now have accurate numbers\n      // for the soft delete right after we flushed to disk.\n      if (hasInitialSoftDeleted || isFullyHardDeleted){\n        // this operation is only really executed if needed an if soft-deletes are not configured it only be executed\n        // if we deleted all docs in this newly flushed segment.\n        ReadersAndUpdates rld = getPooledInstance(newSegment, true);\n        try {\n          if (isFullyDeleted(rld)) {\n            dropDeletedSegment(newSegment);\n          }\n        } finally {\n          release(rld);\n        }\n      }\n\n    } finally {\n      if (published == false) {\n        adjustPendingNumDocs(-newSegment.info.maxDoc());\n      }\n      flushCount.incrementAndGet();\n      doAfterFlush();\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"09c01278fcf71c4b50f2729bade4b16ed7d48f2f","date":1526927557,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#publishFlushedSegment(SegmentCommitInfo,FieldInfos,FrozenBufferedUpdates,FrozenBufferedUpdates,Sorter.DocMap).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#publishFlushedSegment(SegmentCommitInfo,FieldInfos,FrozenBufferedUpdates,FrozenBufferedUpdates,Sorter.DocMap).mjava","sourceNew":"  /**\n   * Atomically adds the segment private delete packet and publishes the flushed\n   * segments SegmentInfo to the index writer.\n   */\n  private synchronized void publishFlushedSegment(SegmentCommitInfo newSegment, FieldInfos fieldInfos,\n                                                  FrozenBufferedUpdates packet, FrozenBufferedUpdates globalPacket,\n                                                  Sorter.DocMap sortMap) throws IOException {\n    boolean published = false;\n    try {\n      // Lock order IW -> BDS\n      ensureOpen(false);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publishFlushedSegment \" + newSegment);\n      }\n\n      if (globalPacket != null && globalPacket.any()) {\n        publishFrozenUpdates(globalPacket);\n      }\n\n      // Publishing the segment must be sync'd on IW -> BDS to make the sure\n      // that no merge prunes away the seg. private delete packet\n      final long nextGen;\n      if (packet != null && packet.any()) {\n        nextGen = publishFrozenUpdates(packet);\n      } else {\n        // Since we don't have a delete packet to apply we can get a new\n        // generation right away\n        nextGen = bufferedUpdatesStream.getNextGen();\n        // No deletes/updates here, so marked finished immediately:\n        bufferedUpdatesStream.finishedSegment(nextGen);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publish sets newSegment delGen=\" + nextGen + \" seg=\" + segString(newSegment));\n      }\n      newSegment.setBufferedDeletesGen(nextGen);\n      segmentInfos.add(newSegment);\n      published = true;\n      checkpoint();\n      if (packet != null && packet.any() && sortMap != null) {\n        // TODO: not great we do this heavyish op while holding IW's monitor lock,\n        // but it only applies if you are using sorted indices and updating doc values:\n        ReadersAndUpdates rld = getPooledInstance(newSegment, true);\n        rld.sortMap = sortMap;\n        // DON't release this ReadersAndUpdates we need to stick with that sortMap\n      }\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(config.softDeletesField); // will return null if no soft deletes are present\n      // this is a corner case where documents delete them-self with soft deletes. This is used to\n      // build delete tombstones etc. in this case we haven't seen any updates to the DV in this fresh flushed segment.\n      // if we have seen updates the update code checks if the segment is fully deleted.\n      boolean hasInitialSoftDeleted = (fieldInfo != null\n          && fieldInfo.getDocValuesGen() == -1\n          && fieldInfo.getDocValuesType() != DocValuesType.NONE);\n      final boolean isFullyHardDeleted = newSegment.getDelCount() == newSegment.info.maxDoc();\n      // we either have a fully hard-deleted segment or one or more docs are soft-deleted. In both cases we need\n      // to go and check if they are fully deleted. This has the nice side-effect that we now have accurate numbers\n      // for the soft delete right after we flushed to disk.\n      if (hasInitialSoftDeleted || isFullyHardDeleted){\n        // this operation is only really executed if needed an if soft-deletes are not configured it only be executed\n        // if we deleted all docs in this newly flushed segment.\n        ReadersAndUpdates rld = getPooledInstance(newSegment, true);\n        try {\n          if (isFullyDeleted(rld)) {\n            dropDeletedSegment(newSegment);\n            checkpoint();\n          }\n        } finally {\n          release(rld);\n        }\n      }\n\n    } finally {\n      if (published == false) {\n        adjustPendingNumDocs(-newSegment.info.maxDoc());\n      }\n      flushCount.incrementAndGet();\n      doAfterFlush();\n    }\n\n  }\n\n","sourceOld":"  /**\n   * Atomically adds the segment private delete packet and publishes the flushed\n   * segments SegmentInfo to the index writer.\n   */\n  private synchronized void publishFlushedSegment(SegmentCommitInfo newSegment, FieldInfos fieldInfos,\n                                                  FrozenBufferedUpdates packet, FrozenBufferedUpdates globalPacket,\n                                                  Sorter.DocMap sortMap) throws IOException {\n    boolean published = false;\n    try {\n      // Lock order IW -> BDS\n      ensureOpen(false);\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publishFlushedSegment \" + newSegment);\n      }\n\n      if (globalPacket != null && globalPacket.any()) {\n        publishFrozenUpdates(globalPacket);\n      }\n\n      // Publishing the segment must be sync'd on IW -> BDS to make the sure\n      // that no merge prunes away the seg. private delete packet\n      final long nextGen;\n      if (packet != null && packet.any()) {\n        nextGen = publishFrozenUpdates(packet);\n      } else {\n        // Since we don't have a delete packet to apply we can get a new\n        // generation right away\n        nextGen = bufferedUpdatesStream.getNextGen();\n        // No deletes/updates here, so marked finished immediately:\n        bufferedUpdatesStream.finishedSegment(nextGen);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"publish sets newSegment delGen=\" + nextGen + \" seg=\" + segString(newSegment));\n      }\n      newSegment.setBufferedDeletesGen(nextGen);\n      segmentInfos.add(newSegment);\n      published = true;\n      checkpoint();\n      if (packet != null && packet.any() && sortMap != null) {\n        // TODO: not great we do this heavyish op while holding IW's monitor lock,\n        // but it only applies if you are using sorted indices and updating doc values:\n        ReadersAndUpdates rld = getPooledInstance(newSegment, true);\n        rld.sortMap = sortMap;\n        // DON't release this ReadersAndUpdates we need to stick with that sortMap\n      }\n      FieldInfo fieldInfo = fieldInfos.fieldInfo(config.softDeletesField); // will return null if no soft deletes are present\n      // this is a corner case where documents delete them-self with soft deletes. This is used to\n      // build delete tombstones etc. in this case we haven't seen any updates to the DV in this fresh flushed segment.\n      // if we have seen updates the update code checks if the segment is fully deleted.\n      boolean hasInitialSoftDeleted = (fieldInfo != null\n          && fieldInfo.getDocValuesGen() == -1\n          && fieldInfo.getDocValuesType() != DocValuesType.NONE);\n      final boolean isFullyHardDeleted = newSegment.getDelCount() == newSegment.info.maxDoc();\n      // we either have a fully hard-deleted segment or one or more docs are soft-deleted. In both cases we need\n      // to go and check if they are fully deleted. This has the nice side-effect that we now have accurate numbers\n      // for the soft delete right after we flushed to disk.\n      if (hasInitialSoftDeleted || isFullyHardDeleted){\n        // this operation is only really executed if needed an if soft-deletes are not configured it only be executed\n        // if we deleted all docs in this newly flushed segment.\n        ReadersAndUpdates rld = getPooledInstance(newSegment, true);\n        try {\n          if (isFullyDeleted(rld)) {\n            dropDeletedSegment(newSegment);\n          }\n        } finally {\n          release(rld);\n        }\n      }\n\n    } finally {\n      if (published == false) {\n        adjustPendingNumDocs(-newSegment.info.maxDoc());\n      }\n      flushCount.incrementAndGet();\n      doAfterFlush();\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"6b8498afacfc8322268ca0d659d274fcce08d557":["86a2e8a56b368d37ef3ba7180541fa317d6fd6c7"],"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7":["1926100d9b67becc9701c54266fee3ba7878a5f0"],"09c01278fcf71c4b50f2729bade4b16ed7d48f2f":["6b8498afacfc8322268ca0d659d274fcce08d557"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"15e716649e2bd79a98b5e68c464154ea4c44677a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"1926100d9b67becc9701c54266fee3ba7878a5f0":["15e716649e2bd79a98b5e68c464154ea4c44677a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["09c01278fcf71c4b50f2729bade4b16ed7d48f2f"]},"commit2Childs":{"6b8498afacfc8322268ca0d659d274fcce08d557":["09c01278fcf71c4b50f2729bade4b16ed7d48f2f"],"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7":["6b8498afacfc8322268ca0d659d274fcce08d557"],"09c01278fcf71c4b50f2729bade4b16ed7d48f2f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["15e716649e2bd79a98b5e68c464154ea4c44677a"],"15e716649e2bd79a98b5e68c464154ea4c44677a":["1926100d9b67becc9701c54266fee3ba7878a5f0"],"1926100d9b67becc9701c54266fee3ba7878a5f0":["86a2e8a56b368d37ef3ba7180541fa317d6fd6c7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}