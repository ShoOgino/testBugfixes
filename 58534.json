{"path":"solr/core/src/test/org/apache/solr/search/facet/TestJsonFacets#doTestPrelimSorting(Client,boolean,boolean).mjava","commits":[{"id":"94ce69d020f939568b84a1dbbfbd11bfd9907b73","date":1543618146,"type":0,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/search/facet/TestJsonFacets#doTestPrelimSorting(Client,boolean,boolean).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Helper method that indexes a fixed set of docs to exactly <em>two</em> of the SolrClients \n   * involved in the current Client such that each shard is identical for the purposes of simplified \n   * doc/facet counting/assertions -- if there is only one SolrClient (Client.local) then it sends that \n   * single shard twice as many docs so the counts/assertions will be consistent.\n   *\n   * Note: this test doesn't demonstrate practical uses of prelim_sort.\n   * The scenerios it tests are actualy fairly absurd, but help to ensure that edge cases are covered.\n   *\n   * @param client client to use -- may be local or multishard\n   * @param extraAgg if an extra aggregation function should be included, this hits slightly diff code paths\n   * @param extraSubFacet if an extra sub facet should be included, this hits slightly diff code paths\n   */\n  public void doTestPrelimSorting(final Client client,\n                                  final boolean extraAgg,\n                                  final boolean extraSubFacet) throws Exception {\n    \n    client.deleteByQuery(\"*:*\", null);\n    \n    List<SolrClient> clients = client.getClientProvider().all();\n    \n    // carefully craft two balanced shards (assuming we have at least two) and leave any other shards\n    // empty to help check the code paths of some shards returning no buckets.\n    //\n    // if we are in a single node sitaution, these clients will be the same, and we'll have the same\n    // total docs in our collection, but the numShardsWithData will be diff\n    // (which will affect some assertions)\n    final SolrClient shardA = clients.get(0);\n    final SolrClient shardB = clients.get(clients.size()-1);\n    final int numShardsWithData = (shardA == shardB) ? 1 : 2;\n\n    // for simplicity, each foo_s \"term\" exists on each shard in the same number of docs as it's numeric \n    // value (so count should be double the term) and bar_i is always 1 per doc (so sum(bar_i)\n    // should always be the same as count)\n    int id = 0;\n    for (int i = 1; i <= 20; i++) {\n      for (int j = 1; j <= i; j++) {\n        shardA.add(new SolrInputDocument(\"id\", \"\"+(++id), \"foo_s\", \"foo_\" + i, \"bar_i\", \"1\"));\n        shardB.add(new SolrInputDocument(\"id\", \"\"+(++id), \"foo_s\", \"foo_\" + i, \"bar_i\", \"1\"));\n      }\n    }\n    assertEquals(420, id); // sanity check\n    client.commit();\n    DebugAgg.Acc.collectDocs.set(0);\n    DebugAgg.Acc.collectDocSets.set(0);\n\n    // NOTE: sorting by index can cause some optimizations when using type=enum|stream\n    // that cause our stat to be collected differently, so we have to account for that when\n    // looking at DebugAdd collect stats if/when the test framework picks those\n    // ...BUT... this only affects cloud, for single node prelim_sort overrides streaming\n    final boolean indexSortDebugAggFudge = ( 1 < numShardsWithData ) &&\n      (FacetField.FacetMethod.DEFAULT_METHOD.equals(FacetField.FacetMethod.STREAM) ||\n       FacetField.FacetMethod.DEFAULT_METHOD.equals(FacetField.FacetMethod.ENUM));\n    \n    \n    final String common = \"refine:true, type:field, field:'foo_s', facet: { \"\n      + \"x: 'debug(wrap,sum(bar_i))' \"\n      + (extraAgg ? \", y:'min(bar_i)'\" : \"\")\n      + (extraSubFacet ? \", z:{type:query, q:'bar_i:0'}\" : \"\")\n      + \"}\";\n    final String yz = (extraAgg ? \"y:1, \" : \"\") + (extraSubFacet ? \"z:{count:0}, \" : \"\");\n    \n    // really basic: top 5 by (prelim_sort) count, (re)sorted by a stat\n    client.testJQ(params(\"q\", \"*:*\", \"rows\", \"0\", \"json.facet\"\n                         , \"{ foo_a:{ \"+ common+\", limit:5, overrequest:0, \"\n                         + \"          prelim_sort:'count desc', sort:'x asc' }\"\n                         + \"  foo_b:{ \"+ common+\", limit:5, overrequest:0, \"\n                         + \"          prelim_sort:'count asc', sort:'x desc' } }\")\n                  , \"facets=={ 'count':420, \"\n                  + \"  'foo_a':{ 'buckets':[\" \n                  + \"    { val:foo_16, count:32, \" + yz + \"x:32.0},\"\n                  + \"    { val:foo_17, count:34, \" + yz + \"x:34.0},\"\n                  + \"    { val:foo_18, count:36, \" + yz + \"x:36.0},\"\n                  + \"    { val:foo_19, count:38, \" + yz + \"x:38.0},\"\n                  + \"    { val:foo_20, count:40, \" + yz + \"x:40.0},\"\n                  + \"] },\"\n                  + \"  'foo_b':{ 'buckets':[\" \n                  + \"    { val:foo_5, count:10, \" + yz + \"x:10.0},\"\n                  + \"    { val:foo_4, count:8,  \" + yz + \"x:8.0},\"\n                  + \"    { val:foo_3, count:6,  \" + yz + \"x:6.0},\"\n                  + \"    { val:foo_2, count:4,  \" + yz + \"x:4.0},\"\n                  + \"    { val:foo_1, count:2,  \" + yz + \"x:2.0},\"\n                  + \"] },\"\n                  + \"}\"\n                  );\n    // (re)sorting should prevent 'sum(bar_i)' from being computed for every doc\n    // only the choosen buckets should be collected (as a set) once per node...\n    assertEqualsAndReset(0, DebugAgg.Acc.collectDocs);\n    // 2 facets, 5 bucket, on each shard\n    assertEqualsAndReset(numShardsWithData * 2 * 5, DebugAgg.Acc.collectDocSets);\n\n    { // same really basic top 5 by (prelim_sort) count, (re)sorted by a stat -- w/allBuckets:true\n      // check code paths with and w/o allBuckets\n      // NOTE: allBuckets includes stats, but not other sub-facets...\n      final String aout = \"allBuckets:{ count:420, \"+ (extraAgg ? \"y:1, \" : \"\") + \"x:420.0 }\";\n      client.testJQ(params(\"q\", \"*:*\", \"rows\", \"0\", \"json.facet\"\n                           , \"{ foo_a:{ \" + common+\", allBuckets:true, limit:5, overrequest:0, \"\n                           + \"          prelim_sort:'count desc', sort:'x asc' }\"\n                           + \"  foo_b:{ \" + common+\", allBuckets:true, limit:5, overrequest:0, \"\n                           + \"          prelim_sort:'count asc', sort:'x desc' } }\")\n                    , \"facets=={ 'count':420, \"\n                    + \"  'foo_a':{ \" + aout + \" 'buckets':[\" \n                    + \"    { val:foo_16, count:32, \" + yz + \"x:32.0},\"\n                    + \"    { val:foo_17, count:34, \" + yz + \"x:34.0},\"\n                    + \"    { val:foo_18, count:36, \" + yz + \"x:36.0},\"\n                    + \"    { val:foo_19, count:38, \" + yz + \"x:38.0},\"\n                    + \"    { val:foo_20, count:40, \" + yz + \"x:40.0},\"\n                    + \"] },\"\n                    + \"  'foo_b':{ \" + aout + \" 'buckets':[\" \n                    + \"    { val:foo_5, count:10, \" + yz + \"x:10.0},\"\n                    + \"    { val:foo_4, count:8,  \" + yz + \"x:8.0},\"\n                    + \"    { val:foo_3, count:6,  \" + yz + \"x:6.0},\"\n                    + \"    { val:foo_2, count:4,  \" + yz + \"x:4.0},\"\n                    + \"    { val:foo_1, count:2,  \" + yz + \"x:2.0},\"\n                    + \"] },\"\n                    + \"}\"\n                    );\n      // because of allBuckets, we collect every doc on everyshard (x2 facets) in a single \"all\" slot...\n      assertEqualsAndReset(2 * 420, DebugAgg.Acc.collectDocs);\n      // ... in addition to collecting each of the choosen buckets (as sets) once per node...\n      // 2 facets, 5 bucket, on each shard\n      assertEqualsAndReset(numShardsWithData * 2 * 5, DebugAgg.Acc.collectDocSets);\n    }\n    \n    // pagination (with offset) should happen against the re-sorted list (up to the effective limit)\n    client.testJQ(params(\"q\", \"*:*\", \"rows\", \"0\", \"json.facet\"\n                         , \"{ foo_a:{ \"+common+\", offset:2, limit:3, overrequest:0, \"\n                         + \"          prelim_sort:'count desc', sort:'x asc' }\"\n                         + \"  foo_b:{ \"+common+\", offset:2, limit:3, overrequest:0, \"\n                         + \"          prelim_sort:'count asc', sort:'x desc' } }\")\n                  , \"facets=={ 'count':420, \"\n                  + \"  'foo_a':{ 'buckets':[\" \n                  + \"    { val:foo_18, count:36, \" + yz + \"x:36.0},\"\n                  + \"    { val:foo_19, count:38, \" + yz + \"x:38.0},\"\n                  + \"    { val:foo_20, count:40, \" + yz + \"x:40.0},\"\n                  + \"] },\"\n                  + \"  'foo_b':{ 'buckets':[\" \n                  + \"    { val:foo_3, count:6,  \" + yz + \"x:6.0},\"\n                  + \"    { val:foo_2, count:4,  \" + yz + \"x:4.0},\"\n                  + \"    { val:foo_1, count:2,  \" + yz + \"x:2.0},\"\n                  + \"] },\"\n                  + \"}\"\n                  );\n    assertEqualsAndReset(0, DebugAgg.Acc.collectDocs);\n    // 2 facets, 5 buckets (including offset), on each shard\n    assertEqualsAndReset(numShardsWithData * 2 * 5, DebugAgg.Acc.collectDocSets);\n    \n    // when overrequesting is used, the full list of candidate buckets should be considered\n    client.testJQ(params(\"q\", \"*:*\", \"rows\", \"0\", \"json.facet\"\n                         , \"{ foo_a:{ \"+common+\", limit:5, overrequest:5, \"\n                         + \"          prelim_sort:'count desc', sort:'x asc' }\"\n                         + \"  foo_b:{ \"+common+\", limit:5, overrequest:5, \"\n                         + \"          prelim_sort:'count asc', sort:'x desc' } }\")\n                  , \"facets=={ 'count':420, \"\n                  + \"  'foo_a':{ 'buckets':[\" \n                  + \"    { val:foo_11, count:22, \" + yz + \"x:22.0},\"\n                  + \"    { val:foo_12, count:24, \" + yz + \"x:24.0},\"\n                  + \"    { val:foo_13, count:26, \" + yz + \"x:26.0},\"\n                  + \"    { val:foo_14, count:28, \" + yz + \"x:28.0},\"\n                  + \"    { val:foo_15, count:30, \" + yz + \"x:30.0},\"\n                  + \"] },\"\n                  + \"  'foo_b':{ 'buckets':[\" \n                  + \"    { val:foo_10, count:20, \" + yz + \"x:20.0},\"\n                  + \"    { val:foo_9, count:18,  \" + yz + \"x:18.0},\"\n                  + \"    { val:foo_8, count:16,  \" + yz + \"x:16.0},\"\n                  + \"    { val:foo_7, count:14,  \" + yz + \"x:14.0},\"\n                  + \"    { val:foo_6, count:12,  \" + yz + \"x:12.0},\"\n                  + \"] },\"\n                  + \"}\"\n                  );\n    assertEqualsAndReset(0, DebugAgg.Acc.collectDocs);\n    // 2 facets, 10 buckets (including overrequest), on each shard\n    assertEqualsAndReset(numShardsWithData * 2 * 10, DebugAgg.Acc.collectDocSets);\n\n    { // for an (effectively) unlimited facet, then from the black box perspective of the client,\n      // preliminary sorting should be completely ignored...\n      final StringBuilder expected = new StringBuilder(\"facets=={ 'count':420, 'foo_a':{ 'buckets':[\\n\");\n      for (int i = 20; 0 < i; i--) {\n        final int x = i * 2;\n        expected.append(\"{ val:foo_\"+i+\", count:\"+x+\", \" + yz + \"x:\"+x+\".0},\\n\");\n      }\n      expected.append(\"] } }\");\n      for (int limit : Arrays.asList(-1, 100000)) {\n        for (String sortOpts : Arrays.asList(\"sort:'x desc'\",\n                                             \"prelim_sort:'count asc', sort:'x desc'\",\n                                             \"prelim_sort:'index asc', sort:'x desc'\")) {\n          final String snippet = \"limit: \" + limit + \", \" + sortOpts;\n          client.testJQ(params(\"q\", \"*:*\", \"rows\", \"0\", \"json.facet\"\n                               , \"{ foo_a:{ \"+common+\", \" + snippet + \"}}\")\n                        , expected.toString());\n\n          // the only difference from a white box perspective, is when/if we are \n          // optimized to use the sort SlotAcc during collection instead of the prelim_sort SlotAcc..\n          // (ie: sub facet preventing single pass (re)sort in single node mode)\n          if (((0 < limit || extraSubFacet) && snippet.contains(\"prelim_sort\")) &&\n              ! (indexSortDebugAggFudge && snippet.contains(\"index asc\"))) {\n            // by-pass single pass collection, do everything as sets...\n            assertEqualsAndReset(snippet, numShardsWithData * 20, DebugAgg.Acc.collectDocSets);\n            assertEqualsAndReset(snippet, 0, DebugAgg.Acc.collectDocs);\n          } else { // simple sort on x, or optimized single pass (re)sort, or indexSortDebugAggFudge\n            // no sets should have been (post) collected for our stat\n            assertEqualsAndReset(snippet, 0, DebugAgg.Acc.collectDocSets);\n            // every doc should be collected...\n            assertEqualsAndReset(snippet, 420, DebugAgg.Acc.collectDocs);\n          }\n        }\n      }\n    }\n\n    // test all permutations of (prelim_sort | sort) on (index | count | stat) since there are\n    // custom sort codepaths for index & count that work differnetly then general stats\n    //\n    // NOTE: there's very little value in re-sort by count/index after prelim_sort on something more complex,\n    // typically better to just ignore the prelim_sort, but we're testing it for completeness\n    // (and because you *might* want to prelim_sort by some function, for the purpose of \"sampling\" the\n    // top results and then (re)sorting by count/index)\n    for (String numSort : Arrays.asList(\"count\", \"x\")) { // equivilent ordering\n      client.testJQ(params(\"q\", \"*:*\", \"rows\", \"0\", \"json.facet\"\n                           , \"{ foo_a:{ \"+common+\", limit:10, overrequest:0, \"\n                           + \"          prelim_sort:'\"+numSort+\" asc', sort:'index desc' }\"\n                           + \"  foo_b:{ \"+common+\", limit:10, overrequest:0, \"\n                           + \"          prelim_sort:'index asc', sort:'\"+numSort+\" desc' } }\")\n                    , \"facets=={ 'count':420, \"\n                    + \"  'foo_a':{ 'buckets':[\" \n                    + \"    { val:foo_9,  count:18, \" + yz + \"x:18.0},\"\n                    + \"    { val:foo_8,  count:16, \" + yz + \"x:16.0},\"\n                    + \"    { val:foo_7,  count:14, \" + yz + \"x:14.0},\"\n                    + \"    { val:foo_6,  count:12, \" + yz + \"x:12.0},\"\n                    + \"    { val:foo_5,  count:10, \" + yz + \"x:10.0},\"\n                    + \"    { val:foo_4,  count:8,  \" + yz + \"x:8.0},\"\n                    + \"    { val:foo_3,  count:6,  \" + yz + \"x:6.0},\"\n                    + \"    { val:foo_2,  count:4,  \" + yz + \"x:4.0},\"\n                    + \"    { val:foo_10, count:20, \" + yz + \"x:20.0},\"\n                    + \"    { val:foo_1,  count:2,  \" + yz + \"x:2.0},\"\n                    + \"] },\"\n                    + \"  'foo_b':{ 'buckets':[\" \n                    + \"    { val:foo_18, count:36, \" + yz + \"x:36.0},\"\n                    + \"    { val:foo_17, count:34, \" + yz + \"x:34.0},\"\n                    + \"    { val:foo_16, count:32, \" + yz + \"x:32.0},\"\n                    + \"    { val:foo_15, count:30, \" + yz + \"x:30.0},\"\n                    + \"    { val:foo_14, count:28, \" + yz + \"x:28.0},\"\n                    + \"    { val:foo_13, count:26, \" + yz + \"x:26.0},\"\n                    + \"    { val:foo_12, count:24, \" + yz + \"x:24.0},\"\n                    + \"    { val:foo_11, count:22, \" + yz + \"x:22.0},\"\n                    + \"    { val:foo_10, count:20, \" + yz + \"x:20.0},\"\n                    + \"    { val:foo_1,  count:2,  \" + yz + \"x:2.0},\"\n                    + \"] },\"\n                    + \"}\"\n                    );\n      // since these behave differently, defer DebugAgg counter checks until all are done...\n    }\n    // These 3 permutations defer the compuation of x as docsets,\n    // so it's 3 x (10 buckets on each shard) (but 0 direct docs)\n    //      prelim_sort:count, sort:index\n    //      prelim_sort:index, sort:x\n    //      prelim_sort:index, sort:count\n    // ...except when streaming, prelim_sort:index does no docsets.\n    assertEqualsAndReset((indexSortDebugAggFudge ? 1 : 3) * numShardsWithData * 10,\n                         DebugAgg.Acc.collectDocSets);\n    // This is the only situation that should (always) result in every doc being collected (but 0 docsets)...\n    //      prelim_sort:x,     sort:index\n    // ...but the (2) prelim_sort:index streaming situations above will also cause all the docs in the first\n    // 10+1 buckets to be collected (enum checks limit+1 to know if there are \"more\"...\n    assertEqualsAndReset(420 + (indexSortDebugAggFudge ?\n                                2 * numShardsWithData * (1+10+11+12+13+14+15+16+17+18+19) : 0),\n                         DebugAgg.Acc.collectDocs);\n\n    // sanity check of prelim_sorting in a sub facet\n    client.testJQ(params(\"q\", \"*:*\", \"rows\", \"0\", \"json.facet\"\n                         , \"{ bar:{ type:query, query:'foo_s:[foo_10 TO foo_19]', facet: {\"\n                         + \"        foo:{ \"+ common+\", limit:5, overrequest:0, \"\n                         + \"              prelim_sort:'count desc', sort:'x asc' } } } }\")\n                  , \"facets=={ 'count':420, \"\n                  + \" 'bar':{ 'count':290, \"\n                  + \"    'foo':{ 'buckets':[\" \n                  + \"      { val:foo_15, count:30, \" + yz + \"x:30.0},\"\n                  + \"      { val:foo_16, count:32, \" + yz + \"x:32.0},\"\n                  + \"      { val:foo_17, count:34, \" + yz + \"x:34.0},\"\n                  + \"      { val:foo_18, count:36, \" + yz + \"x:36.0},\"\n                  + \"      { val:foo_19, count:38, \" + yz + \"x:38.0},\"\n                  + \"    ] },\"\n                  + \"  },\"\n                  + \"}\"\n                  );\n    // the prelim_sort should prevent 'sum(bar_i)' from being computed for every doc\n    // only the choosen buckets should be collected (as a set) once per node...\n    assertEqualsAndReset(0, DebugAgg.Acc.collectDocs);\n    // 5 bucket, on each shard\n    assertEqualsAndReset(numShardsWithData * 5, DebugAgg.Acc.collectDocSets);\n\n    { // sanity check how defered stats are handled\n      \n      // here we'll prelim_sort & sort on things that are both \"not x\" and using the debug() counters\n      // (wrapping x) to assert that 'x' is correctly defered and only collected for the final top buckets\n      final List<String> sorts = new ArrayList<String>(Arrays.asList(\"index asc\", \"count asc\"));\n      if (extraAgg) {\n        sorts.add(\"y asc\"); // same for every bucket, but index order tie breaker should kick in\n      }\n      for (String s : sorts) {\n        client.testJQ(params(\"q\", \"*:*\", \"rows\", \"0\", \"json.facet\"\n                             , \"{ foo:{ \"+ common+\", limit:5, overrequest:0, \"\n                             + \"          prelim_sort:'count desc', sort:'\"+s+\"' } }\")\n                      , \"facets=={ 'count':420, \"\n                      + \"  'foo':{ 'buckets':[\" \n                      + \"    { val:foo_16, count:32, \" + yz + \"x:32.0},\"\n                      + \"    { val:foo_17, count:34, \" + yz + \"x:34.0},\"\n                      + \"    { val:foo_18, count:36, \" + yz + \"x:36.0},\"\n                      + \"    { val:foo_19, count:38, \" + yz + \"x:38.0},\"\n                      + \"    { val:foo_20, count:40, \" + yz + \"x:40.0},\"\n                      + \"] } }\"\n                      );\n        // Neither prelim_sort nor sort should need 'sum(bar_i)' to be computed for every doc\n        // only the choosen buckets should be collected (as a set) once per node...\n        assertEqualsAndReset(0, DebugAgg.Acc.collectDocs);\n        // 5 bucket, on each shard\n        assertEqualsAndReset(numShardsWithData * 5, DebugAgg.Acc.collectDocSets);\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"94ce69d020f939568b84a1dbbfbd11bfd9907b73":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["94ce69d020f939568b84a1dbbfbd11bfd9907b73"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["94ce69d020f939568b84a1dbbfbd11bfd9907b73"],"94ce69d020f939568b84a1dbbfbd11bfd9907b73":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}