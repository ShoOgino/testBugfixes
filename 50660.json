{"path":"src/test/org/apache/solr/analysis/TestSynonymFilter#testOffsetsWithOrig().mjava","commits":[{"id":"0d3072fffb39e4aa50fce38815821919b35fc194","date":1138579199,"type":0,"author":"Yoav Shapira","isMerge":false,"pathNew":"src/test/org/apache/solr/analysis/TestSynonymFilter#testOffsetsWithOrig().mjava","pathOld":"/dev/null","sourceNew":"  public void testOffsetsWithOrig() throws IOException {\r\n    SynonymMap map = new SynonymMap();\r\n\r\n    boolean orig = true;\r\n    boolean merge = true;\r\n\r\n    // test that generated tokens start at the same offset as the original\r\n    map.add(strings(\"a\"), tokens(\"aa\"), orig, merge);\r\n    assertTokEqual(getTokList(map,\"a,5\",false), tokens(\"a,5/aa\"));\r\n    assertTokEqual(getTokList(map,\"a,0\",false), tokens(\"a,0/aa\"));\r\n\r\n    // test that offset of first replacement is ignored (always takes the orig offset)\r\n    map.add(strings(\"b\"), tokens(\"bb,100\"), orig, merge);\r\n    assertTokEqual(getTokList(map,\"b,5\",false), tokens(\"bb,5/b\"));\r\n    assertTokEqual(getTokList(map,\"b,0\",false), tokens(\"bb,0/b\"));\r\n\r\n    // test that subsequent tokens are adjusted accordingly\r\n    map.add(strings(\"c\"), tokens(\"cc,100 c2,2\"), orig, merge);\r\n    assertTokEqual(getTokList(map,\"c,5\",false), tokens(\"cc,5/c c2,2\"));\r\n    assertTokEqual(getTokList(map,\"c,0\",false), tokens(\"cc,0/c c2,2\"));\r\n  }\r\n\n","sourceOld":null,"bugFix":null,"bugIntro":["c1781d63ce2d5e979c8b3f35682b60a86f5df553"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c1781d63ce2d5e979c8b3f35682b60a86f5df553","date":1153254917,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/test/org/apache/solr/analysis/TestSynonymFilter#testOffsetsWithOrig().mjava","pathOld":"src/test/org/apache/solr/analysis/TestSynonymFilter#testOffsetsWithOrig().mjava","sourceNew":"  public void testOffsetsWithOrig() throws IOException {\n    SynonymMap map = new SynonymMap();\n\n    boolean orig = true;\n    boolean merge = true;\n\n    // test that generated tokens start at the same offset as the original\n    map.add(strings(\"a\"), tokens(\"aa\"), orig, merge);\n    assertTokEqual(getTokList(map,\"a,5\",false), tokens(\"a,5/aa\"));\n    assertTokEqual(getTokList(map,\"a,0\",false), tokens(\"a,0/aa\"));\n\n    // test that offset of first replacement is ignored (always takes the orig offset)\n    map.add(strings(\"b\"), tokens(\"bb,100\"), orig, merge);\n    assertTokEqual(getTokList(map,\"b,5\",false), tokens(\"bb,5/b\"));\n    assertTokEqual(getTokList(map,\"b,0\",false), tokens(\"bb,0/b\"));\n\n    // test that subsequent tokens are adjusted accordingly\n    map.add(strings(\"c\"), tokens(\"cc,100 c2,2\"), orig, merge);\n    assertTokEqual(getTokList(map,\"c,5\",false), tokens(\"cc,5/c c2,2\"));\n    assertTokEqual(getTokList(map,\"c,0\",false), tokens(\"cc,0/c c2,2\"));\n  }\n\n","sourceOld":"  public void testOffsetsWithOrig() throws IOException {\r\n    SynonymMap map = new SynonymMap();\r\n\r\n    boolean orig = true;\r\n    boolean merge = true;\r\n\r\n    // test that generated tokens start at the same offset as the original\r\n    map.add(strings(\"a\"), tokens(\"aa\"), orig, merge);\r\n    assertTokEqual(getTokList(map,\"a,5\",false), tokens(\"a,5/aa\"));\r\n    assertTokEqual(getTokList(map,\"a,0\",false), tokens(\"a,0/aa\"));\r\n\r\n    // test that offset of first replacement is ignored (always takes the orig offset)\r\n    map.add(strings(\"b\"), tokens(\"bb,100\"), orig, merge);\r\n    assertTokEqual(getTokList(map,\"b,5\",false), tokens(\"bb,5/b\"));\r\n    assertTokEqual(getTokList(map,\"b,0\",false), tokens(\"bb,0/b\"));\r\n\r\n    // test that subsequent tokens are adjusted accordingly\r\n    map.add(strings(\"c\"), tokens(\"cc,100 c2,2\"), orig, merge);\r\n    assertTokEqual(getTokList(map,\"c,5\",false), tokens(\"cc,5/c c2,2\"));\r\n    assertTokEqual(getTokList(map,\"c,0\",false), tokens(\"cc,0/c c2,2\"));\r\n  }\r\n\n","bugFix":["0d3072fffb39e4aa50fce38815821919b35fc194"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2fd023a662cc25ae7e0ad0f33d71c476a16d0579","date":1261403630,"type":5,"author":"Mark Robert Miller","isMerge":false,"pathNew":"src/test/org/apache/solr/analysis/TestSynonymFilter#testPositionIncrementsWithOrig().mjava","pathOld":"src/test/org/apache/solr/analysis/TestSynonymFilter#testOffsetsWithOrig().mjava","sourceNew":"  public void testPositionIncrementsWithOrig() throws IOException {\n    SynonymMap map = new SynonymMap();\n\n    boolean orig = true;\n    boolean merge = true;\n\n    // test that generated tokens start at the same offset as the original\n    map.add(strings(\"a\"), tokens(\"aa\"), orig, merge);\n    assertTokenizesTo(map, tokens(\"a,5\"),\n        new String[] { \"a\", \"aa\" },\n        new int[] { 5, 0 });\n    assertTokenizesTo(map, tokens(\"a,0\"),\n        new String[] { \"a\", \"aa\" },\n        new int[] { 0, 0 });\n\n    // test that offset of first replacement is ignored (always takes the orig offset)\n    map.add(strings(\"b\"), tokens(\"bb,100\"), orig, merge);\n    assertTokenizesTo(map, tokens(\"b,5\"),\n        new String[] { \"b\", \"bb\" },\n        new int[] { 5, 0 });\n    assertTokenizesTo(map, tokens(\"b,0\"),\n        new String[] { \"b\", \"bb\" },\n        new int[] { 0, 0 });\n\n    // test that subsequent tokens are adjusted accordingly\n    map.add(strings(\"c\"), tokens(\"cc,100 c2,2\"), orig, merge);\n    assertTokenizesTo(map, tokens(\"c,5\"),\n        new String[] { \"c\", \"cc\", \"c2\" },\n        new int[] { 5, 0, 2 });\n    assertTokenizesTo(map, tokens(\"c,0\"),\n        new String[] { \"c\", \"cc\", \"c2\" },\n        new int[] { 0, 0, 2 });\n  }\n\n","sourceOld":"  public void testOffsetsWithOrig() throws IOException {\n    SynonymMap map = new SynonymMap();\n\n    boolean orig = true;\n    boolean merge = true;\n\n    // test that generated tokens start at the same offset as the original\n    map.add(strings(\"a\"), tokens(\"aa\"), orig, merge);\n    assertTokEqual(getTokList(map,\"a,5\",false), tokens(\"a,5/aa\"));\n    assertTokEqual(getTokList(map,\"a,0\",false), tokens(\"a,0/aa\"));\n\n    // test that offset of first replacement is ignored (always takes the orig offset)\n    map.add(strings(\"b\"), tokens(\"bb,100\"), orig, merge);\n    assertTokEqual(getTokList(map,\"b,5\",false), tokens(\"bb,5/b\"));\n    assertTokEqual(getTokList(map,\"b,0\",false), tokens(\"bb,0/b\"));\n\n    // test that subsequent tokens are adjusted accordingly\n    map.add(strings(\"c\"), tokens(\"cc,100 c2,2\"), orig, merge);\n    assertTokEqual(getTokList(map,\"c,5\",false), tokens(\"cc,5/c c2,2\"));\n    assertTokEqual(getTokList(map,\"c,0\",false), tokens(\"cc,0/c c2,2\"));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"c1781d63ce2d5e979c8b3f35682b60a86f5df553":["0d3072fffb39e4aa50fce38815821919b35fc194"],"2fd023a662cc25ae7e0ad0f33d71c476a16d0579":["c1781d63ce2d5e979c8b3f35682b60a86f5df553"],"0d3072fffb39e4aa50fce38815821919b35fc194":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["0d3072fffb39e4aa50fce38815821919b35fc194"],"c1781d63ce2d5e979c8b3f35682b60a86f5df553":["2fd023a662cc25ae7e0ad0f33d71c476a16d0579"],"0d3072fffb39e4aa50fce38815821919b35fc194":["c1781d63ce2d5e979c8b3f35682b60a86f5df553"],"2fd023a662cc25ae7e0ad0f33d71c476a16d0579":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["2fd023a662cc25ae7e0ad0f33d71c476a16d0579","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"pathCommit":null}