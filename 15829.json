{"path":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#_nextBucket().mjava","commits":[{"id":"ac53eb8ed1d40ceac7330e9dc2e5c258e8fc155d","date":1426480823,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#_nextBucket().mjava","pathOld":"/dev/null","sourceNew":"  public SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    int effectiveMincount = (int)(fcontext.isShard() ? Math.min(1 , freq.mincount) : freq.mincount);\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(null, postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4a7c13535572b8e97cc477fc3388a57321a7751a","date":1427500960,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#_nextBucket().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#_nextBucket().mjava","sourceNew":"  public SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(null, postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":"  public SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    int effectiveMincount = (int)(fcontext.isShard() ? Math.min(1 , freq.mincount) : freq.mincount);\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(null, postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":0,"author":"Ryan Ernst","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#_nextBucket().mjava","pathOld":"/dev/null","sourceNew":"  public SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(null, postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"faf1236ae092482293a7e0659e347d172185ef6f","date":1430314113,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#_nextBucket().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#_nextBucket().mjava","sourceNew":"  public SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(null, postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        BytesRef termCopy = BytesRef.deepCopyOf(term);\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          TermQuery filter = new TermQuery(new Term(freq.field, termCopy));\n          processSubs(bucket, filter, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":"  public SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(null, postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#_nextBucket().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#_nextBucket().mjava","sourceNew":"  public SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        BytesRef termCopy = BytesRef.deepCopyOf(term);\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          TermQuery filter = new TermQuery(new Term(freq.field, termCopy));\n          processSubs(bucket, filter, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":"  public SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(null, postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        BytesRef termCopy = BytesRef.deepCopyOf(term);\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          TermQuery filter = new TermQuery(new Term(freq.field, termCopy));\n          processSubs(bucket, filter, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2c5ae3000a048dea45f71032386fb1adf814986a","date":1445375139,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#_nextBucket().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#_nextBucket().mjava","sourceNew":"  public SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":"  public SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        BytesRef termCopy = BytesRef.deepCopyOf(term);\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          TermQuery filter = new TermQuery(new Term(freq.field, termCopy));\n          processSubs(bucket, filter, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"79759974460bc59933cd169acc94f5c6b16368d5","date":1471318443,"type":5,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#_nextBucket().mjava","sourceNew":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":"  public SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","date":1471496851,"type":5,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#_nextBucket().mjava","sourceNew":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":"  public SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"403d05f7f8d69b65659157eff1bc1d2717f04c66","date":1471692961,"type":5,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#_nextBucket().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#_nextBucket().mjava","sourceNew":"  private SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n    // end of the iteration\n    return null;\n  }\n\n","sourceOld":"  public SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#_nextBucket().mjava","sourceNew":null,"sourceOld":"  public SimpleOrderedMap<Object> _nextBucket() throws IOException {\n    DocSet termSet = null;\n\n    try {\n      while (term != null) {\n\n        if (startTermBytes != null && !StringHelper.startsWith(term, startTermBytes)) {\n          break;\n        }\n\n        int df = termsEnum.docFreq();\n        if (df < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (termSet != null) {\n          // termSet.decref(); // OFF-HEAP\n          termSet = null;\n        }\n\n        int c = 0;\n\n        if (hasSubFacets || df >= minDfFilterCache) {\n          // use the filter cache\n\n          if (deState == null) {\n            deState = new SolrIndexSearcher.DocsEnumState();\n            deState.fieldName = sf.getName();\n            deState.liveDocs = fcontext.searcher.getLeafReader().getLiveDocs();\n            deState.termsEnum = termsEnum;\n            deState.postingsEnum = postingsEnum;\n            deState.minSetSizeCached = minDfFilterCache;\n          }\n\n            if (hasSubFacets || !countOnly) {\n              DocSet termsAll = fcontext.searcher.getDocSet(deState);\n              termSet = docs.intersection(termsAll);\n              // termsAll.decref(); // OFF-HEAP\n              c = termSet.size();\n            } else {\n              c = fcontext.searcher.numDocs(docs, deState);\n            }\n            postingsEnum = deState.postingsEnum;\n\n            resetStats();\n\n            if (!countOnly) {\n              collect(termSet, 0);\n            }\n\n        } else {\n          // We don't need the docset here (meaning no sub-facets).\n          // if countOnly, then we are calculating some other stats...\n          resetStats();\n\n          // lazy convert to fastForRandomSet\n          if (fastForRandomSet == null) {\n            fastForRandomSet = docs;\n            if (docs instanceof SortedIntDocSet) {  // OFF-HEAP todo: also check for native version\n              SortedIntDocSet sset = (SortedIntDocSet) docs;\n              fastForRandomSet = new HashDocSet(sset.getDocs(), 0, sset.size());\n            }\n          }\n          // iterate over TermDocs to calculate the intersection\n          postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n\n          if (postingsEnum instanceof MultiPostingsEnum) {\n            MultiPostingsEnum.EnumWithSlice[] subs = ((MultiPostingsEnum) postingsEnum).getSubs();\n            int numSubs = ((MultiPostingsEnum) postingsEnum).getNumSubs();\n            for (int subindex = 0; subindex < numSubs; subindex++) {\n              MultiPostingsEnum.EnumWithSlice sub = subs[subindex];\n              if (sub.postingsEnum == null) continue;\n              int base = sub.slice.start;\n              int docid;\n\n              if (countOnly) {\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) c++;\n                }\n              } else {\n                setNextReader(leaves[sub.slice.readerIndex]);\n                while ((docid = sub.postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                  if (fastForRandomSet.exists(docid + base)) {\n                    c++;\n                    collect(docid, 0);\n                  }\n                }\n              }\n\n            }\n          } else {\n            int docid;\n            if (countOnly) {\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) c++;\n              }\n            } else {\n              setNextReader(leaves[0]);\n              while ((docid = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (fastForRandomSet.exists(docid)) {\n                  c++;\n                  collect(docid, 0);\n                }\n              }\n            }\n          }\n\n        }\n\n\n\n        if (c < effectiveMincount) {\n          term = termsEnum.next();\n          continue;\n        }\n\n        // handle offset and limit\n        if (bucketsToSkip > 0) {\n          bucketsToSkip--;\n          term = termsEnum.next();\n          continue;\n        }\n\n        if (freq.limit >= 0 && ++bucketsReturned > freq.limit) {\n          return null;\n        }\n\n        // set count in case other stats depend on it\n        countAcc.incrementCount(0, c);\n\n        // OK, we have a good bucket to return... first get bucket value before moving to next term\n        Object bucketVal = sf.getType().toObject(sf, term);\n        TermQuery bucketQuery = hasSubFacets ? new TermQuery(new Term(freq.field, term)) : null;\n        term = termsEnum.next();\n\n        SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n        bucket.add(\"val\", bucketVal);\n        addStats(bucket, 0);\n        if (hasSubFacets) {\n          processSubs(bucket, bucketQuery, termSet);\n        }\n\n        // TODO... termSet needs to stick around for streaming sub-facets?\n\n        return bucket;\n\n      }\n\n    } finally {\n      if (termSet != null) {\n        // termSet.decref();  // OFF-HEAP\n        termSet = null;\n      }\n    }\n\n\n    // end of the iteration\n    return null;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","4a7c13535572b8e97cc477fc3388a57321a7751a"],"2c5ae3000a048dea45f71032386fb1adf814986a":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["faf1236ae092482293a7e0659e347d172185ef6f"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["2c5ae3000a048dea45f71032386fb1adf814986a","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"ac53eb8ed1d40ceac7330e9dc2e5c258e8fc155d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["2c5ae3000a048dea45f71032386fb1adf814986a","79759974460bc59933cd169acc94f5c6b16368d5"],"79759974460bc59933cd169acc94f5c6b16368d5":["2c5ae3000a048dea45f71032386fb1adf814986a"],"faf1236ae092482293a7e0659e347d172185ef6f":["4a7c13535572b8e97cc477fc3388a57321a7751a"],"4a7c13535572b8e97cc477fc3388a57321a7751a":["ac53eb8ed1d40ceac7330e9dc2e5c258e8fc155d"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["2c5ae3000a048dea45f71032386fb1adf814986a","403d05f7f8d69b65659157eff1bc1d2717f04c66"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["403d05f7f8d69b65659157eff1bc1d2717f04c66"]},"commit2Childs":{"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"2c5ae3000a048dea45f71032386fb1adf814986a":["403d05f7f8d69b65659157eff1bc1d2717f04c66","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","79759974460bc59933cd169acc94f5c6b16368d5","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["2c5ae3000a048dea45f71032386fb1adf814986a"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","ac53eb8ed1d40ceac7330e9dc2e5c258e8fc155d"],"ac53eb8ed1d40ceac7330e9dc2e5c258e8fc155d":["4a7c13535572b8e97cc477fc3388a57321a7751a"],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"79759974460bc59933cd169acc94f5c6b16368d5":["2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"4a7c13535572b8e97cc477fc3388a57321a7751a":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","faf1236ae092482293a7e0659e347d172185ef6f"],"faf1236ae092482293a7e0659e347d172185ef6f":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}