{"path":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","commits":[{"id":"6e36353d7461af8d2329a78a71457cf8e3c1e88f","date":1411572107,"type":1,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3WithLeaderFailover().mjava","sourceNew":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n\n    assertDocExists(getHttpSolrServer(leader, testCollectionName), testCollectionName, \"5\");\n    assertDocExists(getHttpSolrServer(notLeaders.get(1), testCollectionName), testCollectionName, \"5\");\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(60, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      cloudClient.getZkStateReader().updateClusterState(true);\n\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    sendDoc(6);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 20);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(),\n        notLeaders.size() == 2);\n        \n    sendDoc(1);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n\n    assertDocExists(getHttpSolrServer(leader, testCollectionName), testCollectionName, \"5\");\n    assertDocExists(getHttpSolrServer(notLeaders.get(1), testCollectionName), testCollectionName, \"5\");\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    // kill the leader\n    leaderJetty.stop();\n    \n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(), \n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(60, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      cloudClient.getZkStateReader().updateClusterState(true);\n\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() == 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+printClusterStateInfo(),\n        participatingReplicas.size() == 2);\n\n    sendDoc(6);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 20);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ec3dbdc850ca18bf4aef9acb85f2ea0554306d","date":1419896224,"type":3,"author":"Anshum Gupta","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","sourceNew":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n\n    assertDocExists(getHttpSolrServer(leader, testCollectionName), testCollectionName, \"5\");\n    assertDocExists(getHttpSolrServer(notLeaders.get(1), testCollectionName), testCollectionName, \"5\");\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(60, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      cloudClient.getZkStateReader().updateClusterState(true);\n\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    sendDoc(6);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 20);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n\n    assertDocExists(getHttpSolrServer(leader, testCollectionName), testCollectionName, \"5\");\n    assertDocExists(getHttpSolrServer(notLeaders.get(1), testCollectionName), testCollectionName, \"5\");\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(60, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      cloudClient.getZkStateReader().updateClusterState(true);\n\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    sendDoc(6);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 20);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bafca15d8e408346a67f4282ad1143b88023893b","date":1420034748,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","sourceNew":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n\n    assertDocExists(getHttpSolrClient(leader, testCollectionName), testCollectionName, \"5\");\n    assertDocExists(getHttpSolrClient(notLeaders.get(1), testCollectionName), testCollectionName, \"5\");\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(60, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      cloudClient.getZkStateReader().updateClusterState(true);\n\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    sendDoc(6);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 20);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n\n    assertDocExists(getHttpSolrServer(leader, testCollectionName), testCollectionName, \"5\");\n    assertDocExists(getHttpSolrServer(notLeaders.get(1), testCollectionName), testCollectionName, \"5\");\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(60, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      cloudClient.getZkStateReader().updateClusterState(true);\n\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    sendDoc(6);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 20);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":["6784d0cc613dc1ee97030eaaa5e0754edc22d164"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6784d0cc613dc1ee97030eaaa5e0754edc22d164","date":1420824784,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","sourceNew":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    HttpSolrClient server = getHttpSolrClient(leader, testCollectionName);\n    try {\n      assertDocExists(server, testCollectionName, \"5\");\n    } finally {\n      server.shutdown();\n    }\n    try {\n      server = getHttpSolrClient(notLeaders.get(1), testCollectionName);\n      assertDocExists(server, testCollectionName, \"5\");\n    } finally {\n      server.shutdown();\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(60, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      cloudClient.getZkStateReader().updateClusterState(true);\n\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    \n    sendDoc(6);\n\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n\n    assertDocExists(getHttpSolrClient(leader, testCollectionName), testCollectionName, \"5\");\n    assertDocExists(getHttpSolrClient(notLeaders.get(1), testCollectionName), testCollectionName, \"5\");\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(60, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      cloudClient.getZkStateReader().updateClusterState(true);\n\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    sendDoc(6);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 20);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":["bafca15d8e408346a67f4282ad1143b88023893b","0cd90adb2be0ad7d69e4f6e26f0fab7675176721"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cc3b13b430571c2e169f98fe38e1e7666f88522d","date":1422446157,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","sourceNew":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(60, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      cloudClient.getZkStateReader().updateClusterState(true);\n\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    \n    sendDoc(6);\n\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    HttpSolrClient server = getHttpSolrClient(leader, testCollectionName);\n    try {\n      assertDocExists(server, testCollectionName, \"5\");\n    } finally {\n      server.shutdown();\n    }\n    try {\n      server = getHttpSolrClient(notLeaders.get(1), testCollectionName);\n      assertDocExists(server, testCollectionName, \"5\");\n    } finally {\n      server.shutdown();\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(60, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      cloudClient.getZkStateReader().updateClusterState(true);\n\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    \n    sendDoc(6);\n\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"102da6baafc0f534a59f31729343dbab9d3b9e9a","date":1438410244,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","sourceNew":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(60, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      cloudClient.getZkStateReader().updateClusterState();\n\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    \n    sendDoc(6);\n\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(60, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      cloudClient.getZkStateReader().updateClusterState(true);\n\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    \n    sendDoc(6);\n\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f","date":1457343183,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","sourceNew":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(60, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    \n    sendDoc(6);\n\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(60, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      cloudClient.getZkStateReader().updateClusterState();\n\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    \n    sendDoc(6);\n\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"859081acf00749f5dd462772c571d611d4a4d2db","date":1459527719,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","sourceNew":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(60, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(60, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    \n    sendDoc(6);\n\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9abd8870470e1183a7a910439bde72a7bf00238f","date":1483009165,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","sourceNew":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(90, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(60, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f03e4bed5023ec3ef93a771b8888cae991cf448d","date":1483469262,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","sourceNew":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(90, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(60, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ac97ea104d893f16aab430d9904473bc1f233f3c","date":1496249396,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","sourceNew":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(90, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(90, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3217321f3e1d7922898c6c633d17acfa840d6875","date":1496257480,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","sourceNew":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(90, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(90, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"42dc7f2d60851668d9efa2d12baa1d4ebe54b12f","date":1496281877,"type":3,"author":"Cao Manh Dat","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","sourceNew":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(90, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(90, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e9017cf144952056066919f1ebc7897ff9bd71b1","date":1496757600,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","sourceNew":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(90, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(90, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"197bbedf08450ade98a11f4a0001448059666bec","date":1498534625,"type":3,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","sourceNew":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(90, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(90, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4","date":1498540685,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","sourceNew":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(90, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(90, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","sourceNew":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(90, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(90, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a966532d92cf9ba2856f15a8140151bb6b518e4b","date":1588290631,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","sourceNew":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node {}\",leaderNode);\n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(90, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node \"+leaderNode);      \n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(90, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e","date":1594223844,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/LeaderFailoverAfterPartitionTest#testRf3WithLeaderFailover().mjava","sourceNew":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, \"conf1\", 1, 3);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node {}\",leaderNode);\n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(90, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf3WithLeaderFailover() throws Exception {\n    // now let's create a partition in one of the replicas and outright\n    // kill the leader ... see what happens\n    // create a collection that has 1 shard but 3 replicas\n    String testCollectionName = \"c8n_1x3_lf\"; // _lf is leader fails\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n        \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = null;\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n        \n    Replica leader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\");\n    String leaderNode = leader.getNodeName();\n    assertNotNull(\"Could not find leader for shard1 of \"+\n      testCollectionName+\"; clusterState: \"+printClusterStateInfo(testCollectionName), leader);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    \n    // since maxShardsPerNode is 1, we're safe to kill the leader\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);    \n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n        \n    // indexing during a partition\n    // doc should be on leader and 1 replica\n    sendDoc(5);\n    \n    try (HttpSolrClient server = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n\n    try (HttpSolrClient server = getHttpSolrClient(notLeaders.get(1), testCollectionName)) {\n      assertDocExists(server, testCollectionName, \"5\");\n    }\n  \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    String shouldNotBeNewLeaderNode = notLeaders.get(0).getNodeName();\n\n    //chaosMonkey.expireSession(leaderJetty);\n    // kill the leader\n    leaderJetty.stop();\n    if (leaderJetty.isRunning())\n      fail(\"Failed to stop the leader on \"+leaderNode);\n        \n    SocketProxy oldLeaderProxy = getProxyForReplica(leader);\n    if (oldLeaderProxy != null) {\n      oldLeaderProxy.close();      \n    } else {\n      log.warn(\"No SocketProxy found for old leader node {}\",leaderNode);\n    }\n\n    Thread.sleep(10000); // give chance for new leader to be elected.\n    \n    Replica newLeader = \n        cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 60000);\n        \n    assertNotNull(\"No new leader was elected after 60 seconds; clusterState: \"+\n      printClusterStateInfo(testCollectionName),newLeader);\n        \n    assertTrue(\"Expected node \"+shouldNotBeNewLeaderNode+\n        \" to NOT be the new leader b/c it was out-of-sync with the old leader! ClusterState: \"+\n        printClusterStateInfo(testCollectionName),\n        !shouldNotBeNewLeaderNode.equals(newLeader.getNodeName()));\n    \n    proxy0.reopen();\n    \n    long timeout = System.nanoTime() + TimeUnit.NANOSECONDS.convert(90, TimeUnit.SECONDS);\n    while (System.nanoTime() < timeout) {\n      List<Replica> activeReps = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n      if (activeReps.size() >= 2) break;\n      Thread.sleep(1000);\n    }\n\n    List<Replica> participatingReplicas = getActiveOrRecoveringReplicas(testCollectionName, \"shard1\");\n    assertTrue(\"Expected 2 of 3 replicas to be active but only found \"+\n            participatingReplicas.size()+\"; \"+participatingReplicas+\"; clusterState: \"+\n            printClusterStateInfo(testCollectionName),\n        participatingReplicas.size() >= 2);\n\n    SolrInputDocument doc = new SolrInputDocument();\n    doc.addField(id, String.valueOf(6));\n    doc.addField(\"a_t\", \"hello\" + 6);\n    sendDocsWithRetry(Collections.singletonList(doc), 1, 3, 1);\n\n    Set<String> replicasToCheck = new HashSet<>();\n    for (Replica stillUp : participatingReplicas)\n      replicasToCheck.add(stillUp.getName());\n    waitToSeeReplicasActive(testCollectionName, \"shard1\", replicasToCheck, 90);\n    assertDocsExistInAllReplicas(participatingReplicas, testCollectionName, 1, 6);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"197bbedf08450ade98a11f4a0001448059666bec":["42dc7f2d60851668d9efa2d12baa1d4ebe54b12f"],"102da6baafc0f534a59f31729343dbab9d3b9e9a":["cc3b13b430571c2e169f98fe38e1e7666f88522d"],"ac97ea104d893f16aab430d9904473bc1f233f3c":["9abd8870470e1183a7a910439bde72a7bf00238f"],"bafca15d8e408346a67f4282ad1143b88023893b":["d0ec3dbdc850ca18bf4aef9acb85f2ea0554306d"],"3217321f3e1d7922898c6c633d17acfa840d6875":["9abd8870470e1183a7a910439bde72a7bf00238f","ac97ea104d893f16aab430d9904473bc1f233f3c"],"d0ec3dbdc850ca18bf4aef9acb85f2ea0554306d":["6e36353d7461af8d2329a78a71457cf8e3c1e88f"],"7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f":["102da6baafc0f534a59f31729343dbab9d3b9e9a"],"a966532d92cf9ba2856f15a8140151bb6b518e4b":["28288370235ed02234a64753cdbf0c6ec096304a"],"9abd8870470e1183a7a910439bde72a7bf00238f":["859081acf00749f5dd462772c571d611d4a4d2db"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["a966532d92cf9ba2856f15a8140151bb6b518e4b"],"28288370235ed02234a64753cdbf0c6ec096304a":["3217321f3e1d7922898c6c633d17acfa840d6875","197bbedf08450ade98a11f4a0001448059666bec"],"42dc7f2d60851668d9efa2d12baa1d4ebe54b12f":["9abd8870470e1183a7a910439bde72a7bf00238f","3217321f3e1d7922898c6c633d17acfa840d6875"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":["859081acf00749f5dd462772c571d611d4a4d2db","9abd8870470e1183a7a910439bde72a7bf00238f"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["9abd8870470e1183a7a910439bde72a7bf00238f","42dc7f2d60851668d9efa2d12baa1d4ebe54b12f"],"859081acf00749f5dd462772c571d611d4a4d2db":["7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4":["e9017cf144952056066919f1ebc7897ff9bd71b1","197bbedf08450ade98a11f4a0001448059666bec"],"cc3b13b430571c2e169f98fe38e1e7666f88522d":["6784d0cc613dc1ee97030eaaa5e0754edc22d164"],"6784d0cc613dc1ee97030eaaa5e0754edc22d164":["bafca15d8e408346a67f4282ad1143b88023893b"],"6e36353d7461af8d2329a78a71457cf8e3c1e88f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"]},"commit2Childs":{"197bbedf08450ade98a11f4a0001448059666bec":["28288370235ed02234a64753cdbf0c6ec096304a","fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4"],"102da6baafc0f534a59f31729343dbab9d3b9e9a":["7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f"],"ac97ea104d893f16aab430d9904473bc1f233f3c":["3217321f3e1d7922898c6c633d17acfa840d6875"],"bafca15d8e408346a67f4282ad1143b88023893b":["6784d0cc613dc1ee97030eaaa5e0754edc22d164"],"3217321f3e1d7922898c6c633d17acfa840d6875":["28288370235ed02234a64753cdbf0c6ec096304a","42dc7f2d60851668d9efa2d12baa1d4ebe54b12f"],"d0ec3dbdc850ca18bf4aef9acb85f2ea0554306d":["bafca15d8e408346a67f4282ad1143b88023893b"],"7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f":["859081acf00749f5dd462772c571d611d4a4d2db"],"9abd8870470e1183a7a910439bde72a7bf00238f":["ac97ea104d893f16aab430d9904473bc1f233f3c","3217321f3e1d7922898c6c633d17acfa840d6875","42dc7f2d60851668d9efa2d12baa1d4ebe54b12f","f03e4bed5023ec3ef93a771b8888cae991cf448d","e9017cf144952056066919f1ebc7897ff9bd71b1"],"a966532d92cf9ba2856f15a8140151bb6b518e4b":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"],"42dc7f2d60851668d9efa2d12baa1d4ebe54b12f":["197bbedf08450ade98a11f4a0001448059666bec","e9017cf144952056066919f1ebc7897ff9bd71b1"],"28288370235ed02234a64753cdbf0c6ec096304a":["a966532d92cf9ba2856f15a8140151bb6b518e4b"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":[],"e9017cf144952056066919f1ebc7897ff9bd71b1":["fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4"],"859081acf00749f5dd462772c571d611d4a4d2db":["9abd8870470e1183a7a910439bde72a7bf00238f","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["6e36353d7461af8d2329a78a71457cf8e3c1e88f"],"fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4":[],"cc3b13b430571c2e169f98fe38e1e7666f88522d":["102da6baafc0f534a59f31729343dbab9d3b9e9a"],"6784d0cc613dc1ee97030eaaa5e0754edc22d164":["cc3b13b430571c2e169f98fe38e1e7666f88522d"],"6e36353d7461af8d2329a78a71457cf8e3c1e88f":["d0ec3dbdc850ca18bf4aef9acb85f2ea0554306d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["f03e4bed5023ec3ef93a771b8888cae991cf448d","fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}