{"path":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","commits":[{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":1,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final Term protoTerm = new Term(fieldName);\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(protoTerm.createTerm(text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.deletedDocs == null) {\n            state.deletedDocs = new BitVector(state.numDocs);\n          }\n          state.deletedDocs.set(docID);\n        }\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n        postingsConsumer.startDoc(docID, termDocFreq);\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":1,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriter#appendPostings(String,SegmentWriteState,FreqProxTermsWriterPerField[],FieldsConsumer).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final Term protoTerm = new Term(fieldName);\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(protoTerm.createTerm(text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.deletedDocs == null) {\n            state.deletedDocs = new BitVector(state.numDocs);\n          }\n          state.deletedDocs.set(docID);\n        }\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void appendPostings(String fieldName, SegmentWriteState state,\n                      FreqProxTermsWriterPerField[] fields,\n                      FieldsConsumer consumer)\n    throws CorruptIndexException, IOException {\n\n    int numFields = fields.length;\n\n    final BytesRef text = new BytesRef();\n\n    final FreqProxFieldMergeState[] mergeStates = new FreqProxFieldMergeState[numFields];\n\n    final TermsConsumer termsConsumer = consumer.addField(fields[0].fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    for(int i=0;i<numFields;i++) {\n      FreqProxFieldMergeState fms = mergeStates[i] = new FreqProxFieldMergeState(fields[i], termComp);\n\n      assert fms.field.fieldInfo == fields[0].fieldInfo;\n\n      // Should always be true\n      boolean result = fms.nextTerm();\n      assert result;\n    }\n\n    final Term protoTerm = new Term(fieldName);\n\n    FreqProxFieldMergeState[] termStates = new FreqProxFieldMergeState[numFields];\n\n    final boolean currentFieldOmitTermFreqAndPositions = fields[0].fieldInfo.omitTermFreqAndPositions;\n    //System.out.println(\"flush terms field=\" + fields[0].fieldInfo.name);\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    // TODO: really TermsHashPerField should take over most\n    // of this loop, including merge sort of terms from\n    // multiple threads and interacting with the\n    // TermsConsumer, only calling out to us (passing us the\n    // DocsConsumer) to handle delivery of docs/positions\n    long sumTotalTermFreq = 0;\n    while(numFields > 0) {\n\n      // Get the next term to merge\n      termStates[0] = mergeStates[0];\n      int numToMerge = 1;\n\n      // TODO: pqueue\n      for(int i=1;i<numFields;i++) {\n        final int cmp = termComp.compare(mergeStates[i].text, termStates[0].text);\n        if (cmp < 0) {\n          termStates[0] = mergeStates[i];\n          numToMerge = 1;\n        } else if (cmp == 0) {\n          termStates[numToMerge++] = mergeStates[i];\n        }\n      }\n\n      // Need shallow copy here because termStates[0].text\n      // changes by the time we call finishTerm\n      text.bytes = termStates[0].text.bytes;\n      text.offset = termStates[0].text.offset;\n      text.length = termStates[0].text.length;  \n\n      //System.out.println(\"  term=\" + text.toUnicodeString());\n      //System.out.println(\"  term=\" + text.toString());\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(protoTerm.createTerm(text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      while(numToMerge > 0) {\n        \n        FreqProxFieldMergeState minState = termStates[0];\n        for(int i=1;i<numToMerge;i++) {\n          if (termStates[i].docID < minState.docID) {\n            minState = termStates[i];\n          }\n        }\n\n        final int termDocFreq = minState.termFreq;\n        numDocs++;\n\n        assert minState.docID < flushedDocCount: \"doc=\" + minState.docID + \" maxDoc=\" + flushedDocCount;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n\n        postingsConsumer.startDoc(minState.docID, termDocFreq);\n        if (minState.docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.deletedDocs == null) {\n            state.deletedDocs = new BitVector(state.numDocs);\n          }\n          state.deletedDocs.set(minState.docID);\n        }\n\n        final ByteSliceReader prox = minState.prox;\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload          \n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n            //System.out.println(\"    pos=\" + position);\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();  \n              \n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          } //End for\n\n          postingsConsumer.finishDoc();\n        }\n\n        if (!minState.nextDoc()) {\n\n          // Remove from termStates\n          int upto = 0;\n          // TODO: inefficient O(N) where N = number of\n          // threads that had seen this term:\n          for(int i=0;i<numToMerge;i++) {\n            if (termStates[i] != minState) {\n              termStates[upto++] = termStates[i];\n            }\n          }\n          numToMerge--;\n          assert upto == numToMerge;\n\n          // Advance this state to the next term\n\n          if (!minState.nextTerm()) {\n            // OK, no more terms, so remove from mergeStates\n            // as well\n            upto = 0;\n            for(int i=0;i<numFields;i++)\n              if (mergeStates[i] != minState)\n                mergeStates[upto++] = mergeStates[i];\n            numFields--;\n            assert upto == numFields;\n          }\n        }\n      }\n\n      assert numDocs > 0;\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"/dev/null","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final Term protoTerm = new Term(fieldName);\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(protoTerm.createTerm(text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.deletedDocs == null) {\n            state.deletedDocs = new BitVector(state.numDocs);\n          }\n          state.deletedDocs.set(docID);\n        }\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"/dev/null","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final Term protoTerm = new Term(fieldName);\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(protoTerm.createTerm(text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.deletedDocs == null) {\n            state.deletedDocs = new BitVector(state.numDocs);\n          }\n          state.deletedDocs.set(docID);\n        }\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153","date":1308670974,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.deletedDocs == null) {\n            state.deletedDocs = new BitVector(state.numDocs);\n          }\n          state.deletedDocs.set(docID);\n        }\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final Term protoTerm = new Term(fieldName);\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(protoTerm.createTerm(text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.deletedDocs == null) {\n            state.deletedDocs = new BitVector(state.numDocs);\n          }\n          state.deletedDocs.set(docID);\n        }\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.deletedDocs == null) {\n            state.deletedDocs = new BitVector(state.numDocs);\n          }\n          state.deletedDocs.set(docID);\n        }\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final Term protoTerm = new Term(fieldName);\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(protoTerm.createTerm(text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.deletedDocs == null) {\n            state.deletedDocs = new BitVector(state.numDocs);\n          }\n          state.deletedDocs.set(docID);\n        }\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","date":1309960478,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.liveDocs == null) {\n            state.liveDocs = new BitVector(state.numDocs);\n            state.liveDocs.invertAll();\n          }\n          state.liveDocs.clear(docID);\n        }\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.deletedDocs == null) {\n            state.deletedDocs = new BitVector(state.numDocs);\n          }\n          state.deletedDocs.set(docID);\n        }\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.liveDocs == null) {\n            state.liveDocs = new BitVector(state.numDocs);\n            state.liveDocs.invertAll();\n          }\n          state.liveDocs.clear(docID);\n        }\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.deletedDocs == null) {\n            state.deletedDocs = new BitVector(state.numDocs);\n          }\n          state.deletedDocs.set(docID);\n        }\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.liveDocs == null) {\n            state.liveDocs = new BitVector(state.numDocs);\n            state.liveDocs.invertAll();\n          }\n          state.liveDocs.clear(docID);\n        }\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final Term protoTerm = new Term(fieldName);\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(protoTerm.createTerm(text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.deletedDocs == null) {\n            state.deletedDocs = new BitVector(state.numDocs);\n          }\n          state.deletedDocs.set(docID);\n        }\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f4f2f9fd0a641ccc0cc6a4fb4e53d7ec1ab14a94","date":1310159023,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.liveDocs == null) {\n            state.liveDocs = new BitVector(state.numDocs);\n            state.liveDocs.invertAll();\n          }\n          state.liveDocs.clear(docID);\n        }\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq);\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.liveDocs == null) {\n            state.liveDocs = new BitVector(state.numDocs);\n            state.liveDocs.invertAll();\n          }\n          state.liveDocs.clear(docID);\n        }\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"63639dd66fd5bd9b90bc24dd596ae01575f27cc4","date":1310237454,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.liveDocs == null) {\n            state.liveDocs = new BitVector(state.numDocs);\n            state.liveDocs.invertAll();\n          }\n          state.liveDocs.clear(docID);\n        }\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq);\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.liveDocs == null) {\n            state.liveDocs = new BitVector(state.numDocs);\n            state.liveDocs.invertAll();\n          }\n          state.liveDocs.clear(docID);\n        }\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2afd23a6f1242190c3409d8d81d5c5912d607fc9","date":1310477482,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (indexOptions != IndexOptions.DOCS_ONLY) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (indexOptions == IndexOptions.DOCS_ONLY) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.liveDocs == null) {\n            state.liveDocs = new BitVector(state.numDocs);\n            state.liveDocs.invertAll();\n          }\n          state.liveDocs.clear(docID);\n        }\n\n        if (currentFieldIndexOptions != IndexOptions.DOCS_ONLY) {\n          totTF += termDocFreq;\n        }\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (currentFieldIndexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n          // we do write positions & payload\n          int position = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq);\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final boolean currentFieldOmitTermFreqAndPositions = fieldInfo.omitTermFreqAndPositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (!fieldInfo.omitTermFreqAndPositions) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (!omitTermFreqAndPositions) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (omitTermFreqAndPositions) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.liveDocs == null) {\n            state.liveDocs = new BitVector(state.numDocs);\n            state.liveDocs.invertAll();\n          }\n          state.liveDocs.clear(docID);\n        }\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        if (!currentFieldOmitTermFreqAndPositions) {\n          // omitTermFreqAndPositions == false so we do write positions &\n          // payload\n          int position = 0;\n          totTF += termDocFreq;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"25433c5cacacb7a2055d62d4d36b0daf210e0a10","date":1315747522,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (indexOptions != IndexOptions.DOCS_ONLY) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (indexOptions == IndexOptions.DOCS_ONLY) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.liveDocs == null) {\n            state.liveDocs = new BitVector(state.numDocs);\n            state.liveDocs.invertAll();\n          }\n          state.liveDocs.clear(docID);\n        }\n\n        if (currentFieldIndexOptions != IndexOptions.DOCS_ONLY) {\n          totTF += termDocFreq;\n        }\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (currentFieldIndexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n          // we do write positions & payload\n          int position = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (indexOptions != IndexOptions.DOCS_ONLY) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (indexOptions == IndexOptions.DOCS_ONLY) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.liveDocs == null) {\n            state.liveDocs = new BitVector(state.numDocs);\n            state.liveDocs.invertAll();\n          }\n          state.liveDocs.clear(docID);\n        }\n\n        if (currentFieldIndexOptions != IndexOptions.DOCS_ONLY) {\n          totTF += termDocFreq;\n        }\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (currentFieldIndexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n          // we do write positions & payload\n          int position = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq);\n  }\n\n","bugFix":null,"bugIntro":["29e23e367cc757f42cdfce2bcbf21e68cd209cda","29e23e367cc757f42cdfce2bcbf21e68cd209cda"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"31f025ae60076ae95274433f3fe8e6ace2857a87","date":1326669465,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.liveDocs == null) {\n            state.liveDocs = new BitVector(state.numDocs);\n            state.liveDocs.invertAll();\n          }\n          state.liveDocs.clear(docID);\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n      int termFreq = 0;\n\n      while(true) {\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (indexOptions != IndexOptions.DOCS_ONLY) {\n              termFreq = postings.docFreqs[termID];\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (indexOptions == IndexOptions.DOCS_ONLY) {\n            docID += code;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n        final int termDocFreq = termFreq;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.liveDocs == null) {\n            state.liveDocs = new BitVector(state.numDocs);\n            state.liveDocs.invertAll();\n          }\n          state.liveDocs.clear(docID);\n        }\n\n        if (currentFieldIndexOptions != IndexOptions.DOCS_ONLY) {\n          totTF += termDocFreq;\n        }\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (currentFieldIndexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {\n          // we do write positions & payload\n          int position = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final int code = prox.readVInt();\n            position += code >> 1;\n\n            final int payloadLength;\n            final BytesRef thisPayload;\n\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n\n              if (payload == null) {\n                payload = new BytesRef();\n                payload.bytes = new byte[payloadLength];\n              } else if (payload.bytes.length < payloadLength) {\n                payload.grow(payloadLength);\n              }\n\n              prox.readBytes(payload.bytes, 0, payloadLength);\n              payload.length = payloadLength;\n              thisPayload = payload;\n\n            } else {\n              payloadLength = 0;\n              thisPayload = null;\n            }\n\n            postingsConsumer.addPosition(position, thisPayload);\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":["877cc2c5a8035f48e78b21f0814a79e6607df85d","877cc2c5a8035f48e78b21f0814a79e6607df85d","83ede60c0b5bb96ad193414bbd663193b56689b3","83ede60c0b5bb96ad193414bbd663193b56689b3","d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626","d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2c6f93d9250f0d2c776d11fa7fa6ac4ce921052e","date":1326983914,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.liveDocs == null) {\n            state.liveDocs = new BitVector(state.numDocs);\n            state.liveDocs.invertAll();\n          }\n          state.liveDocs.clear(docID);\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.liveDocs == null) {\n            state.liveDocs = new BitVector(state.numDocs);\n            state.liveDocs.invertAll();\n          }\n          state.liveDocs.clear(docID);\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n\n          postingsConsumer.finishDoc();\n        }\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e5fd20ed668e3a00e9b27810fa855182276b3b58","date":1326994874,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // nocommit: totally wrong to do this reach-around here, and this way\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          state.liveDocs.clear(docID);\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.liveDocs == null) {\n            state.liveDocs = new BitVector(state.numDocs);\n            state.liveDocs.invertAll();\n          }\n          state.liveDocs.clear(docID);\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"da5ca503d2914753714af960bdfea65c0fe0fc59","date":1327688040,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // nocommit: totally wrong to do this reach-around here, and this way\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // nocommit: totally wrong to do this reach-around here, and this way\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          state.liveDocs.clear(docID);\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"302bdbd454dd339f66b0e37355127ed9f7d47048","date":1327761356,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // nocommit: totally wrong to do this reach-around here, and this way\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31","date":1327836826,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.liveDocs == null) {\n            state.liveDocs = new BitVector(state.numDocs);\n            state.liveDocs.invertAll();\n          }\n          state.liveDocs.clear(docID);\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fd92b8bcc88e969302510acf77bd6970da3994c4","date":1327839530,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          if (state.liveDocs == null) {\n            state.liveDocs = new BitVector(state.numDocs);\n            state.liveDocs.invertAll();\n          }\n          state.liveDocs.clear(docID);\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"2afd23a6f1242190c3409d8d81d5c5912d607fc9":["f4f2f9fd0a641ccc0cc6a4fb4e53d7ec1ab14a94"],"2c6f93d9250f0d2c776d11fa7fa6ac4ce921052e":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"25433c5cacacb7a2055d62d4d36b0daf210e0a10":["2afd23a6f1242190c3409d8d81d5c5912d607fc9"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31"],"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["fafef7c83fe8e0b3ca9298d5d75d6b943dc28153"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"fd92b8bcc88e969302510acf77bd6970da3994c4":["2c6f93d9250f0d2c776d11fa7fa6ac4ce921052e","c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"f4f2f9fd0a641ccc0cc6a4fb4e53d7ec1ab14a94":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"2553b00f699380c64959ccb27991289aae87be2e":["a3776dccca01c11e7046323cfad46a3b4a471233","fafef7c83fe8e0b3ca9298d5d75d6b943dc28153"],"63639dd66fd5bd9b90bc24dd596ae01575f27cc4":["817d8435e9135b756f08ce6710ab0baac51bdf88","f4f2f9fd0a641ccc0cc6a4fb4e53d7ec1ab14a94"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["b3e06be49006ecac364d39d12b9c9f74882f9b9f","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["2553b00f699380c64959ccb27991289aae87be2e","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"da5ca503d2914753714af960bdfea65c0fe0fc59":["e5fd20ed668e3a00e9b27810fa855182276b3b58"],"302bdbd454dd339f66b0e37355127ed9f7d47048":["da5ca503d2914753714af960bdfea65c0fe0fc59"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31":["2c6f93d9250f0d2c776d11fa7fa6ac4ce921052e","302bdbd454dd339f66b0e37355127ed9f7d47048"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["25433c5cacacb7a2055d62d4d36b0daf210e0a10"],"e5fd20ed668e3a00e9b27810fa855182276b3b58":["2c6f93d9250f0d2c776d11fa7fa6ac4ce921052e"]},"commit2Childs":{"2afd23a6f1242190c3409d8d81d5c5912d607fc9":["25433c5cacacb7a2055d62d4d36b0daf210e0a10"],"2c6f93d9250f0d2c776d11fa7fa6ac4ce921052e":["fd92b8bcc88e969302510acf77bd6970da3994c4","c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31","e5fd20ed668e3a00e9b27810fa855182276b3b58"],"25433c5cacacb7a2055d62d4d36b0daf210e0a10":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","2553b00f699380c64959ccb27991289aae87be2e"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["f4f2f9fd0a641ccc0cc6a4fb4e53d7ec1ab14a94","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"fd92b8bcc88e969302510acf77bd6970da3994c4":[],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["fafef7c83fe8e0b3ca9298d5d75d6b943dc28153","135621f3a0670a9394eb563224a3b76cc4dddc0f","d083e83f225b11e5fdd900e83d26ddb385b6955c","a3776dccca01c11e7046323cfad46a3b4a471233"],"f4f2f9fd0a641ccc0cc6a4fb4e53d7ec1ab14a94":["2afd23a6f1242190c3409d8d81d5c5912d607fc9","63639dd66fd5bd9b90bc24dd596ae01575f27cc4"],"2553b00f699380c64959ccb27991289aae87be2e":["817d8435e9135b756f08ce6710ab0baac51bdf88"],"63639dd66fd5bd9b90bc24dd596ae01575f27cc4":[],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"817d8435e9135b756f08ce6710ab0baac51bdf88":["63639dd66fd5bd9b90bc24dd596ae01575f27cc4"],"da5ca503d2914753714af960bdfea65c0fe0fc59":["302bdbd454dd339f66b0e37355127ed9f7d47048"],"302bdbd454dd339f66b0e37355127ed9f7d47048":["c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31"],"a3776dccca01c11e7046323cfad46a3b4a471233":["2553b00f699380c64959ccb27991289aae87be2e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["135621f3a0670a9394eb563224a3b76cc4dddc0f","b3e06be49006ecac364d39d12b9c9f74882f9b9f","a3776dccca01c11e7046323cfad46a3b4a471233","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","fd92b8bcc88e969302510acf77bd6970da3994c4"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["2c6f93d9250f0d2c776d11fa7fa6ac4ce921052e"],"e5fd20ed668e3a00e9b27810fa855182276b3b58":["da5ca503d2914753714af960bdfea65c0fe0fc59"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["135621f3a0670a9394eb563224a3b76cc4dddc0f","fd92b8bcc88e969302510acf77bd6970da3994c4","63639dd66fd5bd9b90bc24dd596ae01575f27cc4","d083e83f225b11e5fdd900e83d26ddb385b6955c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}