{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","sourceNew":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 },\n        null, null, null, null, false);\n\n    wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 },\n        null, null, null, null, false);\n  }\n\n","sourceOld":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 },\n        null, null, null, null, false);\n\n    wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 },\n        null, null, null, null, false);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c85fa43e6918808743daa7847ba0264373af687f","date":1395166336,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","sourceNew":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(TEST_VERSION_CURRENT, new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"foobar\", \"bar\" },\n        new int[] { 5, 5, 9 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(TEST_VERSION_CURRENT, new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","sourceOld":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 },\n        null, null, null, null, false);\n\n    wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 },\n        null, null, null, null, false);\n  }\n\n","bugFix":["ec1acb945fb5751735f5c9482576c8760d97b6ab","2fd023a662cc25ae7e0ad0f33d71c476a16d0579","888c2d6bca1edd8d9293631d6e1d188b036e0f05"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"379db3ad24c4f0214f30a122265a6d6be003a99d","date":1407537768,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","sourceNew":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"foobar\", \"bar\" },\n        new int[] { 5, 5, 9 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","sourceOld":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(TEST_VERSION_CURRENT, new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"foobar\", \"bar\" },\n        new int[] { 5, 5, 9 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(TEST_VERSION_CURRENT, new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b1eb427f2c6beed80d1724555fc1db003ccf3030","date":1417137397,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","sourceNew":"  @Test\n  public void testOffsets() throws IOException {\n    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"foobar\", \"bar\" },\n        new int[] { 5, 5, 9 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","sourceOld":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"foobar\", \"bar\" },\n        new int[] { 5, 5, 9 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6fc92bf7f56dcfe9d5bd75dfdc902b9597ea95b3","date":1417215914,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","sourceNew":"  @Test\n  public void testOffsets() throws IOException {\n    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(new CannedTokenStream(new Token(\"foo-bar\", 5, 12)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"foobar\", \"bar\" },\n        new int[] { 5, 5, 9 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(new CannedTokenStream(new Token(\"foo-bar\", 5, 6)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","sourceOld":"  @Test\n  public void testOffsets() throws IOException {\n    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"foobar\", \"bar\" },\n        new int[] { 5, 5, 9 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eaa82f7ae8119e5850fcdeb0a7f2362a7d732bac","date":1524923226,"type":3,"author":"Michael Sokolov","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","sourceNew":"  public void testOffsets() throws IOException {\n    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(new CannedTokenStream(new Token(\"foo-bar\", 5, 12)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"foobar\", \"bar\" },\n        new int[] { 5, 5, 9 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(new CannedTokenStream(new Token(\"foo-bar\", 5, 6)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","sourceOld":"  @Test\n  public void testOffsets() throws IOException {\n    int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(new CannedTokenStream(new Token(\"foo-bar\", 5, 12)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"foobar\", \"bar\" },\n        new int[] { 5, 5, 9 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(new CannedTokenStream(new Token(\"foo-bar\", 5, 6)), DEFAULT_WORD_DELIM_TABLE, flags, null);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b1eb427f2c6beed80d1724555fc1db003ccf3030":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"379db3ad24c4f0214f30a122265a6d6be003a99d":["c85fa43e6918808743daa7847ba0264373af687f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"6fc92bf7f56dcfe9d5bd75dfdc902b9597ea95b3":["b1eb427f2c6beed80d1724555fc1db003ccf3030"],"eaa82f7ae8119e5850fcdeb0a7f2362a7d732bac":["6fc92bf7f56dcfe9d5bd75dfdc902b9597ea95b3"],"c85fa43e6918808743daa7847ba0264373af687f":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["eaa82f7ae8119e5850fcdeb0a7f2362a7d732bac"]},"commit2Childs":{"b1eb427f2c6beed80d1724555fc1db003ccf3030":["6fc92bf7f56dcfe9d5bd75dfdc902b9597ea95b3"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["c85fa43e6918808743daa7847ba0264373af687f"],"379db3ad24c4f0214f30a122265a6d6be003a99d":["b1eb427f2c6beed80d1724555fc1db003ccf3030"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"6fc92bf7f56dcfe9d5bd75dfdc902b9597ea95b3":["eaa82f7ae8119e5850fcdeb0a7f2362a7d732bac"],"eaa82f7ae8119e5850fcdeb0a7f2362a7d732bac":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c85fa43e6918808743daa7847ba0264373af687f":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}