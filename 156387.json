{"path":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,long,long,long[]).mjava","commits":[{"id":"9b832cbed6eb3d54a8bb9339296bdda8eeb53014","date":1279708040,"type":0,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,long,long,long[]).mjava","pathOld":"/dev/null","sourceNew":"  // Apply buffered delete terms, queries and docIDs to the\n  // provided reader\n  final boolean applyDeletes(IndexReader reader, long minSequenceID, long maxSequenceID, long[] sequenceIDs)\n      throws CorruptIndexException, IOException {\n\n    assert sequenceIDs == null || sequenceIDs.length >= reader.maxDoc() : \"reader.maxDoc=\"\n        + reader.maxDoc() + \",sequenceIDs.length=\" + sequenceIDs.length;\n\n    boolean any = false;\n\n    // first: delete the documents that had non-aborting exceptions\n    if (sequenceIDs != null) {\n      for (int i = 0; i < reader.maxDoc(); i++) {\n        if (sequenceIDs[i] == -1) {\n          reader.deleteDocument(i);\n          any = true;\n        }\n      }\n    }\n    \n    if (deletesInRAM.hasDeletes()) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n\n      SortedMap<Long, BufferedDeletesInRAM.Delete> deletes = deletesInRAM.deletes.getReadCopy();\n      \n      SortedMap<Term, Long> deleteTerms = new TreeMap<Term, Long>();\n      for (Entry<Long, BufferedDeletesInRAM.Delete> entry : deletes.entrySet()) {\n        if (minSequenceID < entry.getKey()) {\n          BufferedDeletesInRAM.Delete delete = entry.getValue();\n          if (delete instanceof BufferedDeletesInRAM.DeleteTerm) {\n            BufferedDeletesInRAM.DeleteTerm deleteTerm = (BufferedDeletesInRAM.DeleteTerm) delete;\n            deleteTerms.put(deleteTerm.term, entry.getKey());\n          } else if (delete instanceof BufferedDeletesInRAM.DeleteTerms) {\n            BufferedDeletesInRAM.DeleteTerms terms = (BufferedDeletesInRAM.DeleteTerms) delete;\n            for (Term t : terms.terms) {\n              deleteTerms.put(t, entry.getKey());\n            }\n          } else {\n            // delete query\n            BufferedDeletesInRAM.DeleteQuery deleteQuery = (BufferedDeletesInRAM.DeleteQuery) delete;\n            Query query = deleteQuery.query;\n            Weight weight = query.weight(searcher);\n            Scorer scorer = weight.scorer(reader, true, false);\n            if (scorer != null) {\n              while (true) {\n                int doc = scorer.nextDoc();\n                if (doc == DocsEnum.NO_MORE_DOCS) {\n                  break;\n                }\n                if ( (sequenceIDs != null && sequenceIDs[doc] < entry.getKey())\n                    || (sequenceIDs == null && maxSequenceID < entry.getKey())) {\n                  reader.deleteDocument(doc);\n                  any = true;\n                }\n              }\n            }\n          }\n        }\n      }\n\n      // Delete by term\n      if (deleteTerms.size() > 0) {\n        Fields fields = reader.fields();\n        if (fields == null) {\n          // This reader has no postings\n          return false;\n        }\n\n        TermsEnum termsEnum = null;\n\n        String currentField = null;\n        BytesRef termRef = new BytesRef();\n        DocsEnum docs = null;\n\n        for (Entry<Term, Long> entry : deleteTerms.entrySet()) {\n          Term term = entry.getKey();\n          // Since we visit terms sorted, we gain performance\n          // by re-using the same TermsEnum and seeking only\n          // forwards\n          if (term.field() != currentField) {\n            assert currentField == null || currentField.compareTo(term.field()) < 0;\n            currentField = term.field();\n            Terms terms = fields.terms(currentField);\n            if (terms != null) {\n              termsEnum = terms.iterator();\n            } else {\n              termsEnum = null;\n            }\n          }\n\n          if (termsEnum == null) {\n            continue;\n          }\n          // assert checkDeleteTerm(term);\n\n          termRef.copy(term.text());\n\n          if (termsEnum.seek(termRef, false) == TermsEnum.SeekStatus.FOUND) {\n            DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n\n            if (docsEnum != null) {\n              docs = docsEnum;\n              // int limit = entry.getValue().getNum();\n              while (true) {\n                final int doc = docs.nextDoc();\n                // if (docID == DocsEnum.NO_MORE_DOCS || docIDStart+docID >= limit) {\n                if (doc == DocsEnum.NO_MORE_DOCS) {\n                  break;\n                }\n                if ( (sequenceIDs != null && sequenceIDs[doc] < entry.getValue())\n                    || (sequenceIDs == null && maxSequenceID < entry.getValue())) {\n                  reader.deleteDocument(doc);\n                  any = true;\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n\n    return any;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"334c1175813aea771a71728cd2c4ee4754fd0603","date":1279710173,"type":4,"author":"Uwe Schindler","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,long,long,long[]).mjava","sourceNew":null,"sourceOld":"  // Apply buffered delete terms, queries and docIDs to the\n  // provided reader\n  final boolean applyDeletes(IndexReader reader, long minSequenceID, long maxSequenceID, long[] sequenceIDs)\n      throws CorruptIndexException, IOException {\n\n    assert sequenceIDs == null || sequenceIDs.length >= reader.maxDoc() : \"reader.maxDoc=\"\n        + reader.maxDoc() + \",sequenceIDs.length=\" + sequenceIDs.length;\n\n    boolean any = false;\n\n    // first: delete the documents that had non-aborting exceptions\n    if (sequenceIDs != null) {\n      for (int i = 0; i < reader.maxDoc(); i++) {\n        if (sequenceIDs[i] == -1) {\n          reader.deleteDocument(i);\n          any = true;\n        }\n      }\n    }\n    \n    if (deletesInRAM.hasDeletes()) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n\n      SortedMap<Long, BufferedDeletesInRAM.Delete> deletes = deletesInRAM.deletes.getReadCopy();\n      \n      SortedMap<Term, Long> deleteTerms = new TreeMap<Term, Long>();\n      for (Entry<Long, BufferedDeletesInRAM.Delete> entry : deletes.entrySet()) {\n        if (minSequenceID < entry.getKey()) {\n          BufferedDeletesInRAM.Delete delete = entry.getValue();\n          if (delete instanceof BufferedDeletesInRAM.DeleteTerm) {\n            BufferedDeletesInRAM.DeleteTerm deleteTerm = (BufferedDeletesInRAM.DeleteTerm) delete;\n            deleteTerms.put(deleteTerm.term, entry.getKey());\n          } else if (delete instanceof BufferedDeletesInRAM.DeleteTerms) {\n            BufferedDeletesInRAM.DeleteTerms terms = (BufferedDeletesInRAM.DeleteTerms) delete;\n            for (Term t : terms.terms) {\n              deleteTerms.put(t, entry.getKey());\n            }\n          } else {\n            // delete query\n            BufferedDeletesInRAM.DeleteQuery deleteQuery = (BufferedDeletesInRAM.DeleteQuery) delete;\n            Query query = deleteQuery.query;\n            Weight weight = query.weight(searcher);\n            Scorer scorer = weight.scorer(reader, true, false);\n            if (scorer != null) {\n              while (true) {\n                int doc = scorer.nextDoc();\n                if (doc == DocsEnum.NO_MORE_DOCS) {\n                  break;\n                }\n                if ( (sequenceIDs != null && sequenceIDs[doc] < entry.getKey())\n                    || (sequenceIDs == null && maxSequenceID < entry.getKey())) {\n                  reader.deleteDocument(doc);\n                  any = true;\n                }\n              }\n            }\n          }\n        }\n      }\n\n      // Delete by term\n      if (deleteTerms.size() > 0) {\n        Fields fields = reader.fields();\n        if (fields == null) {\n          // This reader has no postings\n          return false;\n        }\n\n        TermsEnum termsEnum = null;\n\n        String currentField = null;\n        BytesRef termRef = new BytesRef();\n        DocsEnum docs = null;\n\n        for (Entry<Term, Long> entry : deleteTerms.entrySet()) {\n          Term term = entry.getKey();\n          // Since we visit terms sorted, we gain performance\n          // by re-using the same TermsEnum and seeking only\n          // forwards\n          if (term.field() != currentField) {\n            assert currentField == null || currentField.compareTo(term.field()) < 0;\n            currentField = term.field();\n            Terms terms = fields.terms(currentField);\n            if (terms != null) {\n              termsEnum = terms.iterator();\n            } else {\n              termsEnum = null;\n            }\n          }\n\n          if (termsEnum == null) {\n            continue;\n          }\n          // assert checkDeleteTerm(term);\n\n          termRef.copy(term.text());\n\n          if (termsEnum.seek(termRef, false) == TermsEnum.SeekStatus.FOUND) {\n            DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n\n            if (docsEnum != null) {\n              docs = docsEnum;\n              // int limit = entry.getValue().getNum();\n              while (true) {\n                final int doc = docs.nextDoc();\n                // if (docID == DocsEnum.NO_MORE_DOCS || docIDStart+docID >= limit) {\n                if (doc == DocsEnum.NO_MORE_DOCS) {\n                  break;\n                }\n                if ( (sequenceIDs != null && sequenceIDs[doc] < entry.getValue())\n                    || (sequenceIDs == null && maxSequenceID < entry.getValue())) {\n                  reader.deleteDocument(doc);\n                  any = true;\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n\n    return any;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8fe956d65251358d755c56f14fe8380644790e47","date":1279711318,"type":0,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,long,long,long[]).mjava","pathOld":"/dev/null","sourceNew":"  // Apply buffered delete terms, queries and docIDs to the\n  // provided reader\n  final boolean applyDeletes(IndexReader reader, long minSequenceID, long maxSequenceID, long[] sequenceIDs)\n      throws CorruptIndexException, IOException {\n\n    assert sequenceIDs == null || sequenceIDs.length >= reader.maxDoc() : \"reader.maxDoc=\"\n        + reader.maxDoc() + \",sequenceIDs.length=\" + sequenceIDs.length;\n\n    boolean any = false;\n\n    // first: delete the documents that had non-aborting exceptions\n    if (sequenceIDs != null) {\n      for (int i = 0; i < reader.maxDoc(); i++) {\n        if (sequenceIDs[i] == -1) {\n          reader.deleteDocument(i);\n          any = true;\n        }\n      }\n    }\n    \n    if (deletesInRAM.hasDeletes()) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n\n      SortedMap<Long, BufferedDeletesInRAM.Delete> deletes = deletesInRAM.deletes.getReadCopy();\n      \n      SortedMap<Term, Long> deleteTerms = new TreeMap<Term, Long>();\n      for (Entry<Long, BufferedDeletesInRAM.Delete> entry : deletes.entrySet()) {\n        if (minSequenceID < entry.getKey()) {\n          BufferedDeletesInRAM.Delete delete = entry.getValue();\n          if (delete instanceof BufferedDeletesInRAM.DeleteTerm) {\n            BufferedDeletesInRAM.DeleteTerm deleteTerm = (BufferedDeletesInRAM.DeleteTerm) delete;\n            deleteTerms.put(deleteTerm.term, entry.getKey());\n          } else if (delete instanceof BufferedDeletesInRAM.DeleteTerms) {\n            BufferedDeletesInRAM.DeleteTerms terms = (BufferedDeletesInRAM.DeleteTerms) delete;\n            for (Term t : terms.terms) {\n              deleteTerms.put(t, entry.getKey());\n            }\n          } else {\n            // delete query\n            BufferedDeletesInRAM.DeleteQuery deleteQuery = (BufferedDeletesInRAM.DeleteQuery) delete;\n            Query query = deleteQuery.query;\n            Weight weight = query.weight(searcher);\n            Scorer scorer = weight.scorer(reader, true, false);\n            if (scorer != null) {\n              while (true) {\n                int doc = scorer.nextDoc();\n                if (doc == DocsEnum.NO_MORE_DOCS) {\n                  break;\n                }\n                if ( (sequenceIDs != null && sequenceIDs[doc] < entry.getKey())\n                    || (sequenceIDs == null && maxSequenceID < entry.getKey())) {\n                  reader.deleteDocument(doc);\n                  any = true;\n                }\n              }\n            }\n          }\n        }\n      }\n\n      // Delete by term\n      if (deleteTerms.size() > 0) {\n        Fields fields = reader.fields();\n        if (fields == null) {\n          // This reader has no postings\n          return false;\n        }\n\n        TermsEnum termsEnum = null;\n\n        String currentField = null;\n        BytesRef termRef = new BytesRef();\n        DocsEnum docs = null;\n\n        for (Entry<Term, Long> entry : deleteTerms.entrySet()) {\n          Term term = entry.getKey();\n          // Since we visit terms sorted, we gain performance\n          // by re-using the same TermsEnum and seeking only\n          // forwards\n          if (term.field() != currentField) {\n            assert currentField == null || currentField.compareTo(term.field()) < 0;\n            currentField = term.field();\n            Terms terms = fields.terms(currentField);\n            if (terms != null) {\n              termsEnum = terms.iterator();\n            } else {\n              termsEnum = null;\n            }\n          }\n\n          if (termsEnum == null) {\n            continue;\n          }\n          // assert checkDeleteTerm(term);\n\n          termRef.copy(term.text());\n\n          if (termsEnum.seek(termRef, false) == TermsEnum.SeekStatus.FOUND) {\n            DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n\n            if (docsEnum != null) {\n              docs = docsEnum;\n              // int limit = entry.getValue().getNum();\n              while (true) {\n                final int doc = docs.nextDoc();\n                // if (docID == DocsEnum.NO_MORE_DOCS || docIDStart+docID >= limit) {\n                if (doc == DocsEnum.NO_MORE_DOCS) {\n                  break;\n                }\n                if ( (sequenceIDs != null && sequenceIDs[doc] < entry.getValue())\n                    || (sequenceIDs == null && maxSequenceID < entry.getValue())) {\n                  reader.deleteDocument(doc);\n                  any = true;\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n\n    return any;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":4,"author":"Michael Busch","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#applyDeletes(IndexReader,long,long,long[]).mjava","sourceNew":null,"sourceOld":"  // Apply buffered delete terms, queries and docIDs to the\n  // provided reader\n  final boolean applyDeletes(IndexReader reader, long minSequenceID, long maxSequenceID, long[] sequenceIDs)\n      throws CorruptIndexException, IOException {\n\n    assert sequenceIDs == null || sequenceIDs.length >= reader.maxDoc() : \"reader.maxDoc=\"\n        + reader.maxDoc() + \",sequenceIDs.length=\" + sequenceIDs.length;\n\n    boolean any = false;\n\n    // first: delete the documents that had non-aborting exceptions\n    if (sequenceIDs != null) {\n      for (int i = 0; i < reader.maxDoc(); i++) {\n        if (sequenceIDs[i] == -1) {\n          reader.deleteDocument(i);\n          any = true;\n        }\n      }\n    }\n    \n    if (deletesInRAM.hasDeletes()) {\n      IndexSearcher searcher = new IndexSearcher(reader);\n\n      SortedMap<Long, BufferedDeletesInRAM.Delete> deletes = deletesInRAM.deletes.getReadCopy();\n      \n      SortedMap<Term, Long> deleteTerms = new TreeMap<Term, Long>();\n      for (Entry<Long, BufferedDeletesInRAM.Delete> entry : deletes.entrySet()) {\n        if (minSequenceID < entry.getKey()) {\n          BufferedDeletesInRAM.Delete delete = entry.getValue();\n          if (delete instanceof BufferedDeletesInRAM.DeleteTerm) {\n            BufferedDeletesInRAM.DeleteTerm deleteTerm = (BufferedDeletesInRAM.DeleteTerm) delete;\n            deleteTerms.put(deleteTerm.term, entry.getKey());\n          } else if (delete instanceof BufferedDeletesInRAM.DeleteTerms) {\n            BufferedDeletesInRAM.DeleteTerms terms = (BufferedDeletesInRAM.DeleteTerms) delete;\n            for (Term t : terms.terms) {\n              deleteTerms.put(t, entry.getKey());\n            }\n          } else {\n            // delete query\n            BufferedDeletesInRAM.DeleteQuery deleteQuery = (BufferedDeletesInRAM.DeleteQuery) delete;\n            Query query = deleteQuery.query;\n            Weight weight = query.weight(searcher);\n            Scorer scorer = weight.scorer(reader, true, false);\n            if (scorer != null) {\n              while (true) {\n                int doc = scorer.nextDoc();\n                if (doc == DocsEnum.NO_MORE_DOCS) {\n                  break;\n                }\n                if ( (sequenceIDs != null && sequenceIDs[doc] < entry.getKey())\n                    || (sequenceIDs == null && maxSequenceID < entry.getKey())) {\n                  reader.deleteDocument(doc);\n                  any = true;\n                }\n              }\n            }\n          }\n        }\n      }\n\n      // Delete by term\n      if (deleteTerms.size() > 0) {\n        Fields fields = reader.fields();\n        if (fields == null) {\n          // This reader has no postings\n          return false;\n        }\n\n        TermsEnum termsEnum = null;\n\n        String currentField = null;\n        BytesRef termRef = new BytesRef();\n        DocsEnum docs = null;\n\n        for (Entry<Term, Long> entry : deleteTerms.entrySet()) {\n          Term term = entry.getKey();\n          // Since we visit terms sorted, we gain performance\n          // by re-using the same TermsEnum and seeking only\n          // forwards\n          if (term.field() != currentField) {\n            assert currentField == null || currentField.compareTo(term.field()) < 0;\n            currentField = term.field();\n            Terms terms = fields.terms(currentField);\n            if (terms != null) {\n              termsEnum = terms.iterator();\n            } else {\n              termsEnum = null;\n            }\n          }\n\n          if (termsEnum == null) {\n            continue;\n          }\n          // assert checkDeleteTerm(term);\n\n          termRef.copy(term.text());\n\n          if (termsEnum.seek(termRef, false) == TermsEnum.SeekStatus.FOUND) {\n            DocsEnum docsEnum = termsEnum.docs(reader.getDeletedDocs(), docs);\n\n            if (docsEnum != null) {\n              docs = docsEnum;\n              // int limit = entry.getValue().getNum();\n              while (true) {\n                final int doc = docs.nextDoc();\n                // if (docID == DocsEnum.NO_MORE_DOCS || docIDStart+docID >= limit) {\n                if (doc == DocsEnum.NO_MORE_DOCS) {\n                  break;\n                }\n                if ( (sequenceIDs != null && sequenceIDs[doc] < entry.getValue())\n                    || (sequenceIDs == null && maxSequenceID < entry.getValue())) {\n                  reader.deleteDocument(doc);\n                  any = true;\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n\n    return any;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8fe956d65251358d755c56f14fe8380644790e47":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"334c1175813aea771a71728cd2c4ee4754fd0603":["9b832cbed6eb3d54a8bb9339296bdda8eeb53014"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["8fe956d65251358d755c56f14fe8380644790e47","334c1175813aea771a71728cd2c4ee4754fd0603"],"9b832cbed6eb3d54a8bb9339296bdda8eeb53014":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["334c1175813aea771a71728cd2c4ee4754fd0603"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["8fe956d65251358d755c56f14fe8380644790e47","9b832cbed6eb3d54a8bb9339296bdda8eeb53014"],"8fe956d65251358d755c56f14fe8380644790e47":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"334c1175813aea771a71728cd2c4ee4754fd0603":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":[],"9b832cbed6eb3d54a8bb9339296bdda8eeb53014":["334c1175813aea771a71728cd2c4ee4754fd0603"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}