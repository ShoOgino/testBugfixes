{"path":"contrib/analyzers/src/java/org/apache/lucene/analysis/compound/hyphenation/HyphenationTree#searchPatterns(char[],int,byte[]).mjava","commits":[{"id":"dbb53146e3651ed4ebca43f69ee26f9150c6cb5a","date":1210940570,"type":0,"author":"Grant Ingersoll","isMerge":false,"pathNew":"contrib/analyzers/src/java/org/apache/lucene/analysis/compound/hyphenation/HyphenationTree#searchPatterns(char[],int,byte[]).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * <p>\n   * Search for all possible partial matches of word starting at index an update\n   * interletter values. In other words, it does something like:\n   * </p>\n   * <code>\n   * for(i=0; i<patterns.length; i++) {\n   * if ( word.substring(index).startsWidth(patterns[i]) )\n   * update_interletter_values(patterns[i]);\n   * }\n   * </code>\n   * <p>\n   * But it is done in an efficient way since the patterns are stored in a\n   * ternary tree. In fact, this is the whole purpose of having the tree: doing\n   * this search without having to test every single pattern. The number of\n   * patterns for languages such as English range from 4000 to 10000. Thus,\n   * doing thousands of string comparisons for each word to hyphenate would be\n   * really slow without the tree. The tradeoff is memory, but using a ternary\n   * tree instead of a trie, almost halves the the memory used by Lout or TeX.\n   * It's also faster than using a hash table\n   * </p>\n   * \n   * @param word null terminated word to match\n   * @param index start index from word\n   * @param il interletter values array to update\n   */\n  protected void searchPatterns(char[] word, int index, byte[] il) {\n    byte[] values;\n    int i = index;\n    char p, q;\n    char sp = word[i];\n    p = root;\n\n    while (p > 0 && p < sc.length) {\n      if (sc[p] == 0xFFFF) {\n        if (hstrcmp(word, i, kv.getArray(), lo[p]) == 0) {\n          values = getValues(eq[p]); // data pointer is in eq[]\n          int j = index;\n          for (int k = 0; k < values.length; k++) {\n            if (j < il.length && values[k] > il[j]) {\n              il[j] = values[k];\n            }\n            j++;\n          }\n        }\n        return;\n      }\n      int d = sp - sc[p];\n      if (d == 0) {\n        if (sp == 0) {\n          break;\n        }\n        sp = word[++i];\n        p = eq[p];\n        q = p;\n\n        // look for a pattern ending at this position by searching for\n        // the null char ( splitchar == 0 )\n        while (q > 0 && q < sc.length) {\n          if (sc[q] == 0xFFFF) { // stop at compressed branch\n            break;\n          }\n          if (sc[q] == 0) {\n            values = getValues(eq[q]);\n            int j = index;\n            for (int k = 0; k < values.length; k++) {\n              if (j < il.length && values[k] > il[j]) {\n                il[j] = values[k];\n              }\n              j++;\n            }\n            break;\n          } else {\n            q = lo[q];\n\n            /**\n             * actually the code should be: q = sc[q] < 0 ? hi[q] : lo[q]; but\n             * java chars are unsigned\n             */\n          }\n        }\n      } else {\n        p = d < 0 ? lo[p] : hi[p];\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["3570e776df3671c1bce4f54b07bf03ca5a2c23de"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"dd745d580729e528151b58aeda87ef82f1b95c9b","date":1248369082,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"contrib/analyzers/common/src/java/org/apache/lucene/analysis/compound/hyphenation/HyphenationTree#searchPatterns(char[],int,byte[]).mjava","pathOld":"contrib/analyzers/src/java/org/apache/lucene/analysis/compound/hyphenation/HyphenationTree#searchPatterns(char[],int,byte[]).mjava","sourceNew":"  /**\n   * <p>\n   * Search for all possible partial matches of word starting at index an update\n   * interletter values. In other words, it does something like:\n   * </p>\n   * <code>\n   * for(i=0; i<patterns.length; i++) {\n   * if ( word.substring(index).startsWidth(patterns[i]) )\n   * update_interletter_values(patterns[i]);\n   * }\n   * </code>\n   * <p>\n   * But it is done in an efficient way since the patterns are stored in a\n   * ternary tree. In fact, this is the whole purpose of having the tree: doing\n   * this search without having to test every single pattern. The number of\n   * patterns for languages such as English range from 4000 to 10000. Thus,\n   * doing thousands of string comparisons for each word to hyphenate would be\n   * really slow without the tree. The tradeoff is memory, but using a ternary\n   * tree instead of a trie, almost halves the the memory used by Lout or TeX.\n   * It's also faster than using a hash table\n   * </p>\n   * \n   * @param word null terminated word to match\n   * @param index start index from word\n   * @param il interletter values array to update\n   */\n  protected void searchPatterns(char[] word, int index, byte[] il) {\n    byte[] values;\n    int i = index;\n    char p, q;\n    char sp = word[i];\n    p = root;\n\n    while (p > 0 && p < sc.length) {\n      if (sc[p] == 0xFFFF) {\n        if (hstrcmp(word, i, kv.getArray(), lo[p]) == 0) {\n          values = getValues(eq[p]); // data pointer is in eq[]\n          int j = index;\n          for (int k = 0; k < values.length; k++) {\n            if (j < il.length && values[k] > il[j]) {\n              il[j] = values[k];\n            }\n            j++;\n          }\n        }\n        return;\n      }\n      int d = sp - sc[p];\n      if (d == 0) {\n        if (sp == 0) {\n          break;\n        }\n        sp = word[++i];\n        p = eq[p];\n        q = p;\n\n        // look for a pattern ending at this position by searching for\n        // the null char ( splitchar == 0 )\n        while (q > 0 && q < sc.length) {\n          if (sc[q] == 0xFFFF) { // stop at compressed branch\n            break;\n          }\n          if (sc[q] == 0) {\n            values = getValues(eq[q]);\n            int j = index;\n            for (int k = 0; k < values.length; k++) {\n              if (j < il.length && values[k] > il[j]) {\n                il[j] = values[k];\n              }\n              j++;\n            }\n            break;\n          } else {\n            q = lo[q];\n\n            /**\n             * actually the code should be: q = sc[q] < 0 ? hi[q] : lo[q]; but\n             * java chars are unsigned\n             */\n          }\n        }\n      } else {\n        p = d < 0 ? lo[p] : hi[p];\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * <p>\n   * Search for all possible partial matches of word starting at index an update\n   * interletter values. In other words, it does something like:\n   * </p>\n   * <code>\n   * for(i=0; i<patterns.length; i++) {\n   * if ( word.substring(index).startsWidth(patterns[i]) )\n   * update_interletter_values(patterns[i]);\n   * }\n   * </code>\n   * <p>\n   * But it is done in an efficient way since the patterns are stored in a\n   * ternary tree. In fact, this is the whole purpose of having the tree: doing\n   * this search without having to test every single pattern. The number of\n   * patterns for languages such as English range from 4000 to 10000. Thus,\n   * doing thousands of string comparisons for each word to hyphenate would be\n   * really slow without the tree. The tradeoff is memory, but using a ternary\n   * tree instead of a trie, almost halves the the memory used by Lout or TeX.\n   * It's also faster than using a hash table\n   * </p>\n   * \n   * @param word null terminated word to match\n   * @param index start index from word\n   * @param il interletter values array to update\n   */\n  protected void searchPatterns(char[] word, int index, byte[] il) {\n    byte[] values;\n    int i = index;\n    char p, q;\n    char sp = word[i];\n    p = root;\n\n    while (p > 0 && p < sc.length) {\n      if (sc[p] == 0xFFFF) {\n        if (hstrcmp(word, i, kv.getArray(), lo[p]) == 0) {\n          values = getValues(eq[p]); // data pointer is in eq[]\n          int j = index;\n          for (int k = 0; k < values.length; k++) {\n            if (j < il.length && values[k] > il[j]) {\n              il[j] = values[k];\n            }\n            j++;\n          }\n        }\n        return;\n      }\n      int d = sp - sc[p];\n      if (d == 0) {\n        if (sp == 0) {\n          break;\n        }\n        sp = word[++i];\n        p = eq[p];\n        q = p;\n\n        // look for a pattern ending at this position by searching for\n        // the null char ( splitchar == 0 )\n        while (q > 0 && q < sc.length) {\n          if (sc[q] == 0xFFFF) { // stop at compressed branch\n            break;\n          }\n          if (sc[q] == 0) {\n            values = getValues(eq[q]);\n            int j = index;\n            for (int k = 0; k < values.length; k++) {\n              if (j < il.length && values[k] > il[j]) {\n                il[j] = values[k];\n              }\n              j++;\n            }\n            break;\n          } else {\n            q = lo[q];\n\n            /**\n             * actually the code should be: q = sc[q] < 0 ? hi[q] : lo[q]; but\n             * java chars are unsigned\n             */\n          }\n        }\n      } else {\n        p = d < 0 ? lo[p] : hi[p];\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"dbb53146e3651ed4ebca43f69ee26f9150c6cb5a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"dd745d580729e528151b58aeda87ef82f1b95c9b":["dbb53146e3651ed4ebca43f69ee26f9150c6cb5a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["dd745d580729e528151b58aeda87ef82f1b95c9b"]},"commit2Childs":{"dbb53146e3651ed4ebca43f69ee26f9150c6cb5a":["dd745d580729e528151b58aeda87ef82f1b95c9b"],"dd745d580729e528151b58aeda87ef82f1b95c9b":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["dbb53146e3651ed4ebca43f69ee26f9150c6cb5a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}