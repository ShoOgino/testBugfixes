{"path":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","commits":[{"id":"6066dbe072ec5334ff5824f474e9d3abd1620fb7","date":1278709584,"type":0,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"/dev/null","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory();\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundDocStore(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(new Field(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"/dev/null","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory();\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundDocStore(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(new Field(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8dc26bfa5ebbc55b5a04fbec545dfcec647b046b","date":1280297653,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory();\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(new Field(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory();\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundDocStore(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(new Field(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a05409176bd65129d67a785ee70e881e238a9aef","date":1282582843,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundDocStore(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(new Field(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory();\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundDocStore(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(new Field(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"132903c28af3aa6f67284b78de91c0f0a99488c2","date":1284282129,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundDocStore(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundDocStore(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(new Field(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"53a31399f2471493d67b19a95c028a74e0113b6a","date":1289817072,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundDocStore(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundDocStore(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9ab1f5591dc05f1f2b5407d809c9699f75554a32","date":1290008586,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundDocStore(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundDocStore(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5","date":1292695408,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundDocStore(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundDocStore(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory();\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(new Field(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"01e5948db9a07144112d2f08f28ca2e3cd880348","date":1301759232,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"45669a651c970812a680841b97a77cce06af559f","date":1301922222,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer());\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((LogMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fd9cc9d77712aba3662f24632df7539ab75e3667","date":1309095238,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seek(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    FieldType storedTextType = new FieldType(TextField.TYPE_STORED);\n    storedTextType.setStoreTermVectors(true);\n    storedTextType.setStoreTermVectorPositions(true);\n    storedTextType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"f\", text, storedTextType));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    Document doc2 = reader.document(0);\n    assertEquals(text, doc2.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    doc.add(newField(\"f\", text, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    doc = reader.document(0);\n    assertEquals(text, doc.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodec(new AppendingCodec());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    FieldType storedTextType = new FieldType(TextField.TYPE_STORED);\n    storedTextType.setStoreTermVectors(true);\n    storedTextType.setStoreTermVectorPositions(true);\n    storedTextType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"f\", text, storedTextType));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1);\n    assertEquals(2, reader.numDocs());\n    Document doc2 = reader.document(0);\n    assertEquals(text, doc2.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodecProvider(new AppendingCodecProvider());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    FieldType storedTextType = new FieldType(TextField.TYPE_STORED);\n    storedTextType.setStoreTermVectors(true);\n    storedTextType.setStoreTermVectorPositions(true);\n    storedTextType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"f\", text, storedTextType));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1, new AppendingCodecProvider());\n    assertEquals(2, reader.numDocs());\n    Document doc2 = reader.document(0);\n    assertEquals(text, doc2.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d14e8d18c0e3970c20354dbeeb49da11bd587fbd","date":1321041051,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodec(new AppendingCodec());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    FieldType storedTextType = new FieldType(TextField.TYPE_STORED);\n    storedTextType.setStoreTermVectors(true);\n    storedTextType.setStoreTermVectorPositions(true);\n    storedTextType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"f\", text, storedTextType));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1);\n    assertEquals(2, reader.numDocs());\n    Document doc2 = reader.document(0);\n    assertEquals(text, doc2.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodec(new AppendingCodec());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    FieldType storedTextType = new FieldType(TextField.TYPE_STORED);\n    storedTextType.setStoreTermVectors(true);\n    storedTextType.setStoreTermVectorPositions(true);\n    storedTextType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"f\", text, storedTextType));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.optimize();\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1);\n    assertEquals(2, reader.numDocs());\n    Document doc2 = reader.document(0);\n    assertEquals(text, doc2.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodec(new AppendingCodec());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    FieldType storedTextType = new FieldType(TextField.TYPE_STORED);\n    storedTextType.setStoreTermVectors(true);\n    storedTextType.setStoreTermVectorPositions(true);\n    storedTextType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"f\", text, storedTextType));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1);\n    assertEquals(2, reader.numDocs());\n    Document doc2 = reader.document(0);\n    assertEquals(text, doc2.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator(null);\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodec(new AppendingCodec());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    FieldType storedTextType = new FieldType(TextField.TYPE_STORED);\n    storedTextType.setStoreTermVectors(true);\n    storedTextType.setStoreTermVectorPositions(true);\n    storedTextType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"f\", text, storedTextType));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1);\n    assertEquals(2, reader.numDocs());\n    Document doc2 = reader.document(0);\n    assertEquals(text, doc2.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator();\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"872cff1d3a554e0cd64014cd97f88d3002b0f491","date":1323024658,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodec(new AppendingCodec());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    FieldType storedTextType = new FieldType(TextField.TYPE_STORED);\n    storedTextType.setStoreTermVectors(true);\n    storedTextType.setStoreTermVectorPositions(true);\n    storedTextType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"f\", text, storedTextType));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1);\n    assertEquals(2, reader.numDocs());\n    Document doc2 = reader.document(0);\n    assertEquals(text, doc2.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator(null);\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null, true);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodec(new AppendingCodec());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    FieldType storedTextType = new FieldType(TextField.TYPE_STORED);\n    storedTextType.setStoreTermVectors(true);\n    storedTextType.setStoreTermVectorPositions(true);\n    storedTextType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"f\", text, storedTextType));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1);\n    assertEquals(2, reader.numDocs());\n    Document doc2 = reader.document(0);\n    assertEquals(text, doc2.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator(null);\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":["02331260bb246364779cb6f04919ca47900d01bb","02331260bb246364779cb6f04919ca47900d01bb"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b65b350ca9588f9fc76ce7d6804160d06c45ff42","date":1323026297,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodec(new AppendingCodec());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    FieldType storedTextType = new FieldType(TextField.TYPE_STORED);\n    storedTextType.setStoreTermVectors(true);\n    storedTextType.setStoreTermVectorPositions(true);\n    storedTextType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"f\", text, storedTextType));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1);\n    assertEquals(2, reader.numDocs());\n    Document doc2 = reader.document(0);\n    assertEquals(text, doc2.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator(null);\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null, true);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodec(new AppendingCodec());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    FieldType storedTextType = new FieldType(TextField.TYPE_STORED);\n    storedTextType.setStoreTermVectors(true);\n    storedTextType.setStoreTermVectorPositions(true);\n    storedTextType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"f\", text, storedTextType));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1);\n    assertEquals(2, reader.numDocs());\n    Document doc2 = reader.document(0);\n    assertEquals(text, doc2.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator(null);\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1c5b026d03cbbb03ca4c0b97d14e9839682281dc","date":1323049298,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodec(new AppendingCodec());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    FieldType storedTextType = new FieldType(TextField.TYPE_STORED);\n    storedTextType.setStoreTermVectors(true);\n    storedTextType.setStoreTermVectorPositions(true);\n    storedTextType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"f\", text, storedTextType));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, 1);\n    assertEquals(2, reader.numDocs());\n    Document doc2 = reader.document(0);\n    assertEquals(text, doc2.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator(null);\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null, true);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodec(new AppendingCodec());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    FieldType storedTextType = new FieldType(TextField.TYPE_STORED);\n    storedTextType.setStoreTermVectors(true);\n    storedTextType.setStoreTermVectorPositions(true);\n    storedTextType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"f\", text, storedTextType));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1);\n    assertEquals(2, reader.numDocs());\n    Document doc2 = reader.document(0);\n    assertEquals(text, doc2.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator(null);\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null, true);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9eda19c7aeddd50f2d8bc261c8567cdeb7f094d7","date":1323207588,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodec(new AppendingCodec());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    FieldType storedTextType = new FieldType(TextField.TYPE_STORED);\n    storedTextType.setStoreTermVectors(true);\n    storedTextType.setStoreTermVectorPositions(true);\n    storedTextType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"f\", text, storedTextType));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1);\n    assertEquals(2, reader.numDocs());\n    Document doc2 = reader.document(0);\n    assertEquals(text, doc2.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator(null);\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null, true);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodec(new AppendingCodec());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    FieldType storedTextType = new FieldType(TextField.TYPE_STORED);\n    storedTextType.setStoreTermVectors(true);\n    storedTextType.setStoreTermVectorPositions(true);\n    storedTextType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"f\", text, storedTextType));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, null, true, 1);\n    assertEquals(2, reader.numDocs());\n    Document doc2 = reader.document(0);\n    assertEquals(text, doc2.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator(null);\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null, true);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"929d416aad35e36f2d2743c625e05e23908e7563","date":1323209095,"type":5,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","pathOld":"lucene/contrib/misc/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec#testCodec().mjava","sourceNew":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodec(new AppendingCodec());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    FieldType storedTextType = new FieldType(TextField.TYPE_STORED);\n    storedTextType.setStoreTermVectors(true);\n    storedTextType.setStoreTermVectorPositions(true);\n    storedTextType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"f\", text, storedTextType));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, 1);\n    assertEquals(2, reader.numDocs());\n    Document doc2 = reader.document(0);\n    assertEquals(text, doc2.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator(null);\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null, true);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","sourceOld":"  public void testCodec() throws Exception {\n    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());\n    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));\n    \n    cfg.setCodec(new AppendingCodec());\n    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);\n    IndexWriter writer = new IndexWriter(dir, cfg);\n    Document doc = new Document();\n    FieldType storedTextType = new FieldType(TextField.TYPE_STORED);\n    storedTextType.setStoreTermVectors(true);\n    storedTextType.setStoreTermVectorPositions(true);\n    storedTextType.setStoreTermVectorOffsets(true);\n    doc.add(newField(\"f\", text, storedTextType));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.addDocument(doc);\n    writer.forceMerge(1);\n    writer.close();\n    IndexReader reader = IndexReader.open(dir, 1);\n    assertEquals(2, reader.numDocs());\n    Document doc2 = reader.document(0);\n    assertEquals(text, doc2.get(\"f\"));\n    Fields fields = MultiFields.getFields(reader);\n    Terms terms = fields.terms(\"f\");\n    assertNotNull(terms);\n    TermsEnum te = terms.iterator(null);\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"quick\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"brown\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"fox\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"jumped\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"over\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"lazy\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"dog\")));\n    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef(\"the\")));\n    DocsEnum de = te.docs(null, null, true);\n    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);\n    assertEquals(2, de.freq());\n    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);\n    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"1c5b026d03cbbb03ca4c0b97d14e9839682281dc":["b65b350ca9588f9fc76ce7d6804160d06c45ff42"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["7b91922b55d15444d554721b352861d028eb8278"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["a05409176bd65129d67a785ee70e881e238a9aef"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["8dc26bfa5ebbc55b5a04fbec545dfcec647b046b","4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"01e5948db9a07144112d2f08f28ca2e3cd880348":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"929d416aad35e36f2d2743c625e05e23908e7563":["1c5b026d03cbbb03ca4c0b97d14e9839682281dc","9eda19c7aeddd50f2d8bc261c8567cdeb7f094d7"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["01e5948db9a07144112d2f08f28ca2e3cd880348"],"53a31399f2471493d67b19a95c028a74e0113b6a":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a05409176bd65129d67a785ee70e881e238a9aef":["6066dbe072ec5334ff5824f474e9d3abd1620fb7"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["fd9cc9d77712aba3662f24632df7539ab75e3667"],"fd9cc9d77712aba3662f24632df7539ab75e3667":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["53a31399f2471493d67b19a95c028a74e0113b6a"],"9ab1f5591dc05f1f2b5407d809c9699f75554a32":["132903c28af3aa6f67284b78de91c0f0a99488c2","53a31399f2471493d67b19a95c028a74e0113b6a"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"3cc749c053615f5871f3b95715fe292f34e70a53":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["9ab1f5591dc05f1f2b5407d809c9699f75554a32","4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"872cff1d3a554e0cd64014cd97f88d3002b0f491":["3cc749c053615f5871f3b95715fe292f34e70a53"],"5f4e87790277826a2aea119328600dfb07761f32":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","6066dbe072ec5334ff5824f474e9d3abd1620fb7"],"962d04139994fce5193143ef35615499a9a96d78":["45669a651c970812a680841b97a77cce06af559f","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"2553b00f699380c64959ccb27991289aae87be2e":["a3776dccca01c11e7046323cfad46a3b4a471233","fd9cc9d77712aba3662f24632df7539ab75e3667"],"8dc26bfa5ebbc55b5a04fbec545dfcec647b046b":["5f4e87790277826a2aea119328600dfb07761f32"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["f2c5f0cb44df114db4228c8f77861714b5cabaea","fd9cc9d77712aba3662f24632df7539ab75e3667"],"7b91922b55d15444d554721b352861d028eb8278":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"6066dbe072ec5334ff5824f474e9d3abd1620fb7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"b65b350ca9588f9fc76ce7d6804160d06c45ff42":["3cc749c053615f5871f3b95715fe292f34e70a53","872cff1d3a554e0cd64014cd97f88d3002b0f491"],"a3776dccca01c11e7046323cfad46a3b4a471233":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"45669a651c970812a680841b97a77cce06af559f":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","01e5948db9a07144112d2f08f28ca2e3cd880348"],"9eda19c7aeddd50f2d8bc261c8567cdeb7f094d7":["872cff1d3a554e0cd64014cd97f88d3002b0f491"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9eda19c7aeddd50f2d8bc261c8567cdeb7f094d7"]},"commit2Childs":{"1c5b026d03cbbb03ca4c0b97d14e9839682281dc":["929d416aad35e36f2d2743c625e05e23908e7563"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["3cc749c053615f5871f3b95715fe292f34e70a53"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["53a31399f2471493d67b19a95c028a74e0113b6a","9ab1f5591dc05f1f2b5407d809c9699f75554a32"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["45669a651c970812a680841b97a77cce06af559f"],"01e5948db9a07144112d2f08f28ca2e3cd880348":["f2c5f0cb44df114db4228c8f77861714b5cabaea","45669a651c970812a680841b97a77cce06af559f"],"929d416aad35e36f2d2743c625e05e23908e7563":[],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["fd9cc9d77712aba3662f24632df7539ab75e3667","135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","d083e83f225b11e5fdd900e83d26ddb385b6955c","a3776dccca01c11e7046323cfad46a3b4a471233"],"53a31399f2471493d67b19a95c028a74e0113b6a":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5","9ab1f5591dc05f1f2b5407d809c9699f75554a32"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5f4e87790277826a2aea119328600dfb07761f32","6066dbe072ec5334ff5824f474e9d3abd1620fb7"],"a05409176bd65129d67a785ee70e881e238a9aef":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["7b91922b55d15444d554721b352861d028eb8278"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","01e5948db9a07144112d2f08f28ca2e3cd880348","ab5cb6a74aefb78aa0569857970b9151dfe2e787","a3776dccca01c11e7046323cfad46a3b4a471233"],"fd9cc9d77712aba3662f24632df7539ab75e3667":["1509f151d7692d84fae414b2b799ac06ba60fcb4","2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"9ab1f5591dc05f1f2b5407d809c9699f75554a32":["ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"3cc749c053615f5871f3b95715fe292f34e70a53":["872cff1d3a554e0cd64014cd97f88d3002b0f491","b65b350ca9588f9fc76ce7d6804160d06c45ff42"],"872cff1d3a554e0cd64014cd97f88d3002b0f491":["b65b350ca9588f9fc76ce7d6804160d06c45ff42","9eda19c7aeddd50f2d8bc261c8567cdeb7f094d7"],"5f4e87790277826a2aea119328600dfb07761f32":["8dc26bfa5ebbc55b5a04fbec545dfcec647b046b"],"962d04139994fce5193143ef35615499a9a96d78":[],"2553b00f699380c64959ccb27991289aae87be2e":[],"8dc26bfa5ebbc55b5a04fbec545dfcec647b046b":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"7b91922b55d15444d554721b352861d028eb8278":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"6066dbe072ec5334ff5824f474e9d3abd1620fb7":["a05409176bd65129d67a785ee70e881e238a9aef","5f4e87790277826a2aea119328600dfb07761f32"],"b65b350ca9588f9fc76ce7d6804160d06c45ff42":["1c5b026d03cbbb03ca4c0b97d14e9839682281dc"],"a3776dccca01c11e7046323cfad46a3b4a471233":["2553b00f699380c64959ccb27991289aae87be2e"],"45669a651c970812a680841b97a77cce06af559f":["962d04139994fce5193143ef35615499a9a96d78"],"9eda19c7aeddd50f2d8bc261c8567cdeb7f094d7":["929d416aad35e36f2d2743c625e05e23908e7563","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["929d416aad35e36f2d2743c625e05e23908e7563","135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}