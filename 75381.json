{"path":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","commits":[{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least on field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least on field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f0b9507caf22f292ac0e5e59f62db4275adf4511","date":1310107283,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least on field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3738f7d06920ae25ab2884f4efd80c42e95d6271","date":1312707753,"type":3,"author":"Martijn van Groningen","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      FixedBitSet fixedBitSet = allGroupHeadsCollector.retrieveGroupHeads(maxDoc);\n      long[] bits = fixedBitSet.getBits();\n      OpenBitSet openBitSet = new OpenBitSet(bits, bits.length);\n      qr.setDocSet(new BitDocSet(openBitSet));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a250ecd3e83b1c6595bcae0474f5b258df42021b","date":1330943490,"type":3,"author":"Martijn van Groningen","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      FixedBitSet fixedBitSet = allGroupHeadsCollector.retrieveGroupHeads(maxDoc);\n      long[] bits = fixedBitSet.getBits();\n      OpenBitSet openBitSet = new OpenBitSet(bits, bits.length);\n      qr.setDocSet(new BitDocSet(openBitSet));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      FixedBitSet fixedBitSet = allGroupHeadsCollector.retrieveGroupHeads(maxDoc);\n      long[] bits = fixedBitSet.getBits();\n      OpenBitSet openBitSet = new OpenBitSet(bits, bits.length);\n      qr.setDocSet(new BitDocSet(openBitSet));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":["9c7cdfe5a1ea9db97faa404b251fa644faa73597"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","date":1331075828,"type":3,"author":"Ryan McKinley","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      FixedBitSet fixedBitSet = allGroupHeadsCollector.retrieveGroupHeads(maxDoc);\n      long[] bits = fixedBitSet.getBits();\n      OpenBitSet openBitSet = new OpenBitSet(bits, bits.length);\n      qr.setDocSet(new BitDocSet(openBitSet));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      FixedBitSet fixedBitSet = allGroupHeadsCollector.retrieveGroupHeads(maxDoc);\n      long[] bits = fixedBitSet.getBits();\n      OpenBitSet openBitSet = new OpenBitSet(bits, bits.length);\n      qr.setDocSet(new BitDocSet(openBitSet));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae81ef60a860c551619f7702a13ba15f7dc024f5","date":1341796635,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      FixedBitSet fixedBitSet = allGroupHeadsCollector.retrieveGroupHeads(maxDoc);\n      long[] bits = fixedBitSet.getBits();\n      OpenBitSet openBitSet = new OpenBitSet(bits, bits.length);\n      qr.setDocSet(new BitDocSet(openBitSet));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      FixedBitSet fixedBitSet = allGroupHeadsCollector.retrieveGroupHeads(maxDoc);\n      long[] bits = fixedBitSet.getBits();\n      OpenBitSet openBitSet = new OpenBitSet(bits, bits.length);\n      qr.setDocSet(new BitDocSet(openBitSet));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2acf500f78aa12b92e371fd89c719291986b6b90","date":1341846236,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      FixedBitSet fixedBitSet = allGroupHeadsCollector.retrieveGroupHeads(maxDoc);\n      long[] bits = fixedBitSet.getBits();\n      OpenBitSet openBitSet = new OpenBitSet(bits, bits.length);\n      qr.setDocSet(new BitDocSet(openBitSet));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      FixedBitSet fixedBitSet = allGroupHeadsCollector.retrieveGroupHeads(maxDoc);\n      long[] bits = fixedBitSet.getBits();\n      OpenBitSet openBitSet = new OpenBitSet(bits, bits.length);\n      qr.setDocSet(new BitDocSet(openBitSet));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"46d8ada1fff8d18cb197c38c7983225162599948","date":1341853497,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      FixedBitSet fixedBitSet = allGroupHeadsCollector.retrieveGroupHeads(maxDoc);\n      long[] bits = fixedBitSet.getBits();\n      OpenBitSet openBitSet = new OpenBitSet(bits, bits.length);\n      qr.setDocSet(new BitDocSet(openBitSet));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      FixedBitSet fixedBitSet = allGroupHeadsCollector.retrieveGroupHeads(maxDoc);\n      long[] bits = fixedBitSet.getBits();\n      OpenBitSet openBitSet = new OpenBitSet(bits, bits.length);\n      qr.setDocSet(new BitDocSet(openBitSet));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      FixedBitSet fixedBitSet = allGroupHeadsCollector.retrieveGroupHeads(maxDoc);\n      long[] bits = fixedBitSet.getBits();\n      OpenBitSet openBitSet = new OpenBitSet(bits, bits.length);\n      qr.setDocSet(new BitDocSet(openBitSet));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      FixedBitSet fixedBitSet = allGroupHeadsCollector.retrieveGroupHeads(maxDoc);\n      long[] bits = fixedBitSet.getBits();\n      OpenBitSet openBitSet = new OpenBitSet(bits, bits.length);\n      qr.setDocSet(new BitDocSet(openBitSet));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"19275ba31e621f6da1b83bf13af75233876fd3d4","date":1374846698,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      FixedBitSet fixedBitSet = allGroupHeadsCollector.retrieveGroupHeads(maxDoc);\n      long[] bits = fixedBitSet.getBits();\n      OpenBitSet openBitSet = new OpenBitSet(bits, bits.length);\n      qr.setDocSet(new BitDocSet(openBitSet));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      FixedBitSet fixedBitSet = allGroupHeadsCollector.retrieveGroupHeads(maxDoc);\n      long[] bits = fixedBitSet.getBits();\n      OpenBitSet openBitSet = new OpenBitSet(bits, bits.length);\n      qr.setDocSet(new BitDocSet(openBitSet));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","date":1376375609,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      FixedBitSet fixedBitSet = allGroupHeadsCollector.retrieveGroupHeads(maxDoc);\n      long[] bits = fixedBitSet.getBits();\n      OpenBitSet openBitSet = new OpenBitSet(bits, bits.length);\n      qr.setDocSet(new BitDocSet(openBitSet));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      FixedBitSet fixedBitSet = allGroupHeadsCollector.retrieveGroupHeads(maxDoc);\n      long[] bits = fixedBitSet.getBits();\n      OpenBitSet openBitSet = new OpenBitSet(bits, bits.length);\n      qr.setDocSet(new BitDocSet(openBitSet));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"27e1ffb161cee77467d6a2f7924df2cc4a832456","date":1390950445,"type":3,"author":"Erik Hatcher","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      FixedBitSet fixedBitSet = allGroupHeadsCollector.retrieveGroupHeads(maxDoc);\n      long[] bits = fixedBitSet.getBits();\n      OpenBitSet openBitSet = new OpenBitSet(bits, bits.length);\n      qr.setDocSet(new BitDocSet(openBitSet));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n\n          if(secondPhaseCollectors instanceof DelegatingCollector) {\n            ((DelegatingCollector) secondPhaseCollectors).finish();\n          }\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      FixedBitSet fixedBitSet = allGroupHeadsCollector.retrieveGroupHeads(maxDoc);\n      long[] bits = fixedBitSet.getBits();\n      OpenBitSet openBitSet = new OpenBitSet(bits, bits.length);\n      qr.setDocSet(new BitDocSet(openBitSet));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a69cf7f1b4cac5d5b1363402b565cd535f13e6a1","date":1392536197,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n\n          if(secondPhaseCollectors instanceof DelegatingCollector) {\n            ((DelegatingCollector) secondPhaseCollectors).finish();\n          }\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      FixedBitSet fixedBitSet = allGroupHeadsCollector.retrieveGroupHeads(maxDoc);\n      long[] bits = fixedBitSet.getBits();\n      OpenBitSet openBitSet = new OpenBitSet(bits, bits.length);\n      qr.setDocSet(new BitDocSet(openBitSet));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n\n          if(secondPhaseCollectors instanceof DelegatingCollector) {\n            ((DelegatingCollector) secondPhaseCollectors).finish();\n          }\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n\n          if(secondPhaseCollectors instanceof DelegatingCollector) {\n            ((DelegatingCollector) secondPhaseCollectors).finish();\n          }\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n\n          if(secondPhaseCollectors instanceof DelegatingCollector) {\n            ((DelegatingCollector) secondPhaseCollectors).finish();\n          }\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae73da626f97850c922c42736f808d0378e165f0","date":1396625460,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc >> 6, maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n\n          if(secondPhaseCollectors instanceof DelegatingCollector) {\n            ((DelegatingCollector) secondPhaseCollectors).finish();\n          }\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n\n          if(secondPhaseCollectors instanceof DelegatingCollector) {\n            ((DelegatingCollector) secondPhaseCollectors).finish();\n          }\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"15e323346eac5e4685c0a9f2df85eb96b4239bbb","date":1396688577,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc >> 6, maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n\n          if(secondPhaseCollectors instanceof DelegatingCollector) {\n            ((DelegatingCollector) secondPhaseCollectors).finish();\n          }\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n\n          if(secondPhaseCollectors instanceof DelegatingCollector) {\n            ((DelegatingCollector) secondPhaseCollectors).finish();\n          }\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a5602d869a2f645f3108ae38f42d7039a7df6de5","date":1429327956,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc >> 6, maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n        if (secondPhaseCollectors instanceof DelegatingCollector) {\n          ((DelegatingCollector) secondPhaseCollectors).finish();\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc >> 6, maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n\n          if(secondPhaseCollectors instanceof DelegatingCollector) {\n            ((DelegatingCollector) secondPhaseCollectors).finish();\n          }\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ca0cc2f173b07ff75ca951e017f5dd1f319fdad0","date":1442027674,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n        if (secondPhaseCollectors instanceof DelegatingCollector) {\n          ((DelegatingCollector) secondPhaseCollectors).finish();\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc >> 6, maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n        if (secondPhaseCollectors instanceof DelegatingCollector) {\n          ((DelegatingCollector) secondPhaseCollectors).finish();\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"de65fb6e86ba37c5409efafefe587d0c897c2764","date":1445143934,"type":3,"author":"Christine Poerschke","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when withinGroupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).withinGroupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).withinGroupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n        if (secondPhaseCollectors instanceof DelegatingCollector) {\n          ((DelegatingCollector) secondPhaseCollectors).finish();\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n        if (secondPhaseCollectors instanceof DelegatingCollector) {\n          ((DelegatingCollector) secondPhaseCollectors).finish();\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5e8a6094f2ecf9edcd3bb224b087580ac0aa13f2","date":1448513807,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when withinGroupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      Sort withinGroupSort = commands.get(0).withinGroupSort;\n      cacheScores = withinGroupSort == null || withinGroupSort.needsScores();\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n        if (secondPhaseCollectors instanceof DelegatingCollector) {\n          ((DelegatingCollector) secondPhaseCollectors).finish();\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when withinGroupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).withinGroupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).withinGroupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n        if (secondPhaseCollectors instanceof DelegatingCollector) {\n          ((DelegatingCollector) secondPhaseCollectors).finish();\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"20e94e61fe5291647346b70437617e6b6c370408","date":1483783127,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when withinGroupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      Sort withinGroupSort = commands.get(0).withinGroupSort;\n      cacheScores = withinGroupSort == null || withinGroupSort.needsScores();\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n        if (secondPhaseCollectors instanceof DelegatingCollector) {\n          ((DelegatingCollector) secondPhaseCollectors).finish();\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when withinGroupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      Sort withinGroupSort = commands.get(0).withinGroupSort;\n      cacheScores = withinGroupSort == null || withinGroupSort.needsScores();\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n        if (secondPhaseCollectors instanceof DelegatingCollector) {\n          ((DelegatingCollector) secondPhaseCollectors).finish();\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"09ab8ee44ca898536770d0106a7c0ee4be4f0eb7","date":1484239864,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when withinGroupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      Sort withinGroupSort = commands.get(0).withinGroupSort;\n      cacheScores = withinGroupSort == null || withinGroupSort.needsScores();\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n        if (secondPhaseCollectors instanceof DelegatingCollector) {\n          ((DelegatingCollector) secondPhaseCollectors).finish();\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when withinGroupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      Sort withinGroupSort = commands.get(0).withinGroupSort;\n      cacheScores = withinGroupSort == null || withinGroupSort.needsScores();\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AbstractAllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n        if (secondPhaseCollectors instanceof DelegatingCollector) {\n          ((DelegatingCollector) secondPhaseCollectors).finish();\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c80f6f4fe2e841ba56e6ce200951063ab91196d3","date":1533052731,"type":3,"author":"Christine Poerschke","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when withinGroupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      Sort withinGroupSort = commands.get(0).withinGroupSort;\n      cacheScores = withinGroupSort == null || withinGroupSort.needsScores();\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            log.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            log.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n        if (secondPhaseCollectors instanceof DelegatingCollector) {\n          ((DelegatingCollector) secondPhaseCollectors).finish();\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when withinGroupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      Sort withinGroupSort = commands.get(0).withinGroupSort;\n      cacheScores = withinGroupSort == null || withinGroupSort.needsScores();\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n        if (secondPhaseCollectors instanceof DelegatingCollector) {\n          ((DelegatingCollector) secondPhaseCollectors).finish();\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54faedfb0e03479a38f5ee82f2dfaeea536e9404","date":1587251295,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when withinGroupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      Sort withinGroupSort = commands.get(0).withinGroupSort;\n      cacheScores = withinGroupSort == null || withinGroupSort.needsScores();\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            if (log.isWarnEnabled()) {\n              log.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            }\n            log.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n        if (secondPhaseCollectors instanceof DelegatingCollector) {\n          ((DelegatingCollector) secondPhaseCollectors).finish();\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when withinGroupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      Sort withinGroupSort = commands.get(0).withinGroupSort;\n      cacheScores = withinGroupSort == null || withinGroupSort.needsScores();\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            log.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            log.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n        if (secondPhaseCollectors instanceof DelegatingCollector) {\n          ((DelegatingCollector) secondPhaseCollectors).finish();\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"575e66bd4b2349209027f6801184da7fc3cba13f","date":1587609169,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when withinGroupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      Sort withinGroupSort = commands.get(0).withinGroupSort;\n      cacheScores = withinGroupSort == null || withinGroupSort.needsScores();\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            log.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            log.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n        if (secondPhaseCollectors instanceof DelegatingCollector) {\n          ((DelegatingCollector) secondPhaseCollectors).finish();\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when withinGroupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      Sort withinGroupSort = commands.get(0).withinGroupSort;\n      cacheScores = withinGroupSort == null || withinGroupSort.needsScores();\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            if (log.isWarnEnabled()) {\n              log.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            }\n            log.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n        if (secondPhaseCollectors instanceof DelegatingCollector) {\n          ((DelegatingCollector) secondPhaseCollectors).finish();\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"640ded7811e1b7d29236a5e2934ec3cd266a8199","date":1588973147,"type":3,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when withinGroupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      Sort withinGroupSort = commands.get(0).withinGroupSort;\n      cacheScores = withinGroupSort == null || withinGroupSort.needsScores();\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            log.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            log.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n        if (secondPhaseCollectors instanceof DelegatingCollector) {\n          ((DelegatingCollector) secondPhaseCollectors).finish();\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore, TotalHits.Relation.EQUAL_TO));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when withinGroupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      Sort withinGroupSort = commands.get(0).withinGroupSort;\n      cacheScores = withinGroupSort == null || withinGroupSort.needsScores();\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            log.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            log.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n        if (secondPhaseCollectors instanceof DelegatingCollector) {\n          ((DelegatingCollector) secondPhaseCollectors).finish();\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"50dfd19525c8d73e856dca6edb64b7aea074037f","date":1591579225,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when withinGroupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      Sort withinGroupSort = commands.get(0).withinGroupSort;\n      cacheScores = withinGroupSort == null || withinGroupSort.needsScores();\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (@SuppressWarnings({\"rawtypes\"})Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (@SuppressWarnings({\"rawtypes\"})Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (@SuppressWarnings({\"rawtypes\"})Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            log.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            log.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n        if (secondPhaseCollectors instanceof DelegatingCollector) {\n          ((DelegatingCollector) secondPhaseCollectors).finish();\n        }\n      }\n    }\n\n    for (@SuppressWarnings({\"rawtypes\"})Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore, TotalHits.Relation.EQUAL_TO));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when withinGroupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      Sort withinGroupSort = commands.get(0).withinGroupSort;\n      cacheScores = withinGroupSort == null || withinGroupSort.needsScores();\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    AllGroupHeadsCollector<?> allGroupHeadsCollector = null;\n    List<Collector> collectors = new ArrayList<>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null) {\n        collectors.add(collector);\n      }\n      if (getGroupedDocSet && allGroupHeadsCollector == null) {\n        collectors.add(allGroupHeadsCollector = cmd.createAllGroupCollector());\n      }\n    }\n\n    DocSetCollector setCollector = null;\n    if (getDocSet && allGroupHeadsCollector == null) {\n      setCollector = new DocSetCollector(maxDoc);\n      collectors.add(setCollector);\n    }\n    Collector allCollectors = MultiCollector.wrap(collectors);\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searchWithTimeLimiter(luceneFilter, allCollectors);\n\n      if(allCollectors instanceof DelegatingCollector) {\n        ((DelegatingCollector) allCollectors).finish();\n      }\n    }\n\n    if (getGroupedDocSet && allGroupHeadsCollector != null) {\n      qr.setDocSet(new BitDocSet(allGroupHeadsCollector.retrieveGroupHeads(maxDoc)));\n    } else if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            log.warn(String.format(Locale.ROOT, \"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            log.warn(\"Please increase cache size or disable group caching.\");\n            searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searchWithTimeLimiter(luceneFilter, secondPhaseCollectors);\n        }\n        if (secondPhaseCollectors instanceof DelegatingCollector) {\n          ((DelegatingCollector) secondPhaseCollectors).finish();\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore, TotalHits.Relation.EQUAL_TO));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"de65fb6e86ba37c5409efafefe587d0c897c2764":["ca0cc2f173b07ff75ca951e017f5dd1f319fdad0"],"ae81ef60a860c551619f7702a13ba15f7dc024f5":["a250ecd3e83b1c6595bcae0474f5b258df42021b"],"640ded7811e1b7d29236a5e2934ec3cd266a8199":["575e66bd4b2349209027f6801184da7fc3cba13f"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":["3738f7d06920ae25ab2884f4efd80c42e95d6271","a250ecd3e83b1c6595bcae0474f5b258df42021b"],"a250ecd3e83b1c6595bcae0474f5b258df42021b":["3738f7d06920ae25ab2884f4efd80c42e95d6271"],"c26f00b574427b55127e869b935845554afde1fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f0b9507caf22f292ac0e5e59f62db4275adf4511"],"ca0cc2f173b07ff75ca951e017f5dd1f319fdad0":["a5602d869a2f645f3108ae38f42d7039a7df6de5"],"19275ba31e621f6da1b83bf13af75233876fd3d4":["2acf500f78aa12b92e371fd89c719291986b6b90"],"2acf500f78aa12b92e371fd89c719291986b6b90":["a250ecd3e83b1c6595bcae0474f5b258df42021b","ae81ef60a860c551619f7702a13ba15f7dc024f5"],"5e8a6094f2ecf9edcd3bb224b087580ac0aa13f2":["de65fb6e86ba37c5409efafefe587d0c897c2764"],"c80f6f4fe2e841ba56e6ce200951063ab91196d3":["20e94e61fe5291647346b70437617e6b6c370408"],"54faedfb0e03479a38f5ee82f2dfaeea536e9404":["c80f6f4fe2e841ba56e6ce200951063ab91196d3"],"f0b9507caf22f292ac0e5e59f62db4275adf4511":["c903c3d15906a3da96b8c0c2fb704491005fdbdb","a258fbb26824fd104ed795e5d9033d2d040049ee"],"15e323346eac5e4685c0a9f2df85eb96b4239bbb":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","ae73da626f97850c922c42736f808d0378e165f0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["a69cf7f1b4cac5d5b1363402b565cd535f13e6a1"],"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec":["2acf500f78aa12b92e371fd89c719291986b6b90","19275ba31e621f6da1b83bf13af75233876fd3d4"],"a5602d869a2f645f3108ae38f42d7039a7df6de5":["ae73da626f97850c922c42736f808d0378e165f0"],"20e94e61fe5291647346b70437617e6b6c370408":["5e8a6094f2ecf9edcd3bb224b087580ac0aa13f2"],"575e66bd4b2349209027f6801184da7fc3cba13f":["54faedfb0e03479a38f5ee82f2dfaeea536e9404"],"09ab8ee44ca898536770d0106a7c0ee4be4f0eb7":["5e8a6094f2ecf9edcd3bb224b087580ac0aa13f2","20e94e61fe5291647346b70437617e6b6c370408"],"46d8ada1fff8d18cb197c38c7983225162599948":["a250ecd3e83b1c6595bcae0474f5b258df42021b","2acf500f78aa12b92e371fd89c719291986b6b90"],"3738f7d06920ae25ab2884f4efd80c42e95d6271":["c26f00b574427b55127e869b935845554afde1fa"],"27e1ffb161cee77467d6a2f7924df2cc4a832456":["19275ba31e621f6da1b83bf13af75233876fd3d4"],"50dfd19525c8d73e856dca6edb64b7aea074037f":["640ded7811e1b7d29236a5e2934ec3cd266a8199"],"a69cf7f1b4cac5d5b1363402b565cd535f13e6a1":["27e1ffb161cee77467d6a2f7924df2cc4a832456"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["a250ecd3e83b1c6595bcae0474f5b258df42021b","2acf500f78aa12b92e371fd89c719291986b6b90"],"ae73da626f97850c922c42736f808d0378e165f0":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["50dfd19525c8d73e856dca6edb64b7aea074037f"]},"commit2Childs":{"de65fb6e86ba37c5409efafefe587d0c897c2764":["5e8a6094f2ecf9edcd3bb224b087580ac0aa13f2"],"ae81ef60a860c551619f7702a13ba15f7dc024f5":["2acf500f78aa12b92e371fd89c719291986b6b90"],"640ded7811e1b7d29236a5e2934ec3cd266a8199":["50dfd19525c8d73e856dca6edb64b7aea074037f"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":[],"a250ecd3e83b1c6595bcae0474f5b258df42021b":["ae81ef60a860c551619f7702a13ba15f7dc024f5","9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","2acf500f78aa12b92e371fd89c719291986b6b90","46d8ada1fff8d18cb197c38c7983225162599948","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"c26f00b574427b55127e869b935845554afde1fa":["3738f7d06920ae25ab2884f4efd80c42e95d6271"],"ca0cc2f173b07ff75ca951e017f5dd1f319fdad0":["de65fb6e86ba37c5409efafefe587d0c897c2764"],"19275ba31e621f6da1b83bf13af75233876fd3d4":["716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","27e1ffb161cee77467d6a2f7924df2cc4a832456"],"2acf500f78aa12b92e371fd89c719291986b6b90":["19275ba31e621f6da1b83bf13af75233876fd3d4","716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","46d8ada1fff8d18cb197c38c7983225162599948","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"5e8a6094f2ecf9edcd3bb224b087580ac0aa13f2":["20e94e61fe5291647346b70437617e6b6c370408","09ab8ee44ca898536770d0106a7c0ee4be4f0eb7"],"c80f6f4fe2e841ba56e6ce200951063ab91196d3":["54faedfb0e03479a38f5ee82f2dfaeea536e9404"],"54faedfb0e03479a38f5ee82f2dfaeea536e9404":["575e66bd4b2349209027f6801184da7fc3cba13f"],"f0b9507caf22f292ac0e5e59f62db4275adf4511":["c26f00b574427b55127e869b935845554afde1fa"],"15e323346eac5e4685c0a9f2df85eb96b4239bbb":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c26f00b574427b55127e869b935845554afde1fa","c903c3d15906a3da96b8c0c2fb704491005fdbdb","a258fbb26824fd104ed795e5d9033d2d040049ee"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["15e323346eac5e4685c0a9f2df85eb96b4239bbb","ae73da626f97850c922c42736f808d0378e165f0"],"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec":[],"a5602d869a2f645f3108ae38f42d7039a7df6de5":["ca0cc2f173b07ff75ca951e017f5dd1f319fdad0"],"20e94e61fe5291647346b70437617e6b6c370408":["c80f6f4fe2e841ba56e6ce200951063ab91196d3","09ab8ee44ca898536770d0106a7c0ee4be4f0eb7"],"575e66bd4b2349209027f6801184da7fc3cba13f":["640ded7811e1b7d29236a5e2934ec3cd266a8199"],"09ab8ee44ca898536770d0106a7c0ee4be4f0eb7":[],"46d8ada1fff8d18cb197c38c7983225162599948":[],"3738f7d06920ae25ab2884f4efd80c42e95d6271":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","a250ecd3e83b1c6595bcae0474f5b258df42021b"],"27e1ffb161cee77467d6a2f7924df2cc4a832456":["a69cf7f1b4cac5d5b1363402b565cd535f13e6a1"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["f0b9507caf22f292ac0e5e59f62db4275adf4511"],"a69cf7f1b4cac5d5b1363402b565cd535f13e6a1":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"50dfd19525c8d73e856dca6edb64b7aea074037f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":[],"ae73da626f97850c922c42736f808d0378e165f0":["15e323346eac5e4685c0a9f2df85eb96b4239bbb","a5602d869a2f645f3108ae38f42d7039a7df6de5"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["f0b9507caf22f292ac0e5e59f62db4275adf4511"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","15e323346eac5e4685c0a9f2df85eb96b4239bbb","716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","09ab8ee44ca898536770d0106a7c0ee4be4f0eb7","46d8ada1fff8d18cb197c38c7983225162599948","fe33227f6805edab2036cbb80645cc4e2d1fa424","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}