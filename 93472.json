{"path":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector#TokenStreamFromTermPositionVector(Terms).mjava","commits":[{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector#TokenStreamFromTermPositionVector(Terms).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector#TokenStreamFromTermPositionVector(TermPositionVector).mjava","sourceNew":"  /**\n   * Constructor.\n   * \n   * @param vector Terms that contains the data for\n   *        creating the TokenStream. Must have positions and offsets.\n   */\n  public TokenStreamFromTermPositionVector(\n      final Terms vector) throws IOException {\n    termAttribute = addAttribute(CharTermAttribute.class);\n    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);\n    offsetAttribute = addAttribute(OffsetAttribute.class);\n    final TermsEnum termsEnum = vector.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while((text = termsEnum.next()) != null) {\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      final OffsetAttribute offsetAtt;\n      if (dpEnum.attributes().hasAttribute(OffsetAttribute.class)) {\n        offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);\n      } else {\n        offsetAtt = null;\n      }\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        Token token;\n        if (offsetAtt != null) {\n          token = new Token(text.utf8ToString(),\n                            offsetAtt.startOffset(),\n                            offsetAtt.endOffset());\n        } else {\n          token = new Token();\n          token.setEmpty().append(text.utf8ToString());\n        }\n        // Yes - this is the position, not the increment! This is for\n        // sorting. This value\n        // will be corrected before use.\n        token.setPositionIncrement(pos);\n        this.positionedTokens.add(token);\n      }\n    }\n    CollectionUtil.mergeSort(this.positionedTokens, tokenComparator);\n    int lastPosition = -1;\n    for (final Token token : this.positionedTokens) {\n      int thisPosition = token.getPositionIncrement();\n      token.setPositionIncrement(thisPosition - lastPosition);\n      lastPosition = thisPosition;\n    }\n    this.tokensAtCurrentPosition = this.positionedTokens.iterator();\n  }\n\n","sourceOld":"  /**\n   * Constructor.\n   * \n   * @param termPositionVector TermPositionVector that contains the data for\n   *        creating the TokenStream. Must have positions and offsets.\n   */\n  public TokenStreamFromTermPositionVector(\n      final TermPositionVector termPositionVector) {\n    termAttribute = addAttribute(CharTermAttribute.class);\n    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);\n    offsetAttribute = addAttribute(OffsetAttribute.class);\n    final BytesRef[] terms = termPositionVector.getTerms();\n    for (int i = 0; i < terms.length; i++) {\n      final TermVectorOffsetInfo[] offsets = termPositionVector.getOffsets(i);\n      final int[] termPositions = termPositionVector.getTermPositions(i);\n      for (int j = 0; j < termPositions.length; j++) {\n        Token token;\n        if (offsets != null) {\n          token = new Token(terms[i].utf8ToString(),\n              offsets[j].getStartOffset(), offsets[j].getEndOffset());\n        } else {\n          token = new Token();\n          token.setEmpty().append(terms[i].utf8ToString());\n        }\n        // Yes - this is the position, not the increment! This is for\n        // sorting. This value\n        // will be corrected before use.\n        token.setPositionIncrement(termPositions[j]);\n        this.positionedTokens.add(token);\n      }\n    }\n    CollectionUtil.mergeSort(this.positionedTokens, tokenComparator);\n    int lastPosition = -1;\n    for (final Token token : this.positionedTokens) {\n      int thisPosition = token.getPositionIncrement();\n      token.setPositionIncrement(thisPosition - lastPosition);\n      lastPosition = thisPosition;\n    }\n    this.tokensAtCurrentPosition = this.positionedTokens.iterator();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31f025ae60076ae95274433f3fe8e6ace2857a87","date":1326669465,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector#TokenStreamFromTermPositionVector(Terms).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector#TokenStreamFromTermPositionVector(Terms).mjava","sourceNew":"  /**\n   * Constructor.\n   * \n   * @param vector Terms that contains the data for\n   *        creating the TokenStream. Must have positions and offsets.\n   */\n  public TokenStreamFromTermPositionVector(\n      final Terms vector) throws IOException {\n    termAttribute = addAttribute(CharTermAttribute.class);\n    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);\n    offsetAttribute = addAttribute(OffsetAttribute.class);\n    final TermsEnum termsEnum = vector.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while((text = termsEnum.next()) != null) {\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum, true);\n      final boolean hasOffsets;\n      if (dpEnum == null) {\n        hasOffsets = false;\n        dpEnum = termsEnum.docsAndPositions(null, dpEnum, false);\n      } else {\n        hasOffsets = true;\n      }\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        Token token;\n        if (hasOffsets) {\n          token = new Token(text.utf8ToString(),\n                            dpEnum.startOffset(),\n                            dpEnum.endOffset());\n        } else {\n          token = new Token();\n          token.setEmpty().append(text.utf8ToString());\n        }\n        // Yes - this is the position, not the increment! This is for\n        // sorting. This value\n        // will be corrected before use.\n        token.setPositionIncrement(pos);\n        this.positionedTokens.add(token);\n      }\n    }\n    CollectionUtil.mergeSort(this.positionedTokens, tokenComparator);\n    int lastPosition = -1;\n    for (final Token token : this.positionedTokens) {\n      int thisPosition = token.getPositionIncrement();\n      token.setPositionIncrement(thisPosition - lastPosition);\n      lastPosition = thisPosition;\n    }\n    this.tokensAtCurrentPosition = this.positionedTokens.iterator();\n  }\n\n","sourceOld":"  /**\n   * Constructor.\n   * \n   * @param vector Terms that contains the data for\n   *        creating the TokenStream. Must have positions and offsets.\n   */\n  public TokenStreamFromTermPositionVector(\n      final Terms vector) throws IOException {\n    termAttribute = addAttribute(CharTermAttribute.class);\n    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);\n    offsetAttribute = addAttribute(OffsetAttribute.class);\n    final TermsEnum termsEnum = vector.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while((text = termsEnum.next()) != null) {\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      final OffsetAttribute offsetAtt;\n      if (dpEnum.attributes().hasAttribute(OffsetAttribute.class)) {\n        offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);\n      } else {\n        offsetAtt = null;\n      }\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        Token token;\n        if (offsetAtt != null) {\n          token = new Token(text.utf8ToString(),\n                            offsetAtt.startOffset(),\n                            offsetAtt.endOffset());\n        } else {\n          token = new Token();\n          token.setEmpty().append(text.utf8ToString());\n        }\n        // Yes - this is the position, not the increment! This is for\n        // sorting. This value\n        // will be corrected before use.\n        token.setPositionIncrement(pos);\n        this.positionedTokens.add(token);\n      }\n    }\n    CollectionUtil.mergeSort(this.positionedTokens, tokenComparator);\n    int lastPosition = -1;\n    for (final Token token : this.positionedTokens) {\n      int thisPosition = token.getPositionIncrement();\n      token.setPositionIncrement(thisPosition - lastPosition);\n      lastPosition = thisPosition;\n    }\n    this.tokensAtCurrentPosition = this.positionedTokens.iterator();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector#TokenStreamFromTermPositionVector(Terms).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector#TokenStreamFromTermPositionVector(Terms).mjava","sourceNew":"  /**\n   * Constructor.\n   * \n   * @param vector Terms that contains the data for\n   *        creating the TokenStream. Must have positions and offsets.\n   */\n  public TokenStreamFromTermPositionVector(\n      final Terms vector) throws IOException {\n    termAttribute = addAttribute(CharTermAttribute.class);\n    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);\n    offsetAttribute = addAttribute(OffsetAttribute.class);\n    final TermsEnum termsEnum = vector.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while((text = termsEnum.next()) != null) {\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum, true);\n      final boolean hasOffsets;\n      if (dpEnum == null) {\n        hasOffsets = false;\n        dpEnum = termsEnum.docsAndPositions(null, dpEnum, false);\n      } else {\n        hasOffsets = true;\n      }\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        Token token;\n        if (hasOffsets) {\n          token = new Token(text.utf8ToString(),\n                            dpEnum.startOffset(),\n                            dpEnum.endOffset());\n        } else {\n          token = new Token();\n          token.setEmpty().append(text.utf8ToString());\n        }\n        // Yes - this is the position, not the increment! This is for\n        // sorting. This value\n        // will be corrected before use.\n        token.setPositionIncrement(pos);\n        this.positionedTokens.add(token);\n      }\n    }\n    CollectionUtil.mergeSort(this.positionedTokens, tokenComparator);\n    int lastPosition = -1;\n    for (final Token token : this.positionedTokens) {\n      int thisPosition = token.getPositionIncrement();\n      token.setPositionIncrement(thisPosition - lastPosition);\n      lastPosition = thisPosition;\n    }\n    this.tokensAtCurrentPosition = this.positionedTokens.iterator();\n  }\n\n","sourceOld":"  /**\n   * Constructor.\n   * \n   * @param vector Terms that contains the data for\n   *        creating the TokenStream. Must have positions and offsets.\n   */\n  public TokenStreamFromTermPositionVector(\n      final Terms vector) throws IOException {\n    termAttribute = addAttribute(CharTermAttribute.class);\n    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);\n    offsetAttribute = addAttribute(OffsetAttribute.class);\n    final TermsEnum termsEnum = vector.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while((text = termsEnum.next()) != null) {\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum, true);\n      final boolean hasOffsets;\n      if (dpEnum == null) {\n        hasOffsets = false;\n        dpEnum = termsEnum.docsAndPositions(null, dpEnum, false);\n      } else {\n        hasOffsets = true;\n      }\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for (int j = 0; j < freq; j++) {\n        int pos = dpEnum.nextPosition();\n        Token token;\n        if (hasOffsets) {\n          token = new Token(text.utf8ToString(),\n                            dpEnum.startOffset(),\n                            dpEnum.endOffset());\n        } else {\n          token = new Token();\n          token.setEmpty().append(text.utf8ToString());\n        }\n        // Yes - this is the position, not the increment! This is for\n        // sorting. This value\n        // will be corrected before use.\n        token.setPositionIncrement(pos);\n        this.positionedTokens.add(token);\n      }\n    }\n    CollectionUtil.mergeSort(this.positionedTokens, tokenComparator);\n    int lastPosition = -1;\n    for (final Token token : this.positionedTokens) {\n      int thisPosition = token.getPositionIncrement();\n      token.setPositionIncrement(thisPosition - lastPosition);\n      lastPosition = thisPosition;\n    }\n    this.tokensAtCurrentPosition = this.positionedTokens.iterator();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cc749c053615f5871f3b95715fe292f34e70a53":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["3cc749c053615f5871f3b95715fe292f34e70a53"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3cc749c053615f5871f3b95715fe292f34e70a53"],"3cc749c053615f5871f3b95715fe292f34e70a53":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}