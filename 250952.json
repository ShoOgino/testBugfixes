{"path":"lucene/core/src/test/org/apache/lucene/util/compress/LZ4TestCase#doTest(byte[],int,int,LZ4.HashTable).mjava","commits":[{"id":"9407318969e8504257b4c5764c65755a043e5404","date":1579873617,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/util/compress/LZ4TestCase#doTest(byte[],int,int,LZ4.HashTable).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/codecs/compressing/AbstractTestLZ4CompressionMode#test(byte[]).mjava","sourceNew":"  private void doTest(byte[] data, int offset, int length, LZ4.HashTable hashTable) throws IOException {\n    ByteBuffersDataOutput out = new ByteBuffersDataOutput();\n    LZ4.compress(data, offset, length, out, hashTable);\n    byte[] compressed = out.toArrayCopy();\n\n    int off = 0;\n    int decompressedOff = 0;\n    for (;;) {\n      final int token = compressed[off++] & 0xFF;\n      int literalLen = token >>> 4;\n      if (literalLen == 0x0F) {\n        while (compressed[off] == (byte) 0xFF) {\n          literalLen += 0xFF;\n          ++off;\n        }\n        literalLen += compressed[off++] & 0xFF;\n      }\n      // skip literals\n      off += literalLen;\n      decompressedOff += literalLen;\n\n      // check that the stream ends with literals and that there are at least\n      // 5 of them\n      if (off == compressed.length) {\n        assertEquals(length, decompressedOff);\n        assertTrue(\"lastLiterals=\" + literalLen + \", bytes=\" + length,\n            literalLen >= LZ4.LAST_LITERALS || literalLen == length);\n        break;\n      }\n\n      final int matchDec = (compressed[off++] & 0xFF) | ((compressed[off++] & 0xFF) << 8);\n      // check that match dec is not 0\n      assertTrue(matchDec + \" \" + decompressedOff, matchDec > 0 && matchDec <= decompressedOff);\n\n      int matchLen = token & 0x0F;\n      if (matchLen == 0x0F) {\n        while (compressed[off] == (byte) 0xFF) {\n          matchLen += 0xFF;\n          ++off;\n        }\n        matchLen += compressed[off++] & 0xFF;\n      }\n      matchLen += LZ4.MIN_MATCH;\n\n      // if the match ends prematurely, the next sequence should not have\n      // literals or this means we are wasting space\n      if (decompressedOff + matchLen < length - LZ4.LAST_LITERALS) {\n        final boolean moreCommonBytes = data[offset + decompressedOff + matchLen] == data[offset + decompressedOff - matchDec + matchLen];\n        final boolean nextSequenceHasLiterals = ((compressed[off] & 0xFF) >>> 4) != 0;\n        assertTrue(moreCommonBytes == false || nextSequenceHasLiterals == false);\n      }      \n\n      decompressedOff += matchLen;\n    }\n    assertEquals(length, decompressedOff);\n\n    // Compress once again with the same hash table to test reuse\n    ByteBuffersDataOutput out2 = new ByteBuffersDataOutput();\n    LZ4.compress(data, offset, length, out2, hashTable);\n    assertArrayEquals(compressed, out2.toArrayCopy());\n  }\n\n","sourceOld":"  @Override\n  public byte[] test(byte[] decompressed) throws IOException {\n    final byte[] compressed = super.test(decompressed);\n    int off = 0;\n    int decompressedOff = 0;\n    for (;;) {\n      final int token = compressed[off++] & 0xFF;\n      int literalLen = token >>> 4;\n      if (literalLen == 0x0F) {\n        while (compressed[off] == (byte) 0xFF) {\n          literalLen += 0xFF;\n          ++off;\n        }\n        literalLen += compressed[off++] & 0xFF;\n      }\n      // skip literals\n      off += literalLen;\n      decompressedOff += literalLen;\n\n      // check that the stream ends with literals and that there are at least\n      // 5 of them\n      if (off == compressed.length) {\n        assertEquals(decompressed.length, decompressedOff);\n        assertTrue(\"lastLiterals=\" + literalLen + \", bytes=\" + decompressed.length,\n            literalLen >= LZ4.LAST_LITERALS || literalLen == decompressed.length);\n        break;\n      }\n\n      final int matchDec = (compressed[off++] & 0xFF) | ((compressed[off++] & 0xFF) << 8);\n      // check that match dec is not 0\n      assertTrue(matchDec + \" \" + decompressedOff, matchDec > 0 && matchDec <= decompressedOff);\n\n      int matchLen = token & 0x0F;\n      if (matchLen == 0x0F) {\n        while (compressed[off] == (byte) 0xFF) {\n          matchLen += 0xFF;\n          ++off;\n        }\n        matchLen += compressed[off++] & 0xFF;\n      }\n      matchLen += LZ4.MIN_MATCH;\n\n      // if the match ends prematurely, the next sequence should not have\n      // literals or this means we are wasting space\n      if (decompressedOff + matchLen < decompressed.length - LZ4.LAST_LITERALS) {\n        final boolean moreCommonBytes = decompressed[decompressedOff + matchLen] == decompressed[decompressedOff - matchDec + matchLen];\n        final boolean nextSequenceHasLiterals = ((compressed[off] & 0xFF) >>> 4) != 0;\n        assertTrue(!moreCommonBytes || !nextSequenceHasLiterals);\n      }      \n\n      decompressedOff += matchLen;\n    }\n    assertEquals(decompressed.length, decompressedOff);\n    return compressed;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e673a608cacc7de01f6ad30a0fc89a889a2cca86","date":1582885616,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/util/compress/LZ4TestCase#doTest(byte[],int,int,LZ4.HashTable).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/util/compress/LZ4TestCase#doTest(byte[],int,int,LZ4.HashTable).mjava","sourceNew":"  private void doTest(byte[] data, int offset, int length, LZ4.HashTable hashTable) throws IOException {\n    ByteBuffersDataOutput out = new ByteBuffersDataOutput();\n    LZ4.compress(data, offset, length, out, hashTable);\n    byte[] compressed = out.toArrayCopy();\n\n    int off = 0;\n    int decompressedOff = 0;\n    for (;;) {\n      final int token = compressed[off++] & 0xFF;\n      int literalLen = token >>> 4;\n      if (literalLen == 0x0F) {\n        while (compressed[off] == (byte) 0xFF) {\n          literalLen += 0xFF;\n          ++off;\n        }\n        literalLen += compressed[off++] & 0xFF;\n      }\n      // skip literals\n      off += literalLen;\n      decompressedOff += literalLen;\n\n      // check that the stream ends with literals and that there are at least\n      // 5 of them\n      if (off == compressed.length) {\n        assertEquals(length, decompressedOff);\n        assertTrue(\"lastLiterals=\" + literalLen + \", bytes=\" + length,\n            literalLen >= LZ4.LAST_LITERALS || literalLen == length);\n        break;\n      }\n\n      final int matchDec = (compressed[off++] & 0xFF) | ((compressed[off++] & 0xFF) << 8);\n      // check that match dec is not 0\n      assertTrue(matchDec + \" \" + decompressedOff, matchDec > 0 && matchDec <= decompressedOff);\n\n      int matchLen = token & 0x0F;\n      if (matchLen == 0x0F) {\n        while (compressed[off] == (byte) 0xFF) {\n          matchLen += 0xFF;\n          ++off;\n        }\n        matchLen += compressed[off++] & 0xFF;\n      }\n      matchLen += LZ4.MIN_MATCH;\n\n      // if the match ends prematurely, the next sequence should not have\n      // literals or this means we are wasting space\n      if (decompressedOff + matchLen < length - LZ4.LAST_LITERALS) {\n        final boolean moreCommonBytes = data[offset + decompressedOff + matchLen] == data[offset + decompressedOff - matchDec + matchLen];\n        final boolean nextSequenceHasLiterals = ((compressed[off] & 0xFF) >>> 4) != 0;\n        assertTrue(moreCommonBytes == false || nextSequenceHasLiterals == false);\n      }      \n\n      decompressedOff += matchLen;\n    }\n    assertEquals(length, decompressedOff);\n\n    // Compress once again with the same hash table to test reuse\n    ByteBuffersDataOutput out2 = new ByteBuffersDataOutput();\n    LZ4.compress(data, offset, length, out2, hashTable);\n    assertArrayEquals(compressed, out2.toArrayCopy());\n\n    // Now restore and compare bytes\n    byte[] restored = new byte[length + random().nextInt(10)];\n    LZ4.decompress(new ByteArrayDataInput(compressed), length, restored);\n    assertArrayEquals(ArrayUtil.copyOfSubArray(data, offset, offset+length), ArrayUtil.copyOfSubArray(restored, 0, length));\n  }\n\n","sourceOld":"  private void doTest(byte[] data, int offset, int length, LZ4.HashTable hashTable) throws IOException {\n    ByteBuffersDataOutput out = new ByteBuffersDataOutput();\n    LZ4.compress(data, offset, length, out, hashTable);\n    byte[] compressed = out.toArrayCopy();\n\n    int off = 0;\n    int decompressedOff = 0;\n    for (;;) {\n      final int token = compressed[off++] & 0xFF;\n      int literalLen = token >>> 4;\n      if (literalLen == 0x0F) {\n        while (compressed[off] == (byte) 0xFF) {\n          literalLen += 0xFF;\n          ++off;\n        }\n        literalLen += compressed[off++] & 0xFF;\n      }\n      // skip literals\n      off += literalLen;\n      decompressedOff += literalLen;\n\n      // check that the stream ends with literals and that there are at least\n      // 5 of them\n      if (off == compressed.length) {\n        assertEquals(length, decompressedOff);\n        assertTrue(\"lastLiterals=\" + literalLen + \", bytes=\" + length,\n            literalLen >= LZ4.LAST_LITERALS || literalLen == length);\n        break;\n      }\n\n      final int matchDec = (compressed[off++] & 0xFF) | ((compressed[off++] & 0xFF) << 8);\n      // check that match dec is not 0\n      assertTrue(matchDec + \" \" + decompressedOff, matchDec > 0 && matchDec <= decompressedOff);\n\n      int matchLen = token & 0x0F;\n      if (matchLen == 0x0F) {\n        while (compressed[off] == (byte) 0xFF) {\n          matchLen += 0xFF;\n          ++off;\n        }\n        matchLen += compressed[off++] & 0xFF;\n      }\n      matchLen += LZ4.MIN_MATCH;\n\n      // if the match ends prematurely, the next sequence should not have\n      // literals or this means we are wasting space\n      if (decompressedOff + matchLen < length - LZ4.LAST_LITERALS) {\n        final boolean moreCommonBytes = data[offset + decompressedOff + matchLen] == data[offset + decompressedOff - matchDec + matchLen];\n        final boolean nextSequenceHasLiterals = ((compressed[off] & 0xFF) >>> 4) != 0;\n        assertTrue(moreCommonBytes == false || nextSequenceHasLiterals == false);\n      }      \n\n      decompressedOff += matchLen;\n    }\n    assertEquals(length, decompressedOff);\n\n    // Compress once again with the same hash table to test reuse\n    ByteBuffersDataOutput out2 = new ByteBuffersDataOutput();\n    LZ4.compress(data, offset, length, out2, hashTable);\n    assertArrayEquals(compressed, out2.toArrayCopy());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9aefce86de8b17eed91ab011fb54d704d91102ef","date":1599128224,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/util/compress/LZ4TestCase#doTest(byte[],int,int,LZ4.HashTable).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/util/compress/LZ4TestCase#doTest(byte[],int,int,LZ4.HashTable).mjava","sourceNew":"  private void doTest(byte[] data, int offset, int length, LZ4.HashTable hashTable) throws IOException {\n    ByteBuffersDataOutput out = new ByteBuffersDataOutput();\n    LZ4.compress(data, offset, length, out, hashTable);\n    byte[] compressed = out.toArrayCopy();\n\n    int off = 0;\n    int decompressedOff = 0;\n    for (;;) {\n      final int token = compressed[off++] & 0xFF;\n      int literalLen = token >>> 4;\n      if (literalLen == 0x0F) {\n        while (compressed[off] == (byte) 0xFF) {\n          literalLen += 0xFF;\n          ++off;\n        }\n        literalLen += compressed[off++] & 0xFF;\n      }\n      // skip literals\n      off += literalLen;\n      decompressedOff += literalLen;\n\n      // check that the stream ends with literals and that there are at least\n      // 5 of them\n      if (off == compressed.length) {\n        assertEquals(length, decompressedOff);\n        assertTrue(\"lastLiterals=\" + literalLen + \", bytes=\" + length,\n            literalLen >= LZ4.LAST_LITERALS || literalLen == length);\n        break;\n      }\n\n      final int matchDec = (compressed[off++] & 0xFF) | ((compressed[off++] & 0xFF) << 8);\n      // check that match dec is not 0\n      assertTrue(matchDec + \" \" + decompressedOff, matchDec > 0 && matchDec <= decompressedOff);\n\n      int matchLen = token & 0x0F;\n      if (matchLen == 0x0F) {\n        while (compressed[off] == (byte) 0xFF) {\n          matchLen += 0xFF;\n          ++off;\n        }\n        matchLen += compressed[off++] & 0xFF;\n      }\n      matchLen += LZ4.MIN_MATCH;\n\n      // if the match ends prematurely, the next sequence should not have\n      // literals or this means we are wasting space\n      if (decompressedOff + matchLen < length - LZ4.LAST_LITERALS) {\n        final boolean moreCommonBytes = data[offset + decompressedOff + matchLen] == data[offset + decompressedOff - matchDec + matchLen];\n        final boolean nextSequenceHasLiterals = ((compressed[off] & 0xFF) >>> 4) != 0;\n        assertTrue(moreCommonBytes == false || nextSequenceHasLiterals == false);\n      }      \n\n      decompressedOff += matchLen;\n    }\n    assertEquals(length, decompressedOff);\n\n    // Compress once again with the same hash table to test reuse\n    ByteBuffersDataOutput out2 = new ByteBuffersDataOutput();\n    LZ4.compress(data, offset, length, out2, hashTable);\n    assertArrayEquals(compressed, out2.toArrayCopy());\n\n    // Now restore and compare bytes\n    byte[] restored = new byte[length + random().nextInt(10)];\n    LZ4.decompress(new ByteArrayDataInput(compressed), length, restored, 0);\n    assertArrayEquals(ArrayUtil.copyOfSubArray(data, offset, offset+length), ArrayUtil.copyOfSubArray(restored, 0, length));\n\n    // Now restore with an offset\n    int restoreOffset = TestUtil.nextInt(random(), 1, 10);\n    restored = new byte[restoreOffset + length + random().nextInt(10)];\n    LZ4.decompress(new ByteArrayDataInput(compressed), length, restored, restoreOffset);\n    assertArrayEquals(ArrayUtil.copyOfSubArray(data, offset, offset+length), ArrayUtil.copyOfSubArray(restored, restoreOffset, restoreOffset+length));\n  }\n\n","sourceOld":"  private void doTest(byte[] data, int offset, int length, LZ4.HashTable hashTable) throws IOException {\n    ByteBuffersDataOutput out = new ByteBuffersDataOutput();\n    LZ4.compress(data, offset, length, out, hashTable);\n    byte[] compressed = out.toArrayCopy();\n\n    int off = 0;\n    int decompressedOff = 0;\n    for (;;) {\n      final int token = compressed[off++] & 0xFF;\n      int literalLen = token >>> 4;\n      if (literalLen == 0x0F) {\n        while (compressed[off] == (byte) 0xFF) {\n          literalLen += 0xFF;\n          ++off;\n        }\n        literalLen += compressed[off++] & 0xFF;\n      }\n      // skip literals\n      off += literalLen;\n      decompressedOff += literalLen;\n\n      // check that the stream ends with literals and that there are at least\n      // 5 of them\n      if (off == compressed.length) {\n        assertEquals(length, decompressedOff);\n        assertTrue(\"lastLiterals=\" + literalLen + \", bytes=\" + length,\n            literalLen >= LZ4.LAST_LITERALS || literalLen == length);\n        break;\n      }\n\n      final int matchDec = (compressed[off++] & 0xFF) | ((compressed[off++] & 0xFF) << 8);\n      // check that match dec is not 0\n      assertTrue(matchDec + \" \" + decompressedOff, matchDec > 0 && matchDec <= decompressedOff);\n\n      int matchLen = token & 0x0F;\n      if (matchLen == 0x0F) {\n        while (compressed[off] == (byte) 0xFF) {\n          matchLen += 0xFF;\n          ++off;\n        }\n        matchLen += compressed[off++] & 0xFF;\n      }\n      matchLen += LZ4.MIN_MATCH;\n\n      // if the match ends prematurely, the next sequence should not have\n      // literals or this means we are wasting space\n      if (decompressedOff + matchLen < length - LZ4.LAST_LITERALS) {\n        final boolean moreCommonBytes = data[offset + decompressedOff + matchLen] == data[offset + decompressedOff - matchDec + matchLen];\n        final boolean nextSequenceHasLiterals = ((compressed[off] & 0xFF) >>> 4) != 0;\n        assertTrue(moreCommonBytes == false || nextSequenceHasLiterals == false);\n      }      \n\n      decompressedOff += matchLen;\n    }\n    assertEquals(length, decompressedOff);\n\n    // Compress once again with the same hash table to test reuse\n    ByteBuffersDataOutput out2 = new ByteBuffersDataOutput();\n    LZ4.compress(data, offset, length, out2, hashTable);\n    assertArrayEquals(compressed, out2.toArrayCopy());\n\n    // Now restore and compare bytes\n    byte[] restored = new byte[length + random().nextInt(10)];\n    LZ4.decompress(new ByteArrayDataInput(compressed), length, restored);\n    assertArrayEquals(ArrayUtil.copyOfSubArray(data, offset, offset+length), ArrayUtil.copyOfSubArray(restored, 0, length));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e673a608cacc7de01f6ad30a0fc89a889a2cca86":["9407318969e8504257b4c5764c65755a043e5404"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9aefce86de8b17eed91ab011fb54d704d91102ef":["e673a608cacc7de01f6ad30a0fc89a889a2cca86"],"9407318969e8504257b4c5764c65755a043e5404":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9aefce86de8b17eed91ab011fb54d704d91102ef"]},"commit2Childs":{"e673a608cacc7de01f6ad30a0fc89a889a2cca86":["9aefce86de8b17eed91ab011fb54d704d91102ef"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9407318969e8504257b4c5764c65755a043e5404"],"9407318969e8504257b4c5764c65755a043e5404":["e673a608cacc7de01f6ad30a0fc89a889a2cca86"],"9aefce86de8b17eed91ab011fb54d704d91102ef":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}