{"path":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, mergedSegment);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(true, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, true), IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    TermDocs termDocs = mergedReader.termDocs(new Term(DocHelper.TEXT_FIELD_2_KEY, \"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.next() == true);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    String [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i];\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, mergedSegment);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(true, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, true), IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    TermDocs termDocs = mergedReader.termDocs(new Term(DocHelper.TEXT_FIELD_2_KEY, \"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.next() == true);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    String [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i];\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, true,\n        -1, null, false, merger.hasProx(), merger.getCodec()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, null);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    TermDocs termDocs = mergedReader.termDocs(new Term(DocHelper.TEXT_FIELD_2_KEY, \"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.next() == true);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    String [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i];\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, mergedSegment);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(true, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, true), IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    TermDocs termDocs = mergedReader.termDocs(new Term(DocHelper.TEXT_FIELD_2_KEY, \"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.next() == true);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    String [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i];\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"433777d1eaf9998136cd16515dc0e1eb26f5d535","date":1273839120,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, true,\n        -1, null, false, merger.hasProx(), merger.getCodec()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, null);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    TermDocs termDocs = mergedReader.termDocs(new Term(DocHelper.TEXT_FIELD_2_KEY, \"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.next() == true);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    String [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i];\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, true,\n        -1, null, false, merger.hasProx(), merger.getCodec()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, null);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    TermDocs termDocs = mergedReader.termDocs(new Term(DocHelper.TEXT_FIELD_2_KEY, \"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.next() == true);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    String [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i];\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6267e1ce56c2eec111425690cd04e251b6f14952","date":1275222352,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getCodec()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, null);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    TermDocs termDocs = mergedReader.termDocs(new Term(DocHelper.TEXT_FIELD_2_KEY, \"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.next() == true);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    String [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i];\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, true,\n        -1, null, false, merger.hasProx(), merger.getCodec()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, null);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    TermDocs termDocs = mergedReader.termDocs(new Term(DocHelper.TEXT_FIELD_2_KEY, \"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.next() == true);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    String [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i];\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28427ef110c4c5bf5b4057731b83110bd1e13724","date":1276701452,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getCodec()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, null);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    String [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i];\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getCodec()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, null);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    TermDocs termDocs = mergedReader.termDocs(new Term(DocHelper.TEXT_FIELD_2_KEY, \"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.next() == true);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    String [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i];\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4f29ba80b723649f5feb7e37afe1a558dd2c1304","date":1278318805,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getCodec()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, null);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getCodec()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, null);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    String [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i];\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getCodec()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, null);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getCodec()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, null);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    TermDocs termDocs = mergedReader.termDocs(new Term(DocHelper.TEXT_FIELD_2_KEY, \"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.next() == true);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    String [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i];\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"833a7987bc1c94455fde83e3311f72bddedcfb93","date":1279951470,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, \n        merger.hasProx(), merger.getCodec()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, null);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getCodec()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, null);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c","date":1281646583,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getCodec()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, null);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getCodec()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, null);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a493e6d0c3ad86bd55c0a1360d110142e948f2bd","date":1289406991,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getSegmentCodecs()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getCodec()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, null);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"85a883878c0af761245ab048babc63d099f835f3","date":1289553330,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getSegmentCodecs()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getCodec()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, null);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7f367dfb9086b92a13c77e2d31112c715cd4502c","date":1290190924,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getSegmentCodecs()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getSegmentCodecs()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4e8cc373c801e54cec75daf9f52792cb4b17f536","date":1291116159,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getSegmentCodecs()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getSegmentCodecs()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getSegmentCodecs()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getSegmentCodecs()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e1cbd7e289dc1243c7a59e1a83d078163a147fe","date":1292268032,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n                                                                                     null, false, merger.hasProx(), merger.getSegmentCodecs(), merger.hasVectors()),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getSegmentCodecs()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e06c9d5fba0a2f937941d199d64ccb32aac502d1","date":1292411167,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n                                                                                     null, false, merger.fieldInfos().hasProx(),\n                                                                                     merger.getSegmentCodecs(), merger.fieldInfos().hasVectors()),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n                                                                                     null, false, merger.hasProx(), merger.getSegmentCodecs(), merger.hasVectors()),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5","date":1292695408,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, merger.fieldInfos().hasProx(),\n                                                                                     merger.getSegmentCodecs(), merger.fieldInfos().hasVectors()),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n                                                                                     null, false, merger.fieldInfos().hasProx(),\n                                                                                     merger.getSegmentCodecs(), merger.fieldInfos().hasVectors()),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"94cb8b3ec0439dfd8e179637ee4191cd9c6227e5","date":1292711882,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, merger.fieldInfos().hasProx(),\n                                                                                     merger.getSegmentCodecs(), merger.fieldInfos().hasVectors()),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, merger.fieldInfos().hasProx(),\n                                                                                     merger.getSegmentCodecs(), merger.fieldInfos().hasVectors()),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, merger.fieldInfos().hasProx(),\n                                                                                     merger.getSegmentCodecs(), merger.fieldInfos().hasVectors()),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, -1,\n        null, false, merger.hasProx(), merger.getSegmentCodecs()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, merger.fieldInfos().hasProx(),\n                                                                                     merger.getSegmentCodecs(), merger.fieldInfos().hasVectors()),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriter.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null);\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    merger.closeReaders();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, \n        merger.hasProx(), merger.getCodec()), BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, null);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0c7a8f7304b75b1528814c5820fa23a96816c27","date":1298314239,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), merger.fieldInfos()),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, merger.fieldInfos().hasProx(),\n                                                                                     merger.getSegmentCodecs(), merger.fieldInfos().hasVectors()),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), merger.fieldInfos()),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, merger.fieldInfos().hasProx(),\n                                                                                     merger.getSegmentCodecs(), merger.fieldInfos().hasVectors()),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"14ec33385f6fbb6ce172882d14605790418a5d31","date":1298910796,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, merger.fieldInfos().hasProx(),\n                                                                                     merger.getSegmentCodecs(), merger.fieldInfos().hasVectors()),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), merger.fieldInfos()),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1224a4027481acce15495b03bce9b48b93b42722","date":1300792329,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, fieldInfos.hasProx(),\n                                                                                     merger.getSegmentCodecs(), fieldInfos.hasVectors(), fieldInfos),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, merger.fieldInfos().hasProx(),\n                                                                                     merger.getSegmentCodecs(), merger.fieldInfos().hasVectors()),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, fieldInfos.hasProx(),\n                                                                                     merger.getSegmentCodecs(), fieldInfos.hasVectors(), fieldInfos),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, merger.fieldInfos().hasProx(),\n                                                                                     merger.getSegmentCodecs(), merger.fieldInfos().hasVectors()),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0ef0193974807e4bddf5432a6b0287fe4d6c9df","date":1301476645,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, fieldInfos.hasProx(),\n                                                                                     merger.getSegmentCodecs(), fieldInfos.hasVectors(), fieldInfos),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), merger.fieldInfos()),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, fieldInfos.hasProx(),\n                                                                                     merger.getSegmentCodecs(), fieldInfos.hasVectors(), fieldInfos),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, fieldInfos.hasProx(),\n                                                                                     merger.getSegmentCodecs(), fieldInfos.hasVectors(), fieldInfos),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, fieldInfos.hasProx(),\n                                                                                     merger.getSegmentCodecs(), fieldInfos.hasVectors(), fieldInfos),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, fieldInfos.hasProx(),\n                                                                                     merger.getSegmentCodecs(), fieldInfos.hasVectors(), fieldInfos),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3fe2fc74577855eadfb5eae3153c2fffdaaf791","date":1305237079,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), fieldInfos),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, fieldInfos.hasProx(),\n                                                                                     merger.getSegmentCodecs(), fieldInfos.hasVectors(), fieldInfos),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), fieldInfos),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, fieldInfos.hasProx(),\n                                                                                     merger.getSegmentCodecs(), fieldInfos.hasVectors(), fieldInfos),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), fieldInfos),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {                             \n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, CodecProvider.getDefault(), null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false, fieldInfos.hasProx(),\n                                                                                     merger.getSegmentCodecs(), fieldInfos.hasVectors(), fieldInfos),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n    \n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    \n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n    \n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n    \n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }    \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"639c36565ce03aed5b0fce7c9e4448e53a1f7efd","date":1308580104,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, IOContext.DEFAULT);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), fieldInfos),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b6f9be74ca7baaef11857ad002cad40419979516","date":1309449808,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, null, new FieldInfos(), newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, IOContext.DEFAULT);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","date":1309960478,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), fieldInfos),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getLiveDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), fieldInfos),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), fieldInfos),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getLiveDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), fieldInfos),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, null, new FieldInfos(), newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getLiveDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, null, new FieldInfos(), newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getDeletedDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ddc4c914be86e34b54f70023f45a60fa7f04e929","date":1310115160,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, null, new FieldInfos(), newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getLiveDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), fieldInfos),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getLiveDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5d004d0e0b3f65bb40da76d476d659d7888270e8","date":1310158940,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, null, new FieldInfos(), newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getLiveDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, null, new FieldInfos());\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), fieldInfos),\n                                                   BufferedIndexInput.BUFFER_SIZE, true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getLiveDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, null, new FieldInfos(), Codec.getDefault(), newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getCodec(), fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getLiveDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, null, new FieldInfos(), newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getSegmentCodecs(), fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getLiveDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getLiveDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    SegmentMerger merger = new SegmentMerger(mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, null, null, new FieldInfos(), Codec.getDefault(), newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    int docsMerged = merger.merge();\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = merger.fieldInfos();\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     merger.getCodec(), fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getLiveDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getLiveDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getLiveDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    TermFreqVector vector = mergedReader.getTermFreqVector(0, DocHelper.TEXT_FIELD_2_KEY);\n    assertTrue(vector != null);\n    BytesRef [] terms = vector.getTerms();\n    assertTrue(terms != null);\n    //System.out.println(\"Terms size: \" + terms.length);\n    assertTrue(terms.length == 3);\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(freqs != null);\n    //System.out.println(\"Freqs size: \" + freqs.length);\n    assertTrue(vector instanceof TermPositionVector == true);\n\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      int freq = freqs[i];\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"872cff1d3a554e0cd64014cd97f88d3002b0f491","date":1323024658,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getLiveDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":["02331260bb246364779cb6f04919ca47900d01bb"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b65b350ca9588f9fc76ce7d6804160d06c45ff42","date":1323026297,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = MultiFields.getTermDocsEnum(mergedReader,\n                                                    MultiFields.getLiveDocs(mergedReader),\n                                                    DocHelper.TEXT_FIELD_2_KEY,\n                                                    new BytesRef(\"field\"));\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cb4972c6aaf6c714c8f5957b5aeb14dcce34b75f","date":1323210518,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.getRW(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3615ce4a1f785ae1b779244de52c6a7d99227e60","date":1323422019,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.getRW(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","date":1323437438,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.getRW(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.get(false, mergedDir, new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9ce667c6d3400b22523701c549c0d35e26da8b46","date":1324405053,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = SegmentReader.getRW(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   true, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3321cfbf7f8aba27e37e7a4d6901531a97ac2b06","date":1326148180,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    Collection<String> stored = mergedReader.getFieldNames(IndexReader.FieldOption.INDEXED_WITH_TERMVECTOR);\n    assertTrue(stored != null);\n    //System.out.println(\"stored size: \" + stored.size());\n    assertTrue(\"We do not have 3 fields that were indexed with term vector\",stored.size() == 3);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"913fa4c710b6d1168655966e59f0f4de566907a8","date":1327858476,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSegmentMerger#testMerge().mjava","sourceNew":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","sourceOld":"  public void testMerge() throws IOException {\n    final Codec codec = Codec.getDefault();\n    SegmentMerger merger = new SegmentMerger(InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, mergedSegment, MergeState.CheckAbort.NONE, null, new FieldInfos(new FieldInfos.FieldNumberBiMap()), codec, newIOContext(random));\n    merger.add(reader1);\n    merger.add(reader2);\n    MergeState mergeState = merger.merge();\n    int docsMerged = mergeState.mergedDocCount;\n    assertTrue(docsMerged == 2);\n    final FieldInfos fieldInfos = mergeState.fieldInfos;\n    //Should be able to open a new SegmentReader against the new directory\n    SegmentReader mergedReader = new SegmentReader(new SegmentInfo(mergedSegment, docsMerged, mergedDir, false,\n                                                                                     codec, fieldInfos),\n                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n    assertTrue(mergedReader != null);\n    assertTrue(mergedReader.numDocs() == 2);\n    Document newDoc1 = mergedReader.document(0);\n    assertTrue(newDoc1 != null);\n    //There are 2 unstored fields on the document\n    assertTrue(DocHelper.numFields(newDoc1) == DocHelper.numFields(doc1) - DocHelper.unstored.size());\n    Document newDoc2 = mergedReader.document(1);\n    assertTrue(newDoc2 != null);\n    assertTrue(DocHelper.numFields(newDoc2) == DocHelper.numFields(doc2) - DocHelper.unstored.size());\n\n    DocsEnum termDocs = _TestUtil.docs(random, mergedReader,\n                                       DocHelper.TEXT_FIELD_2_KEY,\n                                       new BytesRef(\"field\"),\n                                       MultiFields.getLiveDocs(mergedReader),\n                                       null,\n                                       false);\n    assertTrue(termDocs != null);\n    assertTrue(termDocs.nextDoc() != DocsEnum.NO_MORE_DOCS);\n\n    int tvCount = 0;\n    for(FieldInfo fieldInfo : mergedReader.getFieldInfos()) {\n      if (fieldInfo.storeTermVector) {\n        tvCount++;\n      }\n    }\n    \n    //System.out.println(\"stored size: \" + stored.size());\n    assertEquals(\"We do not have 3 fields that were indexed with term vector\", 3, tvCount);\n\n    Terms vector = mergedReader.getTermVectors(0).terms(DocHelper.TEXT_FIELD_2_KEY);\n    assertNotNull(vector);\n    assertEquals(3, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n\n    int i = 0;\n    while (termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      int freq = (int) termsEnum.totalTermFreq();\n      //System.out.println(\"Term: \" + term + \" Freq: \" + freq);\n      assertTrue(DocHelper.FIELD_2_TEXT.indexOf(term) != -1);\n      assertTrue(DocHelper.FIELD_2_FREQS[i] == freq);\n      i++;\n    }\n\n    TestSegmentReader.checkNorms(mergedReader);\n    mergedReader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"d3fe2fc74577855eadfb5eae3153c2fffdaaf791":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":["872cff1d3a554e0cd64014cd97f88d3002b0f491","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["d3fe2fc74577855eadfb5eae3153c2fffdaaf791"],"a493e6d0c3ad86bd55c0a1360d110142e948f2bd":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["833a7987bc1c94455fde83e3311f72bddedcfb93","94cb8b3ec0439dfd8e179637ee4191cd9c6227e5"],"14ec33385f6fbb6ce172882d14605790418a5d31":["b0c7a8f7304b75b1528814c5820fa23a96816c27"],"94cb8b3ec0439dfd8e179637ee4191cd9c6227e5":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"28427ef110c4c5bf5b4057731b83110bd1e13724":["6267e1ce56c2eec111425690cd04e251b6f14952"],"433777d1eaf9998136cd16515dc0e1eb26f5d535":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["135621f3a0670a9394eb563224a3b76cc4dddc0f","d3fe2fc74577855eadfb5eae3153c2fffdaaf791"],"3321cfbf7f8aba27e37e7a4d6901531a97ac2b06":["9ce667c6d3400b22523701c549c0d35e26da8b46"],"9ce667c6d3400b22523701c549c0d35e26da8b46":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["e06c9d5fba0a2f937941d199d64ccb32aac502d1"],"cb4972c6aaf6c714c8f5957b5aeb14dcce34b75f":["b65b350ca9588f9fc76ce7d6804160d06c45ff42"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","b0c7a8f7304b75b1528814c5820fa23a96816c27"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"06584e6e98d592b34e1329b384182f368d2025e8":["7b91922b55d15444d554721b352861d028eb8278"],"5f4e87790277826a2aea119328600dfb07761f32":["6267e1ce56c2eec111425690cd04e251b6f14952","4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["b6f9be74ca7baaef11857ad002cad40419979516","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["a3776dccca01c11e7046323cfad46a3b4a471233","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"6267e1ce56c2eec111425690cd04e251b6f14952":["433777d1eaf9998136cd16515dc0e1eb26f5d535"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","1224a4027481acce15495b03bce9b48b93b42722"],"a3776dccca01c11e7046323cfad46a3b4a471233":["1224a4027481acce15495b03bce9b48b93b42722","d3fe2fc74577855eadfb5eae3153c2fffdaaf791"],"913fa4c710b6d1168655966e59f0f4de566907a8":["3321cfbf7f8aba27e37e7a4d6901531a97ac2b06"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["872cff1d3a554e0cd64014cd97f88d3002b0f491","cb4972c6aaf6c714c8f5957b5aeb14dcce34b75f"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["d3fe2fc74577855eadfb5eae3153c2fffdaaf791"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["bde51b089eb7f86171eb3406e38a274743f9b7ac","1224a4027481acce15495b03bce9b48b93b42722"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b6f9be74ca7baaef11857ad002cad40419979516":["639c36565ce03aed5b0fce7c9e4448e53a1f7efd"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":["817d8435e9135b756f08ce6710ab0baac51bdf88","ddc4c914be86e34b54f70023f45a60fa7f04e929"],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"7e1cbd7e289dc1243c7a59e1a83d078163a147fe":["4e8cc373c801e54cec75daf9f52792cb4b17f536"],"1224a4027481acce15495b03bce9b48b93b42722":["14ec33385f6fbb6ce172882d14605790418a5d31"],"833a7987bc1c94455fde83e3311f72bddedcfb93":["5f4e87790277826a2aea119328600dfb07761f32"],"e06c9d5fba0a2f937941d199d64ccb32aac502d1":["7e1cbd7e289dc1243c7a59e1a83d078163a147fe"],"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["28427ef110c4c5bf5b4057731b83110bd1e13724"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"85a883878c0af761245ab048babc63d099f835f3":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c","a493e6d0c3ad86bd55c0a1360d110142e948f2bd"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["d619839baa8ce5503e496b94a9e42ad6f079293f","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"3cc749c053615f5871f3b95715fe292f34e70a53":["06584e6e98d592b34e1329b384182f368d2025e8"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["1224a4027481acce15495b03bce9b48b93b42722","c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["3bb13258feba31ab676502787ab2e1779f129b7a","94cb8b3ec0439dfd8e179637ee4191cd9c6227e5"],"872cff1d3a554e0cd64014cd97f88d3002b0f491":["3cc749c053615f5871f3b95715fe292f34e70a53"],"7f367dfb9086b92a13c77e2d31112c715cd4502c":["a493e6d0c3ad86bd55c0a1360d110142e948f2bd"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["3321cfbf7f8aba27e37e7a4d6901531a97ac2b06","913fa4c710b6d1168655966e59f0f4de566907a8"],"7b91922b55d15444d554721b352861d028eb8278":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"b65b350ca9588f9fc76ce7d6804160d06c45ff42":["3cc749c053615f5871f3b95715fe292f34e70a53","872cff1d3a554e0cd64014cd97f88d3002b0f491"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["94cb8b3ec0439dfd8e179637ee4191cd9c6227e5"],"3bb13258feba31ab676502787ab2e1779f129b7a":["85a883878c0af761245ab048babc63d099f835f3","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["7f367dfb9086b92a13c77e2d31112c715cd4502c"]},"commit2Childs":{"d3fe2fc74577855eadfb5eae3153c2fffdaaf791":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","c700f8d0842d3e52bb2bdfbfdc046a137e836edb","a3776dccca01c11e7046323cfad46a3b4a471233","639c36565ce03aed5b0fce7c9e4448e53a1f7efd"],"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","ddc4c914be86e34b54f70023f45a60fa7f04e929"],"a493e6d0c3ad86bd55c0a1360d110142e948f2bd":["85a883878c0af761245ab048babc63d099f835f3","7f367dfb9086b92a13c77e2d31112c715cd4502c"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"14ec33385f6fbb6ce172882d14605790418a5d31":["1224a4027481acce15495b03bce9b48b93b42722"],"94cb8b3ec0439dfd8e179637ee4191cd9c6227e5":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","ab5cb6a74aefb78aa0569857970b9151dfe2e787","b0c7a8f7304b75b1528814c5820fa23a96816c27"],"28427ef110c4c5bf5b4057731b83110bd1e13724":["4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"433777d1eaf9998136cd16515dc0e1eb26f5d535":["6267e1ce56c2eec111425690cd04e251b6f14952"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":[],"3321cfbf7f8aba27e37e7a4d6901531a97ac2b06":["913fa4c710b6d1168655966e59f0f4de566907a8","5cab9a86bd67202d20b6adc463008c8e982b070a"],"9ce667c6d3400b22523701c549c0d35e26da8b46":["3321cfbf7f8aba27e37e7a4d6901531a97ac2b06"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["94cb8b3ec0439dfd8e179637ee4191cd9c6227e5"],"cb4972c6aaf6c714c8f5957b5aeb14dcce34b75f":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"06584e6e98d592b34e1329b384182f368d2025e8":["3cc749c053615f5871f3b95715fe292f34e70a53"],"5f4e87790277826a2aea119328600dfb07761f32":["833a7987bc1c94455fde83e3311f72bddedcfb93"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["5d004d0e0b3f65bb40da76d476d659d7888270e8"],"6267e1ce56c2eec111425690cd04e251b6f14952":["28427ef110c4c5bf5b4057731b83110bd1e13724","5f4e87790277826a2aea119328600dfb07761f32"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a3776dccca01c11e7046323cfad46a3b4a471233":["817d8435e9135b756f08ce6710ab0baac51bdf88"],"913fa4c710b6d1168655966e59f0f4de566907a8":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","9ce667c6d3400b22523701c549c0d35e26da8b46"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["433777d1eaf9998136cd16515dc0e1eb26f5d535"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["b6f9be74ca7baaef11857ad002cad40419979516"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"b6f9be74ca7baaef11857ad002cad40419979516":["d083e83f225b11e5fdd900e83d26ddb385b6955c"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":[],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["5d004d0e0b3f65bb40da76d476d659d7888270e8","7b91922b55d15444d554721b352861d028eb8278"],"7e1cbd7e289dc1243c7a59e1a83d078163a147fe":["e06c9d5fba0a2f937941d199d64ccb32aac502d1"],"833a7987bc1c94455fde83e3311f72bddedcfb93":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"1224a4027481acce15495b03bce9b48b93b42722":["d619839baa8ce5503e496b94a9e42ad6f079293f","a3776dccca01c11e7046323cfad46a3b4a471233","c0ef0193974807e4bddf5432a6b0287fe4d6c9df","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"e06c9d5fba0a2f937941d199d64ccb32aac502d1":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["5f4e87790277826a2aea119328600dfb07761f32","ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["a493e6d0c3ad86bd55c0a1360d110142e948f2bd","85a883878c0af761245ab048babc63d099f835f3"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"85a883878c0af761245ab048babc63d099f835f3":["3bb13258feba31ab676502787ab2e1779f129b7a"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["d3fe2fc74577855eadfb5eae3153c2fffdaaf791","135621f3a0670a9394eb563224a3b76cc4dddc0f"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["d619839baa8ce5503e496b94a9e42ad6f079293f"],"3cc749c053615f5871f3b95715fe292f34e70a53":["872cff1d3a554e0cd64014cd97f88d3002b0f491","b65b350ca9588f9fc76ce7d6804160d06c45ff42"],"872cff1d3a554e0cd64014cd97f88d3002b0f491":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","3615ce4a1f785ae1b779244de52c6a7d99227e60","b65b350ca9588f9fc76ce7d6804160d06c45ff42"],"7f367dfb9086b92a13c77e2d31112c715cd4502c":["4e8cc373c801e54cec75daf9f52792cb4b17f536"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"7b91922b55d15444d554721b352861d028eb8278":["06584e6e98d592b34e1329b384182f368d2025e8"],"b65b350ca9588f9fc76ce7d6804160d06c45ff42":["cb4972c6aaf6c714c8f5957b5aeb14dcce34b75f"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["14ec33385f6fbb6ce172882d14605790418a5d31","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"3bb13258feba31ab676502787ab2e1779f129b7a":["ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["7e1cbd7e289dc1243c7a59e1a83d078163a147fe","3bb13258feba31ab676502787ab2e1779f129b7a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","c700f8d0842d3e52bb2bdfbfdc046a137e836edb","5d004d0e0b3f65bb40da76d476d659d7888270e8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}