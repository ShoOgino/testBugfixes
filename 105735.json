{"path":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,Set[BytesRef],int,boolean,boolean).mjava","commits":[{"id":"58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b","date":1395588343,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,Set[BytesRef],int,boolean,boolean).mjava","pathOld":"/dev/null","sourceNew":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, Set<BytesRef> contexts, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n\n      if (contexts != null) {\n        BooleanQuery sub = new BooleanQuery();\n        query.add(sub, BooleanClause.Occur.MUST);\n        for(BytesRef context : contexts) {\n          // NOTE: we \"should\" wrap this in\n          // ConstantScoreQuery, or maybe send this as a\n          // Filter instead to search, but since all of\n          // these are MUST'd, the change to the score won't\n          // affect the overall ranking.  Since we indexed\n          // as DOCS_ONLY, the perf should be the same\n          // either way (no freq int[] blocks to decode):\n\n          // TODO: if we had a BinaryTermField we could fix\n          // this \"must be valid ut8f\" limitation:\n          sub.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, context.utf8ToString())), BooleanClause.Occur.SHOULD);\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(SORT, num, true, false, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    Collector c2 = new EarlyTerminatingSortingCollector(c, SORT, num);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = (TopFieldDocs) c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, SORT);\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["0977217be8980b17c612feb7ea3d1556ccf4aeed"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0977217be8980b17c612feb7ea3d1556ccf4aeed","date":1415742153,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,Set[BytesRef],int,boolean,boolean).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,Set[BytesRef],int,boolean,boolean).mjava","sourceNew":"  /** Lookup, with context but without booleans. Context booleans default to SHOULD,\n   *  so each suggestion must have at least one of the contexts. */\n  public List<LookupResult> lookup(CharSequence key, Set<BytesRef> contexts, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (contexts == null) {\n      return lookup(key, num, allTermsRequired, doHighlight);\n    }\n\n    Map<BytesRef, BooleanClause.Occur> contextInfo = new HashMap<>();\n    for (BytesRef context : contexts) {\n      contextInfo.put(context, BooleanClause.Occur.SHOULD);\n    }\n    return lookup(key, contextInfo, num, allTermsRequired, doHighlight);\n  }\n\n","sourceOld":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, Set<BytesRef> contexts, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n\n      if (contexts != null) {\n        BooleanQuery sub = new BooleanQuery();\n        query.add(sub, BooleanClause.Occur.MUST);\n        for(BytesRef context : contexts) {\n          // NOTE: we \"should\" wrap this in\n          // ConstantScoreQuery, or maybe send this as a\n          // Filter instead to search, but since all of\n          // these are MUST'd, the change to the score won't\n          // affect the overall ranking.  Since we indexed\n          // as DOCS_ONLY, the perf should be the same\n          // either way (no freq int[] blocks to decode):\n\n          // TODO: if we had a BinaryTermField we could fix\n          // this \"must be valid ut8f\" limitation:\n          sub.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, context.utf8ToString())), BooleanClause.Occur.SHOULD);\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(SORT, num, true, false, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    Collector c2 = new EarlyTerminatingSortingCollector(c, SORT, num);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = (TopFieldDocs) c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, SORT);\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","bugFix":["58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d2204dea8cf3dcfbffdf7b4d3459cf5287cc1c30","date":1431701935,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,Set[BytesRef],int,boolean,boolean).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,Set[BytesRef],int,boolean,boolean).mjava","sourceNew":"  /** Lookup, with context but without booleans. Context booleans default to SHOULD,\n   *  so each suggestion must have at least one of the contexts. */\n  public List<LookupResult> lookup(CharSequence key, Set<BytesRef> contexts, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n    return lookup(key, toQuery(contexts), num, allTermsRequired, doHighlight);\n  }\n\n","sourceOld":"  /** Lookup, with context but without booleans. Context booleans default to SHOULD,\n   *  so each suggestion must have at least one of the contexts. */\n  public List<LookupResult> lookup(CharSequence key, Set<BytesRef> contexts, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (contexts == null) {\n      return lookup(key, num, allTermsRequired, doHighlight);\n    }\n\n    Map<BytesRef, BooleanClause.Occur> contextInfo = new HashMap<>();\n    for (BytesRef context : contexts) {\n      contextInfo.put(context, BooleanClause.Occur.SHOULD);\n    }\n    return lookup(key, contextInfo, num, allTermsRequired, doHighlight);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0977217be8980b17c612feb7ea3d1556ccf4aeed":["58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b"],"58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d2204dea8cf3dcfbffdf7b4d3459cf5287cc1c30":["0977217be8980b17c612feb7ea3d1556ccf4aeed"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d2204dea8cf3dcfbffdf7b4d3459cf5287cc1c30"]},"commit2Childs":{"0977217be8980b17c612feb7ea3d1556ccf4aeed":["d2204dea8cf3dcfbffdf7b4d3459cf5287cc1c30"],"58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b":["0977217be8980b17c612feb7ea3d1556ccf4aeed"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["58d0345a28bb6b4be59c38e6a77e2cc0e615ee4b"],"d2204dea8cf3dcfbffdf7b4d3459cf5287cc1c30":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}