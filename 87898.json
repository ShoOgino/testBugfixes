{"path":"src/java/org/apache/solr/spelling/SpellingQueryConverter#convert(String).mjava","commits":[{"id":"84b6c001c19319635b53dd80ee9fc1ba9a5b4574","date":1213883214,"type":0,"author":"Grant Ingersoll","isMerge":false,"pathNew":"src/java/org/apache/solr/spelling/SpellingQueryConverter#convert(String).mjava","pathOld":"/dev/null","sourceNew":"  public Collection<Token> convert(String original) {\n    Collection<Token> result = new ArrayList<Token>();\n    //TODO: Extract the words using a simple regex, but not query stuff, and then analyze them to produce the token stream\n    Matcher matcher = QUERY_REGEX.matcher(original);\n    TokenStream stream;\n    while (matcher.find()) {\n      String word = matcher.group(0);\n      if (word.equals(\"AND\") == false && word.equals(\"OR\") == false) {\n        try {\n          stream = analyzer.reusableTokenStream(\"\", new StringReader(word));\n          Token token;\n          while ((token = stream.next()) != null) {\n            token.setStartOffset(matcher.start());\n            token.setEndOffset(matcher.end());\n            result.add(token);\n          }\n        } catch (IOException e) {\n        }\n      }\n    }\n    return result;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["4a1d67dfa126cb83a8dfeea520dbc2fae9ba823c","4a1d67dfa126cb83a8dfeea520dbc2fae9ba823c","4a1d67dfa126cb83a8dfeea520dbc2fae9ba823c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"200496f3bb5756f5752f68d23a403a6b78fe7fe9","date":1217027315,"type":3,"author":"Ryan McKinley","isMerge":false,"pathNew":"src/java/org/apache/solr/spelling/SpellingQueryConverter#convert(String).mjava","pathOld":"src/java/org/apache/solr/spelling/SpellingQueryConverter#convert(String).mjava","sourceNew":"  public Collection<Token> convert(String original) {\n    if( original == null ) { // this can happen with q.alt = and no query\n      return Collections.emptyList();\n    }\n    Collection<Token> result = new ArrayList<Token>();\n    //TODO: Extract the words using a simple regex, but not query stuff, and then analyze them to produce the token stream\n    Matcher matcher = QUERY_REGEX.matcher(original);\n    TokenStream stream;\n    while (matcher.find()) {\n      String word = matcher.group(0);\n      if (word.equals(\"AND\") == false && word.equals(\"OR\") == false) {\n        try {\n          stream = analyzer.reusableTokenStream(\"\", new StringReader(word));\n          Token token;\n          while ((token = stream.next()) != null) {\n            token.setStartOffset(matcher.start());\n            token.setEndOffset(matcher.end());\n            result.add(token);\n          }\n        } catch (IOException e) {\n        }\n      }\n    }\n    return result;\n  }\n\n","sourceOld":"  public Collection<Token> convert(String original) {\n    Collection<Token> result = new ArrayList<Token>();\n    //TODO: Extract the words using a simple regex, but not query stuff, and then analyze them to produce the token stream\n    Matcher matcher = QUERY_REGEX.matcher(original);\n    TokenStream stream;\n    while (matcher.find()) {\n      String word = matcher.group(0);\n      if (word.equals(\"AND\") == false && word.equals(\"OR\") == false) {\n        try {\n          stream = analyzer.reusableTokenStream(\"\", new StringReader(word));\n          Token token;\n          while ((token = stream.next()) != null) {\n            token.setStartOffset(matcher.start());\n            token.setEndOffset(matcher.end());\n            result.add(token);\n          }\n        } catch (IOException e) {\n        }\n      }\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"00fc15ea1c4eed67f5d090c697cc57ede49f70a8","date":1218745675,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"src/java/org/apache/solr/spelling/SpellingQueryConverter#convert(String).mjava","pathOld":"src/java/org/apache/solr/spelling/SpellingQueryConverter#convert(String).mjava","sourceNew":"  /**\n   * Converts the original query string to a collection of Lucene Tokens.\n   * @param original the original query string\n   * @return a Collection of Lucene Tokens\n   */\n  public Collection<Token> convert(String original) {\n    if (original == null) { // this can happen with q.alt = and no query\n      return Collections.emptyList();\n    }\n    Collection<Token> result = new ArrayList<Token>();\n    //TODO: Extract the words using a simple regex, but not query stuff, and then analyze them to produce the token stream\n    Matcher matcher = QUERY_REGEX.matcher(original);\n    TokenStream stream;\n    while (matcher.find()) {\n      String word = matcher.group(0);\n      if (word.equals(\"AND\") == false && word.equals(\"OR\") == false) {\n        try {\n          stream = analyzer.reusableTokenStream(\"\", new StringReader(word));\n          Token token;\n          while ((token = stream.next()) != null) {\n            token.setStartOffset(matcher.start());\n            token.setEndOffset(matcher.end());\n            result.add(token);\n          }\n        } catch (IOException e) {\n        }\n      }\n    }\n    return result;\n  }\n\n","sourceOld":"  public Collection<Token> convert(String original) {\n    if( original == null ) { // this can happen with q.alt = and no query\n      return Collections.emptyList();\n    }\n    Collection<Token> result = new ArrayList<Token>();\n    //TODO: Extract the words using a simple regex, but not query stuff, and then analyze them to produce the token stream\n    Matcher matcher = QUERY_REGEX.matcher(original);\n    TokenStream stream;\n    while (matcher.find()) {\n      String word = matcher.group(0);\n      if (word.equals(\"AND\") == false && word.equals(\"OR\") == false) {\n        try {\n          stream = analyzer.reusableTokenStream(\"\", new StringReader(word));\n          Token token;\n          while ((token = stream.next()) != null) {\n            token.setStartOffset(matcher.start());\n            token.setEndOffset(matcher.end());\n            result.add(token);\n          }\n        } catch (IOException e) {\n        }\n      }\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ef28ac95f5f85bbf872801277448c0924b0a6827","date":1268600312,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"src/java/org/apache/solr/spelling/SpellingQueryConverter#convert(String).mjava","pathOld":"src/java/org/apache/solr/spelling/SpellingQueryConverter#convert(String).mjava","sourceNew":"  /**\n   * Converts the original query string to a collection of Lucene Tokens.\n   * @param original the original query string\n   * @return a Collection of Lucene Tokens\n   */\n  public Collection<Token> convert(String original) {\n    if (original == null) { // this can happen with q.alt = and no query\n      return Collections.emptyList();\n    }\n    Collection<Token> result = new ArrayList<Token>();\n    //TODO: Extract the words using a simple regex, but not query stuff, and then analyze them to produce the token stream\n    Matcher matcher = QUERY_REGEX.matcher(original);\n    TokenStream stream;\n    while (matcher.find()) {\n      String word = matcher.group(0);\n      if (word.equals(\"AND\") == false && word.equals(\"OR\") == false) {\n        try {\n          stream = analyzer.reusableTokenStream(\"\", new StringReader(word));\n          // TODO: support custom attributes\n          TermAttribute termAtt = (TermAttribute) stream.addAttribute(TermAttribute.class);\n          FlagsAttribute flagsAtt = (FlagsAttribute) stream.addAttribute(FlagsAttribute.class);\n          TypeAttribute typeAtt = (TypeAttribute) stream.addAttribute(TypeAttribute.class);\n          PayloadAttribute payloadAtt = (PayloadAttribute) stream.addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) stream.addAttribute(PositionIncrementAttribute.class);\n          stream.reset();\n          while (stream.incrementToken()) {\n            Token token = new Token();\n            token.setTermBuffer(termAtt.termBuffer(), 0, termAtt.termLength());\n            token.setStartOffset(matcher.start());\n            token.setEndOffset(matcher.end());\n            token.setFlags(flagsAtt.getFlags());\n            token.setType(typeAtt.type());\n            token.setPayload(payloadAtt.getPayload());\n            token.setPositionIncrement(posIncAtt.getPositionIncrement());\n            result.add(token);\n          }\n        } catch (IOException e) {\n        }\n      }\n    }\n    return result;\n  }\n\n","sourceOld":"  /**\n   * Converts the original query string to a collection of Lucene Tokens.\n   * @param original the original query string\n   * @return a Collection of Lucene Tokens\n   */\n  public Collection<Token> convert(String original) {\n    if (original == null) { // this can happen with q.alt = and no query\n      return Collections.emptyList();\n    }\n    Collection<Token> result = new ArrayList<Token>();\n    //TODO: Extract the words using a simple regex, but not query stuff, and then analyze them to produce the token stream\n    Matcher matcher = QUERY_REGEX.matcher(original);\n    TokenStream stream;\n    while (matcher.find()) {\n      String word = matcher.group(0);\n      if (word.equals(\"AND\") == false && word.equals(\"OR\") == false) {\n        try {\n          stream = analyzer.reusableTokenStream(\"\", new StringReader(word));\n          Token token;\n          while ((token = stream.next()) != null) {\n            token.setStartOffset(matcher.start());\n            token.setEndOffset(matcher.end());\n            result.add(token);\n          }\n        } catch (IOException e) {\n        }\n      }\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/spelling/SpellingQueryConverter#convert(String).mjava","pathOld":"src/java/org/apache/solr/spelling/SpellingQueryConverter#convert(String).mjava","sourceNew":"  /**\n   * Converts the original query string to a collection of Lucene Tokens.\n   * @param original the original query string\n   * @return a Collection of Lucene Tokens\n   */\n  public Collection<Token> convert(String original) {\n    if (original == null) { // this can happen with q.alt = and no query\n      return Collections.emptyList();\n    }\n    Collection<Token> result = new ArrayList<Token>();\n    //TODO: Extract the words using a simple regex, but not query stuff, and then analyze them to produce the token stream\n    Matcher matcher = QUERY_REGEX.matcher(original);\n    TokenStream stream;\n    while (matcher.find()) {\n      String word = matcher.group(0);\n      if (word.equals(\"AND\") == false && word.equals(\"OR\") == false) {\n        try {\n          stream = analyzer.reusableTokenStream(\"\", new StringReader(word));\n          // TODO: support custom attributes\n          TermAttribute termAtt = (TermAttribute) stream.addAttribute(TermAttribute.class);\n          FlagsAttribute flagsAtt = (FlagsAttribute) stream.addAttribute(FlagsAttribute.class);\n          TypeAttribute typeAtt = (TypeAttribute) stream.addAttribute(TypeAttribute.class);\n          PayloadAttribute payloadAtt = (PayloadAttribute) stream.addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) stream.addAttribute(PositionIncrementAttribute.class);\n          stream.reset();\n          while (stream.incrementToken()) {\n            Token token = new Token();\n            token.setTermBuffer(termAtt.termBuffer(), 0, termAtt.termLength());\n            token.setStartOffset(matcher.start());\n            token.setEndOffset(matcher.end());\n            token.setFlags(flagsAtt.getFlags());\n            token.setType(typeAtt.type());\n            token.setPayload(payloadAtt.getPayload());\n            token.setPositionIncrement(posIncAtt.getPositionIncrement());\n            result.add(token);\n          }\n        } catch (IOException e) {\n        }\n      }\n    }\n    return result;\n  }\n\n","sourceOld":"  /**\n   * Converts the original query string to a collection of Lucene Tokens.\n   * @param original the original query string\n   * @return a Collection of Lucene Tokens\n   */\n  public Collection<Token> convert(String original) {\n    if (original == null) { // this can happen with q.alt = and no query\n      return Collections.emptyList();\n    }\n    Collection<Token> result = new ArrayList<Token>();\n    //TODO: Extract the words using a simple regex, but not query stuff, and then analyze them to produce the token stream\n    Matcher matcher = QUERY_REGEX.matcher(original);\n    TokenStream stream;\n    while (matcher.find()) {\n      String word = matcher.group(0);\n      if (word.equals(\"AND\") == false && word.equals(\"OR\") == false) {\n        try {\n          stream = analyzer.reusableTokenStream(\"\", new StringReader(word));\n          // TODO: support custom attributes\n          TermAttribute termAtt = (TermAttribute) stream.addAttribute(TermAttribute.class);\n          FlagsAttribute flagsAtt = (FlagsAttribute) stream.addAttribute(FlagsAttribute.class);\n          TypeAttribute typeAtt = (TypeAttribute) stream.addAttribute(TypeAttribute.class);\n          PayloadAttribute payloadAtt = (PayloadAttribute) stream.addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncAtt = (PositionIncrementAttribute) stream.addAttribute(PositionIncrementAttribute.class);\n          stream.reset();\n          while (stream.incrementToken()) {\n            Token token = new Token();\n            token.setTermBuffer(termAtt.termBuffer(), 0, termAtt.termLength());\n            token.setStartOffset(matcher.start());\n            token.setEndOffset(matcher.end());\n            token.setFlags(flagsAtt.getFlags());\n            token.setType(typeAtt.type());\n            token.setPayload(payloadAtt.getPayload());\n            token.setPositionIncrement(posIncAtt.getPositionIncrement());\n            result.add(token);\n          }\n        } catch (IOException e) {\n        }\n      }\n    }\n    return result;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"84b6c001c19319635b53dd80ee9fc1ba9a5b4574":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"ef28ac95f5f85bbf872801277448c0924b0a6827":["00fc15ea1c4eed67f5d090c697cc57ede49f70a8"],"200496f3bb5756f5752f68d23a403a6b78fe7fe9":["84b6c001c19319635b53dd80ee9fc1ba9a5b4574"],"ad94625fb8d088209f46650c8097196fec67f00c":["ef28ac95f5f85bbf872801277448c0924b0a6827"],"00fc15ea1c4eed67f5d090c697cc57ede49f70a8":["200496f3bb5756f5752f68d23a403a6b78fe7fe9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"84b6c001c19319635b53dd80ee9fc1ba9a5b4574":["200496f3bb5756f5752f68d23a403a6b78fe7fe9"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["84b6c001c19319635b53dd80ee9fc1ba9a5b4574"],"ef28ac95f5f85bbf872801277448c0924b0a6827":["ad94625fb8d088209f46650c8097196fec67f00c"],"200496f3bb5756f5752f68d23a403a6b78fe7fe9":["00fc15ea1c4eed67f5d090c697cc57ede49f70a8"],"ad94625fb8d088209f46650c8097196fec67f00c":[],"00fc15ea1c4eed67f5d090c697cc57ede49f70a8":["ef28ac95f5f85bbf872801277448c0924b0a6827"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ad94625fb8d088209f46650c8097196fec67f00c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"pathCommit":null}