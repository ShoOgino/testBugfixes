{"path":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","commits":[{"id":"ceaef6cfc68c8ab22a684192e469a8280f9e6e70","date":1462354657,"type":0,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","pathOld":"/dev/null","sourceNew":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link MergeState#DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort! */\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    SortField fields[] = sort.getSort();\n    final CrossReaderComparator[] comparators = new CrossReaderComparator[fields.length];\n    for(int i=0;i<fields.length;i++) {\n      comparators[i] = getComparator(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparators.length;i++) {\n            int cmp = comparators[i].compare(a.readerIndex, a.docID, b.readerIndex, b.docID);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          }\n          return a.docID < b.docID;\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      queue.add(new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc()));\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    int mappedDocID = 0;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb0345a2d45479f891041f8b3ce351bc975e64ac","date":1462708700,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","sourceNew":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link MergeState#DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort! */\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // nocommit optimize if only 1 reader is incoming\n\n    SortField fields[] = sort.getSort();\n    final CrossReaderComparator[] comparators = new CrossReaderComparator[fields.length];\n    for(int i=0;i<fields.length;i++) {\n      comparators[i] = getComparator(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparators.length;i++) {\n            int cmp = comparators[i].compare(a.readerIndex, a.docID, b.readerIndex, b.docID);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          }\n          return a.docID < b.docID;\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      queue.add(new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc()));\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    int mappedDocID = 0;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","sourceOld":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link MergeState#DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort! */\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    SortField fields[] = sort.getSort();\n    final CrossReaderComparator[] comparators = new CrossReaderComparator[fields.length];\n    for(int i=0;i<fields.length;i++) {\n      comparators[i] = getComparator(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparators.length;i++) {\n            int cmp = comparators[i].compare(a.readerIndex, a.docID, b.readerIndex, b.docID);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          }\n          return a.docID < b.docID;\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      queue.add(new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc()));\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    int mappedDocID = 0;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5e03940e6e9044943de4b7ac08f8581da37a9534","date":1462870173,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","sourceNew":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link MergeState#DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort! */\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final CrossReaderComparator[] comparators = new CrossReaderComparator[fields.length];\n    for(int i=0;i<fields.length;i++) {\n      comparators[i] = getComparator(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparators.length;i++) {\n            int cmp = comparators[i].compare(a.readerIndex, a.docID, b.readerIndex, b.docID);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          }\n          return a.docID < b.docID;\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      queue.add(new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc()));\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    int mappedDocID = 0;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","sourceOld":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link MergeState#DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort! */\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // nocommit optimize if only 1 reader is incoming\n\n    SortField fields[] = sort.getSort();\n    final CrossReaderComparator[] comparators = new CrossReaderComparator[fields.length];\n    for(int i=0;i<fields.length;i++) {\n      comparators[i] = getComparator(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparators.length;i++) {\n            int cmp = comparators[i].compare(a.readerIndex, a.docID, b.readerIndex, b.docID);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          }\n          return a.docID < b.docID;\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      queue.add(new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc()));\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    int mappedDocID = 0;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9c23672acbb5104509c1c2d6e3dda7a08eaf091f","date":1463128188,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","sourceNew":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort! */\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final CrossReaderComparator[] comparators = new CrossReaderComparator[fields.length];\n    for(int i=0;i<fields.length;i++) {\n      comparators[i] = getComparator(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparators.length;i++) {\n            int cmp = comparators[i].compare(a.readerIndex, a.docID, b.readerIndex, b.docID);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          }\n          return a.docID < b.docID;\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      queue.add(new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc()));\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    int mappedDocID = 0;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","sourceOld":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link MergeState#DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort! */\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final CrossReaderComparator[] comparators = new CrossReaderComparator[fields.length];\n    for(int i=0;i<fields.length;i++) {\n      comparators[i] = getComparator(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparators.length;i++) {\n            int cmp = comparators[i].compare(a.readerIndex, a.docID, b.readerIndex, b.docID);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          }\n          return a.docID < b.docID;\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      queue.add(new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc()));\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    int mappedDocID = 0;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3d33e731a93d4b57e662ff094f64f94a745422d4","date":1463128289,"type":0,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","pathOld":"/dev/null","sourceNew":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort! */\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final CrossReaderComparator[] comparators = new CrossReaderComparator[fields.length];\n    for(int i=0;i<fields.length;i++) {\n      comparators[i] = getComparator(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparators.length;i++) {\n            int cmp = comparators[i].compare(a.readerIndex, a.docID, b.readerIndex, b.docID);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          }\n          return a.docID < b.docID;\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      queue.add(new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc()));\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    int mappedDocID = 0;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ad30c6a479e764150a3316e57263319775f1df2","date":1463395403,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","pathOld":"/dev/null","sourceNew":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort! */\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final CrossReaderComparator[] comparators = new CrossReaderComparator[fields.length];\n    for(int i=0;i<fields.length;i++) {\n      comparators[i] = getComparator(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparators.length;i++) {\n            int cmp = comparators[i].compare(a.readerIndex, a.docID, b.readerIndex, b.docID);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          }\n          return a.docID < b.docID;\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      queue.add(new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc()));\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    int mappedDocID = 0;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":0,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","pathOld":"/dev/null","sourceNew":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort! */\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final CrossReaderComparator[] comparators = new CrossReaderComparator[fields.length];\n    for(int i=0;i<fields.length;i++) {\n      comparators[i] = getComparator(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparators.length;i++) {\n            int cmp = comparators[i].compare(a.readerIndex, a.docID, b.readerIndex, b.docID);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          }\n          return a.docID < b.docID;\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      queue.add(new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc()));\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    int mappedDocID = 0;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6652c74b2358a0b13223817a6a793bf1c9d0749d","date":1474465301,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","sourceNew":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort! */\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final ComparableProvider[][] comparables = new ComparableProvider[fields.length][];\n    for(int i=0;i<fields.length;i++) {\n      comparables[i] = getComparableProviders(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparables.length;i++) {\n            int cmp = a.values[i].compareTo(b.values[i]);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          } else {\n            return a.docID < b.docID;\n          }\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      LeafAndDocID leaf = new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc(), comparables.length);\n      for(int j=0;j<comparables.length;j++) {\n        leaf.values[j] = comparables[j][i].getComparable(leaf.docID);\n        assert leaf.values[j] != null;\n      }\n      queue.add(leaf);\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    // merge sort:\n    int mappedDocID = 0;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        for(int j=0;j<comparables.length;j++) {\n          top.values[j] = comparables[j][top.readerIndex].getComparable(top.docID);\n          assert top.values[j] != null;\n        }\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","sourceOld":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort! */\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final CrossReaderComparator[] comparators = new CrossReaderComparator[fields.length];\n    for(int i=0;i<fields.length;i++) {\n      comparators[i] = getComparator(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparators.length;i++) {\n            int cmp = comparators[i].compare(a.readerIndex, a.docID, b.readerIndex, b.docID);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          }\n          return a.docID < b.docID;\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      queue.add(new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc()));\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    int mappedDocID = 0;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","sourceNew":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort! */\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final ComparableProvider[][] comparables = new ComparableProvider[fields.length][];\n    for(int i=0;i<fields.length;i++) {\n      comparables[i] = getComparableProviders(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparables.length;i++) {\n            int cmp = a.values[i].compareTo(b.values[i]);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          } else {\n            return a.docID < b.docID;\n          }\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      LeafAndDocID leaf = new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc(), comparables.length);\n      for(int j=0;j<comparables.length;j++) {\n        leaf.values[j] = comparables[j][i].getComparable(leaf.docID);\n        assert leaf.values[j] != null;\n      }\n      queue.add(leaf);\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    // merge sort:\n    int mappedDocID = 0;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        for(int j=0;j<comparables.length;j++) {\n          top.values[j] = comparables[j][top.readerIndex].getComparable(top.docID);\n          assert top.values[j] != null;\n        }\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","sourceOld":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort! */\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final CrossReaderComparator[] comparators = new CrossReaderComparator[fields.length];\n    for(int i=0;i<fields.length;i++) {\n      comparators[i] = getComparator(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparators.length;i++) {\n            int cmp = comparators[i].compare(a.readerIndex, a.docID, b.readerIndex, b.docID);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          }\n          return a.docID < b.docID;\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      queue.add(new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc()));\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    int mappedDocID = 0;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","pathOld":"/dev/null","sourceNew":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort! */\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final ComparableProvider[][] comparables = new ComparableProvider[fields.length][];\n    for(int i=0;i<fields.length;i++) {\n      comparables[i] = getComparableProviders(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparables.length;i++) {\n            int cmp = a.values[i].compareTo(b.values[i]);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          } else {\n            return a.docID < b.docID;\n          }\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      LeafAndDocID leaf = new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc(), comparables.length);\n      for(int j=0;j<comparables.length;j++) {\n        leaf.values[j] = comparables[j][i].getComparable(leaf.docID);\n        assert leaf.values[j] != null;\n      }\n      queue.add(leaf);\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    // merge sort:\n    int mappedDocID = 0;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        for(int j=0;j<comparables.length;j++) {\n          top.values[j] = comparables[j][top.readerIndex].getComparable(top.docID);\n          assert top.values[j] != null;\n        }\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"653128722fb3b4713ac331c621491a93f34a4a22","date":1479841816,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","sourceNew":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort!\n   *  Returns null if the merge sort is not needed (segments are already in index sort order).\n   **/\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final ComparableProvider[][] comparables = new ComparableProvider[fields.length][];\n    for(int i=0;i<fields.length;i++) {\n      comparables[i] = getComparableProviders(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparables.length;i++) {\n            int cmp = a.values[i].compareTo(b.values[i]);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          } else {\n            return a.docID < b.docID;\n          }\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      LeafAndDocID leaf = new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc(), comparables.length);\n      for(int j=0;j<comparables.length;j++) {\n        leaf.values[j] = comparables[j][i].getComparable(leaf.docID);\n        assert leaf.values[j] != null;\n      }\n      queue.add(leaf);\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    // merge sort:\n    int mappedDocID = 0;\n    int lastReaderIndex = 0;\n    boolean isSorted = true;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      if (lastReaderIndex > top.readerIndex) {\n        // merge sort is needed\n        isSorted = false;\n      }\n      lastReaderIndex = top.readerIndex;\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        for(int j=0;j<comparables.length;j++) {\n          top.values[j] = comparables[j][top.readerIndex].getComparable(top.docID);\n          assert top.values[j] != null;\n        }\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n    if (isSorted) {\n      return null;\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","sourceOld":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort! */\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final ComparableProvider[][] comparables = new ComparableProvider[fields.length][];\n    for(int i=0;i<fields.length;i++) {\n      comparables[i] = getComparableProviders(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparables.length;i++) {\n            int cmp = a.values[i].compareTo(b.values[i]);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          } else {\n            return a.docID < b.docID;\n          }\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      LeafAndDocID leaf = new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc(), comparables.length);\n      for(int j=0;j<comparables.length;j++) {\n        leaf.values[j] = comparables[j][i].getComparable(leaf.docID);\n        assert leaf.values[j] != null;\n      }\n      queue.add(leaf);\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    // merge sort:\n    int mappedDocID = 0;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        for(int j=0;j<comparables.length;j++) {\n          top.values[j] = comparables[j][top.readerIndex].getComparable(top.docID);\n          assert top.values[j] != null;\n        }\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"727bb765ff2542275f6d31f67be18d7104bae148","date":1480353976,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","sourceNew":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort!\n   *  Returns null if the merge sort is not needed (segments are already in index sort order).\n   **/\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final ComparableProvider[][] comparables = new ComparableProvider[fields.length][];\n    for(int i=0;i<fields.length;i++) {\n      comparables[i] = getComparableProviders(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparables.length;i++) {\n            int cmp = a.values[i].compareTo(b.values[i]);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          } else {\n            return a.docID < b.docID;\n          }\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      LeafAndDocID leaf = new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc(), comparables.length);\n      for(int j=0;j<comparables.length;j++) {\n        leaf.values[j] = comparables[j][i].getComparable(leaf.docID);\n        assert leaf.values[j] != null;\n      }\n      queue.add(leaf);\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    // merge sort:\n    int mappedDocID = 0;\n    int lastReaderIndex = 0;\n    boolean isSorted = true;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      if (lastReaderIndex > top.readerIndex) {\n        // merge sort is needed\n        isSorted = false;\n      }\n      lastReaderIndex = top.readerIndex;\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        for(int j=0;j<comparables.length;j++) {\n          top.values[j] = comparables[j][top.readerIndex].getComparable(top.docID);\n          assert top.values[j] != null;\n        }\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n    if (isSorted) {\n      return null;\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","sourceOld":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort! */\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final ComparableProvider[][] comparables = new ComparableProvider[fields.length][];\n    for(int i=0;i<fields.length;i++) {\n      comparables[i] = getComparableProviders(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparables.length;i++) {\n            int cmp = a.values[i].compareTo(b.values[i]);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          } else {\n            return a.docID < b.docID;\n          }\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      LeafAndDocID leaf = new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc(), comparables.length);\n      for(int j=0;j<comparables.length;j++) {\n        leaf.values[j] = comparables[j][i].getComparable(leaf.docID);\n        assert leaf.values[j] != null;\n      }\n      queue.add(leaf);\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    // merge sort:\n    int mappedDocID = 0;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        for(int j=0;j<comparables.length;j++) {\n          top.values[j] = comparables[j][top.readerIndex].getComparable(top.docID);\n          assert top.values[j] != null;\n        }\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"67d866889b0c200f91ef946ae758a42d324216da","date":1544206319,"type":3,"author":"Jim Ferenczi","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","sourceNew":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort!\n   *  Returns null if the merge sort is not needed (segments are already in index sort order).\n   **/\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final ComparableProvider[][] comparables = new ComparableProvider[fields.length][];\n    final int[] reverseMuls = new int[fields.length];\n    for(int i=0;i<fields.length;i++) {\n      comparables[i] = getComparableProviders(readers, fields[i]);\n      reverseMuls[i] = fields[i].getReverse() ? -1 : 1;\n    }\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparables.length;i++) {\n            int cmp = reverseMuls[i] * a.values[i].compareTo(b.values[i]);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          } else {\n            return a.docID < b.docID;\n          }\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      LeafAndDocID leaf = new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc(), comparables.length);\n      for(int j=0;j<comparables.length;j++) {\n        leaf.values[j] = comparables[j][i].getComparable(leaf.docID);\n        assert leaf.values[j] != null;\n      }\n      queue.add(leaf);\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    // merge sort:\n    int mappedDocID = 0;\n    int lastReaderIndex = 0;\n    boolean isSorted = true;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      if (lastReaderIndex > top.readerIndex) {\n        // merge sort is needed\n        isSorted = false;\n      }\n      lastReaderIndex = top.readerIndex;\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        for(int j=0;j<comparables.length;j++) {\n          top.values[j] = comparables[j][top.readerIndex].getComparable(top.docID);\n          assert top.values[j] != null;\n        }\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n    if (isSorted) {\n      return null;\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","sourceOld":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort!\n   *  Returns null if the merge sort is not needed (segments are already in index sort order).\n   **/\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final ComparableProvider[][] comparables = new ComparableProvider[fields.length][];\n    for(int i=0;i<fields.length;i++) {\n      comparables[i] = getComparableProviders(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparables.length;i++) {\n            int cmp = a.values[i].compareTo(b.values[i]);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          } else {\n            return a.docID < b.docID;\n          }\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      LeafAndDocID leaf = new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc(), comparables.length);\n      for(int j=0;j<comparables.length;j++) {\n        leaf.values[j] = comparables[j][i].getComparable(leaf.docID);\n        assert leaf.values[j] != null;\n      }\n      queue.add(leaf);\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    // merge sort:\n    int mappedDocID = 0;\n    int lastReaderIndex = 0;\n    boolean isSorted = true;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      if (lastReaderIndex > top.readerIndex) {\n        // merge sort is needed\n        isSorted = false;\n      }\n      lastReaderIndex = top.readerIndex;\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        for(int j=0;j<comparables.length;j++) {\n          top.values[j] = comparables[j][top.readerIndex].getComparable(top.docID);\n          assert top.values[j] != null;\n        }\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n    if (isSorted) {\n      return null;\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"66dc286cff642f0c485052d838cfba579b3f84d3","date":1544206544,"type":3,"author":"Jim Ferenczi","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","sourceNew":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort!\n   *  Returns null if the merge sort is not needed (segments are already in index sort order).\n   **/\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final ComparableProvider[][] comparables = new ComparableProvider[fields.length][];\n    for(int i=0;i<fields.length;i++) {\n      comparables[i] = getComparableProviders(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparables.length;i++) {\n            int cmp = a.values[i].compareTo(b.values[i]);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          } else {\n            return a.docID < b.docID;\n          }\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      LeafAndDocID leaf = new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc(), comparables.length);\n      for(int j=0;j<comparables.length;j++) {\n        leaf.values[j] = comparables[j][i].getComparable(leaf.docID);\n        assert leaf.values[j] != null;\n      }\n      queue.add(leaf);\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    // merge sort:\n    int mappedDocID = 0;\n    int lastReaderIndex = 0;\n    boolean isSorted = true;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      if (lastReaderIndex > top.readerIndex) {\n        // merge sort is needed\n        isSorted = false;\n      }\n      lastReaderIndex = top.readerIndex;\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        for(int j=0;j<comparables.length;j++) {\n          top.values[j] = comparables[j][top.readerIndex].getComparable(top.docID);\n          assert top.values[j] != null;\n        }\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n    if (isSorted) {\n      return null;\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","sourceOld":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort!\n   *  Returns null if the merge sort is not needed (segments are already in index sort order).\n   **/\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final ComparableProvider[][] comparables = new ComparableProvider[fields.length][];\n    final int[] reverseMuls = new int[fields.length];\n    for(int i=0;i<fields.length;i++) {\n      comparables[i] = getComparableProviders(readers, fields[i]);\n      reverseMuls[i] = fields[i].getReverse() ? -1 : 1;\n    }\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparables.length;i++) {\n            int cmp = reverseMuls[i] * a.values[i].compareTo(b.values[i]);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          } else {\n            return a.docID < b.docID;\n          }\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      LeafAndDocID leaf = new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc(), comparables.length);\n      for(int j=0;j<comparables.length;j++) {\n        leaf.values[j] = comparables[j][i].getComparable(leaf.docID);\n        assert leaf.values[j] != null;\n      }\n      queue.add(leaf);\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    // merge sort:\n    int mappedDocID = 0;\n    int lastReaderIndex = 0;\n    boolean isSorted = true;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      if (lastReaderIndex > top.readerIndex) {\n        // merge sort is needed\n        isSorted = false;\n      }\n      lastReaderIndex = top.readerIndex;\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        for(int j=0;j<comparables.length;j++) {\n          top.values[j] = comparables[j][top.readerIndex].getComparable(top.docID);\n          assert top.values[j] != null;\n        }\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n    if (isSorted) {\n      return null;\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"76b2395079a51ec9c99b4d38c3967f035ab47cdb","date":1544206602,"type":3,"author":"Jim Ferenczi","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","sourceNew":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort!\n   *  Returns null if the merge sort is not needed (segments are already in index sort order).\n   **/\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final ComparableProvider[][] comparables = new ComparableProvider[fields.length][];\n    final int[] reverseMuls = new int[fields.length];\n    for(int i=0;i<fields.length;i++) {\n      comparables[i] = getComparableProviders(readers, fields[i]);\n      reverseMuls[i] = fields[i].getReverse() ? -1 : 1;\n    }\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparables.length;i++) {\n            int cmp = reverseMuls[i] * a.values[i].compareTo(b.values[i]);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          } else {\n            return a.docID < b.docID;\n          }\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      LeafAndDocID leaf = new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc(), comparables.length);\n      for(int j=0;j<comparables.length;j++) {\n        leaf.values[j] = comparables[j][i].getComparable(leaf.docID);\n        assert leaf.values[j] != null;\n      }\n      queue.add(leaf);\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    // merge sort:\n    int mappedDocID = 0;\n    int lastReaderIndex = 0;\n    boolean isSorted = true;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      if (lastReaderIndex > top.readerIndex) {\n        // merge sort is needed\n        isSorted = false;\n      }\n      lastReaderIndex = top.readerIndex;\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        for(int j=0;j<comparables.length;j++) {\n          top.values[j] = comparables[j][top.readerIndex].getComparable(top.docID);\n          assert top.values[j] != null;\n        }\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n    if (isSorted) {\n      return null;\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","sourceOld":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort!\n   *  Returns null if the merge sort is not needed (segments are already in index sort order).\n   **/\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final ComparableProvider[][] comparables = new ComparableProvider[fields.length][];\n    for(int i=0;i<fields.length;i++) {\n      comparables[i] = getComparableProviders(readers, fields[i]);\n    }\n\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparables.length;i++) {\n            int cmp = a.values[i].compareTo(b.values[i]);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          } else {\n            return a.docID < b.docID;\n          }\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      LeafAndDocID leaf = new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc(), comparables.length);\n      for(int j=0;j<comparables.length;j++) {\n        leaf.values[j] = comparables[j][i].getComparable(leaf.docID);\n        assert leaf.values[j] != null;\n      }\n      queue.add(leaf);\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    // merge sort:\n    int mappedDocID = 0;\n    int lastReaderIndex = 0;\n    boolean isSorted = true;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      if (lastReaderIndex > top.readerIndex) {\n        // merge sort is needed\n        isSorted = false;\n      }\n      lastReaderIndex = top.readerIndex;\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        for(int j=0;j<comparables.length;j++) {\n          top.values[j] = comparables[j][top.readerIndex].getComparable(top.docID);\n          assert top.values[j] != null;\n        }\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n    if (isSorted) {\n      return null;\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aa2458e2941e732b29b4129f7f42cf4f46e4b1f7","date":1562681927,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","sourceNew":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort!\n   *  Returns null if the merge sort is not needed (segments are already in index sort order).\n   **/\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final ComparableProvider[][] comparables = new ComparableProvider[fields.length][];\n    final int[] reverseMuls = new int[fields.length];\n    for(int i=0;i<fields.length;i++) {\n      comparables[i] = getComparableProviders(readers, fields[i]);\n      reverseMuls[i] = fields[i].getReverse() ? -1 : 1;\n    }\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparables.length;i++) {\n            int cmp = Long.compare(a.valuesAsComparableLongs[i], b.valuesAsComparableLongs[i]);\n            if (cmp != 0) {\n              return reverseMuls[i] * cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          } else {\n            return a.docID < b.docID;\n          }\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      LeafAndDocID leaf = new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc(), comparables.length);\n      for(int j=0;j<comparables.length;j++) {\n        leaf.valuesAsComparableLongs[j] = comparables[j][i].getAsComparableLong(leaf.docID);\n      }\n      queue.add(leaf);\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    // merge sort:\n    int mappedDocID = 0;\n    int lastReaderIndex = 0;\n    boolean isSorted = true;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      if (lastReaderIndex > top.readerIndex) {\n        // merge sort is needed\n        isSorted = false;\n      }\n      lastReaderIndex = top.readerIndex;\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        for(int j=0;j<comparables.length;j++) {\n          top.valuesAsComparableLongs[j] = comparables[j][top.readerIndex].getAsComparableLong(top.docID);\n        }\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n    if (isSorted) {\n      return null;\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","sourceOld":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort!\n   *  Returns null if the merge sort is not needed (segments are already in index sort order).\n   **/\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final ComparableProvider[][] comparables = new ComparableProvider[fields.length][];\n    final int[] reverseMuls = new int[fields.length];\n    for(int i=0;i<fields.length;i++) {\n      comparables[i] = getComparableProviders(readers, fields[i]);\n      reverseMuls[i] = fields[i].getReverse() ? -1 : 1;\n    }\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparables.length;i++) {\n            int cmp = reverseMuls[i] * a.values[i].compareTo(b.values[i]);\n            if (cmp != 0) {\n              return cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          } else {\n            return a.docID < b.docID;\n          }\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      LeafAndDocID leaf = new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc(), comparables.length);\n      for(int j=0;j<comparables.length;j++) {\n        leaf.values[j] = comparables[j][i].getComparable(leaf.docID);\n        assert leaf.values[j] != null;\n      }\n      queue.add(leaf);\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    // merge sort:\n    int mappedDocID = 0;\n    int lastReaderIndex = 0;\n    boolean isSorted = true;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      if (lastReaderIndex > top.readerIndex) {\n        // merge sort is needed\n        isSorted = false;\n      }\n      lastReaderIndex = top.readerIndex;\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        for(int j=0;j<comparables.length;j++) {\n          top.values[j] = comparables[j][top.readerIndex].getComparable(top.docID);\n          assert top.values[j] != null;\n        }\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n    if (isSorted) {\n      return null;\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"773bf150032d3ef6c95997a154fb914b82875cb8","date":1590150786,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/MultiSorter#sort(Sort,List[CodecReader]).mjava","sourceNew":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort!\n   *  Returns null if the merge sort is not needed (segments are already in index sort order).\n   **/\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final IndexSorter.ComparableProvider[][] comparables = new IndexSorter.ComparableProvider[fields.length][];\n    final int[] reverseMuls = new int[fields.length];\n    for(int i=0;i<fields.length;i++) {\n      IndexSorter sorter = fields[i].getIndexSorter();\n      if (sorter == null) {\n        throw new IllegalArgumentException(\"Cannot use sort field \" + fields[i] + \" for index sorting\");\n      }\n      comparables[i] = sorter.getComparableProviders(readers);\n      reverseMuls[i] = fields[i].getReverse() ? -1 : 1;\n    }\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparables.length;i++) {\n            int cmp = Long.compare(a.valuesAsComparableLongs[i], b.valuesAsComparableLongs[i]);\n            if (cmp != 0) {\n              return reverseMuls[i] * cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          } else {\n            return a.docID < b.docID;\n          }\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      LeafAndDocID leaf = new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc(), comparables.length);\n      for(int j=0;j<comparables.length;j++) {\n        leaf.valuesAsComparableLongs[j] = comparables[j][i].getAsComparableLong(leaf.docID);\n      }\n      queue.add(leaf);\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    // merge sort:\n    int mappedDocID = 0;\n    int lastReaderIndex = 0;\n    boolean isSorted = true;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      if (lastReaderIndex > top.readerIndex) {\n        // merge sort is needed\n        isSorted = false;\n      }\n      lastReaderIndex = top.readerIndex;\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        for(int j=0;j<comparables.length;j++) {\n          top.valuesAsComparableLongs[j] = comparables[j][top.readerIndex].getAsComparableLong(top.docID);\n        }\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n    if (isSorted) {\n      return null;\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","sourceOld":"  /** Does a merge sort of the leaves of the incoming reader, returning {@link DocMap} to map each leaf's\n   *  documents into the merged segment.  The documents for each incoming leaf reader must already be sorted by the same sort!\n   *  Returns null if the merge sort is not needed (segments are already in index sort order).\n   **/\n  static MergeState.DocMap[] sort(Sort sort, List<CodecReader> readers) throws IOException {\n\n    // TODO: optimize if only 1 reader is incoming, though that's a rare case\n\n    SortField fields[] = sort.getSort();\n    final ComparableProvider[][] comparables = new ComparableProvider[fields.length][];\n    final int[] reverseMuls = new int[fields.length];\n    for(int i=0;i<fields.length;i++) {\n      comparables[i] = getComparableProviders(readers, fields[i]);\n      reverseMuls[i] = fields[i].getReverse() ? -1 : 1;\n    }\n    int leafCount = readers.size();\n\n    PriorityQueue<LeafAndDocID> queue = new PriorityQueue<LeafAndDocID>(leafCount) {\n        @Override\n        public boolean lessThan(LeafAndDocID a, LeafAndDocID b) {\n          for(int i=0;i<comparables.length;i++) {\n            int cmp = Long.compare(a.valuesAsComparableLongs[i], b.valuesAsComparableLongs[i]);\n            if (cmp != 0) {\n              return reverseMuls[i] * cmp < 0;\n            }\n          }\n\n          // tie-break by docID natural order:\n          if (a.readerIndex != b.readerIndex) {\n            return a.readerIndex < b.readerIndex;\n          } else {\n            return a.docID < b.docID;\n          }\n        }\n    };\n\n    PackedLongValues.Builder[] builders = new PackedLongValues.Builder[leafCount];\n\n    for(int i=0;i<leafCount;i++) {\n      CodecReader reader = readers.get(i);\n      LeafAndDocID leaf = new LeafAndDocID(i, reader.getLiveDocs(), reader.maxDoc(), comparables.length);\n      for(int j=0;j<comparables.length;j++) {\n        leaf.valuesAsComparableLongs[j] = comparables[j][i].getAsComparableLong(leaf.docID);\n      }\n      queue.add(leaf);\n      builders[i] = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n    }\n\n    // merge sort:\n    int mappedDocID = 0;\n    int lastReaderIndex = 0;\n    boolean isSorted = true;\n    while (queue.size() != 0) {\n      LeafAndDocID top = queue.top();\n      if (lastReaderIndex > top.readerIndex) {\n        // merge sort is needed\n        isSorted = false;\n      }\n      lastReaderIndex = top.readerIndex;\n      builders[top.readerIndex].add(mappedDocID);\n      if (top.liveDocs == null || top.liveDocs.get(top.docID)) {\n        mappedDocID++;\n      }\n      top.docID++;\n      if (top.docID < top.maxDoc) {\n        for(int j=0;j<comparables.length;j++) {\n          top.valuesAsComparableLongs[j] = comparables[j][top.readerIndex].getAsComparableLong(top.docID);\n        }\n        queue.updateTop();\n      } else {\n        queue.pop();\n      }\n    }\n    if (isSorted) {\n      return null;\n    }\n\n    MergeState.DocMap[] docMaps = new MergeState.DocMap[leafCount];\n    for(int i=0;i<leafCount;i++) {\n      final PackedLongValues remapped = builders[i].build();\n      final Bits liveDocs = readers.get(i).getLiveDocs();\n      docMaps[i] = new MergeState.DocMap() {\n          @Override\n          public int get(int docID) {\n            if (liveDocs == null || liveDocs.get(docID)) {\n              return (int) remapped.get(docID);\n            } else {\n              return -1;\n            }\n          }\n        };\n    }\n\n    return docMaps;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"66dc286cff642f0c485052d838cfba579b3f84d3":["67d866889b0c200f91ef946ae758a42d324216da"],"76b2395079a51ec9c99b4d38c3967f035ab47cdb":["66dc286cff642f0c485052d838cfba579b3f84d3"],"0ad30c6a479e764150a3316e57263319775f1df2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3d33e731a93d4b57e662ff094f64f94a745422d4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","0ad30c6a479e764150a3316e57263319775f1df2"],"fb0345a2d45479f891041f8b3ce351bc975e64ac":["ceaef6cfc68c8ab22a684192e469a8280f9e6e70"],"67d866889b0c200f91ef946ae758a42d324216da":["653128722fb3b4713ac331c621491a93f34a4a22"],"9c23672acbb5104509c1c2d6e3dda7a08eaf091f":["5e03940e6e9044943de4b7ac08f8581da37a9534"],"727bb765ff2542275f6d31f67be18d7104bae148":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","653128722fb3b4713ac331c621491a93f34a4a22"],"aa2458e2941e732b29b4129f7f42cf4f46e4b1f7":["76b2395079a51ec9c99b4d38c3967f035ab47cdb"],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"773bf150032d3ef6c95997a154fb914b82875cb8":["aa2458e2941e732b29b4129f7f42cf4f46e4b1f7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["d470c8182e92b264680e34081b75e70a9f2b3c89","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"5e03940e6e9044943de4b7ac08f8581da37a9534":["fb0345a2d45479f891041f8b3ce351bc975e64ac"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","9c23672acbb5104509c1c2d6e3dda7a08eaf091f"],"653128722fb3b4713ac331c621491a93f34a4a22":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["773bf150032d3ef6c95997a154fb914b82875cb8"]},"commit2Childs":{"66dc286cff642f0c485052d838cfba579b3f84d3":["76b2395079a51ec9c99b4d38c3967f035ab47cdb"],"76b2395079a51ec9c99b4d38c3967f035ab47cdb":["aa2458e2941e732b29b4129f7f42cf4f46e4b1f7"],"0ad30c6a479e764150a3316e57263319775f1df2":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["727bb765ff2542275f6d31f67be18d7104bae148"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"fb0345a2d45479f891041f8b3ce351bc975e64ac":["5e03940e6e9044943de4b7ac08f8581da37a9534"],"67d866889b0c200f91ef946ae758a42d324216da":["66dc286cff642f0c485052d838cfba579b3f84d3"],"9c23672acbb5104509c1c2d6e3dda7a08eaf091f":["3d33e731a93d4b57e662ff094f64f94a745422d4"],"727bb765ff2542275f6d31f67be18d7104bae148":[],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["fb0345a2d45479f891041f8b3ce351bc975e64ac"],"aa2458e2941e732b29b4129f7f42cf4f46e4b1f7":["773bf150032d3ef6c95997a154fb914b82875cb8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0ad30c6a479e764150a3316e57263319775f1df2","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d470c8182e92b264680e34081b75e70a9f2b3c89","ceaef6cfc68c8ab22a684192e469a8280f9e6e70","3d33e731a93d4b57e662ff094f64f94a745422d4"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","653128722fb3b4713ac331c621491a93f34a4a22"],"773bf150032d3ef6c95997a154fb914b82875cb8":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"5e03940e6e9044943de4b7ac08f8581da37a9534":["9c23672acbb5104509c1c2d6e3dda7a08eaf091f"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["0ad30c6a479e764150a3316e57263319775f1df2"],"653128722fb3b4713ac331c621491a93f34a4a22":["67d866889b0c200f91ef946ae758a42d324216da","727bb765ff2542275f6d31f67be18d7104bae148"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["727bb765ff2542275f6d31f67be18d7104bae148","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}