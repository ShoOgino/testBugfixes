{"path":"solr/core/src/test/org/apache/solr/cloud/TestStressLiveNodes#testStress().mjava","commits":[{"id":"d3726445f8e9a7d398466439f0f84b5bb329fdcc","date":1460555680,"type":0,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestStressLiveNodes#testStress().mjava","pathOld":"/dev/null","sourceNew":"  public void testStress() throws Exception {\n\n    // do many iters, so we have \"bursts\" of adding nodes that we then check\n    final int numIters = atLeast(1000);\n    for (int iter = 0; iter < numIters; iter++) {\n\n      // sanity check that ZK says there is in fact 1 live node\n      List<String> actualLiveNodes = getTrueLiveNodesFromZk();\n      assertEquals(\"iter\"+iter+\": \" + actualLiveNodes.toString(),\n                   1, actualLiveNodes.size());\n\n      // only here do we forcibly update the cached live nodes so we don't have to wait for it to catch up\n      // with all the ephemeral nodes that vanished after the last iteration\n      CLOUD_CLIENT.getZkStateReader().updateLiveNodes();\n      \n      // sanity check that our Cloud Client's local state knows about the 1 (real) live node in our cluster\n      List<String> cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n      assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                   actualLiveNodes, cachedLiveNodes);\n      \n      \n      // start spining up some threads to add some live_node children in parallel\n\n      // we don't need a lot of threads or nodes (we don't want to swamp the CPUs\n      // just bursts of conccurent adds) but we do want to randomize it a bit so we increase the\n      // odds of concurrent watchers firing regardless of the num CPUs or load on the machine running\n      // the test (but we deliberately don't look at availableProcessors() since we want randomization\n      // consistency across all machines for a given seed)\n      final int numThreads = TestUtil.nextInt(random(), 2, 5);\n      \n      // use same num for all thrashers, to increase likely hood of them all competing\n      // (diff random number would mean heavy concurency only for ~ the first N=lowest num requetss)\n      //\n      // this does not need to be a large number -- in fact, the higher it is, the more\n      // likely we are to see a mistake in early watcher triggers get \"corrected\" by a later one\n      // and overlook a possible bug\n      final int numNodesPerThrasher = TestUtil.nextInt(random(), 1, 5);\n      \n      log.info(\"preparing parallel adds to live nodes: iter={}, numThreads={} numNodesPerThread={}\",\n               iter, numThreads, numNodesPerThrasher);\n      \n      // NOTE: using ephemeral nodes\n      // so we can't close any of these thrashers until we are done with our assertions\n      final List<LiveNodeTrasher> thrashers = new ArrayList<>(numThreads);\n      for (int i = 0; i < numThreads; i++) {\n        thrashers.add(new LiveNodeTrasher(\"T\"+iter+\"_\"+i, numNodesPerThrasher));\n      }\n      try {\n        final ExecutorService executorService = ExecutorUtil.newMDCAwareFixedThreadPool\n          (thrashers.size()+1, new DefaultSolrThreadFactory(\"test_live_nodes_thrasher_iter\"+iter));\n        \n        executorService.invokeAll(thrashers);\n        executorService.shutdown();\n        if (! executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS)) {\n          for (LiveNodeTrasher thrasher : thrashers) {\n            thrasher.stop();\n          }\n        }\n        assertTrue(\"iter\"+iter+\": thrashers didn't finish even after explicitly stopping\",\n                   executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS));\n\n        // sanity check the *real* live_nodes entries from ZK match what the thrashers added\n        int totalAdded = 1; // 1 real live node when we started\n        for (LiveNodeTrasher thrasher : thrashers) {\n          totalAdded += thrasher.getNumAdded();\n        }\n        actualLiveNodes = getTrueLiveNodesFromZk();\n        assertEquals(\"iter\"+iter, totalAdded, actualLiveNodes.size());\n        \n        // verify our local client knows the correct set of live nodes\n        cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n        assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                     actualLiveNodes, cachedLiveNodes);\n        \n      } finally {\n        for (LiveNodeTrasher thrasher : thrashers) {\n          // shutdown our zk connection, freeing our ephemeral nodes\n          thrasher.close();\n        }\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9ea9249ab9a9f76eb4132ceb4631d15315721f6a","date":1460578553,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestStressLiveNodes#testStress().mjava","pathOld":"/dev/null","sourceNew":"  public void testStress() throws Exception {\n\n    // do many iters, so we have \"bursts\" of adding nodes that we then check\n    final int numIters = atLeast(1000);\n    for (int iter = 0; iter < numIters; iter++) {\n\n      // sanity check that ZK says there is in fact 1 live node\n      List<String> actualLiveNodes = getTrueLiveNodesFromZk();\n      assertEquals(\"iter\"+iter+\": \" + actualLiveNodes.toString(),\n                   1, actualLiveNodes.size());\n\n      // only here do we forcibly update the cached live nodes so we don't have to wait for it to catch up\n      // with all the ephemeral nodes that vanished after the last iteration\n      CLOUD_CLIENT.getZkStateReader().updateLiveNodes();\n      \n      // sanity check that our Cloud Client's local state knows about the 1 (real) live node in our cluster\n      List<String> cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n      assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                   actualLiveNodes, cachedLiveNodes);\n      \n      \n      // start spining up some threads to add some live_node children in parallel\n\n      // we don't need a lot of threads or nodes (we don't want to swamp the CPUs\n      // just bursts of conccurent adds) but we do want to randomize it a bit so we increase the\n      // odds of concurrent watchers firing regardless of the num CPUs or load on the machine running\n      // the test (but we deliberately don't look at availableProcessors() since we want randomization\n      // consistency across all machines for a given seed)\n      final int numThreads = TestUtil.nextInt(random(), 2, 5);\n      \n      // use same num for all thrashers, to increase likely hood of them all competing\n      // (diff random number would mean heavy concurency only for ~ the first N=lowest num requetss)\n      //\n      // this does not need to be a large number -- in fact, the higher it is, the more\n      // likely we are to see a mistake in early watcher triggers get \"corrected\" by a later one\n      // and overlook a possible bug\n      final int numNodesPerThrasher = TestUtil.nextInt(random(), 1, 5);\n      \n      log.info(\"preparing parallel adds to live nodes: iter={}, numThreads={} numNodesPerThread={}\",\n               iter, numThreads, numNodesPerThrasher);\n      \n      // NOTE: using ephemeral nodes\n      // so we can't close any of these thrashers until we are done with our assertions\n      final List<LiveNodeTrasher> thrashers = new ArrayList<>(numThreads);\n      for (int i = 0; i < numThreads; i++) {\n        thrashers.add(new LiveNodeTrasher(\"T\"+iter+\"_\"+i, numNodesPerThrasher));\n      }\n      try {\n        final ExecutorService executorService = ExecutorUtil.newMDCAwareFixedThreadPool\n          (thrashers.size()+1, new DefaultSolrThreadFactory(\"test_live_nodes_thrasher_iter\"+iter));\n        \n        executorService.invokeAll(thrashers);\n        executorService.shutdown();\n        if (! executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS)) {\n          for (LiveNodeTrasher thrasher : thrashers) {\n            thrasher.stop();\n          }\n        }\n        assertTrue(\"iter\"+iter+\": thrashers didn't finish even after explicitly stopping\",\n                   executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS));\n\n        // sanity check the *real* live_nodes entries from ZK match what the thrashers added\n        int totalAdded = 1; // 1 real live node when we started\n        for (LiveNodeTrasher thrasher : thrashers) {\n          totalAdded += thrasher.getNumAdded();\n        }\n        actualLiveNodes = getTrueLiveNodesFromZk();\n        assertEquals(\"iter\"+iter, totalAdded, actualLiveNodes.size());\n        \n        // verify our local client knows the correct set of live nodes\n        cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n        assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                     actualLiveNodes, cachedLiveNodes);\n        \n      } finally {\n        for (LiveNodeTrasher thrasher : thrashers) {\n          // shutdown our zk connection, freeing our ephemeral nodes\n          thrasher.close();\n        }\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0158ced21948b6626f733c1c42c1e18d94449789","date":1462994341,"type":3,"author":"Bartosz KrasiÅ„ski","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestStressLiveNodes#testStress().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestStressLiveNodes#testStress().mjava","sourceNew":"  public void testStress() throws Exception {\n\n    // do many iters, so we have \"bursts\" of adding nodes that we then check\n    final int numIters = atLeast(1000);\n    for (int iter = 0; iter < numIters; iter++) {\n\n      // sanity check that ZK says there is in fact 1 live node\n      List<String> actualLiveNodes = getTrueLiveNodesFromZk();\n      assertEquals(\"iter\"+iter+\": \" + actualLiveNodes.toString(),\n                   1, actualLiveNodes.size());\n\n      // only here do we forcibly update the cached live nodes so we don't have to wait for it to catch up\n      // with all the ephemeral nodes that vanished after the last iteration\n      CLOUD_CLIENT.getZkStateReader().updateLiveNodes();\n      \n      // sanity check that our Cloud Client's local state knows about the 1 (real) live node in our cluster\n      List<String> cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n      assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                   actualLiveNodes, cachedLiveNodes);\n      \n      \n      // start spinning up some threads to add some live_node children in parallel\n\n      // we don't need a lot of threads or nodes (we don't want to swamp the CPUs\n      // just bursts of concurrent adds) but we do want to randomize it a bit so we increase the\n      // odds of concurrent watchers firing regardless of the num CPUs or load on the machine running\n      // the test (but we deliberately don't look at availableProcessors() since we want randomization\n      // consistency across all machines for a given seed)\n      final int numThreads = TestUtil.nextInt(random(), 2, 5);\n      \n      // use same num for all thrashers, to increase likely hood of them all competing\n      // (diff random number would mean heavy concurrency only for ~ the first N=lowest num requests)\n      //\n      // this does not need to be a large number -- in fact, the higher it is, the more\n      // likely we are to see a mistake in early watcher triggers get \"corrected\" by a later one\n      // and overlook a possible bug\n      final int numNodesPerThrasher = TestUtil.nextInt(random(), 1, 5);\n      \n      log.info(\"preparing parallel adds to live nodes: iter={}, numThreads={} numNodesPerThread={}\",\n               iter, numThreads, numNodesPerThrasher);\n      \n      // NOTE: using ephemeral nodes\n      // so we can't close any of these thrashers until we are done with our assertions\n      final List<LiveNodeTrasher> thrashers = new ArrayList<>(numThreads);\n      for (int i = 0; i < numThreads; i++) {\n        thrashers.add(new LiveNodeTrasher(\"T\"+iter+\"_\"+i, numNodesPerThrasher));\n      }\n      try {\n        final ExecutorService executorService = ExecutorUtil.newMDCAwareFixedThreadPool\n          (thrashers.size()+1, new DefaultSolrThreadFactory(\"test_live_nodes_thrasher_iter\"+iter));\n        \n        executorService.invokeAll(thrashers);\n        executorService.shutdown();\n        if (! executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS)) {\n          for (LiveNodeTrasher thrasher : thrashers) {\n            thrasher.stop();\n          }\n        }\n        assertTrue(\"iter\"+iter+\": thrashers didn't finish even after explicitly stopping\",\n                   executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS));\n\n        // sanity check the *real* live_nodes entries from ZK match what the thrashers added\n        int totalAdded = 1; // 1 real live node when we started\n        for (LiveNodeTrasher thrasher : thrashers) {\n          totalAdded += thrasher.getNumAdded();\n        }\n        actualLiveNodes = getTrueLiveNodesFromZk();\n        assertEquals(\"iter\"+iter, totalAdded, actualLiveNodes.size());\n        \n        // verify our local client knows the correct set of live nodes\n        cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n        assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                     actualLiveNodes, cachedLiveNodes);\n        \n      } finally {\n        for (LiveNodeTrasher thrasher : thrashers) {\n          // shutdown our zk connection, freeing our ephemeral nodes\n          thrasher.close();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public void testStress() throws Exception {\n\n    // do many iters, so we have \"bursts\" of adding nodes that we then check\n    final int numIters = atLeast(1000);\n    for (int iter = 0; iter < numIters; iter++) {\n\n      // sanity check that ZK says there is in fact 1 live node\n      List<String> actualLiveNodes = getTrueLiveNodesFromZk();\n      assertEquals(\"iter\"+iter+\": \" + actualLiveNodes.toString(),\n                   1, actualLiveNodes.size());\n\n      // only here do we forcibly update the cached live nodes so we don't have to wait for it to catch up\n      // with all the ephemeral nodes that vanished after the last iteration\n      CLOUD_CLIENT.getZkStateReader().updateLiveNodes();\n      \n      // sanity check that our Cloud Client's local state knows about the 1 (real) live node in our cluster\n      List<String> cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n      assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                   actualLiveNodes, cachedLiveNodes);\n      \n      \n      // start spining up some threads to add some live_node children in parallel\n\n      // we don't need a lot of threads or nodes (we don't want to swamp the CPUs\n      // just bursts of conccurent adds) but we do want to randomize it a bit so we increase the\n      // odds of concurrent watchers firing regardless of the num CPUs or load on the machine running\n      // the test (but we deliberately don't look at availableProcessors() since we want randomization\n      // consistency across all machines for a given seed)\n      final int numThreads = TestUtil.nextInt(random(), 2, 5);\n      \n      // use same num for all thrashers, to increase likely hood of them all competing\n      // (diff random number would mean heavy concurency only for ~ the first N=lowest num requetss)\n      //\n      // this does not need to be a large number -- in fact, the higher it is, the more\n      // likely we are to see a mistake in early watcher triggers get \"corrected\" by a later one\n      // and overlook a possible bug\n      final int numNodesPerThrasher = TestUtil.nextInt(random(), 1, 5);\n      \n      log.info(\"preparing parallel adds to live nodes: iter={}, numThreads={} numNodesPerThread={}\",\n               iter, numThreads, numNodesPerThrasher);\n      \n      // NOTE: using ephemeral nodes\n      // so we can't close any of these thrashers until we are done with our assertions\n      final List<LiveNodeTrasher> thrashers = new ArrayList<>(numThreads);\n      for (int i = 0; i < numThreads; i++) {\n        thrashers.add(new LiveNodeTrasher(\"T\"+iter+\"_\"+i, numNodesPerThrasher));\n      }\n      try {\n        final ExecutorService executorService = ExecutorUtil.newMDCAwareFixedThreadPool\n          (thrashers.size()+1, new DefaultSolrThreadFactory(\"test_live_nodes_thrasher_iter\"+iter));\n        \n        executorService.invokeAll(thrashers);\n        executorService.shutdown();\n        if (! executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS)) {\n          for (LiveNodeTrasher thrasher : thrashers) {\n            thrasher.stop();\n          }\n        }\n        assertTrue(\"iter\"+iter+\": thrashers didn't finish even after explicitly stopping\",\n                   executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS));\n\n        // sanity check the *real* live_nodes entries from ZK match what the thrashers added\n        int totalAdded = 1; // 1 real live node when we started\n        for (LiveNodeTrasher thrasher : thrashers) {\n          totalAdded += thrasher.getNumAdded();\n        }\n        actualLiveNodes = getTrueLiveNodesFromZk();\n        assertEquals(\"iter\"+iter, totalAdded, actualLiveNodes.size());\n        \n        // verify our local client knows the correct set of live nodes\n        cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n        assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                     actualLiveNodes, cachedLiveNodes);\n        \n      } finally {\n        for (LiveNodeTrasher thrasher : thrashers) {\n          // shutdown our zk connection, freeing our ephemeral nodes\n          thrasher.close();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestStressLiveNodes#testStress().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestStressLiveNodes#testStress().mjava","sourceNew":"  public void testStress() throws Exception {\n\n    // do many iters, so we have \"bursts\" of adding nodes that we then check\n    final int numIters = atLeast(1000);\n    for (int iter = 0; iter < numIters; iter++) {\n\n      // sanity check that ZK says there is in fact 1 live node\n      List<String> actualLiveNodes = getTrueLiveNodesFromZk();\n      assertEquals(\"iter\"+iter+\": \" + actualLiveNodes.toString(),\n                   1, actualLiveNodes.size());\n\n      // only here do we forcibly update the cached live nodes so we don't have to wait for it to catch up\n      // with all the ephemeral nodes that vanished after the last iteration\n      CLOUD_CLIENT.getZkStateReader().updateLiveNodes();\n      \n      // sanity check that our Cloud Client's local state knows about the 1 (real) live node in our cluster\n      List<String> cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n      assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                   actualLiveNodes, cachedLiveNodes);\n      \n      \n      // start spinning up some threads to add some live_node children in parallel\n\n      // we don't need a lot of threads or nodes (we don't want to swamp the CPUs\n      // just bursts of concurrent adds) but we do want to randomize it a bit so we increase the\n      // odds of concurrent watchers firing regardless of the num CPUs or load on the machine running\n      // the test (but we deliberately don't look at availableProcessors() since we want randomization\n      // consistency across all machines for a given seed)\n      final int numThreads = TestUtil.nextInt(random(), 2, 5);\n      \n      // use same num for all thrashers, to increase likely hood of them all competing\n      // (diff random number would mean heavy concurrency only for ~ the first N=lowest num requests)\n      //\n      // this does not need to be a large number -- in fact, the higher it is, the more\n      // likely we are to see a mistake in early watcher triggers get \"corrected\" by a later one\n      // and overlook a possible bug\n      final int numNodesPerThrasher = TestUtil.nextInt(random(), 1, 5);\n      \n      log.info(\"preparing parallel adds to live nodes: iter={}, numThreads={} numNodesPerThread={}\",\n               iter, numThreads, numNodesPerThrasher);\n      \n      // NOTE: using ephemeral nodes\n      // so we can't close any of these thrashers until we are done with our assertions\n      final List<LiveNodeTrasher> thrashers = new ArrayList<>(numThreads);\n      for (int i = 0; i < numThreads; i++) {\n        thrashers.add(new LiveNodeTrasher(\"T\"+iter+\"_\"+i, numNodesPerThrasher));\n      }\n      try {\n        final ExecutorService executorService = ExecutorUtil.newMDCAwareFixedThreadPool\n          (thrashers.size()+1, new DefaultSolrThreadFactory(\"test_live_nodes_thrasher_iter\"+iter));\n        \n        executorService.invokeAll(thrashers);\n        executorService.shutdown();\n        if (! executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS)) {\n          for (LiveNodeTrasher thrasher : thrashers) {\n            thrasher.stop();\n          }\n        }\n        assertTrue(\"iter\"+iter+\": thrashers didn't finish even after explicitly stopping\",\n                   executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS));\n\n        // sanity check the *real* live_nodes entries from ZK match what the thrashers added\n        int totalAdded = 1; // 1 real live node when we started\n        for (LiveNodeTrasher thrasher : thrashers) {\n          totalAdded += thrasher.getNumAdded();\n        }\n        actualLiveNodes = getTrueLiveNodesFromZk();\n        assertEquals(\"iter\"+iter, totalAdded, actualLiveNodes.size());\n        \n        // verify our local client knows the correct set of live nodes\n        cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n        assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                     actualLiveNodes, cachedLiveNodes);\n        \n      } finally {\n        for (LiveNodeTrasher thrasher : thrashers) {\n          // shutdown our zk connection, freeing our ephemeral nodes\n          thrasher.close();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public void testStress() throws Exception {\n\n    // do many iters, so we have \"bursts\" of adding nodes that we then check\n    final int numIters = atLeast(1000);\n    for (int iter = 0; iter < numIters; iter++) {\n\n      // sanity check that ZK says there is in fact 1 live node\n      List<String> actualLiveNodes = getTrueLiveNodesFromZk();\n      assertEquals(\"iter\"+iter+\": \" + actualLiveNodes.toString(),\n                   1, actualLiveNodes.size());\n\n      // only here do we forcibly update the cached live nodes so we don't have to wait for it to catch up\n      // with all the ephemeral nodes that vanished after the last iteration\n      CLOUD_CLIENT.getZkStateReader().updateLiveNodes();\n      \n      // sanity check that our Cloud Client's local state knows about the 1 (real) live node in our cluster\n      List<String> cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n      assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                   actualLiveNodes, cachedLiveNodes);\n      \n      \n      // start spining up some threads to add some live_node children in parallel\n\n      // we don't need a lot of threads or nodes (we don't want to swamp the CPUs\n      // just bursts of conccurent adds) but we do want to randomize it a bit so we increase the\n      // odds of concurrent watchers firing regardless of the num CPUs or load on the machine running\n      // the test (but we deliberately don't look at availableProcessors() since we want randomization\n      // consistency across all machines for a given seed)\n      final int numThreads = TestUtil.nextInt(random(), 2, 5);\n      \n      // use same num for all thrashers, to increase likely hood of them all competing\n      // (diff random number would mean heavy concurency only for ~ the first N=lowest num requetss)\n      //\n      // this does not need to be a large number -- in fact, the higher it is, the more\n      // likely we are to see a mistake in early watcher triggers get \"corrected\" by a later one\n      // and overlook a possible bug\n      final int numNodesPerThrasher = TestUtil.nextInt(random(), 1, 5);\n      \n      log.info(\"preparing parallel adds to live nodes: iter={}, numThreads={} numNodesPerThread={}\",\n               iter, numThreads, numNodesPerThrasher);\n      \n      // NOTE: using ephemeral nodes\n      // so we can't close any of these thrashers until we are done with our assertions\n      final List<LiveNodeTrasher> thrashers = new ArrayList<>(numThreads);\n      for (int i = 0; i < numThreads; i++) {\n        thrashers.add(new LiveNodeTrasher(\"T\"+iter+\"_\"+i, numNodesPerThrasher));\n      }\n      try {\n        final ExecutorService executorService = ExecutorUtil.newMDCAwareFixedThreadPool\n          (thrashers.size()+1, new DefaultSolrThreadFactory(\"test_live_nodes_thrasher_iter\"+iter));\n        \n        executorService.invokeAll(thrashers);\n        executorService.shutdown();\n        if (! executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS)) {\n          for (LiveNodeTrasher thrasher : thrashers) {\n            thrasher.stop();\n          }\n        }\n        assertTrue(\"iter\"+iter+\": thrashers didn't finish even after explicitly stopping\",\n                   executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS));\n\n        // sanity check the *real* live_nodes entries from ZK match what the thrashers added\n        int totalAdded = 1; // 1 real live node when we started\n        for (LiveNodeTrasher thrasher : thrashers) {\n          totalAdded += thrasher.getNumAdded();\n        }\n        actualLiveNodes = getTrueLiveNodesFromZk();\n        assertEquals(\"iter\"+iter, totalAdded, actualLiveNodes.size());\n        \n        // verify our local client knows the correct set of live nodes\n        cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n        assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                     actualLiveNodes, cachedLiveNodes);\n        \n      } finally {\n        for (LiveNodeTrasher thrasher : thrashers) {\n          // shutdown our zk connection, freeing our ephemeral nodes\n          thrasher.close();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestStressLiveNodes#testStress().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestStressLiveNodes#testStress().mjava","sourceNew":"  public void testStress() throws Exception {\n\n    // do many iters, so we have \"bursts\" of adding nodes that we then check\n    final int numIters = atLeast(1000);\n    for (int iter = 0; iter < numIters; iter++) {\n\n      // sanity check that ZK says there is in fact 1 live node\n      List<String> actualLiveNodes = getTrueLiveNodesFromZk();\n      assertEquals(\"iter\"+iter+\": \" + actualLiveNodes.toString(),\n                   1, actualLiveNodes.size());\n\n      // only here do we forcibly update the cached live nodes so we don't have to wait for it to catch up\n      // with all the ephemeral nodes that vanished after the last iteration\n      CLOUD_CLIENT.getZkStateReader().updateLiveNodes();\n      \n      // sanity check that our Cloud Client's local state knows about the 1 (real) live node in our cluster\n      List<String> cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n      assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                   actualLiveNodes, cachedLiveNodes);\n      \n      \n      // start spinning up some threads to add some live_node children in parallel\n\n      // we don't need a lot of threads or nodes (we don't want to swamp the CPUs\n      // just bursts of concurrent adds) but we do want to randomize it a bit so we increase the\n      // odds of concurrent watchers firing regardless of the num CPUs or load on the machine running\n      // the test (but we deliberately don't look at availableProcessors() since we want randomization\n      // consistency across all machines for a given seed)\n      final int numThreads = TestUtil.nextInt(random(), 2, 5);\n      \n      // use same num for all thrashers, to increase likely hood of them all competing\n      // (diff random number would mean heavy concurrency only for ~ the first N=lowest num requests)\n      //\n      // this does not need to be a large number -- in fact, the higher it is, the more\n      // likely we are to see a mistake in early watcher triggers get \"corrected\" by a later one\n      // and overlook a possible bug\n      final int numNodesPerThrasher = TestUtil.nextInt(random(), 1, 5);\n      \n      log.info(\"preparing parallel adds to live nodes: iter={}, numThreads={} numNodesPerThread={}\",\n               iter, numThreads, numNodesPerThrasher);\n      \n      // NOTE: using ephemeral nodes\n      // so we can't close any of these thrashers until we are done with our assertions\n      final List<LiveNodeTrasher> thrashers = new ArrayList<>(numThreads);\n      for (int i = 0; i < numThreads; i++) {\n        thrashers.add(new LiveNodeTrasher(\"T\"+iter+\"_\"+i, numNodesPerThrasher));\n      }\n      try {\n        final ExecutorService executorService = ExecutorUtil.newMDCAwareFixedThreadPool\n          (thrashers.size()+1, new DefaultSolrThreadFactory(\"test_live_nodes_thrasher_iter\"+iter));\n        \n        executorService.invokeAll(thrashers);\n        executorService.shutdown();\n        if (! executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS)) {\n          for (LiveNodeTrasher thrasher : thrashers) {\n            thrasher.stop();\n          }\n        }\n        assertTrue(\"iter\"+iter+\": thrashers didn't finish even after explicitly stopping\",\n                   executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS));\n\n        // sanity check the *real* live_nodes entries from ZK match what the thrashers added\n        int totalAdded = 1; // 1 real live node when we started\n        for (LiveNodeTrasher thrasher : thrashers) {\n          totalAdded += thrasher.getNumAdded();\n        }\n        actualLiveNodes = getTrueLiveNodesFromZk();\n        assertEquals(\"iter\"+iter, totalAdded, actualLiveNodes.size());\n        \n        // verify our local client knows the correct set of live nodes\n        cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n        assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                     actualLiveNodes, cachedLiveNodes);\n        \n      } finally {\n        for (LiveNodeTrasher thrasher : thrashers) {\n          // shutdown our zk connection, freeing our ephemeral nodes\n          thrasher.close();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public void testStress() throws Exception {\n\n    // do many iters, so we have \"bursts\" of adding nodes that we then check\n    final int numIters = atLeast(1000);\n    for (int iter = 0; iter < numIters; iter++) {\n\n      // sanity check that ZK says there is in fact 1 live node\n      List<String> actualLiveNodes = getTrueLiveNodesFromZk();\n      assertEquals(\"iter\"+iter+\": \" + actualLiveNodes.toString(),\n                   1, actualLiveNodes.size());\n\n      // only here do we forcibly update the cached live nodes so we don't have to wait for it to catch up\n      // with all the ephemeral nodes that vanished after the last iteration\n      CLOUD_CLIENT.getZkStateReader().updateLiveNodes();\n      \n      // sanity check that our Cloud Client's local state knows about the 1 (real) live node in our cluster\n      List<String> cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n      assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                   actualLiveNodes, cachedLiveNodes);\n      \n      \n      // start spining up some threads to add some live_node children in parallel\n\n      // we don't need a lot of threads or nodes (we don't want to swamp the CPUs\n      // just bursts of conccurent adds) but we do want to randomize it a bit so we increase the\n      // odds of concurrent watchers firing regardless of the num CPUs or load on the machine running\n      // the test (but we deliberately don't look at availableProcessors() since we want randomization\n      // consistency across all machines for a given seed)\n      final int numThreads = TestUtil.nextInt(random(), 2, 5);\n      \n      // use same num for all thrashers, to increase likely hood of them all competing\n      // (diff random number would mean heavy concurency only for ~ the first N=lowest num requetss)\n      //\n      // this does not need to be a large number -- in fact, the higher it is, the more\n      // likely we are to see a mistake in early watcher triggers get \"corrected\" by a later one\n      // and overlook a possible bug\n      final int numNodesPerThrasher = TestUtil.nextInt(random(), 1, 5);\n      \n      log.info(\"preparing parallel adds to live nodes: iter={}, numThreads={} numNodesPerThread={}\",\n               iter, numThreads, numNodesPerThrasher);\n      \n      // NOTE: using ephemeral nodes\n      // so we can't close any of these thrashers until we are done with our assertions\n      final List<LiveNodeTrasher> thrashers = new ArrayList<>(numThreads);\n      for (int i = 0; i < numThreads; i++) {\n        thrashers.add(new LiveNodeTrasher(\"T\"+iter+\"_\"+i, numNodesPerThrasher));\n      }\n      try {\n        final ExecutorService executorService = ExecutorUtil.newMDCAwareFixedThreadPool\n          (thrashers.size()+1, new DefaultSolrThreadFactory(\"test_live_nodes_thrasher_iter\"+iter));\n        \n        executorService.invokeAll(thrashers);\n        executorService.shutdown();\n        if (! executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS)) {\n          for (LiveNodeTrasher thrasher : thrashers) {\n            thrasher.stop();\n          }\n        }\n        assertTrue(\"iter\"+iter+\": thrashers didn't finish even after explicitly stopping\",\n                   executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS));\n\n        // sanity check the *real* live_nodes entries from ZK match what the thrashers added\n        int totalAdded = 1; // 1 real live node when we started\n        for (LiveNodeTrasher thrasher : thrashers) {\n          totalAdded += thrasher.getNumAdded();\n        }\n        actualLiveNodes = getTrueLiveNodesFromZk();\n        assertEquals(\"iter\"+iter, totalAdded, actualLiveNodes.size());\n        \n        // verify our local client knows the correct set of live nodes\n        cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n        assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                     actualLiveNodes, cachedLiveNodes);\n        \n      } finally {\n        for (LiveNodeTrasher thrasher : thrashers) {\n          // shutdown our zk connection, freeing our ephemeral nodes\n          thrasher.close();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fefb36f0e1484796bb2f0af9ab119e927d1b5b11","date":1483011734,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestStressLiveNodes#testStress().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestStressLiveNodes#testStress().mjava","sourceNew":"  public void testStress() throws Exception {\n\n    // do many iters, so we have \"bursts\" of adding nodes that we then check\n    final int numIters = atLeast(TEST_NIGHTLY ? 1000 : 100);\n    for (int iter = 0; iter < numIters; iter++) {\n\n      // sanity check that ZK says there is in fact 1 live node\n      List<String> actualLiveNodes = getTrueLiveNodesFromZk();\n      assertEquals(\"iter\"+iter+\": \" + actualLiveNodes.toString(),\n                   1, actualLiveNodes.size());\n\n      // only here do we forcibly update the cached live nodes so we don't have to wait for it to catch up\n      // with all the ephemeral nodes that vanished after the last iteration\n      CLOUD_CLIENT.getZkStateReader().updateLiveNodes();\n      \n      // sanity check that our Cloud Client's local state knows about the 1 (real) live node in our cluster\n      List<String> cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n      assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                   actualLiveNodes, cachedLiveNodes);\n      \n      \n      // start spinning up some threads to add some live_node children in parallel\n\n      // we don't need a lot of threads or nodes (we don't want to swamp the CPUs\n      // just bursts of concurrent adds) but we do want to randomize it a bit so we increase the\n      // odds of concurrent watchers firing regardless of the num CPUs or load on the machine running\n      // the test (but we deliberately don't look at availableProcessors() since we want randomization\n      // consistency across all machines for a given seed)\n      final int numThreads = TestUtil.nextInt(random(), 2, 5);\n      \n      // use same num for all thrashers, to increase likely hood of them all competing\n      // (diff random number would mean heavy concurrency only for ~ the first N=lowest num requests)\n      //\n      // this does not need to be a large number -- in fact, the higher it is, the more\n      // likely we are to see a mistake in early watcher triggers get \"corrected\" by a later one\n      // and overlook a possible bug\n      final int numNodesPerThrasher = TestUtil.nextInt(random(), 1, 5);\n      \n      log.info(\"preparing parallel adds to live nodes: iter={}, numThreads={} numNodesPerThread={}\",\n               iter, numThreads, numNodesPerThrasher);\n      \n      // NOTE: using ephemeral nodes\n      // so we can't close any of these thrashers until we are done with our assertions\n      final List<LiveNodeTrasher> thrashers = new ArrayList<>(numThreads);\n      for (int i = 0; i < numThreads; i++) {\n        thrashers.add(new LiveNodeTrasher(\"T\"+iter+\"_\"+i, numNodesPerThrasher));\n      }\n      try {\n        final ExecutorService executorService = ExecutorUtil.newMDCAwareFixedThreadPool\n          (thrashers.size()+1, new DefaultSolrThreadFactory(\"test_live_nodes_thrasher_iter\"+iter));\n        \n        executorService.invokeAll(thrashers);\n        executorService.shutdown();\n        if (! executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS)) {\n          for (LiveNodeTrasher thrasher : thrashers) {\n            thrasher.stop();\n          }\n        }\n        assertTrue(\"iter\"+iter+\": thrashers didn't finish even after explicitly stopping\",\n                   executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS));\n\n        // sanity check the *real* live_nodes entries from ZK match what the thrashers added\n        int totalAdded = 1; // 1 real live node when we started\n        for (LiveNodeTrasher thrasher : thrashers) {\n          totalAdded += thrasher.getNumAdded();\n        }\n        actualLiveNodes = getTrueLiveNodesFromZk();\n        assertEquals(\"iter\"+iter, totalAdded, actualLiveNodes.size());\n        \n        // verify our local client knows the correct set of live nodes\n        cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n        assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                     actualLiveNodes, cachedLiveNodes);\n        \n      } finally {\n        for (LiveNodeTrasher thrasher : thrashers) {\n          // shutdown our zk connection, freeing our ephemeral nodes\n          thrasher.close();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public void testStress() throws Exception {\n\n    // do many iters, so we have \"bursts\" of adding nodes that we then check\n    final int numIters = atLeast(1000);\n    for (int iter = 0; iter < numIters; iter++) {\n\n      // sanity check that ZK says there is in fact 1 live node\n      List<String> actualLiveNodes = getTrueLiveNodesFromZk();\n      assertEquals(\"iter\"+iter+\": \" + actualLiveNodes.toString(),\n                   1, actualLiveNodes.size());\n\n      // only here do we forcibly update the cached live nodes so we don't have to wait for it to catch up\n      // with all the ephemeral nodes that vanished after the last iteration\n      CLOUD_CLIENT.getZkStateReader().updateLiveNodes();\n      \n      // sanity check that our Cloud Client's local state knows about the 1 (real) live node in our cluster\n      List<String> cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n      assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                   actualLiveNodes, cachedLiveNodes);\n      \n      \n      // start spinning up some threads to add some live_node children in parallel\n\n      // we don't need a lot of threads or nodes (we don't want to swamp the CPUs\n      // just bursts of concurrent adds) but we do want to randomize it a bit so we increase the\n      // odds of concurrent watchers firing regardless of the num CPUs or load on the machine running\n      // the test (but we deliberately don't look at availableProcessors() since we want randomization\n      // consistency across all machines for a given seed)\n      final int numThreads = TestUtil.nextInt(random(), 2, 5);\n      \n      // use same num for all thrashers, to increase likely hood of them all competing\n      // (diff random number would mean heavy concurrency only for ~ the first N=lowest num requests)\n      //\n      // this does not need to be a large number -- in fact, the higher it is, the more\n      // likely we are to see a mistake in early watcher triggers get \"corrected\" by a later one\n      // and overlook a possible bug\n      final int numNodesPerThrasher = TestUtil.nextInt(random(), 1, 5);\n      \n      log.info(\"preparing parallel adds to live nodes: iter={}, numThreads={} numNodesPerThread={}\",\n               iter, numThreads, numNodesPerThrasher);\n      \n      // NOTE: using ephemeral nodes\n      // so we can't close any of these thrashers until we are done with our assertions\n      final List<LiveNodeTrasher> thrashers = new ArrayList<>(numThreads);\n      for (int i = 0; i < numThreads; i++) {\n        thrashers.add(new LiveNodeTrasher(\"T\"+iter+\"_\"+i, numNodesPerThrasher));\n      }\n      try {\n        final ExecutorService executorService = ExecutorUtil.newMDCAwareFixedThreadPool\n          (thrashers.size()+1, new DefaultSolrThreadFactory(\"test_live_nodes_thrasher_iter\"+iter));\n        \n        executorService.invokeAll(thrashers);\n        executorService.shutdown();\n        if (! executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS)) {\n          for (LiveNodeTrasher thrasher : thrashers) {\n            thrasher.stop();\n          }\n        }\n        assertTrue(\"iter\"+iter+\": thrashers didn't finish even after explicitly stopping\",\n                   executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS));\n\n        // sanity check the *real* live_nodes entries from ZK match what the thrashers added\n        int totalAdded = 1; // 1 real live node when we started\n        for (LiveNodeTrasher thrasher : thrashers) {\n          totalAdded += thrasher.getNumAdded();\n        }\n        actualLiveNodes = getTrueLiveNodesFromZk();\n        assertEquals(\"iter\"+iter, totalAdded, actualLiveNodes.size());\n        \n        // verify our local client knows the correct set of live nodes\n        cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n        assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                     actualLiveNodes, cachedLiveNodes);\n        \n      } finally {\n        for (LiveNodeTrasher thrasher : thrashers) {\n          // shutdown our zk connection, freeing our ephemeral nodes\n          thrasher.close();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f03e4bed5023ec3ef93a771b8888cae991cf448d","date":1483469262,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestStressLiveNodes#testStress().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestStressLiveNodes#testStress().mjava","sourceNew":"  public void testStress() throws Exception {\n\n    // do many iters, so we have \"bursts\" of adding nodes that we then check\n    final int numIters = atLeast(TEST_NIGHTLY ? 1000 : 100);\n    for (int iter = 0; iter < numIters; iter++) {\n\n      // sanity check that ZK says there is in fact 1 live node\n      List<String> actualLiveNodes = getTrueLiveNodesFromZk();\n      assertEquals(\"iter\"+iter+\": \" + actualLiveNodes.toString(),\n                   1, actualLiveNodes.size());\n\n      // only here do we forcibly update the cached live nodes so we don't have to wait for it to catch up\n      // with all the ephemeral nodes that vanished after the last iteration\n      CLOUD_CLIENT.getZkStateReader().updateLiveNodes();\n      \n      // sanity check that our Cloud Client's local state knows about the 1 (real) live node in our cluster\n      List<String> cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n      assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                   actualLiveNodes, cachedLiveNodes);\n      \n      \n      // start spinning up some threads to add some live_node children in parallel\n\n      // we don't need a lot of threads or nodes (we don't want to swamp the CPUs\n      // just bursts of concurrent adds) but we do want to randomize it a bit so we increase the\n      // odds of concurrent watchers firing regardless of the num CPUs or load on the machine running\n      // the test (but we deliberately don't look at availableProcessors() since we want randomization\n      // consistency across all machines for a given seed)\n      final int numThreads = TestUtil.nextInt(random(), 2, 5);\n      \n      // use same num for all thrashers, to increase likely hood of them all competing\n      // (diff random number would mean heavy concurrency only for ~ the first N=lowest num requests)\n      //\n      // this does not need to be a large number -- in fact, the higher it is, the more\n      // likely we are to see a mistake in early watcher triggers get \"corrected\" by a later one\n      // and overlook a possible bug\n      final int numNodesPerThrasher = TestUtil.nextInt(random(), 1, 5);\n      \n      log.info(\"preparing parallel adds to live nodes: iter={}, numThreads={} numNodesPerThread={}\",\n               iter, numThreads, numNodesPerThrasher);\n      \n      // NOTE: using ephemeral nodes\n      // so we can't close any of these thrashers until we are done with our assertions\n      final List<LiveNodeTrasher> thrashers = new ArrayList<>(numThreads);\n      for (int i = 0; i < numThreads; i++) {\n        thrashers.add(new LiveNodeTrasher(\"T\"+iter+\"_\"+i, numNodesPerThrasher));\n      }\n      try {\n        final ExecutorService executorService = ExecutorUtil.newMDCAwareFixedThreadPool\n          (thrashers.size()+1, new DefaultSolrThreadFactory(\"test_live_nodes_thrasher_iter\"+iter));\n        \n        executorService.invokeAll(thrashers);\n        executorService.shutdown();\n        if (! executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS)) {\n          for (LiveNodeTrasher thrasher : thrashers) {\n            thrasher.stop();\n          }\n        }\n        assertTrue(\"iter\"+iter+\": thrashers didn't finish even after explicitly stopping\",\n                   executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS));\n\n        // sanity check the *real* live_nodes entries from ZK match what the thrashers added\n        int totalAdded = 1; // 1 real live node when we started\n        for (LiveNodeTrasher thrasher : thrashers) {\n          totalAdded += thrasher.getNumAdded();\n        }\n        actualLiveNodes = getTrueLiveNodesFromZk();\n        assertEquals(\"iter\"+iter, totalAdded, actualLiveNodes.size());\n        \n        // verify our local client knows the correct set of live nodes\n        cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n        assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                     actualLiveNodes, cachedLiveNodes);\n        \n      } finally {\n        for (LiveNodeTrasher thrasher : thrashers) {\n          // shutdown our zk connection, freeing our ephemeral nodes\n          thrasher.close();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public void testStress() throws Exception {\n\n    // do many iters, so we have \"bursts\" of adding nodes that we then check\n    final int numIters = atLeast(1000);\n    for (int iter = 0; iter < numIters; iter++) {\n\n      // sanity check that ZK says there is in fact 1 live node\n      List<String> actualLiveNodes = getTrueLiveNodesFromZk();\n      assertEquals(\"iter\"+iter+\": \" + actualLiveNodes.toString(),\n                   1, actualLiveNodes.size());\n\n      // only here do we forcibly update the cached live nodes so we don't have to wait for it to catch up\n      // with all the ephemeral nodes that vanished after the last iteration\n      CLOUD_CLIENT.getZkStateReader().updateLiveNodes();\n      \n      // sanity check that our Cloud Client's local state knows about the 1 (real) live node in our cluster\n      List<String> cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n      assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                   actualLiveNodes, cachedLiveNodes);\n      \n      \n      // start spinning up some threads to add some live_node children in parallel\n\n      // we don't need a lot of threads or nodes (we don't want to swamp the CPUs\n      // just bursts of concurrent adds) but we do want to randomize it a bit so we increase the\n      // odds of concurrent watchers firing regardless of the num CPUs or load on the machine running\n      // the test (but we deliberately don't look at availableProcessors() since we want randomization\n      // consistency across all machines for a given seed)\n      final int numThreads = TestUtil.nextInt(random(), 2, 5);\n      \n      // use same num for all thrashers, to increase likely hood of them all competing\n      // (diff random number would mean heavy concurrency only for ~ the first N=lowest num requests)\n      //\n      // this does not need to be a large number -- in fact, the higher it is, the more\n      // likely we are to see a mistake in early watcher triggers get \"corrected\" by a later one\n      // and overlook a possible bug\n      final int numNodesPerThrasher = TestUtil.nextInt(random(), 1, 5);\n      \n      log.info(\"preparing parallel adds to live nodes: iter={}, numThreads={} numNodesPerThread={}\",\n               iter, numThreads, numNodesPerThrasher);\n      \n      // NOTE: using ephemeral nodes\n      // so we can't close any of these thrashers until we are done with our assertions\n      final List<LiveNodeTrasher> thrashers = new ArrayList<>(numThreads);\n      for (int i = 0; i < numThreads; i++) {\n        thrashers.add(new LiveNodeTrasher(\"T\"+iter+\"_\"+i, numNodesPerThrasher));\n      }\n      try {\n        final ExecutorService executorService = ExecutorUtil.newMDCAwareFixedThreadPool\n          (thrashers.size()+1, new DefaultSolrThreadFactory(\"test_live_nodes_thrasher_iter\"+iter));\n        \n        executorService.invokeAll(thrashers);\n        executorService.shutdown();\n        if (! executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS)) {\n          for (LiveNodeTrasher thrasher : thrashers) {\n            thrasher.stop();\n          }\n        }\n        assertTrue(\"iter\"+iter+\": thrashers didn't finish even after explicitly stopping\",\n                   executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS));\n\n        // sanity check the *real* live_nodes entries from ZK match what the thrashers added\n        int totalAdded = 1; // 1 real live node when we started\n        for (LiveNodeTrasher thrasher : thrashers) {\n          totalAdded += thrasher.getNumAdded();\n        }\n        actualLiveNodes = getTrueLiveNodesFromZk();\n        assertEquals(\"iter\"+iter, totalAdded, actualLiveNodes.size());\n        \n        // verify our local client knows the correct set of live nodes\n        cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n        assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                     actualLiveNodes, cachedLiveNodes);\n        \n      } finally {\n        for (LiveNodeTrasher thrasher : thrashers) {\n          // shutdown our zk connection, freeing our ephemeral nodes\n          thrasher.close();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb03700c9690d16b15fb4f56f6ec36b128fd894e","date":1586745995,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestStressLiveNodes#testStress().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestStressLiveNodes#testStress().mjava","sourceNew":"  public void testStress() throws Exception {\n\n    // do many iters, so we have \"bursts\" of adding nodes that we then check\n    final int numIters = atLeast(TEST_NIGHTLY ? 1000 : 100);\n    for (int iter = 0; iter < numIters; iter++) {\n\n      // sanity check that ZK says there is in fact 1 live node\n      List<String> actualLiveNodes = getTrueLiveNodesFromZk();\n      assertEquals(\"iter\"+iter+\": \" + actualLiveNodes.toString(),\n                   1, actualLiveNodes.size());\n\n      // only here do we forcibly update the cached live nodes so we don't have to wait for it to catch up\n      // with all the ephemeral nodes that vanished after the last iteration\n      CLOUD_CLIENT.getZkStateReader().updateLiveNodes();\n      \n      // sanity check that our Cloud Client's local state knows about the 1 (real) live node in our cluster\n      List<String> cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n      assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                   actualLiveNodes, cachedLiveNodes);\n      \n      \n      // start spinning up some threads to add some live_node children in parallel\n\n      // we don't need a lot of threads or nodes (we don't want to swamp the CPUs\n      // just bursts of concurrent adds) but we do want to randomize it a bit so we increase the\n      // odds of concurrent watchers firing regardless of the num CPUs or load on the machine running\n      // the test (but we deliberately don't look at availableProcessors() since we want randomization\n      // consistency across all machines for a given seed)\n      final int numThreads = TestUtil.nextInt(random(), 2, 5);\n      \n      // use same num for all thrashers, to increase likely hood of them all competing\n      // (diff random number would mean heavy concurrency only for ~ the first N=lowest num requests)\n      //\n      // this does not need to be a large number -- in fact, the higher it is, the more\n      // likely we are to see a mistake in early watcher triggers get \"corrected\" by a later one\n      // and overlook a possible bug\n      final int numNodesPerThrasher = TestUtil.nextInt(random(), 1, 5);\n      \n      log.info(\"preparing parallel adds to live nodes: iter={}, numThreads={} numNodesPerThread={}\",\n               iter, numThreads, numNodesPerThrasher);\n      \n      // NOTE: using ephemeral nodes\n      // so we can't close any of these thrashers until we are done with our assertions\n      final List<LiveNodeTrasher> thrashers = new ArrayList<>(numThreads);\n      for (int i = 0; i < numThreads; i++) {\n        thrashers.add(new LiveNodeTrasher(\"T\"+iter+\"_\"+i, numNodesPerThrasher));\n      }\n      try {\n        final ExecutorService executorService = ExecutorUtil.newMDCAwareFixedThreadPool\n          (thrashers.size()+1, new SolrNamedThreadFactory(\"test_live_nodes_thrasher_iter\"+iter));\n        \n        executorService.invokeAll(thrashers);\n        executorService.shutdown();\n        if (! executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS)) {\n          for (LiveNodeTrasher thrasher : thrashers) {\n            thrasher.stop();\n          }\n        }\n        assertTrue(\"iter\"+iter+\": thrashers didn't finish even after explicitly stopping\",\n                   executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS));\n\n        // sanity check the *real* live_nodes entries from ZK match what the thrashers added\n        int totalAdded = 1; // 1 real live node when we started\n        for (LiveNodeTrasher thrasher : thrashers) {\n          totalAdded += thrasher.getNumAdded();\n        }\n        actualLiveNodes = getTrueLiveNodesFromZk();\n        assertEquals(\"iter\"+iter, totalAdded, actualLiveNodes.size());\n        \n        // verify our local client knows the correct set of live nodes\n        cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n        assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                     actualLiveNodes, cachedLiveNodes);\n        \n      } finally {\n        for (LiveNodeTrasher thrasher : thrashers) {\n          // shutdown our zk connection, freeing our ephemeral nodes\n          thrasher.close();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  public void testStress() throws Exception {\n\n    // do many iters, so we have \"bursts\" of adding nodes that we then check\n    final int numIters = atLeast(TEST_NIGHTLY ? 1000 : 100);\n    for (int iter = 0; iter < numIters; iter++) {\n\n      // sanity check that ZK says there is in fact 1 live node\n      List<String> actualLiveNodes = getTrueLiveNodesFromZk();\n      assertEquals(\"iter\"+iter+\": \" + actualLiveNodes.toString(),\n                   1, actualLiveNodes.size());\n\n      // only here do we forcibly update the cached live nodes so we don't have to wait for it to catch up\n      // with all the ephemeral nodes that vanished after the last iteration\n      CLOUD_CLIENT.getZkStateReader().updateLiveNodes();\n      \n      // sanity check that our Cloud Client's local state knows about the 1 (real) live node in our cluster\n      List<String> cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n      assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                   actualLiveNodes, cachedLiveNodes);\n      \n      \n      // start spinning up some threads to add some live_node children in parallel\n\n      // we don't need a lot of threads or nodes (we don't want to swamp the CPUs\n      // just bursts of concurrent adds) but we do want to randomize it a bit so we increase the\n      // odds of concurrent watchers firing regardless of the num CPUs or load on the machine running\n      // the test (but we deliberately don't look at availableProcessors() since we want randomization\n      // consistency across all machines for a given seed)\n      final int numThreads = TestUtil.nextInt(random(), 2, 5);\n      \n      // use same num for all thrashers, to increase likely hood of them all competing\n      // (diff random number would mean heavy concurrency only for ~ the first N=lowest num requests)\n      //\n      // this does not need to be a large number -- in fact, the higher it is, the more\n      // likely we are to see a mistake in early watcher triggers get \"corrected\" by a later one\n      // and overlook a possible bug\n      final int numNodesPerThrasher = TestUtil.nextInt(random(), 1, 5);\n      \n      log.info(\"preparing parallel adds to live nodes: iter={}, numThreads={} numNodesPerThread={}\",\n               iter, numThreads, numNodesPerThrasher);\n      \n      // NOTE: using ephemeral nodes\n      // so we can't close any of these thrashers until we are done with our assertions\n      final List<LiveNodeTrasher> thrashers = new ArrayList<>(numThreads);\n      for (int i = 0; i < numThreads; i++) {\n        thrashers.add(new LiveNodeTrasher(\"T\"+iter+\"_\"+i, numNodesPerThrasher));\n      }\n      try {\n        final ExecutorService executorService = ExecutorUtil.newMDCAwareFixedThreadPool\n          (thrashers.size()+1, new DefaultSolrThreadFactory(\"test_live_nodes_thrasher_iter\"+iter));\n        \n        executorService.invokeAll(thrashers);\n        executorService.shutdown();\n        if (! executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS)) {\n          for (LiveNodeTrasher thrasher : thrashers) {\n            thrasher.stop();\n          }\n        }\n        assertTrue(\"iter\"+iter+\": thrashers didn't finish even after explicitly stopping\",\n                   executorService.awaitTermination(WAIT_TIME, TimeUnit.SECONDS));\n\n        // sanity check the *real* live_nodes entries from ZK match what the thrashers added\n        int totalAdded = 1; // 1 real live node when we started\n        for (LiveNodeTrasher thrasher : thrashers) {\n          totalAdded += thrasher.getNumAdded();\n        }\n        actualLiveNodes = getTrueLiveNodesFromZk();\n        assertEquals(\"iter\"+iter, totalAdded, actualLiveNodes.size());\n        \n        // verify our local client knows the correct set of live nodes\n        cachedLiveNodes = getCachedLiveNodesFromLocalState(actualLiveNodes.size());\n        assertEquals(\"iter\"+iter+\" \" + actualLiveNodes.size() + \" != \" + cachedLiveNodes.size(),\n                     actualLiveNodes, cachedLiveNodes);\n        \n      } finally {\n        for (LiveNodeTrasher thrasher : thrashers) {\n          // shutdown our zk connection, freeing our ephemeral nodes\n          thrasher.close();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"9ea9249ab9a9f76eb4132ceb4631d15315721f6a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","d3726445f8e9a7d398466439f0f84b5bb329fdcc"],"fefb36f0e1484796bb2f0af9ab119e927d1b5b11":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"d3726445f8e9a7d398466439f0f84b5bb329fdcc":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fb03700c9690d16b15fb4f56f6ec36b128fd894e":["fefb36f0e1484796bb2f0af9ab119e927d1b5b11"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["9ea9249ab9a9f76eb4132ceb4631d15315721f6a","d470c8182e92b264680e34081b75e70a9f2b3c89"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["fb03700c9690d16b15fb4f56f6ec36b128fd894e"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["9ea9249ab9a9f76eb4132ceb4631d15315721f6a","0158ced21948b6626f733c1c42c1e18d94449789"],"0158ced21948b6626f733c1c42c1e18d94449789":["9ea9249ab9a9f76eb4132ceb4631d15315721f6a"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","fefb36f0e1484796bb2f0af9ab119e927d1b5b11"]},"commit2Childs":{"9ea9249ab9a9f76eb4132ceb4631d15315721f6a":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d470c8182e92b264680e34081b75e70a9f2b3c89","0158ced21948b6626f733c1c42c1e18d94449789"],"fefb36f0e1484796bb2f0af9ab119e927d1b5b11":["fb03700c9690d16b15fb4f56f6ec36b128fd894e","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"d3726445f8e9a7d398466439f0f84b5bb329fdcc":["9ea9249ab9a9f76eb4132ceb4631d15315721f6a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9ea9249ab9a9f76eb4132ceb4631d15315721f6a","d3726445f8e9a7d398466439f0f84b5bb329fdcc"],"fb03700c9690d16b15fb4f56f6ec36b128fd894e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["f03e4bed5023ec3ef93a771b8888cae991cf448d"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["fefb36f0e1484796bb2f0af9ab119e927d1b5b11","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"0158ced21948b6626f733c1c42c1e18d94449789":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}